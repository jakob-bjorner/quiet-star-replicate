{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate Quiet-STAR's main results, and make it model agnostic\n",
    "\n",
    "Eval on the same datasets during training?\n",
    "\n",
    "Also add MMLU?\n",
    "\n",
    "CoT for gsm8k?\n",
    "\n",
    "What does eval need for COT eval? -- going through cot eval script now from them... They use the forward pass just the same very odd!!!! very inefficient!!!!! but whatever I guess it is easier to code.\n",
    "\n",
    "\n",
    "Making this system work for any model, how will I deal with the idea of a mixing head? combining the logits in a single stream and predicting with them?\n",
    "\n",
    "Also want to be able to handle the idea of multiple models used in an ensemble to get data uncertainty estimates. (https://pytorch.org/tutorials/intermediate/ensembling.html.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batched gen required memory for first inference pass 524288\n",
      "additionally the past_key_value will have a batch of 2048 the entire time but the num keys/values will be 1\n",
      "parallel atten impl required mem for first inference pass 2048 here the past_key_value batch size will be 8 and 256 will be in the number of key and values dimension\n",
      "ultimately we don't take advantage of the repeated sequences if we are forced to generate a thought for every token, but we could argue that we can get diversity in place of this. we may want to instead do better with more context than with less context???\n"
     ]
    }
   ],
   "source": [
    "print(\"batched gen required memory for first inference pass\", 256 * 8 * 256) # memory consumption is reduced massively by the parallel attention head implementation.\n",
    "print(\"additionally the past_key_value will have a batch of\", 256 * 8, \"the entire time but the num keys/values will be 1\")\n",
    "print(\"parallel atten impl required mem for first inference pass\", 256 * 8, \"here the past_key_value batch size will be\", 8, \"and 256 will be in the number of key and values dimension\")\n",
    "\n",
    "print(\"ultimately we don't take advantage of the repeated sequences if we are forced to generate a thought for every token, but we could argue that we can get diversity in place of this. we may want to instead do better with more context than with less context???\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper params\n",
    "# policy_model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "# Q_model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "# reward_y_z_x_model_name = \"meta-llama/Llama-3.2-1B\" \n",
    "\n",
    "# policy_model_name = \"openai-community/gpt2\" \n",
    "# Q_model_name = \"openai-community/gpt2\" \n",
    "# reward_y_z_x_model_name = \"openai-community/gpt2\" \n",
    "\n",
    "# \"mistralai/Mistral-7B-v0.1\" ? \n",
    "quiet_star_model_name = \"openai-community/gpt2\" # \"meta-llama/Llama-3.2-1B\" # \n",
    "\n",
    "tokenizer_sot_token = \"<|sot_token|>\"\n",
    "tokenizer_eot_token = \"<|eot_token|>\"\n",
    "start_of_thought_token_init_embedding = \"---\" # None ??\n",
    "end_of_thought_token_init_embedding = \"---\" # None ??\n",
    "\n",
    "embedding_scaling = 1 # 1e2 TODO: get this working in the forward pass\n",
    "reinforce_loss_scaling = 1e6\n",
    "n_sampled_thoughts = 2 # for trice average\n",
    "original_loss_weight = 0.5\n",
    "base_loss_scaling = 1\n",
    "n_talk_tokens = 4\n",
    "n_thought_tokens = 2 # normally 12 '\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "# mean resizing for new tokens will happen if the init embedding is set to None above.\n",
    "### end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(quiet_star_model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(quiet_star_model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "assert tokenizer.pad_token_id is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "def add_special_tokens_to_model_and_tokenizer(specific_model, specific_tokenizer, embedding_scaling):\n",
    "    if tokenizer_sot_token not in specific_tokenizer.get_added_vocab():\n",
    "        # new_tokens = [tokenizer_sot_token, tokenizer_eot_token]\n",
    "        specific_tokenizer.add_tokens(tokenizer_sot_token, special_tokens=True)\n",
    "        specific_model.resize_token_embeddings(len(specific_tokenizer), mean_resizing=True)\n",
    "        tokenizer_sot_token_id = specific_tokenizer.get_added_vocab()[tokenizer_sot_token]\n",
    "\n",
    "        specific_tokenizer.add_tokens(tokenizer_eot_token, special_tokens=True)\n",
    "        specific_model.resize_token_embeddings(len(specific_tokenizer), mean_resizing=True)\n",
    "        tokenizer_eot_token_id = specific_tokenizer.get_added_vocab()[tokenizer_eot_token]\n",
    "        with torch.no_grad():\n",
    "            model_embeddings = specific_model.get_input_embeddings().weight.data\n",
    "            if start_of_thought_token_init_embedding is not None:\n",
    "                sot_embedding = model_embeddings[tokenizer.encode(start_of_thought_token_init_embedding, add_special_tokens=False)[0]]\n",
    "                model_embeddings[tokenizer_sot_token_id] = sot_embedding / embedding_scaling\n",
    "            if end_of_thought_token_init_embedding is not None:\n",
    "                eot_embedding = model_embeddings[tokenizer.encode(end_of_thought_token_init_embedding, add_special_tokens=False)[0]]\n",
    "                model_embeddings[tokenizer_eot_token_id] = eot_embedding / embedding_scaling\n",
    "add_special_tokens_to_model_and_tokenizer(base_model, tokenizer, embedding_scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: to test if this is truly working I would want to support at least the eval datasets that quiet-star works with, as well as the model which quiet-star works with.\n",
    "# For this I would need to support much longer sequences like 128 or 256 ideally. \n",
    "# so change the way my forward pass is done. calculate all the input sequences which will be used in the generation batch, then essentially run those through the model doing gradient accumulation on the loss? \n",
    "# (would require a backward pass). I could change the way the data is preprocessed to take this into account. Then the eval would have to be calculated only when a whole example has been able to pass through.\n",
    "# This would likely require my own training loop and then eventually figuring out how to do distributed training with Mistral, so I should come back to this idea if I have time.\n",
    "# Right now, I can add some parts of the DPO training loop which I need into their codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(4.0268, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(3.9827, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(4.0708, device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor(4.0268, device='cuda:0', grad_fn=<AddBackward0>),\n",
       " tensor(0., device='cuda:0', grad_fn=<MeanBackward0>),\n",
       " tensor([[[ 1.1575e-01, -3.7863e-02,  5.6798e-01,  ...,  4.7920e-01,\n",
       "           -2.8237e-01,  1.6246e-01],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02]],\n",
       " \n",
       "         [[ 1.1575e-01, -3.7863e-02,  5.6798e-01,  ...,  4.7920e-01,\n",
       "           -2.8237e-01,  1.6246e-01],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02]],\n",
       " \n",
       "         [[ 4.1180e-01,  2.4470e-01, -6.9713e-01,  ...,  2.2863e-01,\n",
       "           -3.1366e-01, -1.2472e-01],\n",
       "          [ 1.1575e-01, -3.7863e-02,  5.6798e-01,  ...,  4.7920e-01,\n",
       "           -2.8237e-01,  1.6246e-01],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02],\n",
       "          [-1.0000e+02, -1.0000e+02, -1.0000e+02,  ..., -1.0000e+02,\n",
       "           -1.0000e+02, -1.0000e+02]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[ 1.8160e-01, -1.3633e-01,  1.5047e-01,  ..., -1.2122e-02,\n",
       "            1.0512e-02,  1.6888e-01],\n",
       "          [ 1.4305e-01,  2.9904e-01, -1.5917e+00,  ..., -2.6680e-01,\n",
       "            8.7320e-03,  2.3587e-01],\n",
       "          [-6.5418e-01, -5.9359e-01, -6.6015e-01,  ..., -1.4455e-01,\n",
       "            2.1628e-01,  2.1437e-01],\n",
       "          [-1.1563e+00, -4.4413e-02,  3.8590e-01,  ..., -7.4039e-02,\n",
       "            4.5854e-01, -1.2858e-01]],\n",
       " \n",
       "         [[-5.7374e-02, -1.7653e-02, -4.4888e-01,  ..., -1.8092e-01,\n",
       "           -3.3592e-02, -1.2949e-01],\n",
       "          [ 1.8160e-01, -1.3633e-01,  1.5047e-01,  ..., -1.2122e-02,\n",
       "            1.0512e-02,  1.6888e-01],\n",
       "          [ 1.4305e-01,  2.9904e-01, -1.5917e+00,  ..., -2.6680e-01,\n",
       "            8.7320e-03,  2.3587e-01],\n",
       "          [-6.5418e-01, -5.9359e-01, -6.6015e-01,  ..., -1.4455e-01,\n",
       "            2.1628e-01,  2.1437e-01]],\n",
       " \n",
       "         [[-5.7374e-02, -1.7653e-02, -4.4888e-01,  ..., -1.8092e-01,\n",
       "           -3.3592e-02, -1.2949e-01],\n",
       "          [ 1.8160e-01, -1.3633e-01,  1.5047e-01,  ..., -1.2122e-02,\n",
       "            1.0512e-02,  1.6888e-01],\n",
       "          [ 1.4305e-01,  2.9904e-01, -1.5917e+00,  ..., -2.6680e-01,\n",
       "            8.7320e-03,  2.3587e-01],\n",
       "          [-6.5418e-01, -5.9359e-01, -6.6015e-01,  ..., -1.4455e-01,\n",
       "            2.1628e-01,  2.1437e-01]]], device='cuda:0', grad_fn=<AddBackward0>),\n",
       " None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_to_triangular_sequence_for_parallel_generation(elements_per_batch_element, inputs, fill_value, n_talk_tokens, predict_all=False):\n",
    "    if predict_all:\n",
    "        n_talk_tokens = 0\n",
    "    # predict_all will create thoughts which do not have enough tokens to calculate loss, this is useful for COT setting\n",
    "    max_seq = inputs.size(1)\n",
    "    max_thoughts_producable_with_enough_talk_tokens_remaining = (elements_per_batch_element - (n_talk_tokens)).clamp(min=0).sum().item() # minus 1 because we only predict n_talk_tokens, and without this, we would predict n_talk_tokens + 1 because one for every token and then one for eot token.\n",
    "    expanded_inputs = torch.full(size=(max_thoughts_producable_with_enough_talk_tokens_remaining, max_seq - (n_talk_tokens)),\n",
    "                                 fill_value=fill_value,\n",
    "                                 dtype=inputs.dtype,\n",
    "                                 device=inputs.device)\n",
    "    batch_being_filled = 0\n",
    "    for b_i in range(inputs.size(0)):\n",
    "        # each batch element should be expanded to fit the\n",
    "        elements_in_b_i = elements_per_batch_element[b_i].item()\n",
    "        if n_talk_tokens >= elements_in_b_i and not predict_all:\n",
    "            continue\n",
    "        num_sequences_with_enough_talk_tokens_remaining = elements_in_b_i - (n_talk_tokens)\n",
    "        sequence_b_i_start_index = max_seq - elements_in_b_i\n",
    "        triangular_inputs = inputs[[b_i], ...].expand(elements_in_b_i,-1).gather(1, (sequence_b_i_start_index + torch.arange(num_sequences_with_enough_talk_tokens_remaining, device=inputs.device, dtype=torch.long)[None, :] - torch.arange(num_sequences_with_enough_talk_tokens_remaining, device=inputs.device, dtype=torch.long)[:, None]).triu())\n",
    "        ones_like_triangular_inputs = torch.ones_like(triangular_inputs)\n",
    "        triangular_inputs = triangular_inputs.triu() + (ones_like_triangular_inputs - ones_like_triangular_inputs.triu()) * fill_value            \n",
    "        expanded_inputs[batch_being_filled: batch_being_filled + num_sequences_with_enough_talk_tokens_remaining, -num_sequences_with_enough_talk_tokens_remaining:] = triangular_inputs\n",
    "        batch_being_filled += num_sequences_with_enough_talk_tokens_remaining\n",
    "    return expanded_inputs\n",
    "\n",
    "def get_next_tokens_for_thoughts(elements_per_batch_element, input_ids, n_talk_tokens_to_leave_room_for, fill_value=-100, predict_all=False, get_token_before_label=False): \n",
    "    # get_token_before_label is useful if you want to pluck the hidden state used to predict the next token for the base model predictions.\n",
    "    n_labels_next_tokens = n_talk_tokens_to_leave_room_for\n",
    "    if predict_all:\n",
    "        n_talk_tokens_to_leave_room_for = 0\n",
    "        \n",
    "    # predict_all will populate next_tokens_for_thoughts which do not have enough tokens to calculate loss, this is useful for COT setting\n",
    "    # we assume every thought needs at least n_talk_tokens after it. \n",
    "    max_seq = input_ids.size(1)\n",
    "    max_thoughts_producable_with_enough_talk_tokens_remaining = (elements_per_batch_element - n_talk_tokens_to_leave_room_for).clamp(min=0).sum().item()\n",
    "    remaining_input_shape = tuple(input_ids.shape[2:]) # to support using this same function for base_hidden_state extraction\n",
    "    next_tokens_for_thoughts = torch.full((max_thoughts_producable_with_enough_talk_tokens_remaining, n_labels_next_tokens) + remaining_input_shape, fill_value=fill_value, dtype=input_ids.dtype, device=input_ids.device)\n",
    "    batch_being_filled = 0\n",
    "    input_ids = torch.concat([input_ids, torch.full_like(input_ids[:,[0]], fill_value=fill_value)], dim=1)# This to support -100 in the labels.\n",
    "\n",
    "    for b_i, elements_in_b_i in enumerate(elements_per_batch_element):\n",
    "        elements_in_b_i = elements_in_b_i.item()\n",
    "        if n_talk_tokens_to_leave_room_for >= elements_in_b_i:\n",
    "            continue\n",
    "        num_sequences_with_enough_talk_tokens_remaining = elements_in_b_i - n_talk_tokens_to_leave_room_for\n",
    "        sequence_b_i_start_index = max_seq - elements_in_b_i\n",
    "        indices_of_next_tokens = sequence_b_i_start_index + torch.concat(list(torch.arange(num_sequences_with_enough_talk_tokens_remaining, 0, -1, device=input_ids.device, dtype=torch.long)[:, None] + i for i in range(n_labels_next_tokens)), dim=-1)\n",
    "        if get_token_before_label:\n",
    "            indices_of_next_tokens -= 1\n",
    "        for additional_shape in remaining_input_shape:\n",
    "            indices_of_next_tokens = indices_of_next_tokens[...,None].expand(*(tuple(indices_of_next_tokens.shape) + (additional_shape,)))\n",
    "        # mechanism to support -100 placement at last index will be to append to the input_ids a last sequence element which is just -100s, then make indices which go past the end only grab the end element.\n",
    "        indices_of_next_tokens = indices_of_next_tokens.clamp(max=max_seq)\n",
    "        talk_tokens_for_b_i_thoughts = input_ids[[b_i], ...].expand(*((num_sequences_with_enough_talk_tokens_remaining, -1) + remaining_input_shape)).gather(1, indices_of_next_tokens)\n",
    "        next_tokens_for_thoughts[batch_being_filled: batch_being_filled + num_sequences_with_enough_talk_tokens_remaining] = talk_tokens_for_b_i_thoughts\n",
    "        batch_being_filled += num_sequences_with_enough_talk_tokens_remaining\n",
    "    return next_tokens_for_thoughts\n",
    "\n",
    "def get_input_sized_logits(inputs, logits, elements_per_batch_element):\n",
    "    # this n_sampled_thoughts = 1, and n_talk = 1, and that a thought was generated after every token, and used to predict only the next token.\n",
    "    return_logits = torch.zeros(tuple(inputs.shape) + tuple(logits.shape[-1:]), device=logits.device, dtype=logits.dtype)\n",
    "    # then we have to go through for every input\n",
    "    batch_being_filled = 0\n",
    "    for b_i, elements_in_b_i in enumerate(elements_per_batch_element):\n",
    "        return_logits[b_i, -elements_in_b_i:] = logits[batch_being_filled:batch_being_filled+elements_in_b_i:, -1].flip(dims=(0,))\n",
    "        batch_being_filled += elements_in_b_i\n",
    "    return return_logits\n",
    "# get_input_sized_logits(base_input_ids, talk_logits, elements_per_batch_element)\n",
    "class QuietSTARPolicyModel(nn.Module):\n",
    "    def __init__(self, base_llm_model, n_sampled_thoughts, n_thought_tokens, n_talk_tokens, embedding_scaling, reinforce_loss_scaling, original_loss_weight, base_loss_scaling):\n",
    "        super().__init__()\n",
    "        self.base_llm_model = base_llm_model\n",
    "        self.n_sampled_thoughts = n_sampled_thoughts\n",
    "        self.n_thought_tokens = n_thought_tokens\n",
    "        self.n_talk_tokens = n_talk_tokens\n",
    "        self.embedding_scaling = embedding_scaling\n",
    "        self.reinforce_loss_scaling = reinforce_loss_scaling\n",
    "        self.original_loss_weight = original_loss_weight\n",
    "        self.base_loss_scaling = base_loss_scaling\n",
    "        assert tokenizer.pad_token_id is not None, \"pad token id cannot be None\"\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.lm_head = base_llm_model.lm_head\n",
    "        self.mixing_head = nn.Sequential(\n",
    "            nn.Linear(self.base_llm_model.config.hidden_size * 2, self.base_llm_model.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.base_llm_model.config.hidden_size, self.base_llm_model.config.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.base_llm_model.config.hidden_size, 1, bias=False)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            # zero the last linear layer of the mixing head to start, so we rely only on the base model before trying to use our generated thoughts at all.\n",
    "            list(self.mixing_head.modules())[-1].weight.zero_()\n",
    "        # decorate the forward function\n",
    "        if not hasattr(self.base_llm_model, \"old_forward\"):\n",
    "            self.base_llm_model.old_forward = self.base_llm_model.forward\n",
    "        self.base_llm_model.forward = self.forward\n",
    "    def forward(self, *args, **kwargs): # for the purposes of testing with quiet-star's evals, all I have to do is parallely predict next token likelihoods over all tokens in some sequence.\n",
    "        # for passing through this forward pass to predict the next token on a given vocab, we would want to generate\n",
    "        # potentially only at the ends of the input ids? or for every token like a standard forward pass would do?\n",
    "        # if someone passes in a key_value_cache, we would want to behave appropriately by having it only generate a thought for the tokens in the key value cache\n",
    "        # to begin with we don't really have to replicate this desired effect, just generate a thought for every token, and see if we can get this to work. Even quietSTAR doesn't have a method to only think for the last token!\n",
    "        # in a single forward pass, we take in the input ids, \n",
    "        # print(\"kwargs\", kwargs.keys())\n",
    "\n",
    "        # if \"input_ids\" in kwargs:\n",
    "        #     print(kwargs['input_ids'].shape)\n",
    "        # if \"use_cache\" in kwargs:\n",
    "        #     print(kwargs[\"use_cache\"])\n",
    "        # if \"past_key_values\" in kwargs:\n",
    "        #     print(kwargs[\"past_key_values\"][0][0].shape)\n",
    "        if \"position_ids\" in kwargs:\n",
    "            raise Exception(f\"unexpected position_ids in kwargs {kwargs['position_ids'].shape=} with {kwargs.keys()=}\")\n",
    "        base_input_ids = kwargs.pop('input_ids')\n",
    "        base_attention_mask = kwargs.pop('attention_mask')\n",
    "        base_input_position_ids = (base_attention_mask.cumsum(-1) - 1).clamp(min=0)\n",
    "        base_outputs = self.base_llm_model.old_forward(input_ids=base_input_ids,\n",
    "                                                       attention_mask=base_attention_mask,\n",
    "                                                       position_ids=base_input_position_ids,\n",
    "                                                       return_dict=True,\n",
    "                                                       output_hidden_states=True,\n",
    "                                                       *args, **kwargs)\n",
    "        base_hidden_states = base_outputs['hidden_states'][-1] # len(base_outputs['hidden_states']) = input + one for every layer in that order. so base_outputs['hidden_states'][-1] gets last layer\n",
    "        \n",
    "        thoughts_for_every_token_sub_sequence_outputs, next_tokens_for_thoughts, context_sot_attention_mask, elements_per_batch_element = self.generate_for_every_token(base_input_ids, base_attention_mask)\n",
    "\n",
    "        context_sot_thought = thoughts_for_every_token_sub_sequence_outputs.sequences\n",
    "        thought_input_ids = context_sot_thought[:, -self.n_thought_tokens:] # this if we do some past_key_value based impelmentation. (for speed)\n",
    "        eot_tokens_to_right_pad = torch.full_like(context_sot_thought[:, [0]], fill_value=tokenizer.get_added_vocab()[tokenizer_eot_token])\n",
    "        context_sot_thought_eot = torch.concat([context_sot_thought, eot_tokens_to_right_pad], dim=-1)\n",
    "        # past_key_values = thoughts_for_every_token_sub_sequence_outputs.past_key_values # this will be useful later to speed up parallel generation implementation.\n",
    "        # now to get the probability of the next self.n_talk_tokens by feeding in the \n",
    "\n",
    "        # context_sot_thought_eot_label_padding = torch.full_like(context_sot_thought_eot, fill_value=-100, dtype=torch.long)\n",
    "        # talk_token_labels = torch.concat([context_sot_thought_eot_label_padding, next_tokens_for_thoughts], dim=-1)\n",
    "\n",
    "        next_tokens_for_thoughts_input_ids = next_tokens_for_thoughts.clone()\n",
    "        next_tokens_for_thoughts_input_ids[next_tokens_for_thoughts_input_ids == -100] = self.pad_token_id\n",
    "        context_sot_thought_eot_talk_input_ids = torch.concat([context_sot_thought_eot, next_tokens_for_thoughts_input_ids], dim=-1)\n",
    "        next_tokens_for_thoughts_attention_mask = next_tokens_for_thoughts.clone()\n",
    "        next_tokens_for_thoughts_attention_mask[next_tokens_for_thoughts_attention_mask != -100] = 1\n",
    "        next_tokens_for_thoughts_attention_mask[next_tokens_for_thoughts_attention_mask == -100] = 0\n",
    "        context_sot_thought_eot_talk_attention_mask = torch.concat([context_sot_attention_mask, torch.ones_like(thought_input_ids), torch.ones_like(eot_tokens_to_right_pad), next_tokens_for_thoughts_attention_mask], dim=-1)\n",
    "        \n",
    "        context_sot_thought_eot_talk_position_ids = (context_sot_thought_eot_talk_attention_mask.cumsum(-1) - 1).clamp(min=0)\n",
    "        thought_outputs = self.base_llm_model.old_forward(input_ids=context_sot_thought_eot_talk_input_ids[:, :-1], \n",
    "                                                          attention_mask=context_sot_thought_eot_talk_attention_mask[:, :-1],\n",
    "                                                          position_ids=context_sot_thought_eot_talk_position_ids[:,:-1],\n",
    "                                                        #   labels=talk_token_labels, # technically we don't need the last talk token except to see what the model thinks about the very last token which is only useful for generation. # this was commented out because we manually compute the loss and ignore the extra single next token thing.\n",
    "                                                          output_hidden_states=True,\n",
    "                                                          return_dict=True)\n",
    "        thought_hidden_states = thought_outputs[\"hidden_states\"][-1][:, -self.n_talk_tokens:] # how to ensure that we only have the hidden states for tokens pretaining to the talk tokens. just use the label tokens as a guide?\n",
    "        # expand the base_hidden_states for linearly combining them! this is batch_size x max_seq x 768, but needs to be num_thoughts x max_seq x 768, and it is difficult to know how many times to repeat each row,\n",
    "        expanded_base_hidden_states = get_next_tokens_for_thoughts(elements_per_batch_element, base_hidden_states, self.n_talk_tokens, predict_all=True, get_token_before_label=True) # [list(j for j, g in enumerate(thoughts_per_batch_element) for i in range(g))]\n",
    "        expanded_base_hidden_states = expanded_base_hidden_states.repeat_interleave(self.n_sampled_thoughts, dim=0)\n",
    "        mixing_factor = self.mixing_head(torch.concat([expanded_base_hidden_states, thought_hidden_states], dim=-1))\n",
    "        mixed_hidden_states = expanded_base_hidden_states * (1 - mixing_factor) + thought_hidden_states * mixing_factor\n",
    "        talk_logits = self.lm_head(mixed_hidden_states)\n",
    "        \n",
    "        return_dict: dict = defaultdict(lambda : None)\n",
    "        return_dict[\"last_hidden_states\"] = mixed_hidden_states # , \"logits\": talk_logits} logits will very strange shape, and are only used externally for cot eval. More can be done to combine the different predictions later, but for now, Just prepare logits if n_talk_tokens is 1, and get logits from every prediction, and line them in a row.\n",
    "\n",
    "        # TODO: fix the off by one error where I do not generate a token for the end of the sequence, because I don't generate a thought for the very last token because I assumed that we would always want the full context of self.n_talk_tokens.\n",
    "        #       or just leave it, and return the logits from this method only when the n_talk is 1, because that is the only time it makes sense, because we have one prediction for every seq token \n",
    "        # during the COT eval, how do they generate the next token?\n",
    "        if \"labels\" in kwargs: # without the labels defined, we still want to get the logits for every token position. when n_talk_tokens is greater than 1, we can just average them? or just return the logit corresponding to when this is the first thought. What quietstar does is they just output the logits which would predict the next token after n_thought. so at that point it is only logits for the seq_len - n_talk + 1\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            next_tokens_for_thoughts = next_tokens_for_thoughts.to(talk_logits.device)\n",
    "            # Shift so that tokens < n predict n\n",
    "            talk_shift_logits = talk_logits.contiguous()\n",
    "            talk_shift_labels = next_tokens_for_thoughts.contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss(reduction='none') # this does mean reduction, but we need to comute the non reduced version to be able to calculate reward as defined by change in log likelihood (new_ll - old_ll)\n",
    "            talk_loss_per_talk_token = loss_fct(talk_shift_logits.view(-1, talk_shift_logits.size(-1)), talk_shift_labels.view(-1)).view(*talk_shift_labels.shape)\n",
    "            return_dict[\"talk_loss\"] = talk_loss_per_talk_token.sum() / (talk_shift_labels != -100).sum()\n",
    "\n",
    "            base_labels = kwargs[\"labels\"]\n",
    "            base_shift_logits = base_outputs['logits'][:,:-1].contiguous()\n",
    "            base_shift_labels = base_labels[:,1:].contiguous()\n",
    "            base_talk_loss = loss_fct(base_shift_logits.view(-1, base_shift_logits.size(-1)), base_shift_labels.view(-1)).view(*base_shift_labels.shape)\n",
    "            expanded_base_talk_loss = get_next_tokens_for_thoughts(elements_per_batch_element, base_talk_loss, self.n_talk_tokens, fill_value=0, predict_all=True, get_token_before_label=False)\n",
    "            expanded_base_talk_loss = expanded_base_talk_loss.repeat_interleave(self.n_sampled_thoughts, dim=0)\n",
    "            return_dict[\"base_talk_loss\"] = base_talk_loss.sum() / (base_shift_labels != -100).sum()\n",
    "            change_in_log_likelihood = expanded_base_talk_loss - talk_loss_per_talk_token\n",
    "            # reward per thought is stored in the rows\n",
    "            # num_predicted_labels_per_thought = (talk_shift_labels != -100).sum(-1)\n",
    "            reward_per_thought = change_in_log_likelihood.sum(-1) \n",
    "            # / num_predicted_labels_per_thought # I think I should just do the sum over the nll reduction because average would remove information about number of tokens predicted.\n",
    "            # reward_per_thought[num_predicted_labels_per_thought == 0] = 0.0 # remove the nan division by zeros\n",
    "            \n",
    "            # goal is to perform trice, which should be average performance on the prefix\n",
    "            control_variate = reward_per_thought.view(-1, self.n_sampled_thoughts).mean(-1).repeat_interleave(self.n_sampled_thoughts, dim=0) # if there are some\n",
    "            control_adjusted_reward_per_thought = reward_per_thought - control_variate\n",
    "            positive_control_adjusted_reward_per_thought = control_adjusted_reward_per_thought.clamp(min=0).detach()\n",
    "            # now we need log_thought_probabilities, to be able to get the reinforce loss\n",
    "            start_of_thought_index_in_context_sot_thought_eot_labels_m1 = -self.n_thought_tokens-self.n_talk_tokens-1\n",
    "            end_of_thought_index_in_context_sot_thought_eot_labels_m1 = -self.n_talk_tokens-1\n",
    "            thought_log_probabilities = thought_outputs[\"logits\"][:, start_of_thought_index_in_context_sot_thought_eot_labels_m1 : end_of_thought_index_in_context_sot_thought_eot_labels_m1].log_softmax(-1).gather(-1, thought_input_ids[..., None]).squeeze(-1).sum(-1)\n",
    "            policy_loss = - positive_control_adjusted_reward_per_thought * thought_log_probabilities\n",
    "            policy_loss = policy_loss.mean()\n",
    "            # base_outputs['loss'] # this should be defined if there are labels.\n",
    "            return_dict['base_and_talk_loss'] = (return_dict[\"base_talk_loss\"] * self.original_loss_weight + return_dict[\"talk_loss\"]  * (1 - self.original_loss_weight))\n",
    "            return_dict['policy_loss'] = policy_loss\n",
    "            return_dict['loss'] = return_dict['policy_loss'] * self.reinforce_loss_scaling + return_dict['base_and_talk_loss'] * self.base_loss_scaling\n",
    "            \n",
    "        else:\n",
    "            # no loss computation when labels aren't given, just give the language models predictions for the next token, which we will define to be the one right after the last talk token. \n",
    "            # This is nice for our generation evals, but doesn't reflect our loss well if we report it in the evals :<\n",
    "            assert self.n_talk_tokens == 1, \"num_talk_tokens must be one to return logits, and that is the only thing that makes sense if we are doing a forward pass without labels.\"\n",
    "            raise NotImplemented\n",
    "\n",
    "        if self.n_talk_tokens == 1 and self.n_sampled_thoughts == 1: # we want one logit per token, so we have to gather the corresponding token locations.\n",
    "            # logits on the corresponding token locations.\n",
    "            return_dict['logits'] = get_input_sized_logits(base_input_ids, talk_logits, elements_per_batch_element)\n",
    "        # we now have the hidden states num_thoughts x max_seq x 768 now using the same code we used to get the label mask, we can get the base hidden states corresponding to each label position.\n",
    "        return tuple(return_dict[key] for key in ['loss', \"talk_loss\", \"base_talk_loss\", \"base_and_talk_loss\", \"policy_loss\", 'last_hidden_states', 'logits']) # must return a tuple when working with huggingface trainer.\n",
    "    def base_model_generate(self, *args, **kwargs):\n",
    "        return self.generate(self.base_llm_model.old_forward, *args, **kwargs)\n",
    "    def generate(self, forward_function, *args, **kwargs):\n",
    "        temp_forward = forward_function\n",
    "        try:\n",
    "            self.base_llm_model.forward = self.base_llm_model.old_forward\n",
    "            gen_output = self.base_llm_model.generate(*args, **kwargs)\n",
    "        except:\n",
    "            raise\n",
    "        finally:\n",
    "            self.base_llm_model.forward = temp_forward\n",
    "        return gen_output\n",
    "        # now the choice is do I generate a thought per token, or do I just generate a single next token. The standard interface would be to support sampling the next token with this method, so I will create a different method to support the operation I want.\n",
    "\n",
    "    def generate_for_every_token(self, input_ids, attention_mask):\n",
    "        # this is simply just spreading out every token as a generation task.\n",
    "        # in the future, I can speed this up with past_key_values from a single standard forward pass on the input_ids as one \n",
    "        elements_per_batch_element = attention_mask.sum(-1)\n",
    "        # (num_tokens) x max_seq # the effective batch size is way larger. possibly max_seq * original_batch_size\n",
    "\n",
    "        # the strategy is to make only as many thoughts to where you have at least n_talk remaining tokens for labels.\n",
    "        thoughts_per_batch_element = (elements_per_batch_element - self.n_talk_tokens).clamp(min=0)\n",
    "        next_tokens_for_thoughts = get_next_tokens_for_thoughts(elements_per_batch_element, input_ids, self.n_talk_tokens, predict_all=True) # input_ids\n",
    "        expanded_input_ids = expand_to_triangular_sequence_for_parallel_generation(elements_per_batch_element, input_ids, fill_value=self.pad_token_id, n_talk_tokens=self.n_talk_tokens, predict_all=True)\n",
    "        expanded_attention_mask = expand_to_triangular_sequence_for_parallel_generation(elements_per_batch_element, attention_mask, fill_value=0, n_talk_tokens=self.n_talk_tokens, predict_all=True)\n",
    "        \n",
    "        sot_tokens_to_right_pad = torch.full_like(expanded_input_ids[:, [0]], fill_value=tokenizer.get_added_vocab()[tokenizer_sot_token])\n",
    "        expanded_input_ids = torch.concat([expanded_input_ids, sot_tokens_to_right_pad], dim=-1)\n",
    "        expanded_input_ids = expanded_input_ids.repeat_interleave(self.n_sampled_thoughts, 0)\n",
    "        mask_to_right_pad = torch.full_like(expanded_attention_mask[:, [0]], fill_value=1)\n",
    "        expanded_attention_mask = torch.concat([expanded_attention_mask, mask_to_right_pad], dim=-1)\n",
    "        expanded_attention_mask = expanded_attention_mask.repeat_interleave(self.n_sampled_thoughts, 0)\n",
    "\n",
    "        next_tokens_for_thoughts = next_tokens_for_thoughts.repeat_interleave(self.n_sampled_thoughts, 0)\n",
    "        # print(expanded_input_ids)\n",
    "        # print(expanded_attention_mask)\n",
    "        # import ipdb; ipdb.set_trace()\n",
    "        outputs = self.base_model_generate(input_ids=expanded_input_ids, attention_mask=expanded_attention_mask, max_new_tokens=self.n_thought_tokens, do_sample=True, return_dict_in_generate=True)\n",
    "        # print(outputs.keys())\n",
    "        return outputs, next_tokens_for_thoughts,  expanded_attention_mask, elements_per_batch_element\n",
    "\n",
    "quiet_star_model = QuietSTARPolicyModel(deepcopy(base_model), n_sampled_thoughts, n_thought_tokens, n_talk_tokens, embedding_scaling, reinforce_loss_scaling, original_loss_weight, base_loss_scaling).to(device)\n",
    "# quiet_star_model = QuietSTARPolicyModel(deepcopy(base_model), 1, n_thought_tokens, 1, embedding_scaling, reinforce_loss_scaling, original_loss_weight, base_loss_scaling).to(device)\n",
    "base_inputs = tokenizer([\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", \"how is it going Schrammy?\"], return_tensors='pt', padding=True, padding_side=\"left\", add_special_tokens=True).to(device)\n",
    "# print(base_inputs)\n",
    "base_input_labels = base_inputs.input_ids.clone()\n",
    "base_input_labels[base_inputs.attention_mask == 0] = -100\n",
    "base_input_labels[torch.arange(base_input_labels.size(0)), base_inputs.attention_mask.argmax(dim=-1)] = -100 # gets the first location? I don't know  if this is a safe bet, but whatever for now...\n",
    "quiet_star_model.forward(**base_inputs, labels=base_input_labels)\n",
    "# quiet_star_model.generate_for_every_token(**base_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef4a97397b4147f7979a59b198d7b104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/114 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjakobbbjorner\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nethome/jbjorner3/dev/hallucination-fun/quiet_star_replicate/scratch/wandb/run-20241205_114035-in6k5cn9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jakobbbjorner/huggingface/runs/in6k5cn9' target=\"_blank\">quiet_star_replicate_runs/cache/quietstar/1733416833</a></strong> to <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jakobbbjorner/huggingface' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jakobbbjorner/huggingface/runs/in6k5cn9' target=\"_blank\">https://wandb.ai/jakobbbjorner/huggingface/runs/in6k5cn9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='425' max='100000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   425/100000 03:14 < 12:42:11, 2.18 it/s, Epoch 0.85/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>891.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3228.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5515.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>6293.570300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6178.756300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>7252.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>7733.604700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>6586.925000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6455.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5995.643000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5905.518400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5190.112500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>6104.147300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>6078.840200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>5684.982000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>6697.660200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5893.401200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5949.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>6053.866400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6794.015600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5127.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>6277.559400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5045.775400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>6206.814100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5578.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5209.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5066.967600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5447.372700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5028.905100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6154.089100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5191.184000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>4584.929300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>4363.572700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>4739.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>4287.881300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>4106.789100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>4827.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>4800.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>4036.913300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4594.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>4294.825400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>4300.900400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 110\u001b[0m\n\u001b[1;32m     78\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# use_cpu=True,\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# no_cuda=True, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    100\u001b[0m     save_total_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;66;03m#running out of scratch storage, only save latest ckpt\u001b[39;00m\n\u001b[1;32m    101\u001b[0m )\n\u001b[1;32m    102\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    103\u001b[0m     model\u001b[38;5;241m=\u001b[39mquiet_star_model,\n\u001b[1;32m    104\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# model_init=model_init,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m )\n\u001b[0;32m--> 110\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/accelerate/utils/memory.py:158\u001b[0m, in \u001b[0;36mfind_executable_batch_size.<locals>.decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo executable batch size found, reached zero.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2479\u001b[0m )\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2487\u001b[0m ):\n\u001b[1;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/trainer.py:3579\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3578\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3579\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3582\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3583\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3584\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   3585\u001b[0m ):\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/trainer.py:3633\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m         loss_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[1;32m   3632\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloss_kwargs}\n\u001b[0;32m-> 3633\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3635\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[6], line 124\u001b[0m, in \u001b[0;36mQuietSTARPolicyModel.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m base_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_llm_model\u001b[38;5;241m.\u001b[39mold_forward(input_ids\u001b[38;5;241m=\u001b[39mbase_input_ids,\n\u001b[1;32m    117\u001b[0m                                                attention_mask\u001b[38;5;241m=\u001b[39mbase_attention_mask,\n\u001b[1;32m    118\u001b[0m                                                position_ids\u001b[38;5;241m=\u001b[39mbase_input_position_ids,\n\u001b[1;32m    119\u001b[0m                                                return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    120\u001b[0m                                                output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    121\u001b[0m                                                \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    122\u001b[0m base_hidden_states \u001b[38;5;241m=\u001b[39m base_outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_states\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# len(base_outputs['hidden_states']) = input + one for every layer in that order. so base_outputs['hidden_states'][-1] gets last layer\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m thoughts_for_every_token_sub_sequence_outputs, next_tokens_for_thoughts, context_sot_attention_mask, elements_per_batch_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_for_every_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m context_sot_thought \u001b[38;5;241m=\u001b[39m thoughts_for_every_token_sub_sequence_outputs\u001b[38;5;241m.\u001b[39msequences\n\u001b[1;32m    127\u001b[0m thought_input_ids \u001b[38;5;241m=\u001b[39m context_sot_thought[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_thought_tokens:] \u001b[38;5;66;03m# this if we do some past_key_value based impelmentation. (for speed)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 253\u001b[0m, in \u001b[0;36mQuietSTARPolicyModel.generate_for_every_token\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m    249\u001b[0m next_tokens_for_thoughts \u001b[38;5;241m=\u001b[39m next_tokens_for_thoughts\u001b[38;5;241m.\u001b[39mrepeat_interleave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_sampled_thoughts, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# print(expanded_input_ids)\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# print(expanded_attention_mask)\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_input_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpanded_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_thought_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# print(outputs.keys())\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs, next_tokens_for_thoughts,  expanded_attention_mask, elements_per_batch_element\n",
      "Cell \u001b[0;32mIn[6], line 217\u001b[0m, in \u001b[0;36mQuietSTARPolicyModel.base_model_generate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbase_model_generate\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_llm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mold_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 222\u001b[0m, in \u001b[0;36mQuietSTARPolicyModel.generate\u001b[0;34m(self, forward_function, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_llm_model\u001b[38;5;241m.\u001b[39mforward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_llm_model\u001b[38;5;241m.\u001b[39mold_forward\n\u001b[0;32m--> 222\u001b[0m     gen_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_llm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3196\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3197\u001b[0m ):\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "random_seed = 42\n",
    "\n",
    "dataset_name = 'open-web-math/open-web-math'\n",
    "n_examples = 1_000\n",
    "max_seq_len = 32\n",
    "\n",
    "\n",
    "dataset = load_dataset(\n",
    "    dataset_name,\n",
    "    \"en\" if \"c4\" in dataset_name else \"default\",\n",
    "    split=f\"train[:{n_examples}]\",\n",
    "    # ignore_verifications=True,\n",
    "    verification_mode=datasets.VerificationMode.NO_CHECKS,\n",
    "    num_proc=4,\n",
    "    # cache_dir=root_prefix + \"cache/datasets/\",\n",
    ")\n",
    "def get_preprocess_function(max_length):\n",
    "    def preprocess_function(examples):\n",
    "        if isinstance(examples[\"text\"], str):\n",
    "            dataset_transform = lambda xs: [xs[\"text\"]]\n",
    "        else:\n",
    "            dataset_transform = lambda xs: xs[\"text\"]\n",
    "        all_tokenized = [tokenizer.encode(t, return_tensors=\"pt\", ) for t in dataset_transform(examples)]\n",
    "        new_tokenized = [{\"input_ids\": t} for t in all_tokenized]\n",
    "        for i, t in enumerate(new_tokenized):\n",
    "            new_tokenized[i][\"input_ids\"] = truncate_or_pad(t['input_ids'], tokenizer.pad_token_id, max_length)\n",
    "        new_input_ids = torch.cat([t[\"input_ids\"] for t in new_tokenized], dim=0)\n",
    "        new_attention_mask = (new_input_ids != tokenizer.pad_token_id).long()\n",
    "        tokenized = {\"input_ids\": new_input_ids, \"attention_mask\": new_attention_mask}\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "        tokenized[\"labels\"][new_attention_mask == 0] = -100\n",
    "        tokenized['labels'][torch.arange(new_attention_mask.size(0)), new_attention_mask.argmax(dim=-1)] = -100\n",
    "        return tokenized\n",
    "    # def preprocess_function(examples):\n",
    "    #     if isinstance(examples[\"text\"], str):\n",
    "    #         dataset_transform = lambda xs: [xs[\"text\"]]\n",
    "    #     else:\n",
    "    #         dataset_transform = lambda xs: xs[\"text\"]\n",
    "    #     print(examples)\n",
    "    #     print()\n",
    "    #     all_tokenized = tokenizer(dataset_transform(examples), return_tensors=\"pt\", padding=True, padding_side='left', max_length=max_seq_len)\n",
    "    #     # all_tokenized = [tokenizer(t, return_tensors=\"pt\", padding_side='left', max_length=max_seq_len) for t in dataset_transform(examples)] # Jakob: changed to padding side right\n",
    "    #     # new_tokenized = [{\"input_ids\": t.input_ids,} for t in all_tokenized]\n",
    "    #     assert tokenizer.pad_token_id is not None, \"Must have a valid pad token, but it was None.\"\n",
    "    #     new_input_ids = all_tokenized.input_ids\n",
    "    #     new_attention_mask = all_tokenized.attention_mask\n",
    "    #     tokenized = {\"input_ids\": new_input_ids, \"attention_mask\": new_attention_mask}\n",
    "    #     tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
    "    #     tokenized['labels'][new_attention_mask == 0] = -100 # Jakob: changed\n",
    "    #     tokenized['labels'][torch.arange(new_attention_mask.size(0)), new_attention_mask.argmax(dim=-1)] = -100 # gets the first location? I don't know  if this is a safe bet, but whatever for now...\n",
    "    #     # tokenized['position_ids'] = (base_inputs.attention_mask.cumsum(-1) - 1).clamp(min=0)\n",
    "    #     # tokenized['labels'][new_attention_mask == 0] = -100 # Jakob: changed\n",
    "    #     return tokenized\n",
    "    return preprocess_function\n",
    "def truncate_or_pad(t, padding_idx=0, max_length=256): # now left padding\n",
    "    if t.shape[1] > max_length:\n",
    "        start = random.randint(0, t.shape[1] - max_length)\n",
    "        t = t[:, start:start + max_length]\n",
    "    else:\n",
    "        padding = torch.zeros(t.shape[0], max_length - t.shape[1], dtype=t.dtype, device=t.device)\n",
    "        t = torch.cat([padding + padding_idx, t], dim=1) # left now.\n",
    "    return t\n",
    "train_dataset = dataset.shuffle(seed=random_seed).map(get_preprocess_function(max_seq_len), batched=True, writer_batch_size=200, remove_columns=[\"text\"])\n",
    "root_prefix = 'quiet_star_replicate_runs/'\n",
    "full_batch_size = 2\n",
    "n_passes_global = 2\n",
    "gradient_accumulation_steps = 1\n",
    "import time\n",
    "run_id = int(time.time())\n",
    "\n",
    "batch_size = full_batch_size // n_passes_global\n",
    "global_gradient_accumulation_steps = full_batch_size // batch_size * gradient_accumulation_steps\n",
    "eval_and_logging_steps = 10\n",
    "training_args = TrainingArguments(\n",
    "    # use_cpu=True,\n",
    "    # no_cuda=True, \n",
    "    output_dir=root_prefix + f\"cache/quietstar/{run_id}\",\n",
    "    learning_rate=1e-6,\n",
    "    optim= \"adamw_torch_fused\" if torch.cuda.is_available() else \"adamw_torch\",  # \"adamw_8bit\", #\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=global_gradient_accumulation_steps,\n",
    "    max_grad_norm=1.0,\n",
    "    max_steps=100000,\n",
    "    warmup_steps=20,\n",
    "    auto_find_batch_size=True, # doesn't seem to work for my a40 Jakob\n",
    "    weight_decay=0.001,\n",
    "    label_names=[\"labels\", \"input_ids\", \"attention_mask\"],\n",
    "    # include_inputs_for_metrics=True,\n",
    "    logging_steps=eval_and_logging_steps,\n",
    "    # eval_steps=eval_and_logging_steps,\n",
    "    # evaluation_strategy=\"steps\",\n",
    "    save_safetensors=False,\n",
    "    # save_steps=save_steps,\n",
    "    # run_name=f\"{debug_prefix}n={n_ahead_global}_nt={n_ahead_talk_global}_np={n_passes_global}_fbz={full_batch_size}_seql={max_seq_len}_gacc={gradient_accumulation_steps}_sot={initial_start_token}_embsc={embedding_scale}_cyc={cycling_sot_token}_rm={reward_modeling}_rmb={rm_beta}_plb={policy_loss_beta}_mp={use_meta_prompt}_{meta_prompt_list}\",\n",
    "    save_total_limit = 1, #running out of scratch storage, only save latest ckpt\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=quiet_star_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_datasets,\n",
    "    # compute_metrics=compute_metrics,\n",
    "    # model_init=model_init,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.6456, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_inputs_left_pad = tokenizer([\"What can you tell me about the Janet egg problem?\", \"how is it going Schrammy?\"], return_tensors='pt', padding=True, padding_side=\"left\", add_special_tokens=True)\n",
    "base_inputs_left_pad.input_ids\n",
    "labels_left_pad = base_inputs_left_pad.input_ids.clone()\n",
    "labels_left_pad[base_inputs_left_pad.attention_mask == 0] = -100\n",
    "labels_left_pad[torch.arange(labels_left_pad.size(0)), base_inputs_left_pad.attention_mask.argmax(dim=-1)] = -100 # gets the first location? I don't know  if this is a safe bet, but whatever for now...\n",
    "position_ids_left = (base_inputs_left_pad.attention_mask.cumsum(-1) - 1).clamp(min=0)\n",
    "base_model(**base_inputs_left_pad, labels=labels_left_pad, position_ids=position_ids_left).loss# .logits[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6456, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ -37.2173,  -70.6897, -124.5740,  -97.8898, -105.1452,  -76.4182,\n",
       "          -90.5668,  -71.3065,  -83.2462,  -77.9504, -135.7136],\n",
       "        [ -29.7428,  -74.9223,  -85.6836,  -48.5544,  -93.3122,  -70.0023,\n",
       "          -64.0737, -119.7977,  -91.2024,  -90.9099,  -90.9580]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_inputs_right_pad = tokenizer([\"What can you tell me about the Janet egg problem?\", \"how is it going Schrammy?\"], return_tensors='pt', padding=True, padding_side=\"right\", add_special_tokens=True)\n",
    "base_inputs_right_pad.input_ids\n",
    "labels_right_pad = base_inputs_right_pad.input_ids.clone()\n",
    "labels_right_pad[base_inputs_right_pad.attention_mask == 0] = -100\n",
    "print(base_model(**base_inputs_right_pad, labels=labels_right_pad).loss)\n",
    "base_model(**base_inputs_right_pad, labels=labels_right_pad).logits[...,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16, 17],\n",
       "        [15, 16],\n",
       "        [14, 15],\n",
       "        [13, 14],\n",
       "        [12, 13],\n",
       "        [11, 12]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(8)[None, :].repeat(8, 1) + 10).gather(1, torch.concat(list(torch.arange(8 - 2 , 0, -1)[:, None] + i for i in range(2)), dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'logits', 'past_key_values', 'hidden_states'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_outputs = base_model(**base_inputs, labels=base_inputs.input_ids, return_dict=True, output_hidden_states=True, past_key)\n",
    "base_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"how is it going?\\n\\nThe answer is that it's not going\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(quiet_star_model.base_model_generate(**base_inputs, max_new_tokens=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specifically so gradients get aggregated to the same forward pass which computed the initial key values. \n",
    "# this for the base model mixed in logits or for parallel generation? (parallel gen has no grads, so mix in portion.)\n",
    "# TODO: is this useful? does it save compute if we need to compute the gradient on the new thoughts anyways?\n",
    "base_outputs.past_key_values[0][1].shape\n",
    "reshaped_past_key_values = tuple()\n",
    "for key, value in base_outputs.past_key_values:\n",
    "    # we get a new key for every batch, and we have to left padd these new keys with something stupid, so we can notice if we have done anything wrong.\n",
    "    batch_sz = key.size(0)\n",
    "    num_tokens = key.size(2)\n",
    "    new_key = torch.zeros_like(key).repeat((num_tokens,1,1,1)) #torch.concat(key[i] for i in range(key.size(-2)))\n",
    "    new_value = torch.zeros_like(value).repeat((num_tokens,1,1,1))\n",
    "    for b_i in range(batch_sz):\n",
    "        for i in range(num_tokens):\n",
    "            new_key[b_i * num_tokens + i, :, -(i+1):, :] = key[b_i, :, :i+1, :] # makes a lower right trangular matrix out of the batch and sequence dimensions.\n",
    "            new_value[b_i * num_tokens + i, :, -(i+1):, :] = value[b_i, :, :i+1, :] \n",
    "            # print(new_key[:,0,:,0])\n",
    "            # print(key[:, 0, :, 0])\n",
    "            # print(key[b_i, :i+1, :, 0])\n",
    "            # print(new_key[b_i * num_tokens + i, -(i+1):, :, 0])\n",
    "    #     break\n",
    "    # break\n",
    "    reshaped_past_key_values += ((new_key, new_value),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n",
       "        [10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n",
       "        [10, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n",
       "        [10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 17, 18],\n",
       "        [10, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16, 17],\n",
       "        [10, 10, 10, 10, 10, 10, 11, 12, 13, 14, 15, 16],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 11, 12, 13, 14, 15],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 11, 12, 13, 14],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 12, 13],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 12],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.arange(12) + 10)[None, :].expand(12,-1).gather(1, (torch.arange(12)[None, :] - torch.arange(12)[:, None]).triu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11, 11,\n",
       "         11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "         13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 14, 14, 14, 14, 14, 14,\n",
       "         14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,\n",
       "         16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17,\n",
       "         17, 17, 17, 17, 17, 17, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,\n",
       "         19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20, 20, 20, 20, 20,\n",
       "         20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.triu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>how is it going? I am so happy to be back in the US. I am '\n",
      " 'so excited to be back in the']\n"
     ]
    }
   ],
   "source": [
    "base_samples = model.generate(**base_inputs, do_sample=False, max_new_tokens=20)\n",
    "pprint(tokenizer.batch_decode(base_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128001, 128001, 128001, 128001, 128000,   5269,   4521],\n",
       "        [128001, 128001, 128001, 128000,   5269,    374,   4521],\n",
       "        [128001, 128001, 128000,   5269,    374,    433,   4521],\n",
       "        [128001, 128000,   5269,    374,    433,   2133,   4521],\n",
       "        [128000,   5269,    374,    433,   2133,     30,   4521]]), 'attention_mask': tensor([[0, 0, 0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer([sent + start_of_thought_token for sent in [\"how\", \"how is\", \"how is it\", \"how is it going\", \"how is it going?\"]], return_tensors='pt', padding=True, padding_side=\"left\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how---to---make---a---screw---driver---with',\n",
      " '<|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how '\n",
      " 'is---the---solar---panel---made---and---how',\n",
      " '<|end_of_text|><|end_of_text|><|begin_of_text|>how is it---the 2nd time i '\n",
      " 'have seen this movie. i',\n",
      " '<|end_of_text|><|begin_of_text|>how is it going---i am still here. i have '\n",
      " 'been busy with my new',\n",
      " \"<|begin_of_text|>how is it going?---i'm back from my trip to the west coast. \"\n",
      " 'i')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=12, do_sample=False, return_dict_in_generate=True)\n",
    "    print(tokenizer.batch_decode(outputs.sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 18, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = (outputs.sequences != tokenizer.pad_token_id).long()\n",
    "# shift the attention matrix by 1 to the left, so we include the begin of sentence token which happens to also equal the end of sentence token.\n",
    "# attention_mask = torch.concat([attention_mask[:, 1:], torch.ones_like(attention_mask[:, [0]])], dim=-1)\n",
    "attention_mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8, 18, 64])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.past_key_values[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'past_key_values'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs_kv_test = model.generate(input_ids=outputs.sequences, attention_mask=attention_mask, max_new_tokens=12, do_sample=False, return_dict_in_generate=True, past_key_values=outputs.past_key_values)\n",
    "outputs_kv_test.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how---to---make---a---screw---driver---with---a---screw---driver---and---a---',\n",
       " '<|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how is---the---solar---panel---made---and---how---does---it---work---?\\nhow is---the',\n",
       " '<|end_of_text|><|end_of_text|><|begin_of_text|>how is it---the 2nd time i have seen this movie. i was not impressed the first time. i think it was a',\n",
       " '<|end_of_text|><|begin_of_text|>how is it going---i am still here. i have been busy with my new job and i am still trying to get my new place in',\n",
       " \"<|begin_of_text|>how is it going?---i'm back from my trip to the west coast. i had a great time, and i'm looking forward to getting\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs_kv_test.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how---to---make---a---screw---driver---with---a---screw---driver---and---a---',\n",
       " '<|end_of_text|><|end_of_text|><|end_of_text|><|begin_of_text|>how is---the---solar---panel---made---and---how---does---it---work---?\\nhow is---the',\n",
       " '<|end_of_text|><|end_of_text|><|begin_of_text|>how is it---the 2nd time i have seen this movie. i was not impressed the first time. i think it was a',\n",
       " '<|end_of_text|><|begin_of_text|>how is it going---i am still here. i have been busy with my new job and i am still trying to get my new place in',\n",
       " \"<|begin_of_text|>how is it going?---i'm back from my trip to the west coast. i had a great time, and i'm looking forward to getting\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(outputs_kv_test.sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(1, 2048)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(128258, 2048)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "\"\"\"GPT2LMHeadModel(\n",
    "  (transformer): GPT2Model(\n",
    "    (wte): Embedding(50257, 768)\n",
    "    (wpe): Embedding(1024, 768)\n",
    "    (drop): Dropout(p=0.1, inplace=False)\n",
    "    (h): ModuleList(\n",
    "      (0-11): 12 x GPT2Block(\n",
    "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (attn): GPT2SdpaAttention(\n",
    "          (c_attn): Conv1D(nf=2304, nx=768)\n",
    "          (c_proj): Conv1D(nf=768, nx=768)\n",
    "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
    "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "        (mlp): GPT2MLP(\n",
    "          (c_fc): Conv1D(nf=3072, nx=768)\n",
    "          (c_proj): Conv1D(nf=768, nx=3072)\n",
    "          (act): NewGELUActivation()\n",
    "          (dropout): Dropout(p=0.1, inplace=False)\n",
    "        )\n",
    "      )\n",
    "    )\n",
    "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
    "  )\n",
    "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
    ")\n",
    "\n",
    "\n",
    "lm_head seems to be common at least in here and in the other hting.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiet-STAR on the hidden representation\n",
    "with character level language model\n",
    "\n",
    "GRU updated:\n",
    "$$h_t = \\tilde{h_t} \\odot z_t + h_{t-1} \\odot (1-z_t)$$\n",
    "$$r_t = \\sigma(W^r[h_{t-1}; x_t])$$\n",
    "$$z_t = \\sigma(W^z[h_{t-1}; x_t])$$\n",
    "$$\\tilde{h_t} = \\tanh(W^{\\tilde{h}}[r_t \\odot h_{t-1}; x_t])$$\n",
    "\n",
    "Adding stochasticity to the $\\tilde{h_t}$ computation, to simulate quiet-star\n",
    "\n",
    "$$\\tilde{h_{base_{t}}} = \\tanh(W^{\\tilde{h_{base}}}[r_t \\odot h_{t-1}; x_t])$$\n",
    "$$\\epsilon \\sim \\mathcal{N}(0,1)$$\n",
    "$$\\tilde{h_t} = W^{\\mu_{\\tilde{h}}}[\\tilde{h_{base_{t}}}] + \\epsilon \\exp(W^{\\sigma_{\\tilde{h}}}[\\tilde{h_{base_{t}}}])$$\n",
    "\n",
    "essentially:\n",
    "$$\\tilde{h_t} \\sim \\mathcal{N}(\\mu_{\\tilde{h_t}},\\sigma_{\\tilde{h_t}})$$\n",
    "\n",
    "\n",
    "Adding stochasticity to the $h_t$ computation, to simulate quiet-star, where they don't have the gating. This kind of seems like an RNN? Yeah, and it removes the impact of gating, so it should have the exploading gradients problem.\n",
    "\n",
    "$$h_{base_{t}} = \\tilde{h_t} \\odot z_t + h_{t-1} \\odot (1-z_t)$$\n",
    "$$\\epsilon \\sim \\mathcal{N}(0,1)$$\n",
    "$$h_t = W^{\\mu_{h}}[h_{base_{t}}] + \\epsilon \\exp(W^{\\sigma_{h}}[h_{base_{t}}])$$\n",
    "\n",
    "essentially:\n",
    "$$h_t \\sim \\mathcal{N}(\\mu_{h_t},\\sigma_{h_t})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import cast\n",
    "from functools import partial\n",
    "import pickle\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any\n",
    "\n",
    "### Hyper parameters\n",
    "device = 'mps'\n",
    "seq_len = 128\n",
    "batch_size = 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to rebalance my data so that it isn't sequential. my true setting isn't concerned with unseen text being future text. assume the text is in training distribution just not trained on directly, and same for the set dedicated to reward model training.\n",
    "# train_str = open(\"./tiny_shakespeare_train.txt\", 'r').read()[-914334:]\n",
    "# train_reward_model_str = open(\"./tiny_shakespeare_train.txt\", 'r').read()[:-914334] # 10% of original train set to the reward modeling set to ensure generality, and not learning instantaniuous reward.\n",
    "# eval_str = open(\"./tiny_shakespeare_eval.txt\", 'r').read()\n",
    "\n",
    "data_str = open('./tiny_shakespeare.txt', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(data_str)\n",
    "vocab = sorted(list(vocab))\n",
    "vocab.insert(0, '<sot>')\n",
    "index_to_char_list = vocab\n",
    "char_to_index_dict = dict((c, i) for i, c in enumerate(index_to_char_list))\n",
    "# chat_to_index_dict, index_to_char_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detokenize(ids):\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        assert len(ids.shape) == 1, \"the size of a detokenized ids tensor can only be one dimensional\"\n",
    "        ids = ids.tolist()\n",
    "    return \"\".join([index_to_char_list[i] for i in ids])\n",
    "def batch_detokenize(batch_ids):\n",
    "    return [detokenize(ids) for ids in batch_ids]\n",
    "def tokenize(string: str):\n",
    "    return [char_to_index_dict[c] for c in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 129])\n",
      "('<sot>ere effusion of thy proper loins,\\n'\n",
      " 'Do curse the gout, serpigo, and the rheum,\\n'\n",
      " 'For ending thee no sooner. Thou hast nor youth nor ')\n"
     ]
    }
   ],
   "source": [
    "class ShakespeareDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_str, seq_len):\n",
    "        self.data = [data_str[i * seq_len: (i+1) * seq_len] for i in range(len(data_str) // seq_len)]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "full_dataset_shakespeare = ShakespeareDataset(data_str, seq_len)\n",
    "\n",
    "train_dataset_shakespeare, train_reward_model_dataset_shakespeare, eval_dataset_shakespeare = torch.utils.data.random_split(full_dataset_shakespeare, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "\n",
    "def shakespeare_collate_fn(examples):\n",
    "    input_ids = []\n",
    "    for example in examples:\n",
    "        input_ids.append([char_to_index_dict['<sot>']] + tokenize(example))\n",
    "    return torch.tensor(input_ids)\n",
    "\n",
    "\n",
    "example_input = None\n",
    "for example_input in torch.utils.data.DataLoader(train_dataset_shakespeare, batch_size=batch_size, collate_fn=shakespeare_collate_fn):\n",
    "    # example_input = example_input.to(device)\n",
    "    print(example_input.shape)\n",
    "    pprint(detokenize(example_input[0]))\n",
    "    break\n",
    "example_input = cast(torch.Tensor, example_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomizedGRUCell(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, sample_h_tilda:bool, sample_identity:bool, reparameterize:bool):\n",
    "        '''has assumed batch by sequence input dimension, and assume randomness is in the hidden representation to be sent to the output and the next layer'''\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear_z = torch.nn.Linear(in_features=hidden_dim + input_size, out_features=hidden_dim)\n",
    "        self.linear_r = torch.nn.Linear(in_features=hidden_dim + input_size, out_features=hidden_dim)\n",
    "        self.linear_h_tilda_x = torch.nn.Linear(in_features=input_size, out_features=hidden_dim)\n",
    "        self.linear_h_tilda_h_m1 = torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        self.sample_h_tilda = sample_h_tilda\n",
    "        self.sample_identity = sample_identity\n",
    "        self.reparameterize = reparameterize\n",
    "\n",
    "        self.distribution_params = torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim*2)\n",
    "\n",
    "    # @torch.compile(options={\"triton.cudagraphs\": True}, fullgraph=True)\n",
    "    def forward(self, x_t, h_t_m1, h_tilda_start=None):\n",
    "        ''' expect [batch, 1, input_dim] for x h_start and h_tilda (if provided)'''\n",
    "        dict_return = dict()\n",
    "        x_concat_h = torch.concat([x_t, h_t_m1], dim=-1)\n",
    "        z_t = self.linear_z(x_concat_h).sigmoid()\n",
    "        r_t = self.linear_r(x_concat_h).sigmoid()\n",
    "        if h_tilda_start is not None and self.sample_h_tilda:\n",
    "            h_tilda_t = h_tilda_start\n",
    "        else:\n",
    "            h_tilda_t = torch.tanh(self.linear_h_tilda_x(x_t) + r_t * self.linear_h_tilda_h_m1(h_t_m1))\n",
    "\n",
    "        if self.sample_h_tilda:\n",
    "            h_tilda_dist, h_tilda_t, log_prob_h_tilda_t = self.create_dist_sample_get_log_prob(h_tilda_t)\n",
    "            h_t = z_t * h_t_m1 + (1 - z_t) * h_tilda_t\n",
    "            dict_return['log_prob_h_tilda_t'] = log_prob_h_tilda_t\n",
    "            dict_return['h_tilda_t'] = h_tilda_t\n",
    "            dict_return['h_tilda_dist'] = h_tilda_dist\n",
    "        else:\n",
    "            h_t_base = z_t * h_t_m1 + (1 - z_t) * h_tilda_t\n",
    "            h_t_dist, h_t, log_prob_h_t = self.create_dist_sample_get_log_prob(h_t_base)\n",
    "            dict_return['log_prob_h_t'] = log_prob_h_t\n",
    "            dict_return['h_t_dist'] = h_t_dist\n",
    "\n",
    "        dict_return[\"h_t\"] = h_t\n",
    "        return dict_return\n",
    "    def create_dist_sample_get_log_prob(self, representation):\n",
    "        if self.sample_identity: # this for testing if base GRU implementation is ok.\n",
    "            return None, representation, torch.zeros_like(representation)\n",
    "        mean, log_var = self.distribution_params(representation).chunk(2, dim=-1)\n",
    "\n",
    "        data_type_info = torch.finfo(torch.float32)\n",
    "        max_log_var_val = 30\n",
    "        max_mean_val = 1000\n",
    "        mean = torch.clamp(mean, min=-max_mean_val, max=max_mean_val)\n",
    "        log_var = torch.clamp(log_var, min=-max_log_var_val, max=max_log_var_val) # do clamping first because the clamp post exp with inf doesn't work the grad turns to nan instead of 0.0\n",
    "        dist = torch.distributions.Normal(loc=mean, scale=(0.5 * log_var).exp()) # numerical stability.\n",
    "        if self.reparameterize:\n",
    "            sample = dist.rsample()\n",
    "        else:\n",
    "            sample = dist.sample()\n",
    "        log_prob_sample = dist.log_prob(sample)\n",
    "        return dist, sample, log_prob_sample\n",
    "        \n",
    "def combine_normals(normal_distributions_list, dim):\n",
    "    locs = torch.concat([n.loc for n in normal_distributions_list], dim=dim)\n",
    "    scales = torch.concat([n.scale for n in normal_distributions_list], dim=dim)\n",
    "    return torch.distributions.Normal(loc=locs, scale=scales)\n",
    "class RandomizedGRU(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_layers, sample_h_tilda:bool, sample_identity:bool, reparameterize:bool):\n",
    "        super().__init__()\n",
    "        # TODO: support multiple layers\n",
    "        # NOTE: this implementation doesn't work with any kind of padding...\n",
    "        self.sample_h_tilda = sample_h_tilda\n",
    "        self.cell = RandomizedGRUCell(input_size, hidden_dim, sample_h_tilda, sample_identity, reparameterize)\n",
    "        self.hidden_dim = hidden_dim\n",
    "    def forward(self, x, h_start=None, h_tilda_start=None, return_dict=False):\n",
    "        if h_start is None:\n",
    "            h_start = torch.zeros_like(x[:, [0]][...,[0]]).repeat(1, 1, self.hidden_dim)\n",
    "        h_t_m1 = h_start\n",
    "        h_ts = []\n",
    "        h_tilda_ts = []\n",
    "        log_prob_h_ts = []\n",
    "        log_prob_h_tilda_ts = []\n",
    "        h_t_dists = []\n",
    "        h_tilda_dists = []\n",
    "        for t in range(x.size(1)):\n",
    "            x_t = x[:, [t], :]\n",
    "            dict_return_t = self.cell(x_t, h_t_m1=h_t_m1, h_tilda_start=h_tilda_start if t == 0 and h_tilda_start is not None and self.sample_h_tilda else None)\n",
    "\n",
    "            if self.sample_h_tilda:\n",
    "                h_tilda_ts.append(dict_return_t['h_tilda_t'])\n",
    "                log_prob_h_tilda_ts.append(dict_return_t['log_prob_h_tilda_t'])\n",
    "                h_tilda_dists.append(dict_return_t['h_tilda_dist'])\n",
    "            else:\n",
    "                log_prob_h_ts.append(dict_return_t['log_prob_h_t'])\n",
    "                h_t_dists.append(dict_return_t['h_t_dist'])\n",
    "            h_ts.append(dict_return_t['h_t'])\n",
    "            h_t_m1 = dict_return_t['h_t'] # .clone() this may be necessary if I want to figure out the compile thing??\n",
    "        h_ts = torch.concat(h_ts, dim=1)\n",
    "        if return_dict:\n",
    "            dict_return: dict = {\"h_ts\": h_ts, \"h_n\": h_ts[:, [-1], :]}\n",
    "            if self.sample_h_tilda:\n",
    "                dict_return[\"h_tilda_ts\"] = torch.concat(h_tilda_ts, dim=1)\n",
    "                dict_return[\"log_prob_h_tilda_ts\"] = torch.concat(log_prob_h_tilda_ts, dim=1)\n",
    "                dict_return['h_tilda_dists'] = combine_normals(h_tilda_dists, dim=1)\n",
    "            else:\n",
    "                dict_return[\"log_prob_h_ts\"] = torch.concat(log_prob_h_ts, dim=1)\n",
    "                dict_return['h_t_dists'] = combine_normals(h_t_dists, dim=1)\n",
    "\n",
    "            return dict_return\n",
    "        else:\n",
    "            return h_ts, h_ts[:, [-1], :]\n",
    "        \n",
    "# r_gru = RandomizedGRU(3, 4, 1, True)\n",
    "# r_gru(torch.zeros((1,5,3)), return_dict=True)\n",
    "# train_model(get_nll, lambda model: eval_loss_fn(model, get_nll), RandomizedGRU(len(vocab), 100, 1, True), epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn with torch\n",
    "\n",
    "class SampleMixin:\n",
    "    token_embedding: torch.nn.Module\n",
    "    model: torch.nn.Module\n",
    "    lm_head: torch.nn.Module\n",
    "\n",
    "    def sample(self, input_ids, max_gen_length=10):\n",
    "        '''greedy sample from the model'''\n",
    "        x_t = self.token_embedding(input_ids)\n",
    "        x_t, hidden_t = self.model(x_t)\n",
    "        x_t = self.lm_head(x_t)\n",
    "        input_id_t = x_t[:, [-1], :].argmax(-1)\n",
    "        # print(input_id_t)\n",
    "        x_t = self.token_embedding(input_id_t)\n",
    "        generated_ids = [input_id_t]\n",
    "        for _ in range(max_gen_length):\n",
    "            x_t, hidden_t = self.model(x_t, hidden_t)\n",
    "            x_t = self.lm_head(x_t)\n",
    "            input_id_t = x_t[:, [-1], :].argmax(-1)\n",
    "            x_t = self.token_embedding(input_id_t)\n",
    "            generated_ids.append(input_id_t)\n",
    "        generated_ids = torch.concat(generated_ids, dim=-1)\n",
    "        return generated_ids\n",
    "def get_model_type(model_type, hidden_dim, num_layers):\n",
    "    if model_type == \"lstm\":\n",
    "        model = torch.nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "    elif model_type == \"gru\":\n",
    "        model = torch.nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "    elif model_type == 'rgruhtilda':\n",
    "        model = RandomizedGRU(input_size=hidden_dim, hidden_dim=hidden_dim, num_layers=1, sample_h_tilda=True, sample_identity=False, reparameterize=False)\n",
    "    elif model_type == 'rgruhtildar':\n",
    "        model = RandomizedGRU(input_size=hidden_dim, hidden_dim=hidden_dim, num_layers=1, sample_h_tilda=True, sample_identity=False, reparameterize=True)\n",
    "    elif model_type == 'rgruh':\n",
    "        model = RandomizedGRU(input_size=hidden_dim, hidden_dim=hidden_dim, num_layers=1, sample_h_tilda=False, sample_identity=False, reparameterize=False)\n",
    "    elif model_type == 'rgruhr':\n",
    "        model = RandomizedGRU(input_size=hidden_dim, hidden_dim=hidden_dim, num_layers=1, sample_h_tilda=False, sample_identity=False, reparameterize=True)\n",
    "    elif model_type == 'rgrui':\n",
    "        model = RandomizedGRU(input_size=hidden_dim, hidden_dim=hidden_dim, num_layers=1, sample_h_tilda=True, sample_identity=True, reparameterize=False)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"{model_type=}. Is not impelmented\")\n",
    "    return model\n",
    "class LanguageModelLSTM(torch.nn.Module, SampleMixin):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, detach_hidden_state=False, model_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.model = get_model_type(model_type, hidden_dim, num_layers)\n",
    "        self.lm_head = torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        self.detach_hidden_state = detach_hidden_state\n",
    "        self.detach_hidden_state_linear_layer = torch.nn.Sequential(\n",
    "            # torch.nn.ReLU(),\n",
    "            torch.nn.Linear(in_features=hidden_dim, out_features=hidden_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x, _ = self.model(x)\n",
    "        if self.detach_hidden_state:\n",
    "            x = self.detach_hidden_state_linear_layer(x).detach() # testing the model performance in same case as quiet-star to get worst case baseline\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nll_from_logits_and_labels(logits, labels):\n",
    "    shifted_logits = logits[:, :-1].contiguous()\n",
    "    shifted_labels = labels[:, 1:].contiguous()\n",
    "    return torch.nn.CrossEntropyLoss(reduction='mean')(shifted_logits.view(-1, len(vocab)), shifted_labels.view(-1))\n",
    "def get_nll(model, inputs):\n",
    "    inputs = inputs.to(device)\n",
    "    labels = inputs.clone()\n",
    "    logits = model(inputs)\n",
    "    return get_nll_from_logits_and_labels(logits, labels)\n",
    "def eval_loss_fn(model, get_loss, dataloader=None):\n",
    "    if dataloader is None:\n",
    "        dataloader = torch.utils.data.DataLoader(eval_dataset_shakespeare, batch_size=batch_size, collate_fn=shakespeare_collate_fn)\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for d in dataloader:\n",
    "            loss = get_loss(model, d)\n",
    "            losses.append(loss)\n",
    "    return float(sum(losses)) / len(losses)\n",
    "def get_grad_norm(model):\n",
    "    params = [p.grad.flatten() for p in model.parameters() if p.grad is not None]\n",
    "    return torch.concat(params).norm()\n",
    "def get_model_param_norm(model):\n",
    "    params = [p.flatten() for p in model.parameters()]\n",
    "    return torch.concat(params).norm()\n",
    "def train_model(get_loss_fn, eval_fn, model, batch_size=batch_size, epochs=10, train_dl=None, eval_every=100, print_stuff=True):\n",
    "    train_losses = []\n",
    "    eval_losses = []\n",
    "    optim = torch.optim.AdamW(model.parameters())\n",
    "    i = 0\n",
    "    d = None\n",
    "    if train_dl is None:\n",
    "        train_dl = torch.utils.data.DataLoader(train_dataset_shakespeare, batch_size=batch_size, collate_fn=shakespeare_collate_fn, shuffle=True)\n",
    "    if print_stuff:\n",
    "        print('Number training steps total:', len(train_dl) * epochs)\n",
    "    for epoch_i in range(epochs):\n",
    "        for d in train_dl:\n",
    "            if i % eval_every == 0 and print_stuff:\n",
    "                eval_loss = eval_fn(model)\n",
    "                print('eval loss', eval_loss)\n",
    "                eval_losses.append((i, eval_loss))\n",
    "            optim.zero_grad()\n",
    "            loss = get_loss_fn(model, d)\n",
    "            loss.backward()\n",
    "            if print_stuff:\n",
    "                print(f\"loss {i:>5}: {loss.item():<8.4f} grad norm: {get_grad_norm(model):<15.4f} model param norm: {get_model_param_norm(model):<15.4f}\")\n",
    "            train_losses.append((i, loss.item()))\n",
    "            # if torch.stack([p.isnan().any() for p in model.parameters()]).any():\n",
    "            #     import ipdb; ipdb.set_trace()\n",
    "            optim.step()\n",
    "            # if torch.stack([p.isnan().any() for p in model.parameters()]).any():\n",
    "            #     import ipdb; ipdb.set_trace()\n",
    "            i += 1\n",
    "\n",
    "    if print_stuff:\n",
    "        eval_loss = eval_fn(model)\n",
    "        print('eval loss', eval_loss)\n",
    "        eval_losses.append((i, eval_loss))\n",
    "        plt.plot(*zip(*train_losses), label='train')\n",
    "        plt.plot(*zip(*eval_losses), label='eval')\n",
    "        plt.show()\n",
    "\n",
    "        if isinstance(d, torch.Tensor) and hasattr(model, \"sample\"): # doesn't work for reward model, but nice to have for the language models\n",
    "            d = d.to(device)\n",
    "            pprint(batch_detokenize(d[:, :100]))\n",
    "            pprint(batch_detokenize(model.sample(d[:, :100], 20)))\n",
    "\n",
    "            print(batch_detokenize(model(d).argmax(dim=-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lm = LanguageModelLSTM(vocab_size=len(vocab), hidden_dim=100, num_layers=1, model_type='rgruhr').to(device) \n",
    "# eval loss: 1.505 30 seconds gru\n",
    "# eval loss: 1.519 took 450 seconds tho rgrui same ish performance (my init might be off, but close enough for me.)\n",
    "# eval loss: 2.50 then nan after 50 steps with r gru h reparameterized. On another run, I was able to get 1.55 without troubles, seems to be inconcistent?\n",
    "# eval loss: 1.48 for r gru h_tilda reparameterized. took 680 seconds maybe longer because the back prop graph was longer.\n",
    "train_model(get_nll, lambda model: eval_loss_fn(model, get_nll), rnn_lm, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train based on the lstm model a quiet-star version... simulates having a good reward function for the quiet-star setting. \n",
    "# need to create some distirbution over the hidden representations that I can train analogous to the rational.\n",
    "\n",
    "class QuietStarLSTMModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, reparameterization_trick, model_type):\n",
    "        super().__init__()\n",
    "        if model_type == 'lstm':\n",
    "            self.model = torch.nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        elif model_type == 'gru':\n",
    "            self.model = torch.nn.GRU(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"no implementation for the model type: {model_type}\")\n",
    "        self.distribution_params = torch.nn.Linear(in_features=hidden_dim, out_features=2*hidden_dim)\n",
    "        self.reparameterization_trick = reparameterization_trick\n",
    "    def forward(self, x, h_c=None):\n",
    "        dist, (h_n, c_n) = self.get_hidden_dist(x, h_c)\n",
    "        if self.reparameterization_trick:\n",
    "            sample = dist.rsample()\n",
    "        else:\n",
    "            sample = dist.sample()\n",
    "        return sample, (h_n, c_n) # doing sample instead of rsample means there is no gradient flowing through this computation.\n",
    "    # sampling is technically a differentiable decision unless you sample all of the space and compute probability over those elements ??\n",
    "    def get_hidden_dist(self, x, h_c=None):\n",
    "        x, (h_n, c_n)= self.model(x, h_c)\n",
    "        x = self.distribution_params(x)\n",
    "        mu, log_var = x.chunk(2, dim=-1)\n",
    "        dist = torch.distributions.Normal(loc=mu, scale=(0.5*log_var).exp())\n",
    "        return dist, (h_n, c_n)\n",
    "    \n",
    "class QuietStarLanguageModel(ABC):\n",
    "    token_embedding: torch.nn.Embedding\n",
    "    model: Any\n",
    "    lm_head: torch.nn.Linear\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x, hidden = self.model(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    @abstractmethod\n",
    "    def get_logits_and_hidden_states_and_log_prob_hidden_states_dist(self, x) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.distributions.Normal]:\n",
    "        ...\n",
    "\n",
    "\n",
    "class QuietStarLanguageModelLSTM(torch.nn.Module, SampleMixin, QuietStarLanguageModel):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, reparameterization_trick=False, model_type='lstm'):\n",
    "        super().__init__()\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.model = QuietStarLSTMModel(hidden_dim, num_layers, reparameterization_trick, model_type)\n",
    "        self.lm_head = torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "    def get_logits_and_hidden_states_and_log_prob_hidden_states_dist(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        dist, _ = cast(QuietStarLSTMModel, self.model).get_hidden_dist(x)\n",
    "        sampled_hidden_states = dist.sample()\n",
    "        x = self.lm_head(sampled_hidden_states)\n",
    "        log_p_hidden_states = dist.log_prob(sampled_hidden_states)\n",
    "        return x, sampled_hidden_states, log_p_hidden_states, dist\n",
    "    # support reward model setting, which seeks to get an estimate for the expected log probability of data under the model given the context, and the chosen hidden representation.\n",
    "    # so need a way to get out hidden_states form the model along with the performance of the model given those hidden states. We can just return the logits, and have the calling \n",
    "    # function compute the performance, because performance can be measured differently, so pass the responsibility to caller.\n",
    "\n",
    "class QuietStarLanguageModelrGRU(SampleMixin, torch.nn.Module, QuietStarLanguageModel):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers, model_type='rgruh'):\n",
    "        super().__init__()\n",
    "        self.model_type = model_type\n",
    "        self.token_embedding = torch.nn.Embedding(vocab_size, hidden_dim)\n",
    "        self.model = cast(RandomizedGRU, get_model_type(model_type, hidden_dim, num_layers))\n",
    "        self.lm_head = torch.nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x)\n",
    "        x, hidden = self.model(x)\n",
    "        x = self.lm_head(x)\n",
    "        return x\n",
    "    def get_logits_and_hidden_states_and_log_prob_hidden_states_dist(self, x, x_embed=None):\n",
    "        if x_embed is not None:\n",
    "            x = x_embed\n",
    "        else:\n",
    "            x = self.token_embedding(x)\n",
    "        dict_returned = self.model(x, return_dict=True)\n",
    "        output_states = dict_returned['h_ts']\n",
    "        x = self.lm_head(output_states)\n",
    "        # dict_return: dict = {\"h_ts\": h_ts, \"h_n\": h_ts[:, [-1], :]}\n",
    "        # if self.sample_h_tilda:\n",
    "        #     dict_return[\"h_tilda_ts\"] = torch.concat(h_tilda_ts, dim=1)\n",
    "        #     dict_return[\"log_prob_h_tilda_ts\"] = torch.concat(log_prob_h_tilda_ts, dim=1)\n",
    "        #     dict_return['h_tilda_dists'] = combine_normals(h_tilda_dists, dim=1)\n",
    "        # else:\n",
    "        #     dict_return[\"log_prob_h_ts\"] = torch.concat(log_prob_h_ts, dim=1)\n",
    "        #     dict_return['h_t_dists'] = combine_normals(h_t_dists, dim=1)\n",
    "        if cast(RandomizedGRU, self.model).sample_h_tilda:\n",
    "            dist = dict_returned['h_tilda_dists']\n",
    "            sampled_states = dict_returned[\"h_tilda_ts\"]\n",
    "            sampled_states_log_prob = dict_returned[\"log_prob_h_tilda_ts\"]# .sum(-1)\n",
    "        else:\n",
    "            # assume the model is the hidden state variety of sampling.\n",
    "            dist = dict_returned['h_t_dists']\n",
    "            sampled_states = dict_returned[\"h_ts\"]\n",
    "            sampled_states_log_prob = dict_returned[\"log_prob_h_ts\"]# .sum(-1)\n",
    "        return x, sampled_states, sampled_states_log_prob, dist\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "m = QuietStarLSTMModel(10, 1, False, 'lstm')\n",
    "m.forward(torch.arange(20.).reshape(2, 10))[0]\n",
    "quiet_star_model = QuietStarLanguageModelLSTM(len(vocab), 100, 1).to(device)\n",
    "quiet_star_model.sample(torch.arange(60, device=device).reshape(6, 10) % len(vocab))\n",
    "quiet_star_model = QuietStarLanguageModelrGRU(len(vocab), 100, 1).to(device)\n",
    "opt = torch.optim.AdamW(quiet_star_model.parameters())\n",
    "for _ in range(00):\n",
    "    opt.zero_grad()\n",
    "    input_ids = torch.arange(250, device=device).reshape(1,250) % len(vocab)\n",
    "    with torch.no_grad():\n",
    "        input_embeds = quiet_star_model.token_embedding(input_ids)\n",
    "    input_embeds.requires_grad = True\n",
    "    logits, hidden_states, log_prob_hidden_states, dist = quiet_star_model.get_logits_and_hidden_states_and_log_prob_hidden_states_dist(x=None, x_embed=input_embeds)\n",
    "    # pseudo_loss = -log_prob_hidden_states[0,-1].sum() \n",
    "    pseudo_loss = log_prob_hidden_states.sum()\n",
    "    pseudo_loss.backward()\n",
    "    print()\n",
    "    print(f\"{pseudo_loss=}\")\n",
    "    print({n: p.grad.norm().item() for n, p in quiet_star_model.named_parameters() if p.grad is not None})\n",
    "    print(\"dist std min max:\", dist.scale.min().item(), dist.scale.mean().item(), dist.scale.max().item())\n",
    "    print(\"hidden_states min max:\", hidden_states.min().item(), hidden_states.max().item())\n",
    "    print(\"hidden_state minus mean squared max:\", (hidden_states - dist.loc).square().max().item())\n",
    "    print(\"hidden_state minus mean divided by std max:\", ((hidden_states - dist.loc)/ dist.scale).max().item())\n",
    "    print(\"log_prob min max:\", log_prob_hidden_states.min().item(), log_prob_hidden_states.max().item())\n",
    "    opt.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the optimization function, flexible enough so I can try a variety of forms? \n",
    "# or just keep it simple and then create the augmentations with if statements, and duplicate code?\n",
    "# for now just test the form that I want to test the most, which is the KL with reward fuction form.\n",
    "# could also test quiet-STAR equivelent form which I think should work basic reinforce set up. \n",
    "# Would still need variance reduction technique like TRICE.\n",
    "def get_quiet_star_loss(model: QuietStarLanguageModel, inputs: torch.Tensor, policy_loss_beta:float=1e6, nll_loss_beta:float=1, trice_samples:int=2, n_tokens_ahead=1, only_positive=False):\n",
    "    # this loss consists of nll for the base model? \n",
    "    # well given that we don't parameterize a basemodel,\n",
    "    # we will ignore that part of the loss for now.\n",
    "    # getting nll for thoughts is still important tho, for training the lm head.\n",
    "    # import ipdb; ipdb.set_trace()\n",
    "    n_tokens_ahead = min(inputs.size(1)-1, n_tokens_ahead)\n",
    "    inputs = inputs.to(device)\n",
    "    labels = inputs.clone()\n",
    "    original_batch_size = inputs.size(0)\n",
    "    repeated_inputs = inputs.repeat_interleave(trice_samples, dim=0) # every example shows up twice, this for trice!\n",
    "    repeated_labels = labels.repeat_interleave(trice_samples, dim=0) # every example shows up twice, this for trice!\n",
    "    repeated_logits, repeated_hidden_states, repeated_log_p_hidden_states, dist = model.get_logits_and_hidden_states_and_log_prob_hidden_states_dist(repeated_inputs)\n",
    "    # if repeated_log_p_hidden_states.isnan().any():\n",
    "    #     import ipdb; ipdb.set_trace()\n",
    "    #     repeated_logits, _, repeated_log_p_hidden_states, dist = model.get_logits_and_hidden_states_and_log_prob_hidden_states_dist(repeated_inputs)\n",
    "    repeated_shifted_logits = repeated_logits[:, :-1].contiguous()\n",
    "    repeated_shifted_labels = repeated_labels[:, 1:].contiguous()\n",
    "    repeated_reward = - torch.nn.CrossEntropyLoss(reduction='none')(repeated_shifted_logits.view(-1, len(vocab)), repeated_shifted_labels.view(-1))\n",
    "    repeated_reward = repeated_reward.reshape(*repeated_shifted_labels.shape)\n",
    "    repeated_reward_n_ahead = torch.clone(repeated_reward)\n",
    "    for i in range(n_tokens_ahead-1):\n",
    "        repeated_reward_n_ahead[:, :-(i+1)] += repeated_reward[:, (i+1):]\n",
    "    # no baseline to regress from, but can still average to create a baseline\n",
    "    trice_baseline_reward_n_ahead = repeated_reward_n_ahead.reshape(original_batch_size, trice_samples, -1)\n",
    "    repeated_reward_n_ahead_minus_baseline = (repeated_reward_n_ahead.view(original_batch_size, trice_samples, -1) - trice_baseline_reward_n_ahead.mean(1, keepdim=True)).view(*repeated_reward_n_ahead.shape)\n",
    "    if only_positive:\n",
    "        repeated_reward_n_ahead_minus_baseline = repeated_reward_n_ahead_minus_baseline.clamp(min=0)\n",
    "    quiet_star_policy_loss = - (repeated_reward_n_ahead_minus_baseline.detach() * repeated_log_p_hidden_states[:, :-1].sum(-1)).mean()\n",
    "    nll_loss = (-repeated_reward).mean()\n",
    "    loss = policy_loss_beta * quiet_star_policy_loss + nll_loss_beta * nll_loss # mean allowed because no pad tokens.\n",
    "    quiet_star_policy_loss = quiet_star_policy_loss.item()\n",
    "    print()\n",
    "    print(f\"{quiet_star_policy_loss= }\")\n",
    "    nll_loss = nll_loss.item()\n",
    "    print(f\"{nll_loss= }\")\n",
    "    avg_std = dist.scale.mean().item()\n",
    "    print(f\"{avg_std= }\")\n",
    "    # print({n: p.grad.norm().item() for n, p in model.named_parameters() if p.grad is not None})\n",
    "    print(\"dist std min max:\", dist.scale.min().item(), dist.scale.mean().item(), dist.scale.max().item())\n",
    "    print(\"hidden_states min max:\", repeated_hidden_states.min().item(), repeated_hidden_states.max().item())\n",
    "    print(\"hidden_state minus mean squared max:\", (repeated_hidden_states - dist.loc).square().max().item())\n",
    "    print(\"hidden_state minus mean divided by std max:\", ((repeated_hidden_states - dist.loc)/ dist.scale).max().item())\n",
    "    print(\"log_prob min max:\", repeated_log_p_hidden_states.min().item(), repeated_log_p_hidden_states.max().item())\n",
    "    # if loss.isnan().any() or loss > 10:\n",
    "    #     import ipdb; ipdb.set_trace()\n",
    "    return loss\n",
    "\n",
    "# example_input = cast(torch.Tensor, example_input)\n",
    "# get_quiet_star_loss(quiet_star_model, example_input, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training steps total: 2800\n",
      "eval loss 4.24504280090332\n",
      "\n",
      "quiet_star_policy_loss= -0.09940767288208008\n",
      "nll_loss= 4.244072914123535\n",
      "avg_std= 1.0069350004196167\n",
      "dist std min max: 0.5519675016403198 1.0069350004196167 1.8664230108261108\n",
      "hidden_states min max: -15.720292091369629 7.074191570281982\n",
      "hidden_state minus mean squared max: 252.16433715820312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29762268066406 -0.33819979429244995\n",
      "loss     0: 4.1447   grad norm: 0.5719          model param norm: 81.9615        \n",
      "\n",
      "quiet_star_policy_loss= -0.023604393005371094\n",
      "nll_loss= 4.216104030609131\n",
      "avg_std= 1.0062047243118286\n",
      "dist std min max: 0.5248814225196838 1.0062047243118286 1.8605033159255981\n",
      "hidden_states min max: -18.564062118530273 7.165224075317383\n",
      "hidden_state minus mean squared max: 316.07574462890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.41056823730469 -0.29316776990890503\n",
      "loss     1: 4.1925   grad norm: 0.5700          model param norm: 81.9601        \n",
      "\n",
      "quiet_star_policy_loss= -0.06280632317066193\n",
      "nll_loss= 4.185205936431885\n",
      "avg_std= 1.0056897401809692\n",
      "dist std min max: 0.5293582677841187 1.0056897401809692 1.8645758628845215\n",
      "hidden_states min max: -14.606643676757812 7.2682342529296875\n",
      "hidden_state minus mean squared max: 217.04290771484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.22261810302734 -0.34067320823669434\n",
      "loss     2: 4.1224   grad norm: 0.5568          model param norm: 81.9591        \n",
      "\n",
      "quiet_star_policy_loss= -0.029958724975585938\n",
      "nll_loss= 4.15455961227417\n",
      "avg_std= 1.0049142837524414\n",
      "dist std min max: 0.5447140336036682 1.0049142837524414 1.85991370677948\n",
      "hidden_states min max: -14.949462890625 6.806451797485352\n",
      "hidden_state minus mean squared max: 228.1240234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.24752807617188 -0.3194442391395569\n",
      "loss     3: 4.1246   grad norm: 0.5757          model param norm: 81.9583        \n",
      "\n",
      "quiet_star_policy_loss= -0.08243732899427414\n",
      "nll_loss= 4.123342514038086\n",
      "avg_std= 1.004132628440857\n",
      "dist std min max: 0.5241639614105225 1.004132628440857 1.8268624544143677\n",
      "hidden_states min max: -16.3681640625 7.271358966827393\n",
      "hidden_state minus mean squared max: 271.2681579589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33411407470703 -0.32398372888565063\n",
      "loss     4: 4.0409   grad norm: 0.5845          model param norm: 81.9581        \n",
      "\n",
      "quiet_star_policy_loss= -0.0752653107047081\n",
      "nll_loss= 4.090844631195068\n",
      "avg_std= 1.0033974647521973\n",
      "dist std min max: 0.5330291390419006 1.0033974647521973 1.831594705581665\n",
      "hidden_states min max: -15.654468536376953 7.0285491943359375\n",
      "hidden_state minus mean squared max: 242.95103454589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.27901458740234 -0.2938995361328125\n",
      "loss     5: 4.0156   grad norm: 0.5803          model param norm: 81.9582        \n",
      "\n",
      "quiet_star_policy_loss= -0.07178287953138351\n",
      "nll_loss= 4.054236888885498\n",
      "avg_std= 1.0024981498718262\n",
      "dist std min max: 0.5294433236122131 1.0024981498718262 1.7944591045379639\n",
      "hidden_states min max: -17.744178771972656 7.967414379119873\n",
      "hidden_state minus mean squared max: 323.1961669921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.42171478271484 -0.3129621744155884\n",
      "loss     6: 3.9825   grad norm: 0.6039          model param norm: 81.9588        \n",
      "\n",
      "quiet_star_policy_loss= -0.09759064018726349\n",
      "nll_loss= 4.016115665435791\n",
      "avg_std= 1.0016182661056519\n",
      "dist std min max: 0.5264873504638672 1.0016182661056519 1.9397552013397217\n",
      "hidden_states min max: -17.68488311767578 7.240309715270996\n",
      "hidden_state minus mean squared max: 306.5102844238281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.39519500732422 -0.31324344873428345\n",
      "loss     7: 3.9185   grad norm: 0.6223          model param norm: 81.9595        \n",
      "\n",
      "quiet_star_policy_loss= -0.08798141777515411\n",
      "nll_loss= 3.9770565032958984\n",
      "avg_std= 1.0006147623062134\n",
      "dist std min max: 0.5168766379356384 1.0006147623062134 1.8503119945526123\n",
      "hidden_states min max: -12.144051551818848 7.463688850402832\n",
      "hidden_state minus mean squared max: 141.73463439941406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.00956726074219 -0.3087727427482605\n",
      "loss     8: 3.8891   grad norm: 0.6448          model param norm: 81.9609        \n",
      "\n",
      "quiet_star_policy_loss= -0.10506057739257812\n",
      "nll_loss= 3.9283416271209717\n",
      "avg_std= 0.9993996024131775\n",
      "dist std min max: 0.5185422301292419 0.9993996024131775 1.8990715742111206\n",
      "hidden_states min max: -15.351806640625 7.355923175811768\n",
      "hidden_state minus mean squared max: 222.03591918945312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.23400115966797 -0.3012370467185974\n",
      "loss     9: 3.8233   grad norm: 0.6850          model param norm: 81.9627        \n",
      "\n",
      "quiet_star_policy_loss= 0.01782207563519478\n",
      "nll_loss= 3.883716583251953\n",
      "avg_std= 0.9980849623680115\n",
      "dist std min max: 0.5141180753707886 0.9980849623680115 1.9695390462875366\n",
      "hidden_states min max: -6.871492385864258 6.938351631164551\n",
      "hidden_state minus mean squared max: 49.592185974121094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.419706344604492 -0.30182284116744995\n",
      "loss    10: 3.9015   grad norm: 0.7031          model param norm: 81.9651        \n",
      "\n",
      "quiet_star_policy_loss= -0.07271003723144531\n",
      "nll_loss= 3.8348336219787598\n",
      "avg_std= 0.996844470500946\n",
      "dist std min max: 0.5107854008674622 0.996844470500946 1.9631564617156982\n",
      "hidden_states min max: -14.675833702087402 6.975379943847656\n",
      "hidden_state minus mean squared max: 215.438232421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21891021728516 -0.29269295930862427\n",
      "loss    11: 3.7621   grad norm: 0.7332          model param norm: 81.9680        \n",
      "\n",
      "quiet_star_policy_loss= -0.10699386894702911\n",
      "nll_loss= 3.766237258911133\n",
      "avg_std= 0.9951283931732178\n",
      "dist std min max: 0.5177867412567139 0.9951283931732178 2.008237361907959\n",
      "hidden_states min max: -17.38732147216797 7.5207695960998535\n",
      "hidden_state minus mean squared max: 293.9993896484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.37438201904297 -0.2798921465873718\n",
      "loss    12: 3.6592   grad norm: 0.7529          model param norm: 81.9713        \n",
      "\n",
      "quiet_star_policy_loss= -0.07791633903980255\n",
      "nll_loss= 3.7006003856658936\n",
      "avg_std= 0.993391752243042\n",
      "dist std min max: 0.518905758857727 0.993391752243042 2.006181478500366\n",
      "hidden_states min max: -15.036349296569824 6.702362060546875\n",
      "hidden_state minus mean squared max: 236.1859588623047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26488494873047 -0.27941375970840454\n",
      "loss    13: 3.6227   grad norm: 0.7666          model param norm: 81.9748        \n",
      "\n",
      "quiet_star_policy_loss= -0.04396381601691246\n",
      "nll_loss= 3.6369729042053223\n",
      "avg_std= 0.9913697838783264\n",
      "dist std min max: 0.5129824876785278 0.9913697838783264 2.086803913116455\n",
      "hidden_states min max: -15.847442626953125 7.7428789138793945\n",
      "hidden_state minus mean squared max: 255.78631591796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30476379394531 -0.271878182888031\n",
      "loss    14: 3.5930   grad norm: 0.7623          model param norm: 81.9787        \n",
      "\n",
      "quiet_star_policy_loss= -0.03586273267865181\n",
      "nll_loss= 3.5710999965667725\n",
      "avg_std= 0.9892090559005737\n",
      "dist std min max: 0.4972553253173828 0.9892090559005737 2.177382230758667\n",
      "hidden_states min max: -16.206676483154297 7.50676965713501\n",
      "hidden_state minus mean squared max: 300.49090576171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.38529205322266 -0.26069748401641846\n",
      "loss    15: 3.5352   grad norm: 0.7667          model param norm: 81.9832        \n",
      "\n",
      "quiet_star_policy_loss= -0.05490408092737198\n",
      "nll_loss= 3.5073745250701904\n",
      "avg_std= 0.9868283867835999\n",
      "dist std min max: 0.5073155164718628 0.9868283867835999 2.2105929851531982\n",
      "hidden_states min max: -19.488317489624023 7.378790855407715\n",
      "hidden_state minus mean squared max: 388.55572509765625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.51380157470703 -0.24438196420669556\n",
      "loss    16: 3.4525   grad norm: 0.7097          model param norm: 81.9882        \n",
      "\n",
      "quiet_star_policy_loss= -0.04585113748908043\n",
      "nll_loss= 3.4492835998535156\n",
      "avg_std= 0.984423816204071\n",
      "dist std min max: 0.48844069242477417 0.984423816204071 2.263681650161743\n",
      "hidden_states min max: -8.252463340759277 7.655120372772217\n",
      "hidden_state minus mean squared max: 65.88164520263672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.508703231811523 -0.21950405836105347\n",
      "loss    17: 3.4034   grad norm: 0.6039          model param norm: 81.9936        \n",
      "\n",
      "quiet_star_policy_loss= -0.09885883331298828\n",
      "nll_loss= 3.3800740242004395\n",
      "avg_std= 0.9811878800392151\n",
      "dist std min max: 0.4741869866847992 0.9811878800392151 2.2949938774108887\n",
      "hidden_states min max: -13.16293716430664 8.502553939819336\n",
      "hidden_state minus mean squared max: 185.2208251953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.14334106445312 -0.20115607976913452\n",
      "loss    18: 3.2812   grad norm: 0.6220          model param norm: 81.9992        \n",
      "\n",
      "quiet_star_policy_loss= -0.01206283550709486\n",
      "nll_loss= 3.376230001449585\n",
      "avg_std= 0.9781824350357056\n",
      "dist std min max: 0.4599445164203644 0.9781824350357056 2.2819597721099854\n",
      "hidden_states min max: -15.75302791595459 7.9037675857543945\n",
      "hidden_state minus mean squared max: 233.2117156982422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25855255126953 -0.16084593534469604\n",
      "loss    19: 3.3642   grad norm: 0.7394          model param norm: 82.0050        \n",
      "\n",
      "quiet_star_policy_loss= -0.006408357527107\n",
      "nll_loss= 3.376286268234253\n",
      "avg_std= 0.9750447273254395\n",
      "dist std min max: 0.44744613766670227 0.9750447273254395 2.3454792499542236\n",
      "hidden_states min max: -16.02991485595703 7.600787162780762\n",
      "hidden_state minus mean squared max: 205.3692169189453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19498443603516 -0.13817638158798218\n",
      "loss    20: 3.3699   grad norm: 0.9793          model param norm: 82.0107        \n",
      "\n",
      "quiet_star_policy_loss= -0.020236779004335403\n",
      "nll_loss= 3.3697285652160645\n",
      "avg_std= 0.9720503687858582\n",
      "dist std min max: 0.42154553532600403 0.9720503687858582 2.3111956119537354\n",
      "hidden_states min max: -18.210405349731445 7.94077205657959\n",
      "hidden_state minus mean squared max: 324.62481689453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.42390441894531 -0.09625881910324097\n",
      "loss    21: 3.3495   grad norm: 1.1146          model param norm: 82.0158        \n",
      "\n",
      "quiet_star_policy_loss= -0.05268993601202965\n",
      "nll_loss= 3.339334487915039\n",
      "avg_std= 0.9691895246505737\n",
      "dist std min max: 0.4167768955230713 0.9691895246505737 2.2900829315185547\n",
      "hidden_states min max: -14.109857559204102 8.41343879699707\n",
      "hidden_state minus mean squared max: 184.25379943847656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.14073944091797 -0.08381807804107666\n",
      "loss    22: 3.2866   grad norm: 1.0632          model param norm: 82.0203        \n",
      "\n",
      "quiet_star_policy_loss= -0.015962982550263405\n",
      "nll_loss= 3.320058584213257\n",
      "avg_std= 0.9669597744941711\n",
      "dist std min max: 0.4197252094745636 0.9669597744941711 2.284153938293457\n",
      "hidden_states min max: -10.676763534545898 8.04414176940918\n",
      "hidden_state minus mean squared max: 138.37303161621094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99755859375 -0.0537455677986145\n",
      "loss    23: 3.3041   grad norm: 0.8965          model param norm: 82.0244        \n",
      "\n",
      "quiet_star_policy_loss= -0.010685634799301624\n",
      "nll_loss= 3.280897617340088\n",
      "avg_std= 0.9644245505332947\n",
      "dist std min max: 0.42318904399871826 0.9644245505332947 2.2600510120391846\n",
      "hidden_states min max: -14.648720741271973 7.9652204513549805\n",
      "hidden_state minus mean squared max: 235.12625122070312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26264953613281 -0.09830069541931152\n",
      "loss    24: 3.2702   grad norm: 0.6681          model param norm: 82.0281        \n",
      "\n",
      "quiet_star_policy_loss= -0.05674400553107262\n",
      "nll_loss= 3.264296054840088\n",
      "avg_std= 0.9628699421882629\n",
      "dist std min max: 0.42628103494644165 0.9628699421882629 2.231503963470459\n",
      "hidden_states min max: -18.960725784301758 7.755165100097656\n",
      "hidden_state minus mean squared max: 344.76385498046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4540023803711 -0.06996041536331177\n",
      "loss    25: 3.2076   grad norm: 0.5356          model param norm: 82.0316        \n",
      "\n",
      "quiet_star_policy_loss= -0.04912834241986275\n",
      "nll_loss= 3.2581429481506348\n",
      "avg_std= 0.961258590221405\n",
      "dist std min max: 0.43149426579475403 0.961258590221405 2.2425055503845215\n",
      "hidden_states min max: -17.503856658935547 7.782995700836182\n",
      "hidden_state minus mean squared max: 305.4464416503906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.39346313476562 -0.08092641830444336\n",
      "loss    26: 3.2090   grad norm: 0.5079          model param norm: 82.0352        \n",
      "\n",
      "quiet_star_policy_loss= 0.06768107414245605\n",
      "nll_loss= 3.251919746398926\n",
      "avg_std= 0.9596024751663208\n",
      "dist std min max: 0.4335547089576721 0.9596024751663208 2.1966195106506348\n",
      "hidden_states min max: -16.186908721923828 8.02863883972168\n",
      "hidden_state minus mean squared max: 283.43017578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3560562133789 -0.13679265975952148\n",
      "loss    27: 3.3196   grad norm: 0.8857          model param norm: 82.0389        \n",
      "\n",
      "quiet_star_policy_loss= 0.013993836008012295\n",
      "nll_loss= 3.2600953578948975\n",
      "avg_std= 0.959272563457489\n",
      "dist std min max: 0.4351853132247925 0.959272563457489 2.27032732963562\n",
      "hidden_states min max: -15.641489028930664 8.35122299194336\n",
      "hidden_state minus mean squared max: 236.74807739257812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26608276367188 -0.12909048795700073\n",
      "loss    28: 3.2741   grad norm: 0.6016          model param norm: 82.0422        \n",
      "\n",
      "quiet_star_policy_loss= -0.060350846499204636\n",
      "nll_loss= 3.23191237449646\n",
      "avg_std= 0.9583286046981812\n",
      "dist std min max: 0.43937963247299194 0.9583286046981812 2.204353094100952\n",
      "hidden_states min max: -16.76898765563965 8.116325378417969\n",
      "hidden_state minus mean squared max: 270.74993896484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33316802978516 -0.11219948530197144\n",
      "loss    29: 3.1716   grad norm: 0.6323          model param norm: 82.0456        \n",
      "\n",
      "quiet_star_policy_loss= -0.050028230994939804\n",
      "nll_loss= 3.1994330883026123\n",
      "avg_std= 0.9572705626487732\n",
      "dist std min max: 0.447842538356781 0.9572705626487732 2.189789056777954\n",
      "hidden_states min max: -13.591431617736816 8.790807723999023\n",
      "hidden_state minus mean squared max: 188.34788513183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15172576904297 -0.13826912641525269\n",
      "loss    30: 3.1494   grad norm: 0.6749          model param norm: 82.0493        \n",
      "\n",
      "quiet_star_policy_loss= -0.02735137939453125\n",
      "nll_loss= 3.2011630535125732\n",
      "avg_std= 0.9569214582443237\n",
      "dist std min max: 0.4541305601596832 0.9569214582443237 2.1549007892608643\n",
      "hidden_states min max: -14.227421760559082 8.220535278320312\n",
      "hidden_state minus mean squared max: 178.0597686767578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12364196777344 -0.14919060468673706\n",
      "loss    31: 3.1738   grad norm: 0.6502          model param norm: 82.0532        \n",
      "\n",
      "quiet_star_policy_loss= -0.07735443115234375\n",
      "nll_loss= 3.190418004989624\n",
      "avg_std= 0.9563322067260742\n",
      "dist std min max: 0.4583170413970947 0.9563322067260742 2.165593385696411\n",
      "hidden_states min max: -19.565065383911133 7.496632099151611\n",
      "hidden_state minus mean squared max: 399.3930969238281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.52755737304688 -0.1556723713874817\n",
      "loss    32: 3.1131   grad norm: 0.6419          model param norm: 82.0572        \n",
      "\n",
      "quiet_star_policy_loss= -0.03435409069061279\n",
      "nll_loss= 3.1868016719818115\n",
      "avg_std= 0.9557649493217468\n",
      "dist std min max: 0.4567871689796448 0.9557649493217468 2.1551287174224854\n",
      "hidden_states min max: -16.07390022277832 8.270425796508789\n",
      "hidden_state minus mean squared max: 250.22052001953125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29373168945312 -0.16343539953231812\n",
      "loss    33: 3.1524   grad norm: 0.6317          model param norm: 82.0615        \n",
      "\n",
      "quiet_star_policy_loss= -0.04874477535486221\n",
      "nll_loss= 3.1481778621673584\n",
      "avg_std= 0.9541776776313782\n",
      "dist std min max: 0.46524959802627563 0.9541776776313782 2.17653751373291\n",
      "hidden_states min max: -21.782081604003906 8.823821067810059\n",
      "hidden_state minus mean squared max: 432.0523986816406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.56685638427734 -0.16676956415176392\n",
      "loss    34: 3.0994   grad norm: 0.6314          model param norm: 82.0660        \n",
      "\n",
      "quiet_star_policy_loss= -0.054411888122558594\n",
      "nll_loss= 3.1584246158599854\n",
      "avg_std= 0.9543054699897766\n",
      "dist std min max: 0.45828065276145935 0.9543054699897766 2.22802734375\n",
      "hidden_states min max: -8.492213249206543 7.490303993225098\n",
      "hidden_state minus mean squared max: 61.17875671386719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.680390357971191 -0.1661076545715332\n",
      "loss    35: 3.1040   grad norm: 0.5690          model param norm: 82.0708        \n",
      "\n",
      "quiet_star_policy_loss= -0.08146324008703232\n",
      "nll_loss= 3.128544569015503\n",
      "avg_std= 0.9529183506965637\n",
      "dist std min max: 0.45800039172172546 0.9529183506965637 2.1784005165100098\n",
      "hidden_states min max: -16.050823211669922 7.8398756980896\n",
      "hidden_state minus mean squared max: 230.03623962402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25169372558594 -0.16041725873947144\n",
      "loss    36: 3.0471   grad norm: 0.5405          model param norm: 82.0760        \n",
      "\n",
      "quiet_star_policy_loss= -0.08758087456226349\n",
      "nll_loss= 3.1113247871398926\n",
      "avg_std= 0.9516966938972473\n",
      "dist std min max: 0.4583227038383484 0.9516966938972473 2.2078824043273926\n",
      "hidden_states min max: -17.42634391784668 7.963741302490234\n",
      "hidden_state minus mean squared max: 285.2160339355469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.35920715332031 -0.13990211486816406\n",
      "loss    37: 3.0237   grad norm: 0.5054          model param norm: 82.0815        \n",
      "\n",
      "quiet_star_policy_loss= -0.05791530758142471\n",
      "nll_loss= 3.1158199310302734\n",
      "avg_std= 0.9508678317070007\n",
      "dist std min max: 0.4506911039352417 0.9508678317070007 2.209592819213867\n",
      "hidden_states min max: -17.26718521118164 8.123624801635742\n",
      "hidden_state minus mean squared max: 277.4161376953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3453369140625 -0.16045093536376953\n",
      "loss    38: 3.0579   grad norm: 0.4760          model param norm: 82.0871        \n",
      "\n",
      "quiet_star_policy_loss= -0.08070927113294601\n",
      "nll_loss= 3.1099343299865723\n",
      "avg_std= 0.9495040774345398\n",
      "dist std min max: 0.456584095954895 0.9495040774345398 2.2313222885131836\n",
      "hidden_states min max: -17.56145477294922 7.45622444152832\n",
      "hidden_state minus mean squared max: 271.6394348144531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3348159790039 -0.1531224250793457\n",
      "loss    39: 3.0292   grad norm: 0.4726          model param norm: 82.0927        \n",
      "\n",
      "quiet_star_policy_loss= -0.03552303463220596\n",
      "nll_loss= 3.09881854057312\n",
      "avg_std= 0.9472942352294922\n",
      "dist std min max: 0.45033618807792664 0.9472942352294922 2.2556538581848145\n",
      "hidden_states min max: -18.47439193725586 7.543379306793213\n",
      "hidden_state minus mean squared max: 322.4066162109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.42047882080078 -0.14878273010253906\n",
      "loss    40: 3.0633   grad norm: 0.5005          model param norm: 82.0985        \n",
      "\n",
      "quiet_star_policy_loss= -0.06777792423963547\n",
      "nll_loss= 3.0899364948272705\n",
      "avg_std= 0.945482611656189\n",
      "dist std min max: 0.4551107883453369 0.945482611656189 2.247359037399292\n",
      "hidden_states min max: -14.769865989685059 7.609711647033691\n",
      "hidden_state minus mean squared max: 226.59115600585938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.24415588378906 -0.14603114128112793\n",
      "loss    41: 3.0222   grad norm: 0.4837          model param norm: 82.1043        \n",
      "\n",
      "quiet_star_policy_loss= -0.07042675465345383\n",
      "nll_loss= 3.061142921447754\n",
      "avg_std= 0.9426436424255371\n",
      "dist std min max: 0.4453370273113251 0.9426436424255371 2.296860694885254\n",
      "hidden_states min max: -21.070661544799805 7.42121696472168\n",
      "hidden_state minus mean squared max: 472.6805725097656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.61177062988281 -0.1438075304031372\n",
      "loss    42: 2.9907   grad norm: 0.5367          model param norm: 82.1102        \n",
      "\n",
      "quiet_star_policy_loss= -0.08033370971679688\n",
      "nll_loss= 3.040147542953491\n",
      "avg_std= 0.9405897855758667\n",
      "dist std min max: 0.4505194127559662 0.9405897855758667 2.267937660217285\n",
      "hidden_states min max: -16.381755828857422 8.082598686218262\n",
      "hidden_state minus mean squared max: 253.7389373779297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30073547363281 -0.13152194023132324\n",
      "loss    43: 2.9598   grad norm: 0.5362          model param norm: 82.1161        \n",
      "\n",
      "quiet_star_policy_loss= -0.0589568130671978\n",
      "nll_loss= 3.0488617420196533\n",
      "avg_std= 0.9390777945518494\n",
      "dist std min max: 0.44926127791404724 0.9390777945518494 2.286280870437622\n",
      "hidden_states min max: -14.005258560180664 7.970432281494141\n",
      "hidden_state minus mean squared max: 233.6859588623047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25957489013672 -0.1442583203315735\n",
      "loss    44: 2.9899   grad norm: 0.5449          model param norm: 82.1220        \n",
      "\n",
      "quiet_star_policy_loss= -0.04598503187298775\n",
      "nll_loss= 3.0397164821624756\n",
      "avg_std= 0.9367238283157349\n",
      "dist std min max: 0.4455992579460144 0.9367238283157349 2.2940115928649902\n",
      "hidden_states min max: -12.502403259277344 8.64350700378418\n",
      "hidden_state minus mean squared max: 152.14227294921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.04498291015625 -0.11766278743743896\n",
      "loss    45: 2.9937   grad norm: 0.5674          model param norm: 82.1279        \n",
      "\n",
      "quiet_star_policy_loss= -0.025497054681181908\n",
      "nll_loss= 3.039163827896118\n",
      "avg_std= 0.9353039860725403\n",
      "dist std min max: 0.44048914313316345 0.9353039860725403 2.31429123878479\n",
      "hidden_states min max: -11.494953155517578 7.288530349731445\n",
      "hidden_state minus mean squared max: 120.18599700927734\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.92710876464844 -0.10920792818069458\n",
      "loss    46: 3.0137   grad norm: 0.5384          model param norm: 82.1339        \n",
      "\n",
      "quiet_star_policy_loss= -0.08534278720617294\n",
      "nll_loss= 2.9974353313446045\n",
      "avg_std= 0.9321150779724121\n",
      "dist std min max: 0.44751083850860596 0.9321150779724121 2.321408271789551\n",
      "hidden_states min max: -14.616765022277832 7.948736190795898\n",
      "hidden_state minus mean squared max: 215.734619140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21959686279297 -0.13374346494674683\n",
      "loss    47: 2.9121   grad norm: 0.5363          model param norm: 82.1401        \n",
      "\n",
      "quiet_star_policy_loss= -0.04760589823126793\n",
      "nll_loss= 3.0008232593536377\n",
      "avg_std= 0.9306163787841797\n",
      "dist std min max: 0.44553762674331665 0.9306163787841797 2.2989344596862793\n",
      "hidden_states min max: -14.77463150024414 7.339677810668945\n",
      "hidden_state minus mean squared max: 218.5010986328125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2259750366211 -0.1252741813659668\n",
      "loss    48: 2.9532   grad norm: 0.5045          model param norm: 82.1464        \n",
      "\n",
      "quiet_star_policy_loss= -0.06230621412396431\n",
      "nll_loss= 2.9963808059692383\n",
      "avg_std= 0.928494930267334\n",
      "dist std min max: 0.4352756142616272 0.928494930267334 2.3193695545196533\n",
      "hidden_states min max: -17.114933013916016 7.396280765533447\n",
      "hidden_state minus mean squared max: 297.75042724609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.38070678710938 -0.10392534732818604\n",
      "loss    49: 2.9341   grad norm: 0.4956          model param norm: 82.1530        \n",
      "\n",
      "quiet_star_policy_loss= -0.060772038996219635\n",
      "nll_loss= 2.9752819538116455\n",
      "avg_std= 0.9262505173683167\n",
      "dist std min max: 0.4337386190891266 0.9262505173683167 2.329967737197876\n",
      "hidden_states min max: -15.525175094604492 8.153264999389648\n",
      "hidden_state minus mean squared max: 251.39862060546875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.29609680175781 -0.1212727427482605\n",
      "loss    50: 2.9145   grad norm: 0.4992          model param norm: 82.1597        \n",
      "\n",
      "quiet_star_policy_loss= -0.11720085144042969\n",
      "nll_loss= 2.962578535079956\n",
      "avg_std= 0.9245136976242065\n",
      "dist std min max: 0.4278787076473236 0.9245136976242065 2.347538709640503\n",
      "hidden_states min max: -20.317350387573242 7.943705081939697\n",
      "hidden_state minus mean squared max: 385.8888854980469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5103530883789 -0.11276048421859741\n",
      "loss    51: 2.8454   grad norm: 0.4883          model param norm: 82.1668        \n",
      "\n",
      "quiet_star_policy_loss= -0.06137847900390625\n",
      "nll_loss= 2.957183599472046\n",
      "avg_std= 0.9215635061264038\n",
      "dist std min max: 0.43720853328704834 0.9215635061264038 2.3343093395233154\n",
      "hidden_states min max: -13.107218742370605 7.990459442138672\n",
      "hidden_state minus mean squared max: 198.76100158691406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17864227294922 -0.11461490392684937\n",
      "loss    52: 2.8958   grad norm: 0.5248          model param norm: 82.1739        \n",
      "\n",
      "quiet_star_policy_loss= 0.020451832562685013\n",
      "nll_loss= 2.953869581222534\n",
      "avg_std= 0.9208845496177673\n",
      "dist std min max: 0.4251340925693512 0.9208845496177673 2.346360921859741\n",
      "hidden_states min max: -15.106958389282227 7.5616865158081055\n",
      "hidden_state minus mean squared max: 231.71075439453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.25532531738281 -0.1033555269241333\n",
      "loss    53: 2.9743   grad norm: 0.5069          model param norm: 82.1812        \n",
      "\n",
      "quiet_star_policy_loss= -0.035558559000492096\n",
      "nll_loss= 2.926652669906616\n",
      "avg_std= 0.9180424809455872\n",
      "dist std min max: 0.4236321747303009 0.9180424809455872 2.3831117153167725\n",
      "hidden_states min max: -18.802631378173828 7.459482669830322\n",
      "hidden_state minus mean squared max: 325.937744140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.42594909667969 -0.10003650188446045\n",
      "loss    54: 2.8911   grad norm: 0.5490          model param norm: 82.1885        \n",
      "\n",
      "quiet_star_policy_loss= -0.07747843861579895\n",
      "nll_loss= 2.910980224609375\n",
      "avg_std= 0.915195643901825\n",
      "dist std min max: 0.4267490804195404 0.915195643901825 2.33868408203125\n",
      "hidden_states min max: -14.025459289550781 7.330782890319824\n",
      "hidden_state minus mean squared max: 169.91073608398438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10021209716797 -0.0804300308227539\n",
      "loss    55: 2.8335   grad norm: 0.9915          model param norm: 82.1961        \n",
      "\n",
      "quiet_star_policy_loss= -0.09147663414478302\n",
      "nll_loss= 2.9107120037078857\n",
      "avg_std= 0.9149245023727417\n",
      "dist std min max: 0.4272047281265259 0.9149245023727417 2.3458871841430664\n",
      "hidden_states min max: -14.543793678283691 7.843595504760742\n",
      "hidden_state minus mean squared max: 252.77691650390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.29883575439453 -0.0840831995010376\n",
      "loss    56: 2.8192   grad norm: 0.5202          model param norm: 82.2034        \n",
      "\n",
      "quiet_star_policy_loss= -0.05037365108728409\n",
      "nll_loss= 2.8977127075195312\n",
      "avg_std= 0.9128360748291016\n",
      "dist std min max: 0.4210371971130371 0.9128360748291016 2.346973419189453\n",
      "hidden_states min max: -14.215656280517578 7.4671149253845215\n",
      "hidden_state minus mean squared max: 188.1214599609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15113830566406 -0.06891316175460815\n",
      "loss    57: 2.8473   grad norm: 0.5242          model param norm: 82.2106        \n",
      "\n",
      "quiet_star_policy_loss= -0.06153922155499458\n",
      "nll_loss= 2.906928062438965\n",
      "avg_std= 0.9124181270599365\n",
      "dist std min max: 0.42209261655807495 0.9124181270599365 2.293334722518921\n",
      "hidden_states min max: -20.51207160949707 8.074313163757324\n",
      "hidden_state minus mean squared max: 386.86981201171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5116195678711 -0.0740593671798706\n",
      "loss    58: 2.8454   grad norm: 0.5065          model param norm: 82.2181        \n",
      "\n",
      "quiet_star_policy_loss= -0.04267406463623047\n",
      "nll_loss= 2.8777482509613037\n",
      "avg_std= 0.909501314163208\n",
      "dist std min max: 0.41933903098106384 0.909501314163208 2.3374013900756836\n",
      "hidden_states min max: -16.68125343322754 7.305140495300293\n",
      "hidden_state minus mean squared max: 219.25181579589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2276840209961 -0.07511246204376221\n",
      "loss    59: 2.8351   grad norm: 0.5296          model param norm: 82.2255        \n",
      "\n",
      "quiet_star_policy_loss= -0.06529694050550461\n",
      "nll_loss= 2.8758785724639893\n",
      "avg_std= 0.9082679152488708\n",
      "dist std min max: 0.41415324807167053 0.9082679152488708 2.2748286724090576\n",
      "hidden_states min max: -15.789932250976562 7.919687747955322\n",
      "hidden_state minus mean squared max: 256.6889343261719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30651092529297 -0.0692562460899353\n",
      "loss    60: 2.8106   grad norm: 0.5241          model param norm: 82.2331        \n",
      "\n",
      "quiet_star_policy_loss= -0.09271659702062607\n",
      "nll_loss= 2.8753411769866943\n",
      "avg_std= 0.9068174362182617\n",
      "dist std min max: 0.4177778661251068 0.9068174362182617 2.291562795639038\n",
      "hidden_states min max: -18.169771194458008 7.326720714569092\n",
      "hidden_state minus mean squared max: 320.45751953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.41744232177734 -0.05359143018722534\n",
      "loss    61: 2.7826   grad norm: 0.5135          model param norm: 82.2406        \n",
      "\n",
      "quiet_star_policy_loss= -0.07713337242603302\n",
      "nll_loss= 2.874377489089966\n",
      "avg_std= 0.9057015776634216\n",
      "dist std min max: 0.4104330241680145 0.9057015776634216 2.279904365539551\n",
      "hidden_states min max: -12.139802932739258 7.516939640045166\n",
      "hidden_state minus mean squared max: 139.99044799804688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.00336456298828 -0.05630481243133545\n",
      "loss    62: 2.7972   grad norm: 0.5409          model param norm: 82.2482        \n",
      "\n",
      "quiet_star_policy_loss= -0.060210611671209335\n",
      "nll_loss= 2.854391098022461\n",
      "avg_std= 0.9037249088287354\n",
      "dist std min max: 0.40544775128364563 0.9037249088287354 2.2396275997161865\n",
      "hidden_states min max: -17.57086944580078 8.864495277404785\n",
      "hidden_state minus mean squared max: 301.5883483886719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.38710021972656 -0.026582062244415283\n",
      "loss    63: 2.7942   grad norm: 0.5404          model param norm: 82.2558        \n",
      "\n",
      "quiet_star_policy_loss= -0.06623291969299316\n",
      "nll_loss= 2.8457653522491455\n",
      "avg_std= 0.9021557569503784\n",
      "dist std min max: 0.40207165479660034 0.9021557569503784 2.2460620403289795\n",
      "hidden_states min max: -13.558951377868652 7.459506988525391\n",
      "hidden_state minus mean squared max: 215.15406799316406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21824645996094 -0.028875231742858887\n",
      "loss    64: 2.7795   grad norm: 0.5710          model param norm: 82.2634        \n",
      "\n",
      "quiet_star_policy_loss= -0.01955108717083931\n",
      "nll_loss= 2.8528308868408203\n",
      "avg_std= 0.9002004861831665\n",
      "dist std min max: 0.39800572395324707 0.9002004861831665 2.2349772453308105\n",
      "hidden_states min max: -18.110057830810547 8.056791305541992\n",
      "hidden_state minus mean squared max: 345.3381042480469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.454833984375 -0.0195387601852417\n",
      "loss    65: 2.8333   grad norm: 0.5363          model param norm: 82.2710        \n",
      "\n",
      "quiet_star_policy_loss= -0.053969383239746094\n",
      "nll_loss= 2.822551727294922\n",
      "avg_std= 0.8980396389961243\n",
      "dist std min max: 0.39568430185317993 0.8980396389961243 2.20289945602417\n",
      "hidden_states min max: -14.189663887023926 7.491935729980469\n",
      "hidden_state minus mean squared max: 198.63194274902344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17831420898438 -0.005015194416046143\n",
      "loss    66: 2.7686   grad norm: 0.5404          model param norm: 82.2787        \n",
      "\n",
      "quiet_star_policy_loss= -0.07215099781751633\n",
      "nll_loss= 2.8102967739105225\n",
      "avg_std= 0.8962792754173279\n",
      "dist std min max: 0.388790488243103 0.8962792754173279 2.225039482116699\n",
      "hidden_states min max: -18.615291595458984 7.804633140563965\n",
      "hidden_state minus mean squared max: 351.86407470703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4642105102539 0.0010127425193786621\n",
      "loss    67: 2.7381   grad norm: 0.5754          model param norm: 82.2864        \n",
      "\n",
      "quiet_star_policy_loss= -0.036365509033203125\n",
      "nll_loss= 2.813678503036499\n",
      "avg_std= 0.8943774104118347\n",
      "dist std min max: 0.38325148820877075 0.8943774104118347 2.1868205070495605\n",
      "hidden_states min max: -10.058396339416504 7.886326313018799\n",
      "hidden_state minus mean squared max: 129.50955200195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.96446228027344 0.012254595756530762\n",
      "loss    68: 2.7773   grad norm: 0.5444          model param norm: 82.2943        \n",
      "\n",
      "quiet_star_policy_loss= -0.02624216116964817\n",
      "nll_loss= 2.8129165172576904\n",
      "avg_std= 0.8937690258026123\n",
      "dist std min max: 0.38640421628952026 0.8937690258026123 2.183954954147339\n",
      "hidden_states min max: -12.71121597290039 8.16533374786377\n",
      "hidden_state minus mean squared max: 174.5743408203125\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -104.11376953125 0.014778614044189453\n",
      "loss    69: 2.7867   grad norm: 0.5671          model param norm: 82.3022        \n",
      "\n",
      "quiet_star_policy_loss= -0.08404870331287384\n",
      "nll_loss= 2.7971913814544678\n",
      "avg_std= 0.8915485739707947\n",
      "dist std min max: 0.38656535744667053 0.8915485739707947 2.1988003253936768\n",
      "hidden_states min max: -16.68711280822754 8.422317504882812\n",
      "hidden_state minus mean squared max: 273.2922058105469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.33784484863281 0.012537956237792969\n",
      "loss    70: 2.7131   grad norm: 0.5478          model param norm: 82.3101        \n",
      "\n",
      "quiet_star_policy_loss= -0.07246704399585724\n",
      "nll_loss= 2.7694039344787598\n",
      "avg_std= 0.8899600505828857\n",
      "dist std min max: 0.38379260897636414 0.8899600505828857 2.1804616451263428\n",
      "hidden_states min max: -17.251211166381836 7.98603630065918\n",
      "hidden_state minus mean squared max: 355.14617919921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4688491821289 0.037882983684539795\n",
      "loss    71: 2.6969   grad norm: 0.5468          model param norm: 82.3179        \n",
      "\n",
      "quiet_star_policy_loss= -0.08179779350757599\n",
      "nll_loss= 2.769683837890625\n",
      "avg_std= 0.8874836564064026\n",
      "dist std min max: 0.38246265053749084 0.8874836564064026 2.172382354736328\n",
      "hidden_states min max: -15.983147621154785 8.044099807739258\n",
      "hidden_state minus mean squared max: 240.93247985839844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.27484130859375 0.025652587413787842\n",
      "loss    72: 2.6879   grad norm: 0.5433          model param norm: 82.3259        \n",
      "\n",
      "quiet_star_policy_loss= -0.0904720351099968\n",
      "nll_loss= 2.772128105163574\n",
      "avg_std= 0.8862142562866211\n",
      "dist std min max: 0.38089048862457275 0.8862142562866211 2.146826982498169\n",
      "hidden_states min max: -15.352499008178711 7.855859756469727\n",
      "hidden_state minus mean squared max: 237.34886169433594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.267333984375 0.026750147342681885\n",
      "loss    73: 2.6817   grad norm: 0.5326          model param norm: 82.3338        \n",
      "\n",
      "quiet_star_policy_loss= -0.007128477096557617\n",
      "nll_loss= 2.7383675575256348\n",
      "avg_std= 0.8833749890327454\n",
      "dist std min max: 0.38035592436790466 0.8833749890327454 2.1995530128479004\n",
      "hidden_states min max: -13.77066707611084 7.791698932647705\n",
      "hidden_state minus mean squared max: 206.6041717529297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.19799041748047 0.044242024421691895\n",
      "loss    74: 2.7312   grad norm: 0.5465          model param norm: 82.3416        \n",
      "\n",
      "quiet_star_policy_loss= -0.06090397760272026\n",
      "nll_loss= 2.7633442878723145\n",
      "avg_std= 0.8832818865776062\n",
      "dist std min max: 0.3799189329147339 0.8832818865776062 2.1603362560272217\n",
      "hidden_states min max: -16.90838623046875 8.184951782226562\n",
      "hidden_state minus mean squared max: 286.52276611328125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.36148834228516 0.03662979602813721\n",
      "loss    75: 2.7024   grad norm: 0.5621          model param norm: 82.3494        \n",
      "\n",
      "quiet_star_policy_loss= -0.03947305679321289\n",
      "nll_loss= 2.7427783012390137\n",
      "avg_std= 0.8807593584060669\n",
      "dist std min max: 0.3783766031265259 0.8807593584060669 2.1771910190582275\n",
      "hidden_states min max: -14.539612770080566 8.026205062866211\n",
      "hidden_state minus mean squared max: 224.71485900878906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.24000549316406 0.047174811363220215\n",
      "loss    76: 2.7033   grad norm: 0.5732          model param norm: 82.3571        \n",
      "\n",
      "quiet_star_policy_loss= -0.05563793331384659\n",
      "nll_loss= 2.7330241203308105\n",
      "avg_std= 0.8794006705284119\n",
      "dist std min max: 0.37760481238365173 0.8794006705284119 2.234684705734253\n",
      "hidden_states min max: -18.681814193725586 7.855250835418701\n",
      "hidden_state minus mean squared max: 333.2787170410156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43706512451172 0.04607754945755005\n",
      "loss    77: 2.6774   grad norm: 0.5480          model param norm: 82.3648        \n",
      "\n",
      "quiet_star_policy_loss= -0.0009717941284179688\n",
      "nll_loss= 2.731372117996216\n",
      "avg_std= 0.8790781497955322\n",
      "dist std min max: 0.3707300126552582 0.8790781497955322 2.200730323791504\n",
      "hidden_states min max: -16.231155395507812 8.505891799926758\n",
      "hidden_state minus mean squared max: 244.20050048828125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.28157043457031 0.03816783428192139\n",
      "loss    78: 2.7304   grad norm: 0.5508          model param norm: 82.3727        \n",
      "\n",
      "quiet_star_policy_loss= -0.03401999548077583\n",
      "nll_loss= 2.731206178665161\n",
      "avg_std= 0.8771798610687256\n",
      "dist std min max: 0.3768172860145569 0.8771798610687256 2.235136032104492\n",
      "hidden_states min max: -13.614728927612305 8.232295989990234\n",
      "hidden_state minus mean squared max: 161.478515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07476806640625 0.0408930778503418\n",
      "loss    79: 2.6972   grad norm: 0.5773          model param norm: 82.3807        \n",
      "\n",
      "quiet_star_policy_loss= -0.052938081324100494\n",
      "nll_loss= 2.6968905925750732\n",
      "avg_std= 0.8742467164993286\n",
      "dist std min max: 0.36974555253982544 0.8742467164993286 2.251288414001465\n",
      "hidden_states min max: -15.087417602539062 7.876106262207031\n",
      "hidden_state minus mean squared max: 268.5022888183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.32899475097656 0.04408913850784302\n",
      "loss    80: 2.6440   grad norm: 0.5473          model param norm: 82.3887        \n",
      "\n",
      "quiet_star_policy_loss= -0.10000305622816086\n",
      "nll_loss= 2.71081280708313\n",
      "avg_std= 0.8730542659759521\n",
      "dist std min max: 0.36981117725372314 0.8730542659759521 2.294816017150879\n",
      "hidden_states min max: -16.69738006591797 8.013277053833008\n",
      "hidden_state minus mean squared max: 236.0037841796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26451873779297 0.05674910545349121\n",
      "loss    81: 2.6108   grad norm: 0.5785          model param norm: 82.3969        \n",
      "\n",
      "quiet_star_policy_loss= -0.08920755237340927\n",
      "nll_loss= 2.705784797668457\n",
      "avg_std= 0.8729334473609924\n",
      "dist std min max: 0.3705511689186096 0.8729334473609924 2.4227688312530518\n",
      "hidden_states min max: -16.994247436523438 8.19975757598877\n",
      "hidden_state minus mean squared max: 315.7146911621094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.40999603271484 0.05754435062408447\n",
      "loss    82: 2.6166   grad norm: 0.5600          model param norm: 82.4051        \n",
      "\n",
      "quiet_star_policy_loss= -0.019515585154294968\n",
      "nll_loss= 2.7112984657287598\n",
      "avg_std= 0.871860146522522\n",
      "dist std min max: 0.3715711534023285 0.871860146522522 2.3233401775360107\n",
      "hidden_states min max: -8.116543769836426 7.478472709655762\n",
      "hidden_state minus mean squared max: 51.58784866333008\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.959280967712402 0.05529987812042236\n",
      "loss    83: 2.6918   grad norm: 1.1351          model param norm: 82.4134        \n",
      "\n",
      "quiet_star_policy_loss= -0.025695418938994408\n",
      "nll_loss= 2.6815266609191895\n",
      "avg_std= 0.8669636845588684\n",
      "dist std min max: 0.362225741147995 0.8669636845588684 2.358041524887085\n",
      "hidden_states min max: -15.86065673828125 8.109206199645996\n",
      "hidden_state minus mean squared max: 258.9462890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.31087493896484 0.06976795196533203\n",
      "loss    84: 2.6558   grad norm: 0.5319          model param norm: 82.4217        \n",
      "\n",
      "quiet_star_policy_loss= -0.07279739528894424\n",
      "nll_loss= 2.6827285289764404\n",
      "avg_std= 0.867022693157196\n",
      "dist std min max: 0.3616725206375122 0.867022693157196 2.33884596824646\n",
      "hidden_states min max: -13.048176765441895 8.498619079589844\n",
      "hidden_state minus mean squared max: 177.3938751220703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12177276611328 0.07940226793289185\n",
      "loss    85: 2.6099   grad norm: 0.5576          model param norm: 82.4299        \n",
      "\n",
      "quiet_star_policy_loss= -0.09927301853895187\n",
      "nll_loss= 2.6901695728302\n",
      "avg_std= 0.8645043969154358\n",
      "dist std min max: 0.3553813099861145 0.8645043969154358 2.3427257537841797\n",
      "hidden_states min max: -14.303502082824707 8.87880802154541\n",
      "hidden_state minus mean squared max: 214.05780029296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.2156982421875 0.10218894481658936\n",
      "loss    86: 2.5909   grad norm: 0.5275          model param norm: 82.4382        \n",
      "\n",
      "quiet_star_policy_loss= -0.05201072618365288\n",
      "nll_loss= 2.66443133354187\n",
      "avg_std= 0.8627736568450928\n",
      "dist std min max: 0.3483967185020447 0.8627736568450928 2.3507511615753174\n",
      "hidden_states min max: -13.408888816833496 8.673380851745605\n",
      "hidden_state minus mean squared max: 173.92173767089844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.11186981201172 0.12191331386566162\n",
      "loss    87: 2.6124   grad norm: 0.5477          model param norm: 82.4464        \n",
      "\n",
      "quiet_star_policy_loss= -0.05116767808794975\n",
      "nll_loss= 2.6585235595703125\n",
      "avg_std= 0.8592016696929932\n",
      "dist std min max: 0.3468567430973053 0.8592016696929932 2.3271350860595703\n",
      "hidden_states min max: -16.960954666137695 8.361745834350586\n",
      "hidden_state minus mean squared max: 262.0300598144531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.31682586669922 0.11555850505828857\n",
      "loss    88: 2.6074   grad norm: 0.5359          model param norm: 82.4548        \n",
      "\n",
      "quiet_star_policy_loss= -0.06036567687988281\n",
      "nll_loss= 2.6556613445281982\n",
      "avg_std= 0.8582296371459961\n",
      "dist std min max: 0.34459859132766724 0.8582296371459961 2.3300909996032715\n",
      "hidden_states min max: -15.814529418945312 10.059148788452148\n",
      "hidden_state minus mean squared max: 270.17083740234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33210754394531 0.13186395168304443\n",
      "loss    89: 2.5953   grad norm: 0.5810          model param norm: 82.4629        \n",
      "\n",
      "quiet_star_policy_loss= -0.01231460552662611\n",
      "nll_loss= 2.6599206924438477\n",
      "avg_std= 0.8565844297409058\n",
      "dist std min max: 0.33692169189453125 0.8565844297409058 2.298621654510498\n",
      "hidden_states min max: -13.745672225952148 9.044231414794922\n",
      "hidden_state minus mean squared max: 199.22845458984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1798095703125 0.16040217876434326\n",
      "loss    90: 2.6476   grad norm: 0.5414          model param norm: 82.4713        \n",
      "\n",
      "quiet_star_policy_loss= -0.07988815754652023\n",
      "nll_loss= 2.631836175918579\n",
      "avg_std= 0.8531689643859863\n",
      "dist std min max: 0.3304647207260132 0.8531689643859863 2.303178548812866\n",
      "hidden_states min max: -15.245065689086914 10.044299125671387\n",
      "hidden_state minus mean squared max: 207.70068359375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20062255859375 0.1704857349395752\n",
      "loss    91: 2.5519   grad norm: 0.5659          model param norm: 82.4794        \n",
      "\n",
      "quiet_star_policy_loss= -0.03115530125796795\n",
      "nll_loss= 2.6356418132781982\n",
      "avg_std= 0.8511019349098206\n",
      "dist std min max: 0.3293634057044983 0.8511019349098206 2.2958710193634033\n",
      "hidden_states min max: -15.475337982177734 8.695743560791016\n",
      "hidden_state minus mean squared max: 221.33457946777344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.232421875 0.1854085922241211\n",
      "loss    92: 2.6045   grad norm: 0.5663          model param norm: 82.4875        \n",
      "\n",
      "quiet_star_policy_loss= -0.08125152438879013\n",
      "nll_loss= 2.6284146308898926\n",
      "avg_std= 0.8505144119262695\n",
      "dist std min max: 0.32573840022087097 0.8505144119262695 2.3073830604553223\n",
      "hidden_states min max: -11.553136825561523 9.059836387634277\n",
      "hidden_state minus mean squared max: 175.8827362060547\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -104.11749267578125 0.1808229684829712\n",
      "loss    93: 2.5472   grad norm: 0.5808          model param norm: 82.4958        \n",
      "\n",
      "quiet_star_policy_loss= -0.023786162957549095\n",
      "nll_loss= 2.6313552856445312\n",
      "avg_std= 0.8479962348937988\n",
      "dist std min max: 0.31573057174682617 0.8479962348937988 2.3054559230804443\n",
      "hidden_states min max: -21.441177368164062 9.617111206054688\n",
      "hidden_state minus mean squared max: 408.15093994140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.53839111328125 0.21366047859191895\n",
      "loss    94: 2.6076   grad norm: 0.5624          model param norm: 82.5042        \n",
      "\n",
      "quiet_star_policy_loss= -0.04063263162970543\n",
      "nll_loss= 2.6120967864990234\n",
      "avg_std= 0.8467099666595459\n",
      "dist std min max: 0.3141697347164154 0.8467099666595459 2.304333209991455\n",
      "hidden_states min max: -19.06999397277832 9.521088600158691\n",
      "hidden_state minus mean squared max: 412.54925537109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.54375457763672 0.2247370481491089\n",
      "loss    95: 2.5715   grad norm: 0.5637          model param norm: 82.5125        \n",
      "\n",
      "quiet_star_policy_loss= -0.02412891387939453\n",
      "nll_loss= 2.6061513423919678\n",
      "avg_std= 0.8452386856079102\n",
      "dist std min max: 0.3099333643913269 0.8452386856079102 2.342144250869751\n",
      "hidden_states min max: -22.725366592407227 9.632102012634277\n",
      "hidden_state minus mean squared max: 462.7543029785156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.60118103027344 0.235343337059021\n",
      "loss    96: 2.5820   grad norm: 0.5664          model param norm: 82.5208        \n",
      "\n",
      "quiet_star_policy_loss= -0.036133863031864166\n",
      "nll_loss= 2.5998189449310303\n",
      "avg_std= 0.8450378775596619\n",
      "dist std min max: 0.3073391020298004 0.8450378775596619 2.3850836753845215\n",
      "hidden_states min max: -12.021463394165039 9.497108459472656\n",
      "hidden_state minus mean squared max: 201.89109802246094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.18643951416016 0.2541886568069458\n",
      "loss    97: 2.5637   grad norm: 0.5480          model param norm: 82.5291        \n",
      "\n",
      "quiet_star_policy_loss= -0.032819367945194244\n",
      "nll_loss= 2.619663953781128\n",
      "avg_std= 0.8449064493179321\n",
      "dist std min max: 0.3063148558139801 0.8449064493179321 2.4006080627441406\n",
      "hidden_states min max: -15.066290855407715 10.719260215759277\n",
      "hidden_state minus mean squared max: 212.37124633789062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21174621582031 0.26087093353271484\n",
      "loss    98: 2.5868   grad norm: 0.5718          model param norm: 82.5374        \n",
      "\n",
      "quiet_star_policy_loss= -0.00853796023875475\n",
      "nll_loss= 2.611461877822876\n",
      "avg_std= 0.8439380526542664\n",
      "dist std min max: 0.30188217759132385 0.8439380526542664 2.472707748413086\n",
      "hidden_states min max: -9.717217445373535 9.582253456115723\n",
      "hidden_state minus mean squared max: 86.41261291503906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.44855499267578 0.2773439884185791\n",
      "loss    99: 2.6029   grad norm: 0.5588          model param norm: 82.5459        \n",
      "eval loss 2.5970826148986816\n",
      "\n",
      "quiet_star_policy_loss= 0.012980557046830654\n",
      "nll_loss= 2.5858943462371826\n",
      "avg_std= 0.8406473398208618\n",
      "dist std min max: 0.3019130229949951 0.8406473398208618 2.481107711791992\n",
      "hidden_states min max: -12.189167022705078 12.115489959716797\n",
      "hidden_state minus mean squared max: 130.46435546875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9572525024414 0.27153706550598145\n",
      "loss   100: 2.5989   grad norm: 0.5402          model param norm: 82.5544        \n",
      "\n",
      "quiet_star_policy_loss= -0.06778331100940704\n",
      "nll_loss= 2.5920238494873047\n",
      "avg_std= 0.8417250514030457\n",
      "dist std min max: 0.30426454544067383 0.8417250514030457 2.4778153896331787\n",
      "hidden_states min max: -16.739625930786133 9.842761039733887\n",
      "hidden_state minus mean squared max: 294.5740966796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.37535095214844 0.25644659996032715\n",
      "loss   101: 2.5242   grad norm: 0.5900          model param norm: 82.5628        \n",
      "\n",
      "quiet_star_policy_loss= -0.06407318264245987\n",
      "nll_loss= 2.5946648120880127\n",
      "avg_std= 0.8409544825553894\n",
      "dist std min max: 0.3032620847225189 0.8409544825553894 2.4957163333892822\n",
      "hidden_states min max: -11.40480899810791 10.26294231414795\n",
      "hidden_state minus mean squared max: 133.675537109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.98029327392578 0.2710186243057251\n",
      "loss   102: 2.5306   grad norm: 0.5381          model param norm: 82.5712        \n",
      "\n",
      "quiet_star_policy_loss= -0.04571933671832085\n",
      "nll_loss= 2.5802206993103027\n",
      "avg_std= 0.8410852551460266\n",
      "dist std min max: 0.3030700087547302 0.8410852551460266 2.514371395111084\n",
      "hidden_states min max: -15.36462116241455 10.397989273071289\n",
      "hidden_state minus mean squared max: 260.3186340332031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.31352996826172 0.25899577140808105\n",
      "loss   103: 2.5345   grad norm: 0.5622          model param norm: 82.5794        \n",
      "\n",
      "quiet_star_policy_loss= -0.02896881103515625\n",
      "nll_loss= 2.563413381576538\n",
      "avg_std= 0.838952362537384\n",
      "dist std min max: 0.30274006724357605 0.838952362537384 2.5657296180725098\n",
      "hidden_states min max: -15.698020935058594 10.723608016967773\n",
      "hidden_state minus mean squared max: 240.9501190185547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2748794555664 0.27507805824279785\n",
      "loss   104: 2.5344   grad norm: 0.5606          model param norm: 82.5875        \n",
      "\n",
      "quiet_star_policy_loss= -0.08473491668701172\n",
      "nll_loss= 2.564835548400879\n",
      "avg_std= 0.838503897190094\n",
      "dist std min max: 0.3003944456577301 0.838503897190094 2.5449817180633545\n",
      "hidden_states min max: -18.290529251098633 10.801648139953613\n",
      "hidden_state minus mean squared max: 306.6103515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3953857421875 0.2659837007522583\n",
      "loss   105: 2.4801   grad norm: 0.5458          model param norm: 82.5956        \n",
      "\n",
      "quiet_star_policy_loss= -0.027259541675448418\n",
      "nll_loss= 2.574605941772461\n",
      "avg_std= 0.8389884829521179\n",
      "dist std min max: 0.300005167722702 0.8389884829521179 2.4960532188415527\n",
      "hidden_states min max: -15.414196014404297 10.005395889282227\n",
      "hidden_state minus mean squared max: 292.4228210449219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3716812133789 0.2748926877975464\n",
      "loss   106: 2.5473   grad norm: 0.5812          model param norm: 82.6036        \n",
      "\n",
      "quiet_star_policy_loss= -0.04986467584967613\n",
      "nll_loss= 2.5651092529296875\n",
      "avg_std= 0.8365219235420227\n",
      "dist std min max: 0.296057790517807 0.8365219235420227 2.546194076538086\n",
      "hidden_states min max: -17.43036460876465 10.752689361572266\n",
      "hidden_state minus mean squared max: 258.35748291015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30975341796875 0.2738295793533325\n",
      "loss   107: 2.5152   grad norm: 0.5456          model param norm: 82.6116        \n",
      "\n",
      "quiet_star_policy_loss= -0.020188523456454277\n",
      "nll_loss= 2.563339948654175\n",
      "avg_std= 0.8366986513137817\n",
      "dist std min max: 0.29538479447364807 0.8366986513137817 2.5135252475738525\n",
      "hidden_states min max: -16.547935485839844 10.379417419433594\n",
      "hidden_state minus mean squared max: 268.3665466308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.32874298095703 0.27612757682800293\n",
      "loss   108: 2.5432   grad norm: 0.5618          model param norm: 82.6195        \n",
      "\n",
      "quiet_star_policy_loss= -0.039348602294921875\n",
      "nll_loss= 2.5396487712860107\n",
      "avg_std= 0.8361939787864685\n",
      "dist std min max: 0.29751330614089966 0.8361939787864685 2.5185866355895996\n",
      "hidden_states min max: -23.1446590423584 9.921038627624512\n",
      "hidden_state minus mean squared max: 487.08795166015625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.6268081665039 0.28830158710479736\n",
      "loss   109: 2.5003   grad norm: 0.5634          model param norm: 82.6272        \n",
      "\n",
      "quiet_star_policy_loss= -0.06375813484191895\n",
      "nll_loss= 2.5442044734954834\n",
      "avg_std= 0.8346972465515137\n",
      "dist std min max: 0.29665252566337585 0.8346972465515137 2.5024936199188232\n",
      "hidden_states min max: -19.00217628479004 9.566594123840332\n",
      "hidden_state minus mean squared max: 344.50445556640625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.45362854003906 0.27886152267456055\n",
      "loss   110: 2.4804   grad norm: 0.5783          model param norm: 82.6352        \n",
      "\n",
      "quiet_star_policy_loss= 0.04079538956284523\n",
      "nll_loss= 2.5428459644317627\n",
      "avg_std= 0.8373349905014038\n",
      "dist std min max: 0.2987588047981262 0.8373349905014038 2.5174102783203125\n",
      "hidden_states min max: -9.886153221130371 9.085648536682129\n",
      "hidden_state minus mean squared max: 91.21449279785156\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.78917694091797 0.2828763723373413\n",
      "loss   111: 2.5836   grad norm: 1.2006          model param norm: 82.6429        \n",
      "\n",
      "quiet_star_policy_loss= -0.09179210662841797\n",
      "nll_loss= 2.550987720489502\n",
      "avg_std= 0.8352825045585632\n",
      "dist std min max: 0.2961680293083191 0.8352825045585632 2.4900107383728027\n",
      "hidden_states min max: -17.99076271057129 10.862306594848633\n",
      "hidden_state minus mean squared max: 333.9696350097656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4381103515625 0.274782657623291\n",
      "loss   112: 2.4592   grad norm: 0.5808          model param norm: 82.6504        \n",
      "\n",
      "quiet_star_policy_loss= -0.01424942072480917\n",
      "nll_loss= 2.5352628231048584\n",
      "avg_std= 0.8337525725364685\n",
      "dist std min max: 0.29771026968955994 0.8337525725364685 2.5157558917999268\n",
      "hidden_states min max: -16.99604606628418 10.174577713012695\n",
      "hidden_state minus mean squared max: 240.6426239013672\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.27422332763672 0.28518640995025635\n",
      "loss   113: 2.5210   grad norm: 0.5675          model param norm: 82.6579        \n",
      "\n",
      "quiet_star_policy_loss= -0.02904052846133709\n",
      "nll_loss= 2.5471436977386475\n",
      "avg_std= 0.8322469592094421\n",
      "dist std min max: 0.2972724735736847 0.8322469592094421 2.519434928894043\n",
      "hidden_states min max: -10.98254680633545 9.721100807189941\n",
      "hidden_state minus mean squared max: 121.86878204345703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9340591430664 0.27147650718688965\n",
      "loss   114: 2.5181   grad norm: 0.6025          model param norm: 82.6653        \n",
      "\n",
      "quiet_star_policy_loss= -0.0888141617178917\n",
      "nll_loss= 2.531278133392334\n",
      "avg_std= 0.8303947448730469\n",
      "dist std min max: 0.3008210361003876 0.8303947448730469 2.5118186473846436\n",
      "hidden_states min max: -17.095762252807617 11.549805641174316\n",
      "hidden_state minus mean squared max: 255.93580627441406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.3050308227539 0.26845383644104004\n",
      "loss   115: 2.4425   grad norm: 0.6100          model param norm: 82.6726        \n",
      "\n",
      "quiet_star_policy_loss= -0.07969546318054199\n",
      "nll_loss= 2.5155320167541504\n",
      "avg_std= 0.8302467465400696\n",
      "dist std min max: 0.3012271821498871 0.8302467465400696 2.5414042472839355\n",
      "hidden_states min max: -12.986166954040527 9.829771041870117\n",
      "hidden_state minus mean squared max: 160.44358825683594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07154846191406 0.2774839401245117\n",
      "loss   116: 2.4358   grad norm: 0.5874          model param norm: 82.6800        \n",
      "\n",
      "quiet_star_policy_loss= -0.07760276645421982\n",
      "nll_loss= 2.527137517929077\n",
      "avg_std= 0.8307561874389648\n",
      "dist std min max: 0.2967849373817444 0.8307561874389648 2.53355073928833\n",
      "hidden_states min max: -16.12083625793457 9.519277572631836\n",
      "hidden_state minus mean squared max: 246.8246612548828\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -104.28691864013672 0.28368914127349854\n",
      "loss   117: 2.4495   grad norm: 0.5932          model param norm: 82.6874        \n",
      "\n",
      "quiet_star_policy_loss= -0.09271889179944992\n",
      "nll_loss= 2.529742479324341\n",
      "avg_std= 0.8285070061683655\n",
      "dist std min max: 0.29668381810188293 0.8285070061683655 2.541942834854126\n",
      "hidden_states min max: -18.436168670654297 11.758599281311035\n",
      "hidden_state minus mean squared max: 340.56719970703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.44789123535156 0.282672643661499\n",
      "loss   118: 2.4370   grad norm: 0.5483          model param norm: 82.6951        \n",
      "\n",
      "quiet_star_policy_loss= 0.011689758859574795\n",
      "nll_loss= 2.5122227668762207\n",
      "avg_std= 0.8259268403053284\n",
      "dist std min max: 0.29368147253990173 0.8259268403053284 2.5559258460998535\n",
      "hidden_states min max: -12.879511833190918 10.269736289978027\n",
      "hidden_state minus mean squared max: 129.89041137695312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.96593475341797 0.297765851020813\n",
      "loss   119: 2.5239   grad norm: 0.5714          model param norm: 82.7028        \n",
      "\n",
      "quiet_star_policy_loss= -0.0474490188062191\n",
      "nll_loss= 2.5140926837921143\n",
      "avg_std= 0.8261345624923706\n",
      "dist std min max: 0.2968581020832062 0.8261345624923706 2.573133945465088\n",
      "hidden_states min max: -16.50018882751465 10.56302261352539\n",
      "hidden_state minus mean squared max: 275.130859375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.34121704101562 0.2812187671661377\n",
      "loss   120: 2.4666   grad norm: 0.5639          model param norm: 82.7107        \n",
      "\n",
      "quiet_star_policy_loss= -0.024074172601103783\n",
      "nll_loss= 2.50388765335083\n",
      "avg_std= 0.8223282098770142\n",
      "dist std min max: 0.29898491501808167 0.8223282098770142 2.589447498321533\n",
      "hidden_states min max: -22.60915184020996 10.117765426635742\n",
      "hidden_state minus mean squared max: 494.6297302246094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.63446807861328 0.2838233709335327\n",
      "loss   121: 2.4798   grad norm: 0.5988          model param norm: 82.7184        \n",
      "\n",
      "quiet_star_policy_loss= -0.08516617119312286\n",
      "nll_loss= 2.513070821762085\n",
      "avg_std= 0.8216966986656189\n",
      "dist std min max: 0.30071160197257996 0.8216966986656189 2.610011577606201\n",
      "hidden_states min max: -11.317623138427734 10.932392120361328\n",
      "hidden_state minus mean squared max: 121.93058776855469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.93431091308594 0.27586889266967773\n",
      "loss   122: 2.4279   grad norm: 0.5719          model param norm: 82.7262        \n",
      "\n",
      "quiet_star_policy_loss= -0.043093491345644\n",
      "nll_loss= 2.516317844390869\n",
      "avg_std= 0.8222953081130981\n",
      "dist std min max: 0.29632940888404846 0.8222953081130981 2.6177918910980225\n",
      "hidden_states min max: -13.128843307495117 11.48910140991211\n",
      "hidden_state minus mean squared max: 171.73948669433594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1055679321289 0.2754307985305786\n",
      "loss   123: 2.4732   grad norm: 0.6094          model param norm: 82.7338        \n",
      "\n",
      "quiet_star_policy_loss= -0.04883270338177681\n",
      "nll_loss= 2.4964234828948975\n",
      "avg_std= 0.8167919516563416\n",
      "dist std min max: 0.2973606288433075 0.8167919516563416 2.627673864364624\n",
      "hidden_states min max: -18.58655548095703 10.016451835632324\n",
      "hidden_state minus mean squared max: 342.36065673828125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.45050048828125 0.28908276557922363\n",
      "loss   124: 2.4476   grad norm: 0.5661          model param norm: 82.7414        \n",
      "\n",
      "quiet_star_policy_loss= -0.027853870764374733\n",
      "nll_loss= 2.49768328666687\n",
      "avg_std= 0.8181837201118469\n",
      "dist std min max: 0.2947043478488922 0.8181837201118469 2.6331212520599365\n",
      "hidden_states min max: -12.729551315307617 10.63699722290039\n",
      "hidden_state minus mean squared max: 157.39529418945312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.06197357177734 0.2783123254776001\n",
      "loss   125: 2.4698   grad norm: 0.6001          model param norm: 82.7488        \n",
      "\n",
      "quiet_star_policy_loss= -0.03939571604132652\n",
      "nll_loss= 2.489396810531616\n",
      "avg_std= 0.8175718188285828\n",
      "dist std min max: 0.2954072058200836 0.8175718188285828 2.6074585914611816\n",
      "hidden_states min max: -15.259737014770508 11.847270965576172\n",
      "hidden_state minus mean squared max: 253.17808532714844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29961395263672 0.2880232334136963\n",
      "loss   126: 2.4500   grad norm: 0.5893          model param norm: 82.7562        \n",
      "\n",
      "quiet_star_policy_loss= -0.068939208984375\n",
      "nll_loss= 2.4864845275878906\n",
      "avg_std= 0.8153080940246582\n",
      "dist std min max: 0.2918332815170288 0.8153080940246582 2.6091413497924805\n",
      "hidden_states min max: -14.553950309753418 10.440522193908691\n",
      "hidden_state minus mean squared max: 201.90296936035156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.18647003173828 0.28766822814941406\n",
      "loss   127: 2.4175   grad norm: 0.5754          model param norm: 82.7637        \n",
      "\n",
      "quiet_star_policy_loss= -0.030966758728027344\n",
      "nll_loss= 2.489227294921875\n",
      "avg_std= 0.8141034245491028\n",
      "dist std min max: 0.2894769608974457 0.8141034245491028 2.5989372730255127\n",
      "hidden_states min max: -12.194156646728516 10.59520435333252\n",
      "hidden_state minus mean squared max: 134.07937622070312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.98179626464844 0.2965271472930908\n",
      "loss   128: 2.4583   grad norm: 0.6039          model param norm: 82.7712        \n",
      "\n",
      "quiet_star_policy_loss= -0.059945203363895416\n",
      "nll_loss= 2.498102903366089\n",
      "avg_std= 0.8125826120376587\n",
      "dist std min max: 0.28186437487602234 0.8125826120376587 2.6116106510162354\n",
      "hidden_states min max: -16.011873245239258 11.218300819396973\n",
      "hidden_state minus mean squared max: 245.08944702148438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.28339385986328 0.3129919767379761\n",
      "loss   129: 2.4382   grad norm: 0.6146          model param norm: 82.7788        \n",
      "\n",
      "quiet_star_policy_loss= -0.11347036808729172\n",
      "nll_loss= 2.4854817390441895\n",
      "avg_std= 0.806159257888794\n",
      "dist std min max: 0.28114697337150574 0.806159257888794 2.6183080673217773\n",
      "hidden_states min max: -15.906597137451172 10.18257999420166\n",
      "hidden_state minus mean squared max: 191.24386596679688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15935516357422 0.33820319175720215\n",
      "loss   130: 2.3720   grad norm: 0.6162          model param norm: 82.7863        \n",
      "\n",
      "quiet_star_policy_loss= -0.06958084553480148\n",
      "nll_loss= 2.5044639110565186\n",
      "avg_std= 0.8082886338233948\n",
      "dist std min max: 0.27540236711502075 0.8082886338233948 2.6484057903289795\n",
      "hidden_states min max: -14.803918838500977 11.472138404846191\n",
      "hidden_state minus mean squared max: 205.58053588867188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19550323486328 0.3596080541610718\n",
      "loss   131: 2.4349   grad norm: 0.5917          model param norm: 82.7937        \n",
      "\n",
      "quiet_star_policy_loss= -0.09526558220386505\n",
      "nll_loss= 2.479940414428711\n",
      "avg_std= 0.8061046600341797\n",
      "dist std min max: 0.2683016359806061 0.8061046600341797 2.6389615535736084\n",
      "hidden_states min max: -11.982199668884277 10.570747375488281\n",
      "hidden_state minus mean squared max: 133.52084350585938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9797134399414 0.38676416873931885\n",
      "loss   132: 2.3847   grad norm: 0.5722          model param norm: 82.8009        \n",
      "\n",
      "quiet_star_policy_loss= -0.02332611195743084\n",
      "nll_loss= 2.4713170528411865\n",
      "avg_std= 0.8045832514762878\n",
      "dist std min max: 0.2643360495567322 0.8045832514762878 2.6686227321624756\n",
      "hidden_states min max: -14.228580474853516 11.742158889770508\n",
      "hidden_state minus mean squared max: 198.1598358154297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.1771240234375 0.39183473587036133\n",
      "loss   133: 2.4480   grad norm: 0.5748          model param norm: 82.8079        \n",
      "\n",
      "quiet_star_policy_loss= 0.0012510300148278475\n",
      "nll_loss= 2.4724414348602295\n",
      "avg_std= 0.8032894730567932\n",
      "dist std min max: 0.2574082016944885 0.8032894730567932 2.718592882156372\n",
      "hidden_states min max: -11.204479217529297 11.47464656829834\n",
      "hidden_state minus mean squared max: 117.66051483154297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.320669174194336 0.4187600612640381\n",
      "loss   134: 2.4737   grad norm: 0.5947          model param norm: 82.8150        \n",
      "\n",
      "quiet_star_policy_loss= 0.005516243167221546\n",
      "nll_loss= 2.4612789154052734\n",
      "avg_std= 0.7994029521942139\n",
      "dist std min max: 0.2542824447154999 0.7994029521942139 2.715212821960449\n",
      "hidden_states min max: -9.407132148742676 11.052972793579102\n",
      "hidden_state minus mean squared max: 104.29310607910156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.686723709106445 0.42082035541534424\n",
      "loss   135: 2.4668   grad norm: 0.5923          model param norm: 82.8222        \n",
      "\n",
      "quiet_star_policy_loss= -0.006266975309699774\n",
      "nll_loss= 2.45994234085083\n",
      "avg_std= 0.8008136749267578\n",
      "dist std min max: 0.24913105368614197 0.8008136749267578 2.71567440032959\n",
      "hidden_states min max: -10.609338760375977 11.431195259094238\n",
      "hidden_state minus mean squared max: 123.36515808105469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.67975616455078 0.45141756534576416\n",
      "loss   136: 2.4537   grad norm: 0.6003          model param norm: 82.8293        \n",
      "\n",
      "quiet_star_policy_loss= -0.05469207838177681\n",
      "nll_loss= 2.459003448486328\n",
      "avg_std= 0.7978967428207397\n",
      "dist std min max: 0.246150404214859 0.7978967428207397 2.7198164463043213\n",
      "hidden_states min max: -12.79034423828125 11.552327156066895\n",
      "hidden_state minus mean squared max: 127.72061157226562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.95271301269531 0.4647064208984375\n",
      "loss   137: 2.4043   grad norm: 0.5970          model param norm: 82.8364        \n",
      "\n",
      "quiet_star_policy_loss= -0.040900420397520065\n",
      "nll_loss= 2.4510014057159424\n",
      "avg_std= 0.7999230623245239\n",
      "dist std min max: 0.24283471703529358 0.7999230623245239 2.7712268829345703\n",
      "hidden_states min max: -16.51506996154785 11.857215881347656\n",
      "hidden_state minus mean squared max: 269.4125061035156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.3306884765625 0.47345638275146484\n",
      "loss   138: 2.4101   grad norm: 0.5946          model param norm: 82.8433        \n",
      "\n",
      "quiet_star_policy_loss= -0.037259623408317566\n",
      "nll_loss= 2.4446139335632324\n",
      "avg_std= 0.7968074679374695\n",
      "dist std min max: 0.24360859394073486 0.7968074679374695 2.7560062408447266\n",
      "hidden_states min max: -13.31252384185791 10.374309539794922\n",
      "hidden_state minus mean squared max: 144.73605346679688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.02003479003906 0.45945096015930176\n",
      "loss   139: 2.4074   grad norm: 1.1783          model param norm: 82.8502        \n",
      "\n",
      "quiet_star_policy_loss= -0.03322143480181694\n",
      "nll_loss= 2.45065975189209\n",
      "avg_std= 0.7963886260986328\n",
      "dist std min max: 0.2407274842262268 0.7963886260986328 2.7219150066375732\n",
      "hidden_states min max: -13.645341873168945 11.069722175598145\n",
      "hidden_state minus mean squared max: 180.0903778076172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12931823730469 0.4785895347595215\n",
      "loss   140: 2.4174   grad norm: 0.6074          model param norm: 82.8569        \n",
      "\n",
      "quiet_star_policy_loss= -0.029692267999053\n",
      "nll_loss= 2.4525554180145264\n",
      "avg_std= 0.7952889204025269\n",
      "dist std min max: 0.23930388689041138 0.7952889204025269 2.6983726024627686\n",
      "hidden_states min max: -15.007102966308594 11.59716510772705\n",
      "hidden_state minus mean squared max: 167.38658142089844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.09271240234375 0.49365127086639404\n",
      "loss   141: 2.4229   grad norm: 0.5863          model param norm: 82.8636        \n",
      "\n",
      "quiet_star_policy_loss= -0.06845312565565109\n",
      "nll_loss= 2.4478344917297363\n",
      "avg_std= 0.7959585785865784\n",
      "dist std min max: 0.24035102128982544 0.7959585785865784 2.6818771362304688\n",
      "hidden_states min max: -10.09514331817627 10.450068473815918\n",
      "hidden_state minus mean squared max: 99.60616302490234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.55473327636719 0.4942358732223511\n",
      "loss   142: 2.3794   grad norm: 0.5973          model param norm: 82.8702        \n",
      "\n",
      "quiet_star_policy_loss= -0.05063629150390625\n",
      "nll_loss= 2.4544403553009033\n",
      "avg_std= 0.7947386503219604\n",
      "dist std min max: 0.2361207753419876 0.7947386503219604 2.658698081970215\n",
      "hidden_states min max: -14.363273620605469 10.588804244995117\n",
      "hidden_state minus mean squared max: 180.94259643554688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.13166809082031 0.491901159286499\n",
      "loss   143: 2.4038   grad norm: 0.6011          model param norm: 82.8768        \n",
      "\n",
      "quiet_star_policy_loss= -0.026355743408203125\n",
      "nll_loss= 2.424888849258423\n",
      "avg_std= 0.794728696346283\n",
      "dist std min max: 0.23359046876430511 0.794728696346283 2.6302170753479004\n",
      "hidden_states min max: -18.22269630432129 10.056171417236328\n",
      "hidden_state minus mean squared max: 355.3006591796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.46904754638672 0.4941021203994751\n",
      "loss   144: 2.3985   grad norm: 0.5910          model param norm: 82.8834        \n",
      "\n",
      "quiet_star_policy_loss= -0.043227385729551315\n",
      "nll_loss= 2.4294323921203613\n",
      "avg_std= 0.7942342758178711\n",
      "dist std min max: 0.23489949107170105 0.7942342758178711 2.6307015419006348\n",
      "hidden_states min max: -10.505902290344238 10.673091888427734\n",
      "hidden_state minus mean squared max: 103.19148254394531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.84915924072266 0.5150730609893799\n",
      "loss   145: 2.3862   grad norm: 0.6160          model param norm: 82.8900        \n",
      "\n",
      "quiet_star_policy_loss= 0.0012163162464275956\n",
      "nll_loss= 2.4437754154205322\n",
      "avg_std= 0.7923591136932373\n",
      "dist std min max: 0.23455137014389038 0.7923591136932373 2.5937349796295166\n",
      "hidden_states min max: -17.804176330566406 10.393367767333984\n",
      "hidden_state minus mean squared max: 305.6426696777344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.39378356933594 0.5307213068008423\n",
      "loss   146: 2.4450   grad norm: 0.5885          model param norm: 82.8965        \n",
      "\n",
      "quiet_star_policy_loss= -0.06695538014173508\n",
      "nll_loss= 2.4079113006591797\n",
      "avg_std= 0.7919058799743652\n",
      "dist std min max: 0.22785143554210663 0.7919058799743652 2.573892593383789\n",
      "hidden_states min max: -18.8351993560791 9.968271255493164\n",
      "hidden_state minus mean squared max: 271.0265197753906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33368682861328 0.5172817707061768\n",
      "loss   147: 2.3410   grad norm: 0.5750          model param norm: 82.9029        \n",
      "\n",
      "quiet_star_policy_loss= -0.0663672462105751\n",
      "nll_loss= 2.439458131790161\n",
      "avg_std= 0.7923964262008667\n",
      "dist std min max: 0.22298163175582886 0.7923964262008667 2.5648961067199707\n",
      "hidden_states min max: -17.287860870361328 10.909736633300781\n",
      "hidden_state minus mean squared max: 271.7853698730469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33506774902344 0.5238844156265259\n",
      "loss   148: 2.3731   grad norm: 0.5810          model param norm: 82.9095        \n",
      "\n",
      "quiet_star_policy_loss= -0.09542713314294815\n",
      "nll_loss= 2.4233381748199463\n",
      "avg_std= 0.787915825843811\n",
      "dist std min max: 0.22733689844608307 0.787915825843811 2.562940835952759\n",
      "hidden_states min max: -14.250805854797363 11.669486999511719\n",
      "hidden_state minus mean squared max: 195.49009704589844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.17034149169922 0.5301285982131958\n",
      "loss   149: 2.3279   grad norm: 0.6108          model param norm: 82.9158        \n",
      "\n",
      "quiet_star_policy_loss= -0.08154592663049698\n",
      "nll_loss= 2.4310402870178223\n",
      "avg_std= 0.7868281602859497\n",
      "dist std min max: 0.2282525897026062 0.7868281602859497 2.5505247116088867\n",
      "hidden_states min max: -15.998961448669434 10.548916816711426\n",
      "hidden_state minus mean squared max: 235.98573303222656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26447296142578 0.5314480066299438\n",
      "loss   150: 2.3495   grad norm: 0.6485          model param norm: 82.9222        \n",
      "\n",
      "quiet_star_policy_loss= -0.05160865932703018\n",
      "nll_loss= 2.4289183616638184\n",
      "avg_std= 0.7867166996002197\n",
      "dist std min max: 0.2241206169128418 0.7867166996002197 2.5704338550567627\n",
      "hidden_states min max: -18.377248764038086 10.670580863952637\n",
      "hidden_state minus mean squared max: 329.5902099609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43150329589844 0.5583293437957764\n",
      "loss   151: 2.3773   grad norm: 0.6383          model param norm: 82.9287        \n",
      "\n",
      "quiet_star_policy_loss= -0.015685273334383965\n",
      "nll_loss= 2.4167423248291016\n",
      "avg_std= 0.7859309911727905\n",
      "dist std min max: 0.22598354518413544 0.7859309911727905 2.66271710395813\n",
      "hidden_states min max: -17.79722785949707 11.038003921508789\n",
      "hidden_state minus mean squared max: 237.54017639160156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26775360107422 0.5623021125793457\n",
      "loss   152: 2.4011   grad norm: 0.6072          model param norm: 82.9354        \n",
      "\n",
      "quiet_star_policy_loss= -0.014597320929169655\n",
      "nll_loss= 2.4313979148864746\n",
      "avg_std= 0.7847782373428345\n",
      "dist std min max: 0.22913935780525208 0.7847782373428345 2.5388972759246826\n",
      "hidden_states min max: -20.679285049438477 10.636038780212402\n",
      "hidden_state minus mean squared max: 379.35186767578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.50181579589844 0.530058741569519\n",
      "loss   153: 2.4168   grad norm: 0.6170          model param norm: 82.9421        \n",
      "\n",
      "quiet_star_policy_loss= -0.05786648020148277\n",
      "nll_loss= 2.4047012329101562\n",
      "avg_std= 0.7799889445304871\n",
      "dist std min max: 0.22735384106636047 0.7799889445304871 2.5709619522094727\n",
      "hidden_states min max: -14.892065048217773 10.422147750854492\n",
      "hidden_state minus mean squared max: 213.765625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21503448486328 0.5398218631744385\n",
      "loss   154: 2.3468   grad norm: 0.6190          model param norm: 82.9486        \n",
      "\n",
      "quiet_star_policy_loss= -0.04546022415161133\n",
      "nll_loss= 2.397409677505493\n",
      "avg_std= 0.7801699042320251\n",
      "dist std min max: 0.2274610996246338 0.7801699042320251 2.5524308681488037\n",
      "hidden_states min max: -9.527316093444824 11.464783668518066\n",
      "hidden_state minus mean squared max: 115.30628204345703\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5749282836914 0.5594749450683594\n",
      "loss   155: 2.3519   grad norm: 0.5981          model param norm: 82.9551        \n",
      "\n",
      "quiet_star_policy_loss= -0.06168670579791069\n",
      "nll_loss= 2.416699171066284\n",
      "avg_std= 0.7801696062088013\n",
      "dist std min max: 0.22987911105155945 0.7801696062088013 2.560845375061035\n",
      "hidden_states min max: -13.255887031555176 12.427976608276367\n",
      "hidden_state minus mean squared max: 134.1512451171875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9779281616211 0.5473083257675171\n",
      "loss   156: 2.3550   grad norm: 0.6102          model param norm: 82.9617        \n",
      "\n",
      "quiet_star_policy_loss= -0.07463865727186203\n",
      "nll_loss= 2.4047722816467285\n",
      "avg_std= 0.7777636647224426\n",
      "dist std min max: 0.22732143104076385 0.7777636647224426 2.5855202674865723\n",
      "hidden_states min max: -20.497316360473633 11.182424545288086\n",
      "hidden_state minus mean squared max: 440.5651550292969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.57660675048828 0.5521218776702881\n",
      "loss   157: 2.3301   grad norm: 0.5994          model param norm: 82.9682        \n",
      "\n",
      "quiet_star_policy_loss= -0.0645214095711708\n",
      "nll_loss= 2.395301342010498\n",
      "avg_std= 0.7794120907783508\n",
      "dist std min max: 0.22743068635463715 0.7794120907783508 2.5500059127807617\n",
      "hidden_states min max: -16.971155166625977 10.099238395690918\n",
      "hidden_state minus mean squared max: 229.4110565185547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2503433227539 0.5517942905426025\n",
      "loss   158: 2.3308   grad norm: 0.6317          model param norm: 82.9745        \n",
      "\n",
      "quiet_star_policy_loss= -0.11433830112218857\n",
      "nll_loss= 2.426149606704712\n",
      "avg_std= 0.7771004438400269\n",
      "dist std min max: 0.23032952845096588 0.7771004438400269 2.5672147274017334\n",
      "hidden_states min max: -14.010554313659668 10.824265480041504\n",
      "hidden_state minus mean squared max: 222.86886596679688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.23587036132812 0.5484014749526978\n",
      "loss   159: 2.3118   grad norm: 0.5815          model param norm: 82.9809        \n",
      "\n",
      "quiet_star_policy_loss= -0.035040855407714844\n",
      "nll_loss= 2.411247968673706\n",
      "avg_std= 0.7753127217292786\n",
      "dist std min max: 0.23062922060489655 0.7753127217292786 2.5642566680908203\n",
      "hidden_states min max: -17.36435317993164 10.578291893005371\n",
      "hidden_state minus mean squared max: 313.1966247558594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.40601348876953 0.5417337417602539\n",
      "loss   160: 2.3762   grad norm: 0.6089          model param norm: 82.9872        \n",
      "\n",
      "quiet_star_policy_loss= -0.07503890991210938\n",
      "nll_loss= 2.400481700897217\n",
      "avg_std= 0.7717223763465881\n",
      "dist std min max: 0.22981734573841095 0.7717223763465881 2.640378475189209\n",
      "hidden_states min max: -17.78837776184082 10.743218421936035\n",
      "hidden_state minus mean squared max: 334.28997802734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43858337402344 0.527389645576477\n",
      "loss   161: 2.3254   grad norm: 0.6378          model param norm: 82.9937        \n",
      "\n",
      "quiet_star_policy_loss= -0.05371437221765518\n",
      "nll_loss= 2.39959979057312\n",
      "avg_std= 0.7735887765884399\n",
      "dist std min max: 0.23000766336917877 0.7735887765884399 2.558405637741089\n",
      "hidden_states min max: -15.13547420501709 10.329856872558594\n",
      "hidden_state minus mean squared max: 231.96678161621094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25587463378906 0.5504604578018188\n",
      "loss   162: 2.3459   grad norm: 0.5931          model param norm: 83.0000        \n",
      "\n",
      "quiet_star_policy_loss= -0.04853839799761772\n",
      "nll_loss= 2.4000327587127686\n",
      "avg_std= 0.7717205882072449\n",
      "dist std min max: 0.23004387319087982 0.7717205882072449 2.573326349258423\n",
      "hidden_states min max: -16.89115333557129 9.874608993530273\n",
      "hidden_state minus mean squared max: 254.1004180908203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30145263671875 0.5294437408447266\n",
      "loss   163: 2.3515   grad norm: 0.6281          model param norm: 83.0062        \n",
      "\n",
      "quiet_star_policy_loss= -0.06740770488977432\n",
      "nll_loss= 2.390134811401367\n",
      "avg_std= 0.7673604488372803\n",
      "dist std min max: 0.23029983043670654 0.7673604488372803 2.6941609382629395\n",
      "hidden_states min max: -15.357889175415039 11.457573890686035\n",
      "hidden_state minus mean squared max: 242.42849731445312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.27793884277344 0.5345686674118042\n",
      "loss   164: 2.3227   grad norm: 0.6314          model param norm: 83.0124        \n",
      "\n",
      "quiet_star_policy_loss= -0.07527618855237961\n",
      "nll_loss= 2.3760268688201904\n",
      "avg_std= 0.7630881071090698\n",
      "dist std min max: 0.23015527427196503 0.7630881071090698 2.5981812477111816\n",
      "hidden_states min max: -10.42426586151123 11.067795753479004\n",
      "hidden_state minus mean squared max: 105.2330322265625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65191650390625 0.5374847650527954\n",
      "loss   165: 2.3008   grad norm: 0.6136          model param norm: 83.0188        \n",
      "\n",
      "quiet_star_policy_loss= -0.05662250518798828\n",
      "nll_loss= 2.3837647438049316\n",
      "avg_std= 0.764215350151062\n",
      "dist std min max: 0.22949925065040588 0.764215350151062 2.5903520584106445\n",
      "hidden_states min max: -10.364059448242188 11.706171989440918\n",
      "hidden_state minus mean squared max: 118.97714233398438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.73255157470703 0.5375995635986328\n",
      "loss   166: 2.3271   grad norm: 0.6067          model param norm: 83.0250        \n",
      "\n",
      "quiet_star_policy_loss= 0.010562032461166382\n",
      "nll_loss= 2.3799843788146973\n",
      "avg_std= 0.7656126022338867\n",
      "dist std min max: 0.2335406094789505 0.7656126022338867 2.587232828140259\n",
      "hidden_states min max: -8.637577056884766 9.327852249145508\n",
      "hidden_state minus mean squared max: 74.41078186035156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.55152130126953 0.5289249420166016\n",
      "loss   167: 2.3905   grad norm: 1.3430          model param norm: 83.0315        \n",
      "\n",
      "quiet_star_policy_loss= -0.003943443298339844\n",
      "nll_loss= 2.387343168258667\n",
      "avg_std= 0.7604267001152039\n",
      "dist std min max: 0.22805532813072205 0.7604267001152039 2.5877952575683594\n",
      "hidden_states min max: -28.330110549926758 10.470122337341309\n",
      "hidden_state minus mean squared max: 889.9114379882812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.92813873291016 0.5452785491943359\n",
      "loss   168: 2.3834   grad norm: 0.6348          model param norm: 83.0381        \n",
      "\n",
      "quiet_star_policy_loss= -0.08067546039819717\n",
      "nll_loss= 2.374663829803467\n",
      "avg_std= 0.7571929693222046\n",
      "dist std min max: 0.22964252531528473 0.7571929693222046 2.7779910564422607\n",
      "hidden_states min max: -22.740642547607422 10.173444747924805\n",
      "hidden_state minus mean squared max: 475.4586486816406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.6147232055664 0.5309123992919922\n",
      "loss   169: 2.2940   grad norm: 0.6194          model param norm: 83.0446        \n",
      "\n",
      "quiet_star_policy_loss= -0.06923389434814453\n",
      "nll_loss= 2.3866167068481445\n",
      "avg_std= 0.7594113349914551\n",
      "dist std min max: 0.23067991435527802 0.7594113349914551 2.6805412769317627\n",
      "hidden_states min max: -17.651493072509766 11.114692687988281\n",
      "hidden_state minus mean squared max: 262.7770080566406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.31824493408203 0.5422501564025879\n",
      "loss   170: 2.3174   grad norm: 0.5885          model param norm: 83.0513        \n",
      "\n",
      "quiet_star_policy_loss= -0.023657608777284622\n",
      "nll_loss= 2.3724770545959473\n",
      "avg_std= 0.7565436959266663\n",
      "dist std min max: 0.23003053665161133 0.7565436959266663 2.5655298233032227\n",
      "hidden_states min max: -15.073135375976562 10.456396102905273\n",
      "hidden_state minus mean squared max: 228.60992431640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.24858093261719 0.5495301485061646\n",
      "loss   171: 2.3488   grad norm: 0.6296          model param norm: 83.0579        \n",
      "\n",
      "quiet_star_policy_loss= -0.07741699367761612\n",
      "nll_loss= 2.3748154640197754\n",
      "avg_std= 0.756872296333313\n",
      "dist std min max: 0.23083388805389404 0.756872296333313 2.793809413909912\n",
      "hidden_states min max: -14.525399208068848 10.666958808898926\n",
      "hidden_state minus mean squared max: 206.3168182373047\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.19728088378906 0.5360807180404663\n",
      "loss   172: 2.2974   grad norm: 0.5982          model param norm: 83.0648        \n",
      "\n",
      "quiet_star_policy_loss= -0.022728754207491875\n",
      "nll_loss= 2.379746198654175\n",
      "avg_std= 0.7558289170265198\n",
      "dist std min max: 0.23088780045509338 0.7558289170265198 2.59328293800354\n",
      "hidden_states min max: -15.16409683227539 10.117181777954102\n",
      "hidden_state minus mean squared max: 181.44114685058594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.1330337524414 0.5253208875656128\n",
      "loss   173: 2.3570   grad norm: 0.5831          model param norm: 83.0718        \n",
      "\n",
      "quiet_star_policy_loss= -0.047349072992801666\n",
      "nll_loss= 2.3749630451202393\n",
      "avg_std= 0.7561120390892029\n",
      "dist std min max: 0.230938121676445 0.7561120390892029 2.6192731857299805\n",
      "hidden_states min max: -11.374943733215332 10.890544891357422\n",
      "hidden_state minus mean squared max: 148.7906494140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.03385162353516 0.5462687015533447\n",
      "loss   174: 2.3276   grad norm: 0.6486          model param norm: 83.0787        \n",
      "\n",
      "quiet_star_policy_loss= -0.06412529945373535\n",
      "nll_loss= 2.363431930541992\n",
      "avg_std= 0.749482274055481\n",
      "dist std min max: 0.22809608280658722 0.749482274055481 2.6169779300689697\n",
      "hidden_states min max: -14.334556579589844 11.21607494354248\n",
      "hidden_state minus mean squared max: 166.681640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.09062194824219 0.5296752452850342\n",
      "loss   175: 2.2993   grad norm: 0.6332          model param norm: 83.0859        \n",
      "\n",
      "quiet_star_policy_loss= -0.004780387971550226\n",
      "nll_loss= 2.3743221759796143\n",
      "avg_std= 0.7524911165237427\n",
      "dist std min max: 0.23160403966903687 0.7524911165237427 2.66595196723938\n",
      "hidden_states min max: -15.323991775512695 11.108719825744629\n",
      "hidden_state minus mean squared max: 238.3309783935547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26940155029297 0.537816047668457\n",
      "loss   176: 2.3695   grad norm: 0.6237          model param norm: 83.0931        \n",
      "\n",
      "quiet_star_policy_loss= -0.06758222728967667\n",
      "nll_loss= 2.3492448329925537\n",
      "avg_std= 0.7527933120727539\n",
      "dist std min max: 0.23061847686767578 0.7527933120727539 2.64591121673584\n",
      "hidden_states min max: -17.96120262145996 10.706544876098633\n",
      "hidden_state minus mean squared max: 277.9851989746094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.34636688232422 0.5248878002166748\n",
      "loss   177: 2.2817   grad norm: 0.6279          model param norm: 83.1001        \n",
      "\n",
      "quiet_star_policy_loss= -0.035057734698057175\n",
      "nll_loss= 2.361365795135498\n",
      "avg_std= 0.7506712675094604\n",
      "dist std min max: 0.23115602135658264 0.7506712675094604 2.7037265300750732\n",
      "hidden_states min max: -10.295682907104492 10.954268455505371\n",
      "hidden_state minus mean squared max: 105.0306396484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.85971069335938 0.5385944843292236\n",
      "loss   178: 2.3263   grad norm: 0.6461          model param norm: 83.1072        \n",
      "\n",
      "quiet_star_policy_loss= -0.06888427585363388\n",
      "nll_loss= 2.3334877490997314\n",
      "avg_std= 0.7505001425743103\n",
      "dist std min max: 0.2280893623828888 0.7505001425743103 2.613863468170166\n",
      "hidden_states min max: -22.098037719726562 12.436149597167969\n",
      "hidden_state minus mean squared max: 510.3868713378906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.650146484375 0.523853063583374\n",
      "loss   179: 2.2646   grad norm: 0.6173          model param norm: 83.1142        \n",
      "\n",
      "quiet_star_policy_loss= -0.024210358038544655\n",
      "nll_loss= 2.360619306564331\n",
      "avg_std= 0.749103307723999\n",
      "dist std min max: 0.23265472054481506 0.749103307723999 2.734372615814209\n",
      "hidden_states min max: -15.5381441116333 11.246185302734375\n",
      "hidden_state minus mean squared max: 227.8024139404297\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -104.24681091308594 0.5209071636199951\n",
      "loss   180: 2.3364   grad norm: 0.6541          model param norm: 83.1212        \n",
      "\n",
      "quiet_star_policy_loss= 0.014539480209350586\n",
      "nll_loss= 2.356820821762085\n",
      "avg_std= 0.745628297328949\n",
      "dist std min max: 0.23091089725494385 0.745628297328949 2.6518781185150146\n",
      "hidden_states min max: -12.990164756774902 12.058591842651367\n",
      "hidden_state minus mean squared max: 175.10707092285156\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -104.11528778076172 0.5213217735290527\n",
      "loss   181: 2.3714   grad norm: 0.6249          model param norm: 83.1283        \n",
      "\n",
      "quiet_star_policy_loss= -0.016516495496034622\n",
      "nll_loss= 2.350951910018921\n",
      "avg_std= 0.7464080452919006\n",
      "dist std min max: 0.23309063911437988 0.7464080452919006 2.733394145965576\n",
      "hidden_states min max: -9.659975051879883 11.78095531463623\n",
      "hidden_state minus mean squared max: 109.22003936767578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.45048522949219 0.52692711353302\n",
      "loss   182: 2.3344   grad norm: 0.6357          model param norm: 83.1352        \n",
      "\n",
      "quiet_star_policy_loss= -0.07452070713043213\n",
      "nll_loss= 2.3518013954162598\n",
      "avg_std= 0.7429037094116211\n",
      "dist std min max: 0.23069138824939728 0.7429037094116211 2.7561051845550537\n",
      "hidden_states min max: -12.201812744140625 12.762809753417969\n",
      "hidden_state minus mean squared max: 217.9434814453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.22469329833984 0.5278464555740356\n",
      "loss   183: 2.2773   grad norm: 0.6406          model param norm: 83.1422        \n",
      "\n",
      "quiet_star_policy_loss= -0.031229782849550247\n",
      "nll_loss= 2.3459272384643555\n",
      "avg_std= 0.7410274147987366\n",
      "dist std min max: 0.23388230800628662 0.7410274147987366 2.7032580375671387\n",
      "hidden_states min max: -30.998512268066406 11.68360710144043\n",
      "hidden_state minus mean squared max: 1067.7999267578125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -105.01924133300781 0.5280617475509644\n",
      "loss   184: 2.3147   grad norm: 0.6201          model param norm: 83.1491        \n",
      "\n",
      "quiet_star_policy_loss= -0.04903726652264595\n",
      "nll_loss= 2.3503873348236084\n",
      "avg_std= 0.7423041462898254\n",
      "dist std min max: 0.23056446015834808 0.7423041462898254 2.611325740814209\n",
      "hidden_states min max: -11.845869064331055 10.473320960998535\n",
      "hidden_state minus mean squared max: 166.25721740722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08934783935547 0.5246098041534424\n",
      "loss   185: 2.3014   grad norm: 0.6490          model param norm: 83.1560        \n",
      "\n",
      "quiet_star_policy_loss= -0.04936986044049263\n",
      "nll_loss= 2.3578813076019287\n",
      "avg_std= 0.7404693365097046\n",
      "dist std min max: 0.22987046837806702 0.7404693365097046 2.6153457164764404\n",
      "hidden_states min max: -15.757213592529297 12.07858657836914\n",
      "hidden_state minus mean squared max: 171.8636932373047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10592651367188 0.5356107950210571\n",
      "loss   186: 2.3085   grad norm: 0.6388          model param norm: 83.1627        \n",
      "\n",
      "quiet_star_policy_loss= -0.07592087239027023\n",
      "nll_loss= 2.342676877975464\n",
      "avg_std= 0.7381969690322876\n",
      "dist std min max: 0.23158617317676544 0.7381969690322876 2.7096076011657715\n",
      "hidden_states min max: -10.451541900634766 12.393057823181152\n",
      "hidden_state minus mean squared max: 113.74735260009766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65129852294922 0.5230673551559448\n",
      "loss   187: 2.2668   grad norm: 0.6324          model param norm: 83.1692        \n",
      "\n",
      "quiet_star_policy_loss= -0.01483230572193861\n",
      "nll_loss= 2.341836929321289\n",
      "avg_std= 0.7405879497528076\n",
      "dist std min max: 0.22990462183952332 0.7405879497528076 2.6587696075439453\n",
      "hidden_states min max: -13.344792366027832 10.658405303955078\n",
      "hidden_state minus mean squared max: 179.8936309814453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1287612915039 0.5477925539016724\n",
      "loss   188: 2.3270   grad norm: 0.6630          model param norm: 83.1757        \n",
      "\n",
      "quiet_star_policy_loss= 0.008926344104111195\n",
      "nll_loss= 2.331634998321533\n",
      "avg_std= 0.7374868392944336\n",
      "dist std min max: 0.22802281379699707 0.7374868392944336 2.858454942703247\n",
      "hidden_states min max: -18.411849975585938 12.65205192565918\n",
      "hidden_state minus mean squared max: 371.55914306640625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.49142456054688 0.5454314947128296\n",
      "loss   189: 2.3406   grad norm: 0.6620          model param norm: 83.1824        \n",
      "\n",
      "quiet_star_policy_loss= -0.00574493408203125\n",
      "nll_loss= 2.3369038105010986\n",
      "avg_std= 0.738310694694519\n",
      "dist std min max: 0.22279399633407593 0.738310694694519 2.6870663166046143\n",
      "hidden_states min max: -10.859518051147461 10.870092391967773\n",
      "hidden_state minus mean squared max: 99.87898254394531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78382110595703 0.5645432472229004\n",
      "loss   190: 2.3312   grad norm: 0.6581          model param norm: 83.1889        \n",
      "\n",
      "quiet_star_policy_loss= -0.04667377471923828\n",
      "nll_loss= 2.3358452320098877\n",
      "avg_std= 0.7361131310462952\n",
      "dist std min max: 0.220415398478508 0.7361131310462952 2.8282864093780518\n",
      "hidden_states min max: -16.957286834716797 11.580911636352539\n",
      "hidden_state minus mean squared max: 255.84912109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30486297607422 0.5776662826538086\n",
      "loss   191: 2.2892   grad norm: 0.6216          model param norm: 83.1954        \n",
      "\n",
      "quiet_star_policy_loss= -0.024479199200868607\n",
      "nll_loss= 2.330354690551758\n",
      "avg_std= 0.7370819449424744\n",
      "dist std min max: 0.2173847109079361 0.7370819449424744 2.676011800765991\n",
      "hidden_states min max: -10.051678657531738 11.409873962402344\n",
      "hidden_state minus mean squared max: 93.51598358154297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72019958496094 0.584820032119751\n",
      "loss   192: 2.3059   grad norm: 0.6528          model param norm: 83.2021        \n",
      "\n",
      "quiet_star_policy_loss= -0.006159019656479359\n",
      "nll_loss= 2.322726011276245\n",
      "avg_std= 0.7375306487083435\n",
      "dist std min max: 0.21645711362361908 0.7375306487083435 2.7121529579162598\n",
      "hidden_states min max: -11.86744213104248 11.986836433410645\n",
      "hidden_state minus mean squared max: 136.5782470703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99102783203125 0.6074001789093018\n",
      "loss   193: 2.3166   grad norm: 0.6464          model param norm: 83.2086        \n",
      "\n",
      "quiet_star_policy_loss= -0.03864545747637749\n",
      "nll_loss= 2.32800555229187\n",
      "avg_std= 0.7319877743721008\n",
      "dist std min max: 0.20952753722667694 0.7319877743721008 2.7765066623687744\n",
      "hidden_states min max: -8.924079895019531 11.57058334350586\n",
      "hidden_state minus mean squared max: 109.47356414794922\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.7103271484375 0.6141170263290405\n",
      "loss   194: 2.2894   grad norm: 0.6563          model param norm: 83.2151        \n",
      "\n",
      "quiet_star_policy_loss= 0.010495401918888092\n",
      "nll_loss= 2.332526922225952\n",
      "avg_std= 0.7346361875534058\n",
      "dist std min max: 0.21023300290107727 0.7346361875534058 2.6059770584106445\n",
      "hidden_states min max: -9.486244201660156 9.96888542175293\n",
      "hidden_state minus mean squared max: 76.76205444335938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69920349121094 0.618401050567627\n",
      "loss   195: 2.3430   grad norm: 1.4193          model param norm: 83.2215        \n",
      "\n",
      "quiet_star_policy_loss= -0.09787755459547043\n",
      "nll_loss= 2.3300657272338867\n",
      "avg_std= 0.7360880374908447\n",
      "dist std min max: 0.20687834918498993 0.7360880374908447 2.722287893295288\n",
      "hidden_states min max: -13.809969902038574 11.750520706176758\n",
      "hidden_state minus mean squared max: 185.9625701904297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.14534759521484 0.6534163951873779\n",
      "loss   196: 2.2322   grad norm: 0.6678          model param norm: 83.2284        \n",
      "\n",
      "quiet_star_policy_loss= -0.021898461505770683\n",
      "nll_loss= 2.3260037899017334\n",
      "avg_std= 0.7324716448783875\n",
      "dist std min max: 0.20671483874320984 0.7324716448783875 2.7146339416503906\n",
      "hidden_states min max: -13.11549186706543 11.883760452270508\n",
      "hidden_state minus mean squared max: 128.14947509765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.95917510986328 0.6480016708374023\n",
      "loss   197: 2.3041   grad norm: 0.6752          model param norm: 83.2353        \n",
      "\n",
      "quiet_star_policy_loss= -0.06236782297492027\n",
      "nll_loss= 2.3219103813171387\n",
      "avg_std= 0.7337357401847839\n",
      "dist std min max: 0.20273439586162567 0.7337357401847839 2.6763904094696045\n",
      "hidden_states min max: -10.500653266906738 13.487159729003906\n",
      "hidden_state minus mean squared max: 137.1226348876953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.780997276306152 0.644530177116394\n",
      "loss   198: 2.2595   grad norm: 0.6393          model param norm: 83.2423        \n",
      "\n",
      "quiet_star_policy_loss= -0.06189699098467827\n",
      "nll_loss= 2.3052613735198975\n",
      "avg_std= 0.7325394749641418\n",
      "dist std min max: 0.2071601003408432 0.7325394749641418 2.698059320449829\n",
      "hidden_states min max: -12.955709457397461 11.44869327545166\n",
      "hidden_state minus mean squared max: 133.43392944335938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97937774658203 0.651952862739563\n",
      "loss   199: 2.2434   grad norm: 0.6701          model param norm: 83.2492        \n",
      "eval loss 2.318236827850342\n",
      "\n",
      "quiet_star_policy_loss= -0.04477052763104439\n",
      "nll_loss= 2.313894033432007\n",
      "avg_std= 0.7285925149917603\n",
      "dist std min max: 0.2062027007341385 0.7285925149917603 2.739009380340576\n",
      "hidden_states min max: -12.13080883026123 12.059525489807129\n",
      "hidden_state minus mean squared max: 185.6409912109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.14449310302734 0.6566731929779053\n",
      "loss   200: 2.2691   grad norm: 0.6846          model param norm: 83.2561        \n",
      "\n",
      "quiet_star_policy_loss= -0.042555999010801315\n",
      "nll_loss= 2.3181798458099365\n",
      "avg_std= 0.7288424372673035\n",
      "dist std min max: 0.20656858384609222 0.7288424372673035 2.727541208267212\n",
      "hidden_states min max: -18.52264976501465 10.87384033203125\n",
      "hidden_state minus mean squared max: 312.30126953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.40457153320312 0.6441177129745483\n",
      "loss   201: 2.2756   grad norm: 0.6704          model param norm: 83.2631        \n",
      "\n",
      "quiet_star_policy_loss= -0.025130271911621094\n",
      "nll_loss= 2.3030431270599365\n",
      "avg_std= 0.7252428531646729\n",
      "dist std min max: 0.2029428631067276 0.7252428531646729 2.7730133533477783\n",
      "hidden_states min max: -12.834375381469727 12.530681610107422\n",
      "hidden_state minus mean squared max: 147.9101104736328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.0308837890625 0.6360948085784912\n",
      "loss   202: 2.2779   grad norm: 0.6711          model param norm: 83.2700        \n",
      "\n",
      "quiet_star_policy_loss= -0.037937212735414505\n",
      "nll_loss= 2.311655044555664\n",
      "avg_std= 0.7251064777374268\n",
      "dist std min max: 0.20739425718784332 0.7251064777374268 2.7906711101531982\n",
      "hidden_states min max: -13.04375171661377 11.924010276794434\n",
      "hidden_state minus mean squared max: 185.9063720703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.14521026611328 0.6343920230865479\n",
      "loss   203: 2.2737   grad norm: 0.6961          model param norm: 83.2766        \n",
      "\n",
      "quiet_star_policy_loss= -0.07650013267993927\n",
      "nll_loss= 2.3179798126220703\n",
      "avg_std= 0.7258660197257996\n",
      "dist std min max: 0.2072380632162094 0.7258660197257996 3.026519775390625\n",
      "hidden_states min max: -14.290160179138184 12.058504104614258\n",
      "hidden_state minus mean squared max: 264.23138427734375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.32097625732422 0.6358330249786377\n",
      "loss   204: 2.2415   grad norm: 0.6852          model param norm: 83.2833        \n",
      "\n",
      "quiet_star_policy_loss= -0.04127540811896324\n",
      "nll_loss= 2.3121159076690674\n",
      "avg_std= 0.7250516414642334\n",
      "dist std min max: 0.20748412609100342 0.7250516414642334 2.7333457469940186\n",
      "hidden_states min max: -17.243345260620117 11.822330474853516\n",
      "hidden_state minus mean squared max: 271.1255798339844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.3338851928711 0.6341859102249146\n",
      "loss   205: 2.2708   grad norm: 0.6720          model param norm: 83.2897        \n",
      "\n",
      "quiet_star_policy_loss= -0.040007926523685455\n",
      "nll_loss= 2.3068933486938477\n",
      "avg_std= 0.7239443063735962\n",
      "dist std min max: 0.20709645748138428 0.7239443063735962 2.8754665851593018\n",
      "hidden_states min max: -15.05354118347168 11.299914360046387\n",
      "hidden_state minus mean squared max: 174.13722229003906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.11249542236328 0.6306055784225464\n",
      "loss   206: 2.2669   grad norm: 0.6842          model param norm: 83.2962        \n",
      "\n",
      "quiet_star_policy_loss= 0.015521049499511719\n",
      "nll_loss= 2.3052196502685547\n",
      "avg_std= 0.7199553847312927\n",
      "dist std min max: 0.2111486941576004 0.7199553847312927 2.856884002685547\n",
      "hidden_states min max: -15.036693572998047 10.708109855651855\n",
      "hidden_state minus mean squared max: 266.5103454589844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.32530212402344 0.6341037750244141\n",
      "loss   207: 2.3207   grad norm: 0.6625          model param norm: 83.3025        \n",
      "\n",
      "quiet_star_policy_loss= -0.04337453842163086\n",
      "nll_loss= 2.2948379516601562\n",
      "avg_std= 0.7200664281845093\n",
      "dist std min max: 0.209220290184021 0.7200664281845093 2.8598897457122803\n",
      "hidden_states min max: -12.68712043762207 11.555758476257324\n",
      "hidden_state minus mean squared max: 127.72134399414062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9020004272461 0.6176575422286987\n",
      "loss   208: 2.2515   grad norm: 0.6801          model param norm: 83.3086        \n",
      "\n",
      "quiet_star_policy_loss= -0.06398587673902512\n",
      "nll_loss= 2.304702043533325\n",
      "avg_std= 0.7185208201408386\n",
      "dist std min max: 0.20892150700092316 0.7185208201408386 2.8864023685455322\n",
      "hidden_states min max: -13.463053703308105 11.142193794250488\n",
      "hidden_state minus mean squared max: 152.52130126953125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.04621887207031 0.6466464996337891\n",
      "loss   209: 2.2407   grad norm: 0.6475          model param norm: 83.3145        \n",
      "\n",
      "quiet_star_policy_loss= 0.018678857013583183\n",
      "nll_loss= 2.285626173019409\n",
      "avg_std= 0.7164046764373779\n",
      "dist std min max: 0.20338700711727142 0.7164046764373779 2.766961097717285\n",
      "hidden_states min max: -11.041691780090332 12.405044555664062\n",
      "hidden_state minus mean squared max: 116.22425079345703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80706787109375 0.6278893947601318\n",
      "loss   210: 2.3043   grad norm: 0.6731          model param norm: 83.3203        \n",
      "\n",
      "quiet_star_policy_loss= -0.02231454849243164\n",
      "nll_loss= 2.2809360027313232\n",
      "avg_std= 0.7150045037269592\n",
      "dist std min max: 0.21062448620796204 0.7150045037269592 2.8642406463623047\n",
      "hidden_states min max: -12.576894760131836 11.64934253692627\n",
      "hidden_state minus mean squared max: 160.1527557373047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07063293457031 0.6342109441757202\n",
      "loss   211: 2.2586   grad norm: 0.6412          model param norm: 83.3260        \n",
      "\n",
      "quiet_star_policy_loss= -0.06425847858190536\n",
      "nll_loss= 2.2895259857177734\n",
      "avg_std= 0.7144383788108826\n",
      "dist std min max: 0.20842555165290833 0.7144383788108826 2.825300693511963\n",
      "hidden_states min max: -13.853955268859863 11.33906078338623\n",
      "hidden_state minus mean squared max: 162.8286590576172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07892608642578 0.6171053647994995\n",
      "loss   212: 2.2253   grad norm: 0.6662          model param norm: 83.3321        \n",
      "\n",
      "quiet_star_policy_loss= -0.06364560127258301\n",
      "nll_loss= 2.297576904296875\n",
      "avg_std= 0.7127813696861267\n",
      "dist std min max: 0.2040071338415146 0.7127813696861267 2.8945884704589844\n",
      "hidden_states min max: -10.39804458618164 10.825145721435547\n",
      "hidden_state minus mean squared max: 97.97895050048828\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.4030532836914 0.6273102760314941\n",
      "loss   213: 2.2339   grad norm: 0.7122          model param norm: 83.3383        \n",
      "\n",
      "quiet_star_policy_loss= -0.045069124549627304\n",
      "nll_loss= 2.287916660308838\n",
      "avg_std= 0.7095866799354553\n",
      "dist std min max: 0.20790275931358337 0.7095866799354553 2.911156177520752\n",
      "hidden_states min max: -12.273689270019531 10.426645278930664\n",
      "hidden_state minus mean squared max: 127.58332061767578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.95695495605469 0.6102168560028076\n",
      "loss   214: 2.2428   grad norm: 0.6569          model param norm: 83.3447        \n",
      "\n",
      "quiet_star_policy_loss= -0.05168202146887779\n",
      "nll_loss= 2.279526472091675\n",
      "avg_std= 0.7127844095230103\n",
      "dist std min max: 0.20726250112056732 0.7127844095230103 2.99613618850708\n",
      "hidden_states min max: -15.262859344482422 11.039987564086914\n",
      "hidden_state minus mean squared max: 207.5021209716797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20014953613281 0.6182934045791626\n",
      "loss   215: 2.2278   grad norm: 0.6691          model param norm: 83.3512        \n",
      "\n",
      "quiet_star_policy_loss= -0.020763589069247246\n",
      "nll_loss= 2.287926197052002\n",
      "avg_std= 0.7137985825538635\n",
      "dist std min max: 0.20649032294750214 0.7137985825538635 2.9845123291015625\n",
      "hidden_states min max: -14.356220245361328 10.888969421386719\n",
      "hidden_state minus mean squared max: 161.5620880126953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07500457763672 0.6490095853805542\n",
      "loss   216: 2.2672   grad norm: 0.6407          model param norm: 83.3578        \n",
      "\n",
      "quiet_star_policy_loss= -0.04506836086511612\n",
      "nll_loss= 2.286043167114258\n",
      "avg_std= 0.711064338684082\n",
      "dist std min max: 0.20514611899852753 0.711064338684082 2.97062349319458\n",
      "hidden_states min max: -15.52164077758789 12.024287223815918\n",
      "hidden_state minus mean squared max: 232.17623901367188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2563247680664 0.6302832365036011\n",
      "loss   217: 2.2410   grad norm: 0.6652          model param norm: 83.3643        \n",
      "\n",
      "quiet_star_policy_loss= -0.010317707434296608\n",
      "nll_loss= 2.2916600704193115\n",
      "avg_std= 0.7122029066085815\n",
      "dist std min max: 0.2029627114534378 0.7122029066085815 2.933534622192383\n",
      "hidden_states min max: -11.120665550231934 10.34307861328125\n",
      "hidden_state minus mean squared max: 117.35225677490234\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.79470825195312 0.6605269908905029\n",
      "loss   218: 2.2813   grad norm: 0.6809          model param norm: 83.3709        \n",
      "\n",
      "quiet_star_policy_loss= -0.01835041120648384\n",
      "nll_loss= 2.280630350112915\n",
      "avg_std= 0.705988347530365\n",
      "dist std min max: 0.2011731117963791 0.705988347530365 3.125032663345337\n",
      "hidden_states min max: -12.382823944091797 11.00361442565918\n",
      "hidden_state minus mean squared max: 201.69705200195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.18596649169922 0.6531003713607788\n",
      "loss   219: 2.2623   grad norm: 0.7213          model param norm: 83.3773        \n",
      "\n",
      "quiet_star_policy_loss= -0.009614372625946999\n",
      "nll_loss= 2.2871665954589844\n",
      "avg_std= 0.7044786810874939\n",
      "dist std min max: 0.20319817960262299 0.7044786810874939 2.9596641063690186\n",
      "hidden_states min max: -14.894365310668945 11.419663429260254\n",
      "hidden_state minus mean squared max: 208.7756805419922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20321655273438 0.6596139669418335\n",
      "loss   220: 2.2776   grad norm: 0.6343          model param norm: 83.3840        \n",
      "\n",
      "quiet_star_policy_loss= 0.019916916266083717\n",
      "nll_loss= 2.286771774291992\n",
      "avg_std= 0.7092922329902649\n",
      "dist std min max: 0.19983701407909393 0.7092922329902649 3.200153350830078\n",
      "hidden_states min max: -10.069805145263672 11.098779678344727\n",
      "hidden_state minus mean squared max: 86.94901275634766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.63082885742188 0.668117880821228\n",
      "loss   221: 2.3067   grad norm: 0.6940          model param norm: 83.3905        \n",
      "\n",
      "quiet_star_policy_loss= -0.0031970979180186987\n",
      "nll_loss= 2.273839235305786\n",
      "avg_std= 0.7036294937133789\n",
      "dist std min max: 0.20185424387454987 0.7036294937133789 3.1371712684631348\n",
      "hidden_states min max: -9.394500732421875 11.024209976196289\n",
      "hidden_state minus mean squared max: 98.92730712890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.57965087890625 0.6613571643829346\n",
      "loss   222: 2.2706   grad norm: 0.6698          model param norm: 83.3969        \n",
      "\n",
      "quiet_star_policy_loss= 0.09211263060569763\n",
      "nll_loss= 2.244407892227173\n",
      "avg_std= 0.7005634307861328\n",
      "dist std min max: 0.20089887082576752 0.7005634307861328 3.017681121826172\n",
      "hidden_states min max: -10.327620506286621 10.064214706420898\n",
      "hidden_state minus mean squared max: 84.83354187011719\n",
      "hidden_state minus mean divided by std max: 4.900964260101318\n",
      "log_prob min max: -103.66685485839844 0.6721221208572388\n",
      "loss   223: 2.3365   grad norm: 1.3300          model param norm: 83.4033        \n",
      "\n",
      "quiet_star_policy_loss= -0.05528412014245987\n",
      "nll_loss= 2.2694270610809326\n",
      "avg_std= 0.7064634561538696\n",
      "dist std min max: 0.1995696872472763 0.7064634561538696 3.102078437805176\n",
      "hidden_states min max: -12.296544075012207 10.70986557006836\n",
      "hidden_state minus mean squared max: 120.37447357177734\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8333969116211 0.6677972078323364\n",
      "loss   224: 2.2141   grad norm: 0.6728          model param norm: 83.4095        \n",
      "\n",
      "quiet_star_policy_loss= -0.0335119254887104\n",
      "nll_loss= 2.260944366455078\n",
      "avg_std= 0.7072058320045471\n",
      "dist std min max: 0.20160269737243652 0.7072058320045471 3.1706743240356445\n",
      "hidden_states min max: -14.00025463104248 10.655593872070312\n",
      "hidden_state minus mean squared max: 203.2531280517578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.18980407714844 0.6757160425186157\n",
      "loss   225: 2.2274   grad norm: 0.7184          model param norm: 83.4160        \n",
      "\n",
      "quiet_star_policy_loss= -0.07206306606531143\n",
      "nll_loss= 2.2810635566711426\n",
      "avg_std= 0.7060104608535767\n",
      "dist std min max: 0.1970304697751999 0.7060104608535767 3.0819287300109863\n",
      "hidden_states min max: -9.816767692565918 10.951301574707031\n",
      "hidden_state minus mean squared max: 86.60245513916016\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.68844604492188 0.6877453327178955\n",
      "loss   226: 2.2090   grad norm: 0.7187          model param norm: 83.4227        \n",
      "\n",
      "quiet_star_policy_loss= -0.052238013595342636\n",
      "nll_loss= 2.2662646770477295\n",
      "avg_std= 0.7036551833152771\n",
      "dist std min max: 0.1959460973739624 0.7036551833152771 3.2546305656433105\n",
      "hidden_states min max: -10.118111610412598 10.239957809448242\n",
      "hidden_state minus mean squared max: 96.75714874267578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.6661376953125 0.7097824811935425\n",
      "loss   227: 2.2140   grad norm: 0.7043          model param norm: 83.4295        \n",
      "\n",
      "quiet_star_policy_loss= -0.04532146453857422\n",
      "nll_loss= 2.258439540863037\n",
      "avg_std= 0.7019999623298645\n",
      "dist std min max: 0.19673827290534973 0.7019999623298645 3.1660990715026855\n",
      "hidden_states min max: -23.93651008605957 12.22870922088623\n",
      "hidden_state minus mean squared max: 582.1764526367188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.7159652709961 0.7051191329956055\n",
      "loss   228: 2.2131   grad norm: 0.7193          model param norm: 83.4363        \n",
      "\n",
      "quiet_star_policy_loss= -0.033002663403749466\n",
      "nll_loss= 2.2644546031951904\n",
      "avg_std= 0.7001684904098511\n",
      "dist std min max: 0.19590210914611816 0.7001684904098511 3.2072248458862305\n",
      "hidden_states min max: -9.737432479858398 10.530207633972168\n",
      "hidden_state minus mean squared max: 93.60277557373047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.48414611816406 0.6948592662811279\n",
      "loss   229: 2.2315   grad norm: 0.6940          model param norm: 83.4430        \n",
      "\n",
      "quiet_star_policy_loss= -0.04468994215130806\n",
      "nll_loss= 2.2423462867736816\n",
      "avg_std= 0.7018256187438965\n",
      "dist std min max: 0.19511546194553375 0.7018256187438965 3.338329792022705\n",
      "hidden_states min max: -15.520804405212402 10.528847694396973\n",
      "hidden_state minus mean squared max: 221.47044372558594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.23271942138672 0.7013826370239258\n",
      "loss   230: 2.1977   grad norm: 0.6947          model param norm: 83.4498        \n",
      "\n",
      "quiet_star_policy_loss= -0.07145319133996964\n",
      "nll_loss= 2.2735674381256104\n",
      "avg_std= 0.6961055994033813\n",
      "dist std min max: 0.1919453889131546 0.6961055994033813 3.4970312118530273\n",
      "hidden_states min max: -11.187264442443848 10.876620292663574\n",
      "hidden_state minus mean squared max: 112.7839584350586\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78238677978516 0.7249207496643066\n",
      "loss   231: 2.2021   grad norm: 0.6971          model param norm: 83.4565        \n",
      "\n",
      "quiet_star_policy_loss= -0.04543624073266983\n",
      "nll_loss= 2.266213893890381\n",
      "avg_std= 0.69573974609375\n",
      "dist std min max: 0.18914663791656494 0.69573974609375 3.230041027069092\n",
      "hidden_states min max: -14.096445083618164 10.161966323852539\n",
      "hidden_state minus mean squared max: 233.46009826660156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25908660888672 0.7167966365814209\n",
      "loss   232: 2.2208   grad norm: 0.7054          model param norm: 83.4630        \n",
      "\n",
      "quiet_star_policy_loss= -0.06548991054296494\n",
      "nll_loss= 2.265413999557495\n",
      "avg_std= 0.6952322125434875\n",
      "dist std min max: 0.18849200010299683 0.6952322125434875 3.50193190574646\n",
      "hidden_states min max: -24.155994415283203 10.523347854614258\n",
      "hidden_state minus mean squared max: 533.0180053710938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.67184448242188 0.7345367670059204\n",
      "loss   233: 2.1999   grad norm: 0.7128          model param norm: 83.4694        \n",
      "\n",
      "quiet_star_policy_loss= -0.0031932832207530737\n",
      "nll_loss= 2.2572684288024902\n",
      "avg_std= 0.6935647130012512\n",
      "dist std min max: 0.18395806849002838 0.6935647130012512 3.2881839275360107\n",
      "hidden_states min max: -14.58836555480957 11.050066947937012\n",
      "hidden_state minus mean squared max: 183.79136657714844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.13948822021484 0.7447888851165771\n",
      "loss   234: 2.2541   grad norm: 0.6814          model param norm: 83.4759        \n",
      "\n",
      "quiet_star_policy_loss= 0.032608795911073685\n",
      "nll_loss= 2.2535400390625\n",
      "avg_std= 0.6940919160842896\n",
      "dist std min max: 0.18515288829803467 0.6940919160842896 3.2873587608337402\n",
      "hidden_states min max: -11.425122261047363 11.235492706298828\n",
      "hidden_state minus mean squared max: 119.03126525878906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78462219238281 0.752297043800354\n",
      "loss   235: 2.2861   grad norm: 0.6949          model param norm: 83.4825        \n",
      "\n",
      "quiet_star_policy_loss= -0.013832665048539639\n",
      "nll_loss= 2.2500178813934326\n",
      "avg_std= 0.6923681497573853\n",
      "dist std min max: 0.18342064321041107 0.6923681497573853 3.3082923889160156\n",
      "hidden_states min max: -21.840421676635742 12.017810821533203\n",
      "hidden_state minus mean squared max: 430.7456970214844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.56533813476562 0.7552422285079956\n",
      "loss   236: 2.2362   grad norm: 0.7112          model param norm: 83.4890        \n",
      "\n",
      "quiet_star_policy_loss= -0.04511613771319389\n",
      "nll_loss= 2.2485930919647217\n",
      "avg_std= 0.6917527914047241\n",
      "dist std min max: 0.18230502307415009 0.6917527914047241 3.5175955295562744\n",
      "hidden_states min max: -11.095040321350098 11.270933151245117\n",
      "hidden_state minus mean squared max: 120.28024291992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83782958984375 0.7610666751861572\n",
      "loss   237: 2.2035   grad norm: 0.7075          model param norm: 83.4956        \n",
      "\n",
      "quiet_star_policy_loss= -0.023787690326571465\n",
      "nll_loss= 2.2465054988861084\n",
      "avg_std= 0.6912147402763367\n",
      "dist std min max: 0.18320603668689728 0.6912147402763367 3.2619564533233643\n",
      "hidden_states min max: -12.554924964904785 11.22568130493164\n",
      "hidden_state minus mean squared max: 159.9889373779297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.07013702392578 0.7692303657531738\n",
      "loss   238: 2.2227   grad norm: 0.7325          model param norm: 83.5021        \n",
      "\n",
      "quiet_star_policy_loss= -0.02168731763958931\n",
      "nll_loss= 2.249213695526123\n",
      "avg_std= 0.6904841661453247\n",
      "dist std min max: 0.1828983873128891 0.6904841661453247 3.291577100753784\n",
      "hidden_states min max: -23.968542098999023 11.230537414550781\n",
      "hidden_state minus mean squared max: 520.035400390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.65951538085938 0.7688947916030884\n",
      "loss   239: 2.2275   grad norm: 0.6848          model param norm: 83.5089        \n",
      "\n",
      "quiet_star_policy_loss= -0.0029469490982592106\n",
      "nll_loss= 2.25504732131958\n",
      "avg_std= 0.6933957934379578\n",
      "dist std min max: 0.1809108555316925 0.6933957934379578 3.1623497009277344\n",
      "hidden_states min max: -12.24482536315918 11.698722839355469\n",
      "hidden_state minus mean squared max: 136.57730102539062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9361343383789 0.7829633951187134\n",
      "loss   240: 2.2521   grad norm: 0.7271          model param norm: 83.5157        \n",
      "\n",
      "quiet_star_policy_loss= -0.031703948974609375\n",
      "nll_loss= 2.259211301803589\n",
      "avg_std= 0.6913781762123108\n",
      "dist std min max: 0.17933811247348785 0.6913781762123108 3.260690450668335\n",
      "hidden_states min max: -10.435787200927734 11.902912139892578\n",
      "hidden_state minus mean squared max: 113.36000061035156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83927154541016 0.7917599678039551\n",
      "loss   241: 2.2275   grad norm: 0.7320          model param norm: 83.5221        \n",
      "\n",
      "quiet_star_policy_loss= -0.032573699951171875\n",
      "nll_loss= 2.2467904090881348\n",
      "avg_std= 0.6937955617904663\n",
      "dist std min max: 0.17809748649597168 0.6937955617904663 3.44307541847229\n",
      "hidden_states min max: -10.945109367370605 10.80052375793457\n",
      "hidden_state minus mean squared max: 119.30892181396484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.92343139648438 0.7882733345031738\n",
      "loss   242: 2.2142   grad norm: 0.7136          model param norm: 83.5285        \n",
      "\n",
      "quiet_star_policy_loss= -0.03607075288891792\n",
      "nll_loss= 2.240770101547241\n",
      "avg_std= 0.6922652125358582\n",
      "dist std min max: 0.178850919008255 0.6922652125358582 3.4314217567443848\n",
      "hidden_states min max: -22.47252655029297 11.840909957885742\n",
      "hidden_state minus mean squared max: 584.7599487304688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.71817016601562 0.797484278678894\n",
      "loss   243: 2.2047   grad norm: 0.6920          model param norm: 83.5349        \n",
      "\n",
      "quiet_star_policy_loss= -0.015089607797563076\n",
      "nll_loss= 2.231426477432251\n",
      "avg_std= 0.6972454786300659\n",
      "dist std min max: 0.17937342822551727 0.6972454786300659 3.3795931339263916\n",
      "hidden_states min max: -16.2996883392334 13.065153121948242\n",
      "hidden_state minus mean squared max: 239.75889587402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.27236938476562 0.7795267105102539\n",
      "loss   244: 2.2163   grad norm: 0.6993          model param norm: 83.5411        \n",
      "\n",
      "quiet_star_policy_loss= -0.05226192623376846\n",
      "nll_loss= 2.226383924484253\n",
      "avg_std= 0.6929225921630859\n",
      "dist std min max: 0.17428743839263916 0.6929225921630859 3.3605406284332275\n",
      "hidden_states min max: -10.652050018310547 12.48132038116455\n",
      "hidden_state minus mean squared max: 131.81202697753906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.91226959228516 0.7952953577041626\n",
      "loss   245: 2.1741   grad norm: 0.7113          model param norm: 83.5474        \n",
      "\n",
      "quiet_star_policy_loss= 0.014793909154832363\n",
      "nll_loss= 2.244121551513672\n",
      "avg_std= 0.6928457021713257\n",
      "dist std min max: 0.17594695091247559 0.6928457021713257 3.2489380836486816\n",
      "hidden_states min max: -11.722763061523438 11.826495170593262\n",
      "hidden_state minus mean squared max: 120.68572998046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.87921905517578 0.798642635345459\n",
      "loss   246: 2.2589   grad norm: 0.6832          model param norm: 83.5535        \n",
      "\n",
      "quiet_star_policy_loss= -0.034693147987127304\n",
      "nll_loss= 2.2349190711975098\n",
      "avg_std= 0.6970833539962769\n",
      "dist std min max: 0.17577314376831055 0.6970833539962769 3.349884271621704\n",
      "hidden_states min max: -10.757399559020996 11.496020317077637\n",
      "hidden_state minus mean squared max: 137.45175170898438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99423217773438 0.8143360614776611\n",
      "loss   247: 2.2002   grad norm: 0.6945          model param norm: 83.5597        \n",
      "\n",
      "quiet_star_policy_loss= -0.0794529914855957\n",
      "nll_loss= 2.2365500926971436\n",
      "avg_std= 0.6923068165779114\n",
      "dist std min max: 0.17425023019313812 0.6923068165779114 3.7929322719573975\n",
      "hidden_states min max: -16.080842971801758 12.279464721679688\n",
      "hidden_state minus mean squared max: 247.88601684570312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2890625 0.8257595300674438\n",
      "loss   248: 2.1571   grad norm: 0.7295          model param norm: 83.5660        \n",
      "\n",
      "quiet_star_policy_loss= -0.051468659192323685\n",
      "nll_loss= 2.22759747505188\n",
      "avg_std= 0.6938233375549316\n",
      "dist std min max: 0.17277759313583374 0.6938233375549316 3.4107954502105713\n",
      "hidden_states min max: -12.571167945861816 13.002304077148438\n",
      "hidden_state minus mean squared max: 160.4543914794922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.51325988769531 0.8250561952590942\n",
      "loss   249: 2.1761   grad norm: 0.7114          model param norm: 83.5719        \n",
      "\n",
      "quiet_star_policy_loss= -0.03782382234930992\n",
      "nll_loss= 2.237638235092163\n",
      "avg_std= 0.693596363067627\n",
      "dist std min max: 0.17238382995128632 0.693596363067627 3.2390003204345703\n",
      "hidden_states min max: -12.988903045654297 13.651455879211426\n",
      "hidden_state minus mean squared max: 199.26736450195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17990112304688 0.8204898834228516\n",
      "loss   250: 2.1998   grad norm: 0.7489          model param norm: 83.5778        \n",
      "\n",
      "quiet_star_policy_loss= -0.04918375611305237\n",
      "nll_loss= 2.2256014347076416\n",
      "avg_std= 0.6919105648994446\n",
      "dist std min max: 0.17326022684574127 0.6919105648994446 3.191770076751709\n",
      "hidden_states min max: -13.04509449005127 11.292753219604492\n",
      "hidden_state minus mean squared max: 135.58331298828125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.98737335205078 0.8218544721603394\n",
      "loss   251: 2.1764   grad norm: 1.4642          model param norm: 83.5836        \n",
      "\n",
      "quiet_star_policy_loss= 0.002120447112247348\n",
      "nll_loss= 2.219139575958252\n",
      "avg_std= 0.6893723607063293\n",
      "dist std min max: 0.16973969340324402 0.6893723607063293 3.3797576427459717\n",
      "hidden_states min max: -25.735450744628906 12.072832107543945\n",
      "hidden_state minus mean squared max: 622.36962890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.74934387207031 0.8453177213668823\n",
      "loss   252: 2.2213   grad norm: 0.7034          model param norm: 83.5894        \n",
      "\n",
      "quiet_star_policy_loss= -0.008756065741181374\n",
      "nll_loss= 2.2359778881073\n",
      "avg_std= 0.691003143787384\n",
      "dist std min max: 0.16878454387187958 0.691003143787384 3.6361303329467773\n",
      "hidden_states min max: -11.586211204528809 12.401839256286621\n",
      "hidden_state minus mean squared max: 129.01605224609375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.810298919677734 0.854333758354187\n",
      "loss   253: 2.2272   grad norm: 0.7527          model param norm: 83.5952        \n",
      "\n",
      "quiet_star_policy_loss= -0.000388908403692767\n",
      "nll_loss= 2.2387044429779053\n",
      "avg_std= 0.6867128014564514\n",
      "dist std min max: 0.16841302812099457 0.6867128014564514 3.3898377418518066\n",
      "hidden_states min max: -15.463229179382324 13.256649017333984\n",
      "hidden_state minus mean squared max: 214.5345916748047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2168197631836 0.8480745553970337\n",
      "loss   254: 2.2383   grad norm: 0.7215          model param norm: 83.6012        \n",
      "\n",
      "quiet_star_policy_loss= 0.004968691151589155\n",
      "nll_loss= 2.2217023372650146\n",
      "avg_std= 0.6903864741325378\n",
      "dist std min max: 0.16667771339416504 0.6903864741325378 3.545762777328491\n",
      "hidden_states min max: -32.60599136352539 12.122801780700684\n",
      "hidden_state minus mean squared max: 998.3816528320312\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -104.98564147949219 0.8579956293106079\n",
      "loss   255: 2.2267   grad norm: 0.7186          model param norm: 83.6073        \n",
      "\n",
      "quiet_star_policy_loss= -0.06237459182739258\n",
      "nll_loss= 2.235520601272583\n",
      "avg_std= 0.6899190545082092\n",
      "dist std min max: 0.16652832925319672 0.6899190545082092 3.472290515899658\n",
      "hidden_states min max: -19.61308479309082 12.501067161560059\n",
      "hidden_state minus mean squared max: 361.5868835449219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.47783660888672 0.8433938026428223\n",
      "loss   256: 2.1731   grad norm: 0.7156          model param norm: 83.6137        \n",
      "\n",
      "quiet_star_policy_loss= -0.009131384082138538\n",
      "nll_loss= 2.2253940105438232\n",
      "avg_std= 0.6877977252006531\n",
      "dist std min max: 0.16644686460494995 0.6877977252006531 3.465017795562744\n",
      "hidden_states min max: -32.61039352416992 11.83163070678711\n",
      "hidden_state minus mean squared max: 966.4266967773438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.9693603515625 0.8619465827941895\n",
      "loss   257: 2.2163   grad norm: 0.6802          model param norm: 83.6201        \n",
      "\n",
      "quiet_star_policy_loss= -0.038690950721502304\n",
      "nll_loss= 2.2459261417388916\n",
      "avg_std= 0.6913990378379822\n",
      "dist std min max: 0.1644809991121292 0.6913990378379822 3.533623695373535\n",
      "hidden_states min max: -11.336405754089355 14.365891456604004\n",
      "hidden_state minus mean squared max: 169.7967071533203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83409118652344 0.8700345754623413\n",
      "loss   258: 2.2072   grad norm: 0.7129          model param norm: 83.6264        \n",
      "\n",
      "quiet_star_policy_loss= -0.0040184021927416325\n",
      "nll_loss= 2.2286674976348877\n",
      "avg_std= 0.6882761716842651\n",
      "dist std min max: 0.1631118655204773 0.6882761716842651 3.5966944694519043\n",
      "hidden_states min max: -22.358217239379883 12.450116157531738\n",
      "hidden_state minus mean squared max: 467.20111083984375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.60594940185547 0.8911228179931641\n",
      "loss   259: 2.2246   grad norm: 0.7594          model param norm: 83.6328        \n",
      "\n",
      "quiet_star_policy_loss= 0.0037115097511559725\n",
      "nll_loss= 2.224055051803589\n",
      "avg_std= 0.6883718371391296\n",
      "dist std min max: 0.16150756180286407 0.6883718371391296 3.395009994506836\n",
      "hidden_states min max: -24.977705001831055 12.976716995239258\n",
      "hidden_state minus mean squared max: 678.9155883789062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.7928237915039 0.8886245489120483\n",
      "loss   260: 2.2278   grad norm: 0.7350          model param norm: 83.6392        \n",
      "\n",
      "quiet_star_policy_loss= -0.04064784198999405\n",
      "nll_loss= 2.210447072982788\n",
      "avg_std= 0.6906389594078064\n",
      "dist std min max: 0.15974929928779602 0.6906389594078064 3.5430965423583984\n",
      "hidden_states min max: -13.187788963317871 12.721899032592773\n",
      "hidden_state minus mean squared max: 187.3857879638672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.149169921875 0.9014291763305664\n",
      "loss   261: 2.1698   grad norm: 0.7308          model param norm: 83.6456        \n",
      "\n",
      "quiet_star_policy_loss= -0.09804830700159073\n",
      "nll_loss= 2.2070419788360596\n",
      "avg_std= 0.6881834268569946\n",
      "dist std min max: 0.15686893463134766 0.6881834268569946 3.5600292682647705\n",
      "hidden_states min max: -11.945584297180176 12.394582748413086\n",
      "hidden_state minus mean squared max: 139.431640625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7417221069336 0.9065060615539551\n",
      "loss   262: 2.1090   grad norm: 0.7118          model param norm: 83.6522        \n",
      "\n",
      "quiet_star_policy_loss= 0.0029117583762854338\n",
      "nll_loss= 2.226806402206421\n",
      "avg_std= 0.6895231008529663\n",
      "dist std min max: 0.1580793857574463 0.6895231008529663 3.5817887783050537\n",
      "hidden_states min max: -13.947918891906738 12.715771675109863\n",
      "hidden_state minus mean squared max: 211.46768188476562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2096176147461 0.9139600992202759\n",
      "loss   263: 2.2297   grad norm: 0.7479          model param norm: 83.6588        \n",
      "\n",
      "quiet_star_policy_loss= -0.014888430014252663\n",
      "nll_loss= 2.214451551437378\n",
      "avg_std= 0.6887426376342773\n",
      "dist std min max: 0.1549796164035797 0.6887426376342773 3.4221749305725098\n",
      "hidden_states min max: -14.117743492126465 13.318465232849121\n",
      "hidden_state minus mean squared max: 211.41757202148438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20950317382812 0.9319441318511963\n",
      "loss   264: 2.1996   grad norm: 0.7191          model param norm: 83.6650        \n",
      "\n",
      "quiet_star_policy_loss= -0.050086405128240585\n",
      "nll_loss= 2.21003794670105\n",
      "avg_std= 0.6887098550796509\n",
      "dist std min max: 0.15290245413780212 0.6887098550796509 3.6940317153930664\n",
      "hidden_states min max: -30.53375816345215 12.077094078063965\n",
      "hidden_state minus mean squared max: 869.7276000976562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.91666412353516 0.9465240240097046\n",
      "loss   265: 2.1600   grad norm: 0.7552          model param norm: 83.6711        \n",
      "\n",
      "quiet_star_policy_loss= -0.03660764917731285\n",
      "nll_loss= 2.1971213817596436\n",
      "avg_std= 0.6885192394256592\n",
      "dist std min max: 0.15098640322685242 0.6885192394256592 3.623976707458496\n",
      "hidden_states min max: -21.674062728881836 13.319015502929688\n",
      "hidden_state minus mean squared max: 421.8634338378906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5549087524414 0.9674816131591797\n",
      "loss   266: 2.1605   grad norm: 0.7621          model param norm: 83.6774        \n",
      "\n",
      "quiet_star_policy_loss= -0.016531754285097122\n",
      "nll_loss= 2.206557035446167\n",
      "avg_std= 0.6936237812042236\n",
      "dist std min max: 0.1491825431585312 0.6936237812042236 3.645512104034424\n",
      "hidden_states min max: -26.464950561523438 12.76778793334961\n",
      "hidden_state minus mean squared max: 622.5286254882812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.74947357177734 0.9686553478240967\n",
      "loss   267: 2.1900   grad norm: 0.6873          model param norm: 83.6839        \n",
      "\n",
      "quiet_star_policy_loss= -0.03178916126489639\n",
      "nll_loss= 2.212938070297241\n",
      "avg_std= 0.6855292320251465\n",
      "dist std min max: 0.14637619256973267 0.6855292320251465 3.4359211921691895\n",
      "hidden_states min max: -12.428970336914062 12.396849632263184\n",
      "hidden_state minus mean squared max: 143.16049194335938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.75123596191406 0.9919576644897461\n",
      "loss   268: 2.1811   grad norm: 0.7509          model param norm: 83.6900        \n",
      "\n",
      "quiet_star_policy_loss= -0.0406557098031044\n",
      "nll_loss= 2.2034504413604736\n",
      "avg_std= 0.6905038952827454\n",
      "dist std min max: 0.14525379240512848 0.6905038952827454 3.538639545440674\n",
      "hidden_states min max: -12.95047378540039 12.374515533447266\n",
      "hidden_state minus mean squared max: 161.93936157226562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.06702423095703 1.0019323825836182\n",
      "loss   269: 2.1628   grad norm: 0.7240          model param norm: 83.6960        \n",
      "\n",
      "quiet_star_policy_loss= -0.06312809139490128\n",
      "nll_loss= 2.21040415763855\n",
      "avg_std= 0.6870140433311462\n",
      "dist std min max: 0.14440971612930298 0.6870140433311462 3.5510635375976562\n",
      "hidden_states min max: -11.749761581420898 12.638626098632812\n",
      "hidden_state minus mean squared max: 158.78004455566406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.02873992919922 1.0124235153198242\n",
      "loss   270: 2.1473   grad norm: 0.7408          model param norm: 83.7019        \n",
      "\n",
      "quiet_star_policy_loss= -0.09241180866956711\n",
      "nll_loss= 2.2006874084472656\n",
      "avg_std= 0.6873708367347717\n",
      "dist std min max: 0.14437666535377502 0.6873708367347717 3.4387285709381104\n",
      "hidden_states min max: -17.65300178527832 11.661115646362305\n",
      "hidden_state minus mean squared max: 365.06005859375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.48260498046875 1.0070226192474365\n",
      "loss   271: 2.1083   grad norm: 0.7530          model param norm: 83.7078        \n",
      "\n",
      "quiet_star_policy_loss= -0.047686006873846054\n",
      "nll_loss= 2.1987977027893066\n",
      "avg_std= 0.686400830745697\n",
      "dist std min max: 0.1438387781381607 0.686400830745697 3.5593345165252686\n",
      "hidden_states min max: -12.254061698913574 12.800443649291992\n",
      "hidden_state minus mean squared max: 149.7736358642578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.87413024902344 1.0108239650726318\n",
      "loss   272: 2.1511   grad norm: 0.7299          model param norm: 83.7137        \n",
      "\n",
      "quiet_star_policy_loss= -0.020738601684570312\n",
      "nll_loss= 2.203386068344116\n",
      "avg_std= 0.6865503787994385\n",
      "dist std min max: 0.1447533816099167 0.6865503787994385 3.491936683654785\n",
      "hidden_states min max: -14.9542236328125 11.81893539428711\n",
      "hidden_state minus mean squared max: 188.0809326171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15101623535156 1.0067780017852783\n",
      "loss   273: 2.1826   grad norm: 0.7208          model param norm: 83.7196        \n",
      "\n",
      "quiet_star_policy_loss= -0.05147528648376465\n",
      "nll_loss= 2.1897671222686768\n",
      "avg_std= 0.6863481998443604\n",
      "dist std min max: 0.14477753639221191 0.6863481998443604 3.68796968460083\n",
      "hidden_states min max: -14.344137191772461 13.188643455505371\n",
      "hidden_state minus mean squared max: 178.051513671875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.12360382080078 1.0083786249160767\n",
      "loss   274: 2.1383   grad norm: 0.7488          model param norm: 83.7257        \n",
      "\n",
      "quiet_star_policy_loss= -0.05981483682990074\n",
      "nll_loss= 2.2088096141815186\n",
      "avg_std= 0.6863656640052795\n",
      "dist std min max: 0.14468003809452057 0.6863656640052795 3.818605899810791\n",
      "hidden_states min max: -13.09704875946045 11.620784759521484\n",
      "hidden_state minus mean squared max: 177.20042419433594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.12123107910156 1.0091826915740967\n",
      "loss   275: 2.1490   grad norm: 0.7459          model param norm: 83.7321        \n",
      "\n",
      "quiet_star_policy_loss= 0.03498678281903267\n",
      "nll_loss= 2.1910436153411865\n",
      "avg_std= 0.6882796883583069\n",
      "dist std min max: 0.14403803646564484 0.6882796883583069 3.4909005165100098\n",
      "hidden_states min max: -12.006429672241211 11.592668533325195\n",
      "hidden_state minus mean squared max: 117.69856262207031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8265151977539 1.016099214553833\n",
      "loss   276: 2.2260   grad norm: 0.7369          model param norm: 83.7382        \n",
      "\n",
      "quiet_star_policy_loss= -0.08872079849243164\n",
      "nll_loss= 2.1825456619262695\n",
      "avg_std= 0.6841439008712769\n",
      "dist std min max: 0.14494110643863678 0.6841439008712769 3.5575125217437744\n",
      "hidden_states min max: -15.155945777893066 12.067689895629883\n",
      "hidden_state minus mean squared max: 264.42303466796875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.32134246826172 1.001368761062622\n",
      "loss   277: 2.0938   grad norm: 0.7379          model param norm: 83.7443        \n",
      "\n",
      "quiet_star_policy_loss= -0.01728668250143528\n",
      "nll_loss= 2.1887402534484863\n",
      "avg_std= 0.6856269240379333\n",
      "dist std min max: 0.14513127505779266 0.6856269240379333 3.6546218395233154\n",
      "hidden_states min max: -23.08428382873535 12.13899040222168\n",
      "hidden_state minus mean squared max: 417.7036437988281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.54996490478516 0.9993023872375488\n",
      "loss   278: 2.1715   grad norm: 0.7143          model param norm: 83.7504        \n",
      "\n",
      "quiet_star_policy_loss= -0.10189545154571533\n",
      "nll_loss= 2.1883225440979004\n",
      "avg_std= 0.687420666217804\n",
      "dist std min max: 0.14699621498584747 0.687420666217804 3.701659679412842\n",
      "hidden_states min max: -13.690788269042969 10.591792106628418\n",
      "hidden_state minus mean squared max: 128.93606567382812\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.96224975585938 0.9932715892791748\n",
      "loss   279: 2.0864   grad norm: 1.5805          model param norm: 83.7567        \n",
      "\n",
      "quiet_star_policy_loss= -0.011366844177246094\n",
      "nll_loss= 2.198932409286499\n",
      "avg_std= 0.6834452748298645\n",
      "dist std min max: 0.14451396465301514 0.6834452748298645 3.510957956314087\n",
      "hidden_states min max: -12.814050674438477 13.198503494262695\n",
      "hidden_state minus mean squared max: 188.54977416992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.94732666015625 0.9960272312164307\n",
      "loss   280: 2.1876   grad norm: 0.7426          model param norm: 83.7637        \n",
      "\n",
      "quiet_star_policy_loss= -0.015887213870882988\n",
      "nll_loss= 2.188093900680542\n",
      "avg_std= 0.6779894828796387\n",
      "dist std min max: 0.14418481290340424 0.6779894828796387 3.641608238220215\n",
      "hidden_states min max: -16.117658615112305 12.028563499450684\n",
      "hidden_state minus mean squared max: 224.94285583496094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.24052429199219 1.0044516324996948\n",
      "loss   281: 2.1722   grad norm: 0.7217          model param norm: 83.7708        \n",
      "\n",
      "quiet_star_policy_loss= -0.03665151819586754\n",
      "nll_loss= 2.1864192485809326\n",
      "avg_std= 0.6793557405471802\n",
      "dist std min max: 0.14380882680416107 0.6793557405471802 3.418348789215088\n",
      "hidden_states min max: -21.774446487426758 11.501630783081055\n",
      "hidden_state minus mean squared max: 438.2829895019531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.57401275634766 1.0020968914031982\n",
      "loss   282: 2.1498   grad norm: 0.7302          model param norm: 83.7775        \n",
      "\n",
      "quiet_star_policy_loss= -0.06856556236743927\n",
      "nll_loss= 2.1804044246673584\n",
      "avg_std= 0.6788625717163086\n",
      "dist std min max: 0.14527879655361176 0.6788625717163086 3.449488639831543\n",
      "hidden_states min max: -13.798205375671387 12.0549955368042\n",
      "hidden_state minus mean squared max: 150.66197204589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.04010009765625 0.9926105737686157\n",
      "loss   283: 2.1118   grad norm: 0.7568          model param norm: 83.7838        \n",
      "\n",
      "quiet_star_policy_loss= 0.009620475582778454\n",
      "nll_loss= 2.1909077167510986\n",
      "avg_std= 0.6774635314941406\n",
      "dist std min max: 0.1465400904417038 0.6774635314941406 3.540701150894165\n",
      "hidden_states min max: -13.600934982299805 11.488484382629395\n",
      "hidden_state minus mean squared max: 153.51905822753906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.04949188232422 0.9885993003845215\n",
      "loss   284: 2.2005   grad norm: 0.7933          model param norm: 83.7901        \n",
      "\n",
      "quiet_star_policy_loss= -0.025140095502138138\n",
      "nll_loss= 2.1853718757629395\n",
      "avg_std= 0.6736218929290771\n",
      "dist std min max: 0.14678725600242615 0.6736218929290771 3.4784083366394043\n",
      "hidden_states min max: -25.182025909423828 11.276590347290039\n",
      "hidden_state minus mean squared max: 582.283203125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.71605682373047 0.9806170463562012\n",
      "loss   285: 2.1602   grad norm: 0.7207          model param norm: 83.7965        \n",
      "\n",
      "quiet_star_policy_loss= -0.021871184930205345\n",
      "nll_loss= 2.189316987991333\n",
      "avg_std= 0.6765363216400146\n",
      "dist std min max: 0.1478811353445053 0.6765363216400146 3.563979387283325\n",
      "hidden_states min max: -11.84347915649414 11.427103996276855\n",
      "hidden_state minus mean squared max: 156.00860595703125\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -104.05753326416016 0.9798295497894287\n",
      "loss   286: 2.1674   grad norm: 0.7271          model param norm: 83.8027        \n",
      "\n",
      "quiet_star_policy_loss= -0.04923982545733452\n",
      "nll_loss= 2.171808958053589\n",
      "avg_std= 0.6728767156600952\n",
      "dist std min max: 0.14746971428394318 0.6728767156600952 3.4129598140716553\n",
      "hidden_states min max: -12.173455238342285 12.709282875061035\n",
      "hidden_state minus mean squared max: 131.52359008789062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.57453155517578 0.9742163419723511\n",
      "loss   287: 2.1226   grad norm: 0.7272          model param norm: 83.8090        \n",
      "\n",
      "quiet_star_policy_loss= -0.04759817197918892\n",
      "nll_loss= 2.1895129680633545\n",
      "avg_std= 0.6720244884490967\n",
      "dist std min max: 0.14742159843444824 0.6720244884490967 3.667942523956299\n",
      "hidden_states min max: -11.378578186035156 11.107901573181152\n",
      "hidden_state minus mean squared max: 129.64859008789062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.73906707763672 0.9718418121337891\n",
      "loss   288: 2.1419   grad norm: 0.7641          model param norm: 83.8155        \n",
      "\n",
      "quiet_star_policy_loss= -0.029992341995239258\n",
      "nll_loss= 2.192232847213745\n",
      "avg_std= 0.6740021705627441\n",
      "dist std min max: 0.15011610090732574 0.6740021705627441 3.6140997409820557\n",
      "hidden_states min max: -16.644039154052734 11.364080429077148\n",
      "hidden_state minus mean squared max: 350.12744140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.46172332763672 0.964033842086792\n",
      "loss   289: 2.1622   grad norm: 0.7750          model param norm: 83.8221        \n",
      "\n",
      "quiet_star_policy_loss= -0.04676571115851402\n",
      "nll_loss= 2.166842222213745\n",
      "avg_std= 0.6699908971786499\n",
      "dist std min max: 0.15116079151630402 0.6699908971786499 3.607180118560791\n",
      "hidden_states min max: -12.339871406555176 11.732868194580078\n",
      "hidden_state minus mean squared max: 119.15180206298828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.86956024169922 0.9591480493545532\n",
      "loss   290: 2.1201   grad norm: 0.7331          model param norm: 83.8286        \n",
      "\n",
      "quiet_star_policy_loss= -0.05129079893231392\n",
      "nll_loss= 2.1836071014404297\n",
      "avg_std= 0.6731129288673401\n",
      "dist std min max: 0.15053816139698029 0.6731129288673401 3.544093132019043\n",
      "hidden_states min max: -39.06753158569336 12.373941421508789\n",
      "hidden_state minus mean squared max: 1471.1754150390625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -105.17949676513672 0.9632354974746704\n",
      "loss   291: 2.1323   grad norm: 0.8263          model param norm: 83.8349        \n",
      "\n",
      "quiet_star_policy_loss= -0.049762558192014694\n",
      "nll_loss= 2.1741764545440674\n",
      "avg_std= 0.666400134563446\n",
      "dist std min max: 0.14965398609638214 0.666400134563446 3.4529526233673096\n",
      "hidden_states min max: -13.5258150100708 12.497014045715332\n",
      "hidden_state minus mean squared max: 203.48928833007812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19038391113281 0.9802570343017578\n",
      "loss   292: 2.1244   grad norm: 0.7550          model param norm: 83.8409        \n",
      "\n",
      "quiet_star_policy_loss= -0.02530384063720703\n",
      "nll_loss= 2.179168701171875\n",
      "avg_std= 0.6675940752029419\n",
      "dist std min max: 0.14733648300170898 0.6675940752029419 3.667311668395996\n",
      "hidden_states min max: -12.81509780883789 12.353882789611816\n",
      "hidden_state minus mean squared max: 155.15481567382812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.91615295410156 0.982650876045227\n",
      "loss   293: 2.1539   grad norm: 0.7819          model param norm: 83.8468        \n",
      "\n",
      "quiet_star_policy_loss= -0.017182517796754837\n",
      "nll_loss= 2.188420057296753\n",
      "avg_std= 0.6687229871749878\n",
      "dist std min max: 0.1481284350156784 0.6687229871749878 3.4725193977355957\n",
      "hidden_states min max: -13.796424865722656 11.320267677307129\n",
      "hidden_state minus mean squared max: 154.57765197753906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.59630584716797 0.9821982383728027\n",
      "loss   294: 2.1712   grad norm: 0.7991          model param norm: 83.8526        \n",
      "\n",
      "quiet_star_policy_loss= 0.005269431974738836\n",
      "nll_loss= 2.166687488555908\n",
      "avg_std= 0.6652909517288208\n",
      "dist std min max: 0.1471484899520874 0.6652909517288208 3.4918575286865234\n",
      "hidden_states min max: -17.939910888671875 12.350067138671875\n",
      "hidden_state minus mean squared max: 339.2269287109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.44591522216797 0.9880355596542358\n",
      "loss   295: 2.1720   grad norm: 0.7860          model param norm: 83.8585        \n",
      "\n",
      "quiet_star_policy_loss= -0.028032494708895683\n",
      "nll_loss= 2.1626102924346924\n",
      "avg_std= 0.668251097202301\n",
      "dist std min max: 0.14614306390285492 0.668251097202301 3.5230088233947754\n",
      "hidden_states min max: -11.79002571105957 12.012750625610352\n",
      "hidden_state minus mean squared max: 138.08236694335938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.64056396484375 0.9837583303451538\n",
      "loss   296: 2.1346   grad norm: 0.7648          model param norm: 83.8641        \n",
      "\n",
      "quiet_star_policy_loss= -0.0006685256958007812\n",
      "nll_loss= 2.1691627502441406\n",
      "avg_std= 0.6657162308692932\n",
      "dist std min max: 0.14657868444919586 0.6657162308692932 3.5801570415496826\n",
      "hidden_states min max: -13.086902618408203 12.258224487304688\n",
      "hidden_state minus mean squared max: 196.59141540527344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17314147949219 0.9936721324920654\n",
      "loss   297: 2.1685   grad norm: 0.7498          model param norm: 83.8699        \n",
      "\n",
      "quiet_star_policy_loss= -0.032427217811346054\n",
      "nll_loss= 2.1725099086761475\n",
      "avg_std= 0.6626454591751099\n",
      "dist std min max: 0.1443541944026947 0.6626454591751099 3.4290783405303955\n",
      "hidden_states min max: -13.101676940917969 11.64315128326416\n",
      "hidden_state minus mean squared max: 144.62071228027344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.0196533203125 1.0011435747146606\n",
      "loss   298: 2.1401   grad norm: 0.7812          model param norm: 83.8757        \n",
      "\n",
      "quiet_star_policy_loss= -0.07466640323400497\n",
      "nll_loss= 2.1659083366394043\n",
      "avg_std= 0.6624987721443176\n",
      "dist std min max: 0.14491000771522522 0.6624987721443176 3.4826292991638184\n",
      "hidden_states min max: -12.596651077270508 11.060126304626465\n",
      "hidden_state minus mean squared max: 124.77693176269531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.71456146240234 0.987944483757019\n",
      "loss   299: 2.0912   grad norm: 0.7798          model param norm: 83.8814        \n",
      "eval loss 2.1695566177368164\n",
      "\n",
      "quiet_star_policy_loss= -0.046260882169008255\n",
      "nll_loss= 2.1575815677642822\n",
      "avg_std= 0.6614672541618347\n",
      "dist std min max: 0.14557848870754242 0.6614672541618347 3.4184730052948\n",
      "hidden_states min max: -11.956072807312012 12.49221420288086\n",
      "hidden_state minus mean squared max: 132.5860137939453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.38011169433594 0.9999028444290161\n",
      "loss   300: 2.1113   grad norm: 0.7884          model param norm: 83.8871        \n",
      "\n",
      "quiet_star_policy_loss= 0.04070329666137695\n",
      "nll_loss= 2.1890056133270264\n",
      "avg_std= 0.660790205001831\n",
      "dist std min max: 0.14325621724128723 0.660790205001831 3.5347585678100586\n",
      "hidden_states min max: -12.506492614746094 12.054067611694336\n",
      "hidden_state minus mean squared max: 169.54808044433594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.49507904052734 1.0241661071777344\n",
      "loss   301: 2.2297   grad norm: 0.7429          model param norm: 83.8928        \n",
      "\n",
      "quiet_star_policy_loss= -0.07679357379674911\n",
      "nll_loss= 2.156069755554199\n",
      "avg_std= 0.6605371832847595\n",
      "dist std min max: 0.1444069892168045 0.6605371832847595 3.572068691253662\n",
      "hidden_states min max: -14.398815155029297 10.968207359313965\n",
      "hidden_state minus mean squared max: 213.3987579345703\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.21414947509766 1.013063907623291\n",
      "loss   302: 2.0793   grad norm: 0.7347          model param norm: 83.8989        \n",
      "\n",
      "quiet_star_policy_loss= -0.08483586460351944\n",
      "nll_loss= 2.162494659423828\n",
      "avg_std= 0.6571228504180908\n",
      "dist std min max: 0.14194351434707642 0.6571228504180908 3.5635690689086914\n",
      "hidden_states min max: -13.078081130981445 11.7235746383667\n",
      "hidden_state minus mean squared max: 145.09739685058594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.85607147216797 1.0206435918807983\n",
      "loss   303: 2.0777   grad norm: 0.7748          model param norm: 83.9050        \n",
      "\n",
      "quiet_star_policy_loss= -0.06008319929242134\n",
      "nll_loss= 2.1695191860198975\n",
      "avg_std= 0.6563515663146973\n",
      "dist std min max: 0.14331719279289246 0.6563515663146973 3.458159923553467\n",
      "hidden_states min max: -15.531959533691406 12.939986228942871\n",
      "hidden_state minus mean squared max: 304.7024841308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.39225006103516 1.0183610916137695\n",
      "loss   304: 2.1094   grad norm: 0.7917          model param norm: 83.9108        \n",
      "\n",
      "quiet_star_policy_loss= 0.018906235694885254\n",
      "nll_loss= 2.171677589416504\n",
      "avg_std= 0.6530466675758362\n",
      "dist std min max: 0.14038382470607758 0.6530466675758362 3.5438549518585205\n",
      "hidden_states min max: -28.370208740234375 11.806830406188965\n",
      "hidden_state minus mean squared max: 906.6326293945312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.93743896484375 1.0223207473754883\n",
      "loss   305: 2.1906   grad norm: 0.8090          model param norm: 83.9168        \n",
      "\n",
      "quiet_star_policy_loss= -0.09255480766296387\n",
      "nll_loss= 2.168213367462158\n",
      "avg_std= 0.6560399532318115\n",
      "dist std min max: 0.14054086804389954 0.6560399532318115 3.579676866531372\n",
      "hidden_states min max: -13.029157638549805 12.720108985900879\n",
      "hidden_state minus mean squared max: 146.6148223876953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.74395751953125 1.030190110206604\n",
      "loss   306: 2.0757   grad norm: 0.7531          model param norm: 83.9226        \n",
      "\n",
      "quiet_star_policy_loss= 0.010750936344265938\n",
      "nll_loss= 2.1768083572387695\n",
      "avg_std= 0.6517878174781799\n",
      "dist std min max: 0.14022386074066162 0.6517878174781799 3.4526472091674805\n",
      "hidden_states min max: -12.66311264038086 12.973939895629883\n",
      "hidden_state minus mean squared max: 126.70740509033203\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -14.505935668945312 1.0354695320129395\n",
      "loss   307: 2.1876   grad norm: 1.4692          model param norm: 83.9285        \n",
      "\n",
      "quiet_star_policy_loss= -0.03661167621612549\n",
      "nll_loss= 2.16255521774292\n",
      "avg_std= 0.6517390012741089\n",
      "dist std min max: 0.13666421175003052 0.6517390012741089 3.86933970451355\n",
      "hidden_states min max: -13.814464569091797 12.955190658569336\n",
      "hidden_state minus mean squared max: 189.37094116210938\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.15443420410156 1.0703883171081543\n",
      "loss   308: 2.1259   grad norm: 0.7736          model param norm: 83.9347        \n",
      "\n",
      "quiet_star_policy_loss= -0.034499358385801315\n",
      "nll_loss= 2.1462886333465576\n",
      "avg_std= 0.6494781374931335\n",
      "dist std min max: 0.1341942995786667 0.6494781374931335 3.6109731197357178\n",
      "hidden_states min max: -13.788612365722656 12.736468315124512\n",
      "hidden_state minus mean squared max: 148.06991577148438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9806900024414 1.059509515762329\n",
      "loss   309: 2.1118   grad norm: 0.7593          model param norm: 83.9406        \n",
      "\n",
      "quiet_star_policy_loss= -0.034003447741270065\n",
      "nll_loss= 2.1594059467315674\n",
      "avg_std= 0.6523196697235107\n",
      "dist std min max: 0.13495130836963654 0.6523196697235107 3.5785820484161377\n",
      "hidden_states min max: -12.996550559997559 13.280141830444336\n",
      "hidden_state minus mean squared max: 145.03387451171875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9725112915039 1.07108736038208\n",
      "loss   310: 2.1254   grad norm: 0.8083          model param norm: 83.9465        \n",
      "\n",
      "quiet_star_policy_loss= -0.0035221099387854338\n",
      "nll_loss= 2.157256603240967\n",
      "avg_std= 0.649035632610321\n",
      "dist std min max: 0.13401417434215546 0.649035632610321 4.006556510925293\n",
      "hidden_states min max: -13.670459747314453 11.398873329162598\n",
      "hidden_state minus mean squared max: 150.70550537109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.88533782958984 1.081436038017273\n",
      "loss   311: 2.1537   grad norm: 0.8002          model param norm: 83.9525        \n",
      "\n",
      "quiet_star_policy_loss= 0.003389740129932761\n",
      "nll_loss= 2.1601696014404297\n",
      "avg_std= 0.6474818587303162\n",
      "dist std min max: 0.13365624845027924 0.6474818587303162 3.5686733722686768\n",
      "hidden_states min max: -13.323095321655273 11.849652290344238\n",
      "hidden_state minus mean squared max: 161.30560302734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29142761230469 1.0815736055374146\n",
      "loss   312: 2.1636   grad norm: 0.8351          model param norm: 83.9585        \n",
      "\n",
      "quiet_star_policy_loss= -0.018618011847138405\n",
      "nll_loss= 2.151946783065796\n",
      "avg_std= 0.6479423642158508\n",
      "dist std min max: 0.13354238867759705 0.6479423642158508 3.875999689102173\n",
      "hidden_states min max: -24.164514541625977 13.492206573486328\n",
      "hidden_state minus mean squared max: 607.8609619140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.73753356933594 1.0878394842147827\n",
      "loss   313: 2.1333   grad norm: 0.7881          model param norm: 83.9646        \n",
      "\n",
      "quiet_star_policy_loss= -0.016721725463867188\n",
      "nll_loss= 2.145777702331543\n",
      "avg_std= 0.6457338333129883\n",
      "dist std min max: 0.13179980218410492 0.6457338333129883 3.6225383281707764\n",
      "hidden_states min max: -14.875889778137207 12.871447563171387\n",
      "hidden_state minus mean squared max: 200.92544555664062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.02947235107422 1.0884097814559937\n",
      "loss   314: 2.1291   grad norm: 0.7900          model param norm: 83.9706        \n",
      "\n",
      "quiet_star_policy_loss= 0.003310299012809992\n",
      "nll_loss= 2.156376361846924\n",
      "avg_std= 0.6445645689964294\n",
      "dist std min max: 0.1292264759540558 0.6445645689964294 3.7893078327178955\n",
      "hidden_states min max: -13.910926818847656 11.350311279296875\n",
      "hidden_state minus mean squared max: 191.2533721923828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12183380126953 1.1133986711502075\n",
      "loss   315: 2.1597   grad norm: 0.8402          model param norm: 83.9767        \n",
      "\n",
      "quiet_star_policy_loss= -0.08658113330602646\n",
      "nll_loss= 2.1463754177093506\n",
      "avg_std= 0.6443269848823547\n",
      "dist std min max: 0.13338753581047058 0.6443269848823547 3.762521505355835\n",
      "hidden_states min max: -13.629323959350586 11.915029525756836\n",
      "hidden_state minus mean squared max: 156.593017578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90080261230469 1.0940626859664917\n",
      "loss   316: 2.0598   grad norm: 0.7791          model param norm: 83.9828        \n",
      "\n",
      "quiet_star_policy_loss= -0.059624481946229935\n",
      "nll_loss= 2.1451470851898193\n",
      "avg_std= 0.643328070640564\n",
      "dist std min max: 0.13228777050971985 0.643328070640564 3.703216791152954\n",
      "hidden_states min max: -14.756452560424805 13.366506576538086\n",
      "hidden_state minus mean squared max: 210.22970581054688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.43196105957031 1.0919893980026245\n",
      "loss   317: 2.0855   grad norm: 0.7785          model param norm: 83.9888        \n",
      "\n",
      "quiet_star_policy_loss= -0.032280635088682175\n",
      "nll_loss= 2.148167848587036\n",
      "avg_std= 0.6455042362213135\n",
      "dist std min max: 0.13300839066505432 0.6455042362213135 3.867145538330078\n",
      "hidden_states min max: -27.75857162475586 13.135683059692383\n",
      "hidden_state minus mean squared max: 787.7357177734375\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -104.86714935302734 1.0853086709976196\n",
      "loss   318: 2.1159   grad norm: 0.7919          model param norm: 83.9947        \n",
      "\n",
      "quiet_star_policy_loss= -0.03009185753762722\n",
      "nll_loss= 2.1513030529022217\n",
      "avg_std= 0.6409096717834473\n",
      "dist std min max: 0.13356101512908936 0.6409096717834473 3.7200422286987305\n",
      "hidden_states min max: -13.94316291809082 11.725330352783203\n",
      "hidden_state minus mean squared max: 147.2509002685547\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.75614929199219 1.0808649063110352\n",
      "loss   319: 2.1212   grad norm: 0.8571          model param norm: 84.0007        \n",
      "\n",
      "quiet_star_policy_loss= -0.03724327310919762\n",
      "nll_loss= 2.1563873291015625\n",
      "avg_std= 0.6393035054206848\n",
      "dist std min max: 0.1344873458147049 0.6393035054206848 3.864403486251831\n",
      "hidden_states min max: -13.960247039794922 11.55561637878418\n",
      "hidden_state minus mean squared max: 141.73458862304688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.05840301513672 1.0826588869094849\n",
      "loss   320: 2.1191   grad norm: 0.7762          model param norm: 84.0068        \n",
      "\n",
      "quiet_star_policy_loss= -0.04367685317993164\n",
      "nll_loss= 2.1487889289855957\n",
      "avg_std= 0.6424257755279541\n",
      "dist std min max: 0.13563375174999237 0.6424257755279541 3.86350417137146\n",
      "hidden_states min max: -28.961782455444336 11.632227897644043\n",
      "hidden_state minus mean squared max: 792.0048217773438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.86985778808594 1.0635461807250977\n",
      "loss   321: 2.1051   grad norm: 0.8465          model param norm: 84.0126        \n",
      "\n",
      "quiet_star_policy_loss= -0.04878120496869087\n",
      "nll_loss= 2.1505954265594482\n",
      "avg_std= 0.6393883228302002\n",
      "dist std min max: 0.13518188893795013 0.6393883228302002 3.8232007026672363\n",
      "hidden_states min max: -14.773170471191406 12.120240211486816\n",
      "hidden_state minus mean squared max: 330.953857421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43357849121094 1.05894935131073\n",
      "loss   322: 2.1018   grad norm: 0.8802          model param norm: 84.0182        \n",
      "\n",
      "quiet_star_policy_loss= 0.0001596450892975554\n",
      "nll_loss= 2.1437435150146484\n",
      "avg_std= 0.6379184126853943\n",
      "dist std min max: 0.13377293944358826 0.6379184126853943 3.8652350902557373\n",
      "hidden_states min max: -13.756257057189941 13.88875961303711\n",
      "hidden_state minus mean squared max: 195.22775268554688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.12786865234375 1.0630332231521606\n",
      "loss   323: 2.1439   grad norm: 0.7717          model param norm: 84.0237        \n",
      "\n",
      "quiet_star_policy_loss= -0.04010367393493652\n",
      "nll_loss= 2.1502530574798584\n",
      "avg_std= 0.6367847323417664\n",
      "dist std min max: 0.13346262276172638 0.6367847323417664 3.8440260887145996\n",
      "hidden_states min max: -13.992842674255371 14.43071460723877\n",
      "hidden_state minus mean squared max: 212.45252990722656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.81631469726562 1.0846301317214966\n",
      "loss   324: 2.1101   grad norm: 0.8395          model param norm: 84.0290        \n",
      "\n",
      "quiet_star_policy_loss= -0.023157214745879173\n",
      "nll_loss= 2.1378586292266846\n",
      "avg_std= 0.6351690292358398\n",
      "dist std min max: 0.13115301728248596 0.6351690292358398 3.8950891494750977\n",
      "hidden_states min max: -13.513579368591309 12.758986473083496\n",
      "hidden_state minus mean squared max: 261.3076171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.31542205810547 1.07671320438385\n",
      "loss   325: 2.1147   grad norm: 0.8436          model param norm: 84.0343        \n",
      "\n",
      "quiet_star_policy_loss= -0.03658699989318848\n",
      "nll_loss= 2.1532390117645264\n",
      "avg_std= 0.6374461054801941\n",
      "dist std min max: 0.12972550094127655 0.6374461054801941 3.8283205032348633\n",
      "hidden_states min max: -23.97201919555664 12.26817798614502\n",
      "hidden_state minus mean squared max: 558.5608520507812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.69524383544922 1.0989760160446167\n",
      "loss   326: 2.1167   grad norm: 0.8121          model param norm: 84.0398        \n",
      "\n",
      "quiet_star_policy_loss= -0.0612889789044857\n",
      "nll_loss= 2.1307125091552734\n",
      "avg_std= 0.6377821564674377\n",
      "dist std min max: 0.12727755308151245 0.6377821564674377 3.8898327350616455\n",
      "hidden_states min max: -16.06239128112793 12.91127872467041\n",
      "hidden_state minus mean squared max: 251.69967651367188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9185791015625 1.1263281106948853\n",
      "loss   327: 2.0694   grad norm: 0.7939          model param norm: 84.0451        \n",
      "\n",
      "quiet_star_policy_loss= -0.035628508776426315\n",
      "nll_loss= 2.1362314224243164\n",
      "avg_std= 0.629594624042511\n",
      "dist std min max: 0.12753106653690338 0.629594624042511 3.8886890411376953\n",
      "hidden_states min max: -14.319178581237793 12.123201370239258\n",
      "hidden_state minus mean squared max: 324.4744567871094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.42367553710938 1.1122153997421265\n",
      "loss   328: 2.1006   grad norm: 0.7609          model param norm: 84.0508        \n",
      "\n",
      "quiet_star_policy_loss= -0.07691078633069992\n",
      "nll_loss= 2.134526014328003\n",
      "avg_std= 0.6347060203552246\n",
      "dist std min max: 0.12199708819389343 0.6347060203552246 3.8472540378570557\n",
      "hidden_states min max: -16.95159339904785 12.846602439880371\n",
      "hidden_state minus mean squared max: 209.88818359375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.13088989257812 1.1619518995285034\n",
      "loss   329: 2.0576   grad norm: 0.8381          model param norm: 84.0565        \n",
      "\n",
      "quiet_star_policy_loss= -0.04026341438293457\n",
      "nll_loss= 2.1337649822235107\n",
      "avg_std= 0.6341378092765808\n",
      "dist std min max: 0.12325362116098404 0.6341378092765808 3.959118127822876\n",
      "hidden_states min max: -12.407276153564453 12.521618843078613\n",
      "hidden_state minus mean squared max: 154.8947296142578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.59009552001953 1.1377030611038208\n",
      "loss   330: 2.0935   grad norm: 0.8266          model param norm: 84.0623        \n",
      "\n",
      "quiet_star_policy_loss= 0.019697094336152077\n",
      "nll_loss= 2.1396992206573486\n",
      "avg_std= 0.6295507550239563\n",
      "dist std min max: 0.12267613410949707 0.6295507550239563 3.9002182483673096\n",
      "hidden_states min max: -18.685199737548828 14.160151481628418\n",
      "hidden_state minus mean squared max: 331.2378234863281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43399047851562 1.1735440492630005\n",
      "loss   331: 2.1594   grad norm: 0.8192          model param norm: 84.0683        \n",
      "\n",
      "quiet_star_policy_loss= 0.03316492959856987\n",
      "nll_loss= 2.1312096118927\n",
      "avg_std= 0.6314659714698792\n",
      "dist std min max: 0.12500296533107758 0.6314659714698792 3.8619067668914795\n",
      "hidden_states min max: -29.44426155090332 13.965119361877441\n",
      "hidden_state minus mean squared max: 790.5883178710938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.86897277832031 1.1339362859725952\n",
      "loss   332: 2.1644   grad norm: 0.9171          model param norm: 84.0740        \n",
      "\n",
      "quiet_star_policy_loss= -0.038274575024843216\n",
      "nll_loss= 2.1329822540283203\n",
      "avg_std= 0.6320915818214417\n",
      "dist std min max: 0.12312115728855133 0.6320915818214417 3.8366639614105225\n",
      "hidden_states min max: -14.804353713989258 12.559968948364258\n",
      "hidden_state minus mean squared max: 251.8807373046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.29707336425781 1.140053391456604\n",
      "loss   333: 2.0947   grad norm: 0.7830          model param norm: 84.0795        \n",
      "\n",
      "quiet_star_policy_loss= -0.03328237682580948\n",
      "nll_loss= 2.121234655380249\n",
      "avg_std= 0.6297987699508667\n",
      "dist std min max: 0.12434930354356766 0.6297987699508667 3.8105075359344482\n",
      "hidden_states min max: -13.571699142456055 13.709041595458984\n",
      "hidden_state minus mean squared max: 152.0015106201172\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.7116928100586 1.1584326028823853\n",
      "loss   334: 2.0880   grad norm: 0.8441          model param norm: 84.0852        \n",
      "\n",
      "quiet_star_policy_loss= -0.019525757059454918\n",
      "nll_loss= 2.138864040374756\n",
      "avg_std= 0.6335607767105103\n",
      "dist std min max: 0.12382913380861282 0.6335607767105103 3.6302123069763184\n",
      "hidden_states min max: -12.494553565979004 12.438329696655273\n",
      "hidden_state minus mean squared max: 128.4414520263672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.001337051391602 1.1583110094070435\n",
      "loss   335: 2.1193   grad norm: 1.6984          model param norm: 84.0912        \n",
      "\n",
      "quiet_star_policy_loss= -0.002184200333431363\n",
      "nll_loss= 2.140733480453491\n",
      "avg_std= 0.6294716596603394\n",
      "dist std min max: 0.1243663802742958 0.6294716596603394 3.9178898334503174\n",
      "hidden_states min max: -16.84449005126953 12.702841758728027\n",
      "hidden_state minus mean squared max: 310.43310546875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.40155029296875 1.1587544679641724\n",
      "loss   336: 2.1385   grad norm: 0.8378          model param norm: 84.0969        \n",
      "\n",
      "quiet_star_policy_loss= -0.017952537164092064\n",
      "nll_loss= 2.146865129470825\n",
      "avg_std= 0.6279165744781494\n",
      "dist std min max: 0.12009336054325104 0.6279165744781494 3.8268232345581055\n",
      "hidden_states min max: -23.921981811523438 12.796867370605469\n",
      "hidden_state minus mean squared max: 622.1015625\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -104.74913024902344 1.1592530012130737\n",
      "loss   337: 2.1289   grad norm: 0.7763          model param norm: 84.1028        \n",
      "\n",
      "quiet_star_policy_loss= -0.07614536583423615\n",
      "nll_loss= 2.1211400032043457\n",
      "avg_std= 0.6263604760169983\n",
      "dist std min max: 0.12236742675304413 0.6263604760169983 3.9018337726593018\n",
      "hidden_states min max: -13.693270683288574 12.726536750793457\n",
      "hidden_state minus mean squared max: 181.6940155029297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7484359741211 1.1646414995193481\n",
      "loss   338: 2.0450   grad norm: 0.8696          model param norm: 84.1086        \n",
      "\n",
      "quiet_star_policy_loss= -0.019839955493807793\n",
      "nll_loss= 2.1235198974609375\n",
      "avg_std= 0.6240136623382568\n",
      "dist std min max: 0.1199445053935051 0.6240136623382568 3.8272457122802734\n",
      "hidden_states min max: -25.74385643005371 13.528075218200684\n",
      "hidden_state minus mean squared max: 706.7599487304688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.81292724609375 1.1938263177871704\n",
      "loss   339: 2.1037   grad norm: 0.8908          model param norm: 84.1141        \n",
      "\n",
      "quiet_star_policy_loss= -0.017276858910918236\n",
      "nll_loss= 2.127423048019409\n",
      "avg_std= 0.6239497661590576\n",
      "dist std min max: 0.1197398453950882 0.6239497661590576 3.91493821144104\n",
      "hidden_states min max: -15.916399002075195 13.041479110717773\n",
      "hidden_state minus mean squared max: 245.1608428955078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.28353881835938 1.1888524293899536\n",
      "loss   340: 2.1101   grad norm: 0.8547          model param norm: 84.1200        \n",
      "\n",
      "quiet_star_policy_loss= -0.008960152044892311\n",
      "nll_loss= 2.1315410137176514\n",
      "avg_std= 0.6203157901763916\n",
      "dist std min max: 0.1224307268857956 0.6203157901763916 3.874469518661499\n",
      "hidden_states min max: -14.994449615478516 14.727090835571289\n",
      "hidden_state minus mean squared max: 221.4044952392578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06880950927734 1.1613997220993042\n",
      "loss   341: 2.1226   grad norm: 0.8372          model param norm: 84.1258        \n",
      "\n",
      "quiet_star_policy_loss= -0.05762319639325142\n",
      "nll_loss= 2.1382062435150146\n",
      "avg_std= 0.6216773986816406\n",
      "dist std min max: 0.12025424093008041 0.6216773986816406 3.840069532394409\n",
      "hidden_states min max: -13.94478988647461 13.729287147521973\n",
      "hidden_state minus mean squared max: 219.4620361328125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.22816467285156 1.1785129308700562\n",
      "loss   342: 2.0806   grad norm: 0.8975          model param norm: 84.1316        \n",
      "\n",
      "quiet_star_policy_loss= -0.09967388957738876\n",
      "nll_loss= 2.1289725303649902\n",
      "avg_std= 0.6201543807983398\n",
      "dist std min max: 0.12061433494091034 0.6201543807983398 3.911240816116333\n",
      "hidden_states min max: -37.57453918457031 13.094074249267578\n",
      "hidden_state minus mean squared max: 1225.4144287109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.08808898925781 1.17119562625885\n",
      "loss   343: 2.0293   grad norm: 0.8835          model param norm: 84.1373        \n",
      "\n",
      "quiet_star_policy_loss= 0.010794544592499733\n",
      "nll_loss= 2.116985321044922\n",
      "avg_std= 0.620635449886322\n",
      "dist std min max: 0.12258800864219666 0.620635449886322 3.826838970184326\n",
      "hidden_states min max: -17.231260299682617 13.866706848144531\n",
      "hidden_state minus mean squared max: 272.9938049316406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33729553222656 1.1655386686325073\n",
      "loss   344: 2.1278   grad norm: 0.8634          model param norm: 84.1431        \n",
      "\n",
      "quiet_star_policy_loss= -0.04468512535095215\n",
      "nll_loss= 2.1180155277252197\n",
      "avg_std= 0.6221733093261719\n",
      "dist std min max: 0.12115349620580673 0.6221733093261719 3.8928303718566895\n",
      "hidden_states min max: -13.09798812866211 12.445784568786621\n",
      "hidden_state minus mean squared max: 161.26837158203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.88016510009766 1.180296540260315\n",
      "loss   345: 2.0733   grad norm: 0.8519          model param norm: 84.1489        \n",
      "\n",
      "quiet_star_policy_loss= -0.05414772033691406\n",
      "nll_loss= 2.124959945678711\n",
      "avg_std= 0.6207062005996704\n",
      "dist std min max: 0.12203598767518997 0.6207062005996704 3.8981308937072754\n",
      "hidden_states min max: -14.582871437072754 12.800195693969727\n",
      "hidden_state minus mean squared max: 256.2169189453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.30559539794922 1.149224877357483\n",
      "loss   346: 2.0708   grad norm: 0.8051          model param norm: 84.1546        \n",
      "\n",
      "quiet_star_policy_loss= -0.05523281171917915\n",
      "nll_loss= 2.1073896884918213\n",
      "avg_std= 0.6193615794181824\n",
      "dist std min max: 0.12227611988782883 0.6193615794181824 3.8793649673461914\n",
      "hidden_states min max: -12.845908164978027 13.912129402160645\n",
      "hidden_state minus mean squared max: 196.62364196777344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.09270477294922 1.1580199003219604\n",
      "loss   347: 2.0522   grad norm: 0.8649          model param norm: 84.1603        \n",
      "\n",
      "quiet_star_policy_loss= 0.007948112674057484\n",
      "nll_loss= 2.1171352863311768\n",
      "avg_std= 0.6166831254959106\n",
      "dist std min max: 0.12321160733699799 0.6166831254959106 3.9205243587493896\n",
      "hidden_states min max: -13.12753963470459 13.494997024536133\n",
      "hidden_state minus mean squared max: 188.16903686523438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.92217254638672 1.1597238779067993\n",
      "loss   348: 2.1251   grad norm: 0.8327          model param norm: 84.1660        \n",
      "\n",
      "quiet_star_policy_loss= -0.008685732260346413\n",
      "nll_loss= 2.1069164276123047\n",
      "avg_std= 0.6137717366218567\n",
      "dist std min max: 0.12325432896614075 0.6137717366218567 4.047276496887207\n",
      "hidden_states min max: -13.385668754577637 13.002334594726562\n",
      "hidden_state minus mean squared max: 160.97007751464844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.86868286132812 1.1732569932937622\n",
      "loss   349: 2.0982   grad norm: 0.8588          model param norm: 84.1718        \n",
      "\n",
      "quiet_star_policy_loss= -0.01349411066621542\n",
      "nll_loss= 2.1152970790863037\n",
      "avg_std= 0.6179563403129578\n",
      "dist std min max: 0.12238359451293945 0.6179563403129578 3.9654486179351807\n",
      "hidden_states min max: -13.972635269165039 12.767526626586914\n",
      "hidden_state minus mean squared max: 190.48289489746094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78718566894531 1.1796330213546753\n",
      "loss   350: 2.1018   grad norm: 0.9337          model param norm: 84.1777        \n",
      "\n",
      "quiet_star_policy_loss= -0.05148282274603844\n",
      "nll_loss= 2.1140835285186768\n",
      "avg_std= 0.6173657774925232\n",
      "dist std min max: 0.12168202549219131 0.6173657774925232 4.061267375946045\n",
      "hidden_states min max: -15.674295425415039 13.999982833862305\n",
      "hidden_state minus mean squared max: 301.6470947265625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.38720703125 1.1769274473190308\n",
      "loss   351: 2.0626   grad norm: 0.8375          model param norm: 84.1834        \n",
      "\n",
      "quiet_star_policy_loss= -0.02873772382736206\n",
      "nll_loss= 2.1036860942840576\n",
      "avg_std= 0.6168962717056274\n",
      "dist std min max: 0.12192556262016296 0.6168962717056274 4.11061954498291\n",
      "hidden_states min max: -16.577838897705078 14.287062644958496\n",
      "hidden_state minus mean squared max: 248.75961303710938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29081726074219 1.1791659593582153\n",
      "loss   352: 2.0749   grad norm: 0.8861          model param norm: 84.1892        \n",
      "\n",
      "quiet_star_policy_loss= -0.055617619305849075\n",
      "nll_loss= 2.104978561401367\n",
      "avg_std= 0.6161389946937561\n",
      "dist std min max: 0.12134037911891937 0.6161389946937561 4.1418633460998535\n",
      "hidden_states min max: -13.050972938537598 13.785451889038086\n",
      "hidden_state minus mean squared max: 183.80535888671875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97761535644531 1.1601797342300415\n",
      "loss   353: 2.0494   grad norm: 0.8189          model param norm: 84.1949        \n",
      "\n",
      "quiet_star_policy_loss= -0.010538292117416859\n",
      "nll_loss= 2.1082446575164795\n",
      "avg_std= 0.6138193607330322\n",
      "dist std min max: 0.12171736359596252 0.6138193607330322 4.042928218841553\n",
      "hidden_states min max: -19.28410530090332 14.998321533203125\n",
      "hidden_state minus mean squared max: 253.4740753173828\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.30020904541016 1.1828111410140991\n",
      "loss   354: 2.0977   grad norm: 0.9002          model param norm: 84.2007        \n",
      "\n",
      "quiet_star_policy_loss= -0.032721519470214844\n",
      "nll_loss= 2.126110076904297\n",
      "avg_std= 0.6104062795639038\n",
      "dist std min max: 0.1215013638138771 0.6104062795639038 3.9950215816497803\n",
      "hidden_states min max: -13.221612930297852 16.119503021240234\n",
      "hidden_state minus mean squared max: 267.77777099609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.08678436279297 1.1884194612503052\n",
      "loss   355: 2.0934   grad norm: 0.8244          model param norm: 84.2066        \n",
      "\n",
      "quiet_star_policy_loss= -0.06714697182178497\n",
      "nll_loss= 2.112567663192749\n",
      "avg_std= 0.6138381958007812\n",
      "dist std min max: 0.12096242606639862 0.6138381958007812 4.129579067230225\n",
      "hidden_states min max: -12.913703918457031 14.31948471069336\n",
      "hidden_state minus mean squared max: 206.79238891601562\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.78211975097656 1.185667634010315\n",
      "loss   356: 2.0454   grad norm: 0.8490          model param norm: 84.2124        \n",
      "\n",
      "quiet_star_policy_loss= -0.011439037509262562\n",
      "nll_loss= 2.1211049556732178\n",
      "avg_std= 0.6096054315567017\n",
      "dist std min max: 0.11804009974002838 0.6096054315567017 4.120322227478027\n",
      "hidden_states min max: -13.133759498596191 13.403203964233398\n",
      "hidden_state minus mean squared max: 172.2281036376953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.63766479492188 1.173230528831482\n",
      "loss   357: 2.1097   grad norm: 0.8455          model param norm: 84.2180        \n",
      "\n",
      "quiet_star_policy_loss= -0.030516862869262695\n",
      "nll_loss= 2.1129565238952637\n",
      "avg_std= 0.607376217842102\n",
      "dist std min max: 0.11762847006320953 0.607376217842102 4.189013481140137\n",
      "hidden_states min max: -33.73035430908203 13.956979751586914\n",
      "hidden_state minus mean squared max: 1269.682373046875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -105.10582733154297 1.1735652685165405\n",
      "loss   358: 2.0824   grad norm: 0.8592          model param norm: 84.2235        \n",
      "\n",
      "quiet_star_policy_loss= -0.0536225326359272\n",
      "nll_loss= 2.106959581375122\n",
      "avg_std= 0.6067548990249634\n",
      "dist std min max: 0.11529861390590668 0.6067548990249634 4.20111608505249\n",
      "hidden_states min max: -17.713409423828125 15.443387031555176\n",
      "hidden_state minus mean squared max: 239.54888916015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.25721740722656 1.1970213651657104\n",
      "loss   359: 2.0533   grad norm: 0.8393          model param norm: 84.2288        \n",
      "\n",
      "quiet_star_policy_loss= -0.06655845791101456\n",
      "nll_loss= 2.1030824184417725\n",
      "avg_std= 0.6073690056800842\n",
      "dist std min max: 0.11203963309526443 0.6073690056800842 4.287319183349609\n",
      "hidden_states min max: -12.98195743560791 14.225048065185547\n",
      "hidden_state minus mean squared max: 200.84381103515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.84291076660156 1.222263216972351\n",
      "loss   360: 2.0365   grad norm: 0.8882          model param norm: 84.2341        \n",
      "\n",
      "quiet_star_policy_loss= -0.041181325912475586\n",
      "nll_loss= 2.1101505756378174\n",
      "avg_std= 0.6062215566635132\n",
      "dist std min max: 0.10751766711473465 0.6062215566635132 4.279272079467773\n",
      "hidden_states min max: -17.730417251586914 14.45421314239502\n",
      "hidden_state minus mean squared max: 236.06411743164062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26463317871094 1.2657305002212524\n",
      "loss   361: 2.0690   grad norm: 0.8467          model param norm: 84.2396        \n",
      "\n",
      "quiet_star_policy_loss= -0.05060319975018501\n",
      "nll_loss= 2.098142147064209\n",
      "avg_std= 0.6098349690437317\n",
      "dist std min max: 0.11643137037754059 0.6098349690437317 4.264313697814941\n",
      "hidden_states min max: -13.837817192077637 13.391139030456543\n",
      "hidden_state minus mean squared max: 189.6099090576172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78338623046875 1.2111176252365112\n",
      "loss   362: 2.0475   grad norm: 0.8827          model param norm: 84.2451        \n",
      "\n",
      "quiet_star_policy_loss= 0.08236053586006165\n",
      "nll_loss= 2.1106202602386475\n",
      "avg_std= 0.5991657376289368\n",
      "dist std min max: 0.1132645234465599 0.5991657376289368 4.292369842529297\n",
      "hidden_states min max: -12.738481521606445 12.98729419708252\n",
      "hidden_state minus mean squared max: 170.61764526367188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.2553939819336 1.2428897619247437\n",
      "loss   363: 2.1930   grad norm: 1.8084          model param norm: 84.2506        \n",
      "\n",
      "quiet_star_policy_loss= -0.004800868220627308\n",
      "nll_loss= 2.1123099327087402\n",
      "avg_std= 0.6049884557723999\n",
      "dist std min max: 0.11209435760974884 0.6049884557723999 4.311633110046387\n",
      "hidden_states min max: -13.492430686950684 16.150087356567383\n",
      "hidden_state minus mean squared max: 260.5786437988281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.04890441894531 1.2522391080856323\n",
      "loss   364: 2.1075   grad norm: 0.8721          model param norm: 84.2558        \n",
      "\n",
      "quiet_star_policy_loss= -0.05474434047937393\n",
      "nll_loss= 2.097318649291992\n",
      "avg_std= 0.6024048924446106\n",
      "dist std min max: 0.11378207802772522 0.6024048924446106 4.316676616668701\n",
      "hidden_states min max: -15.218358993530273 14.650330543518066\n",
      "hidden_state minus mean squared max: 287.20172119140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.36267852783203 1.2374812364578247\n",
      "loss   365: 2.0426   grad norm: 0.8668          model param norm: 84.2608        \n",
      "\n",
      "quiet_star_policy_loss= -0.04887428507208824\n",
      "nll_loss= 2.0860297679901123\n",
      "avg_std= 0.6046620011329651\n",
      "dist std min max: 0.11162790656089783 0.6046620011329651 4.310625076293945\n",
      "hidden_states min max: -18.587833404541016 13.482619285583496\n",
      "hidden_state minus mean squared max: 418.5895690917969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.55101776123047 1.256330132484436\n",
      "loss   366: 2.0372   grad norm: 0.7913          model param norm: 84.2659        \n",
      "\n",
      "quiet_star_policy_loss= -0.029929829761385918\n",
      "nll_loss= 2.1132099628448486\n",
      "avg_std= 0.6025070548057556\n",
      "dist std min max: 0.1086156815290451 0.6025070548057556 4.345697402954102\n",
      "hidden_states min max: -20.674123764038086 14.91547679901123\n",
      "hidden_state minus mean squared max: 434.6922912597656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.56990814208984 1.2683087587356567\n",
      "loss   367: 2.0833   grad norm: 0.8502          model param norm: 84.2710        \n",
      "\n",
      "quiet_star_policy_loss= -0.008544540964066982\n",
      "nll_loss= 2.1082894802093506\n",
      "avg_std= 0.5994862914085388\n",
      "dist std min max: 0.11231151968240738 0.5994862914085388 4.329472064971924\n",
      "hidden_states min max: -13.654192924499512 14.71531867980957\n",
      "hidden_state minus mean squared max: 194.93699645996094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.55301666259766 1.2429057359695435\n",
      "loss   368: 2.0997   grad norm: 0.8917          model param norm: 84.2763        \n",
      "\n",
      "quiet_star_policy_loss= 0.03170270845293999\n",
      "nll_loss= 2.089280366897583\n",
      "avg_std= 0.5987637042999268\n",
      "dist std min max: 0.1114860326051712 0.5987637042999268 4.395699977874756\n",
      "hidden_states min max: -15.428269386291504 13.243346214294434\n",
      "hidden_state minus mean squared max: 239.7444305419922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.94900512695312 1.2598544359207153\n",
      "loss   369: 2.1210   grad norm: 0.8499          model param norm: 84.2816        \n",
      "\n",
      "quiet_star_policy_loss= -0.028847528621554375\n",
      "nll_loss= 2.102468490600586\n",
      "avg_std= 0.5995468497276306\n",
      "dist std min max: 0.11043260991573334 0.5995468497276306 4.276987552642822\n",
      "hidden_states min max: -15.45954704284668 14.085504531860352\n",
      "hidden_state minus mean squared max: 265.54718017578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.32347106933594 1.284354567527771\n",
      "loss   370: 2.0736   grad norm: 0.8392          model param norm: 84.2869        \n",
      "\n",
      "quiet_star_policy_loss= -0.03162651136517525\n",
      "nll_loss= 2.095475912094116\n",
      "avg_std= 0.5985659956932068\n",
      "dist std min max: 0.10366487503051758 0.5985659956932068 4.4847283363342285\n",
      "hidden_states min max: -15.547652244567871 14.597565650939941\n",
      "hidden_state minus mean squared max: 229.56532287597656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.90128326416016 1.3243459463119507\n",
      "loss   371: 2.0638   grad norm: 0.8582          model param norm: 84.2924        \n",
      "\n",
      "quiet_star_policy_loss= -0.03976321220397949\n",
      "nll_loss= 2.0891129970550537\n",
      "avg_std= 0.5991727113723755\n",
      "dist std min max: 0.11013530939817429 0.5991727113723755 4.363959789276123\n",
      "hidden_states min max: -38.13853073120117 13.985157012939453\n",
      "hidden_state minus mean squared max: 1611.7506103515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.22511291503906 1.2361823320388794\n",
      "loss   372: 2.0493   grad norm: 0.8187          model param norm: 84.2979        \n",
      "\n",
      "quiet_star_policy_loss= -0.03878355026245117\n",
      "nll_loss= 2.1011180877685547\n",
      "avg_std= 0.597967803478241\n",
      "dist std min max: 0.10676370561122894 0.597967803478241 4.250547885894775\n",
      "hidden_states min max: -16.33078956604004 14.53679370880127\n",
      "hidden_state minus mean squared max: 254.3499755859375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1197738647461 1.2761982679367065\n",
      "loss   373: 2.0623   grad norm: 0.8285          model param norm: 84.3036        \n",
      "\n",
      "quiet_star_policy_loss= -0.06476669758558273\n",
      "nll_loss= 2.114959478378296\n",
      "avg_std= 0.5969263911247253\n",
      "dist std min max: 0.11244339495897293 0.5969263911247253 4.2789435386657715\n",
      "hidden_states min max: -19.43413543701172 13.519490242004395\n",
      "hidden_state minus mean squared max: 340.2946472167969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.44747924804688 1.2332712411880493\n",
      "loss   374: 2.0502   grad norm: 0.8898          model param norm: 84.3091        \n",
      "\n",
      "quiet_star_policy_loss= -0.033568598330020905\n",
      "nll_loss= 2.082824468612671\n",
      "avg_std= 0.5961224436759949\n",
      "dist std min max: 0.10921438783407211 0.5961224436759949 4.404541015625\n",
      "hidden_states min max: -15.225509643554688 13.620733261108398\n",
      "hidden_state minus mean squared max: 231.82057189941406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.00001525878906 1.2788070440292358\n",
      "loss   375: 2.0493   grad norm: 0.8625          model param norm: 84.3147        \n",
      "\n",
      "quiet_star_policy_loss= -0.0019834518898278475\n",
      "nll_loss= 2.0879955291748047\n",
      "avg_std= 0.5971707701683044\n",
      "dist std min max: 0.11433375626802444 0.5971707701683044 4.348625659942627\n",
      "hidden_states min max: -13.004329681396484 15.466363906860352\n",
      "hidden_state minus mean squared max: 180.3781280517578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.6395263671875 1.2474132776260376\n",
      "loss   376: 2.0860   grad norm: 0.8358          model param norm: 84.3206        \n",
      "\n",
      "quiet_star_policy_loss= -0.003150343894958496\n",
      "nll_loss= 2.0804383754730225\n",
      "avg_std= 0.597085177898407\n",
      "dist std min max: 0.11415381729602814 0.597085177898407 4.377375602722168\n",
      "hidden_states min max: -50.34577941894531 14.281525611877441\n",
      "hidden_state minus mean squared max: 2525.90087890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -105.4497299194336 1.2285560369491577\n",
      "loss   377: 2.0773   grad norm: 0.8483          model param norm: 84.3262        \n",
      "\n",
      "quiet_star_policy_loss= 0.03347950056195259\n",
      "nll_loss= 2.091461181640625\n",
      "avg_std= 0.5961427092552185\n",
      "dist std min max: 0.10924618691205978 0.5961427092552185 4.331488132476807\n",
      "hidden_states min max: -28.81477165222168 14.643696784973145\n",
      "hidden_state minus mean squared max: 759.204833984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.84871673583984 1.2716888189315796\n",
      "loss   378: 2.1249   grad norm: 0.8618          model param norm: 84.3320        \n",
      "\n",
      "quiet_star_policy_loss= -0.014668035320937634\n",
      "nll_loss= 2.0922725200653076\n",
      "avg_std= 0.5925145149230957\n",
      "dist std min max: 0.1135668084025383 0.5925145149230957 4.312246799468994\n",
      "hidden_states min max: -40.39060592651367 14.20617961883545\n",
      "hidden_state minus mean squared max: 1781.635986328125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.27522277832031 1.2564114332199097\n",
      "loss   379: 2.0776   grad norm: 0.8851          model param norm: 84.3380        \n",
      "\n",
      "quiet_star_policy_loss= -0.06529837101697922\n",
      "nll_loss= 2.0788745880126953\n",
      "avg_std= 0.5935735106468201\n",
      "dist std min max: 0.11090879887342453 0.5935735106468201 4.4155988693237305\n",
      "hidden_states min max: -13.861184120178223 14.492758750915527\n",
      "hidden_state minus mean squared max: 191.50637817382812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69505310058594 1.2544409036636353\n",
      "loss   380: 2.0136   grad norm: 0.8585          model param norm: 84.3438        \n",
      "\n",
      "quiet_star_policy_loss= -0.04049968719482422\n",
      "nll_loss= 2.0815200805664062\n",
      "avg_std= 0.5925930142402649\n",
      "dist std min max: 0.11204096674919128 0.5925930142402649 4.3525776863098145\n",
      "hidden_states min max: -16.544334411621094 17.058359146118164\n",
      "hidden_state minus mean squared max: 296.3640441894531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.22425079345703 1.2686055898666382\n",
      "loss   381: 2.0410   grad norm: 0.8436          model param norm: 84.3496        \n",
      "\n",
      "quiet_star_policy_loss= -0.01589975319802761\n",
      "nll_loss= 2.0828158855438232\n",
      "avg_std= 0.5888333320617676\n",
      "dist std min max: 0.10839737206697464 0.5888333320617676 4.364138603210449\n",
      "hidden_states min max: -16.327510833740234 14.875324249267578\n",
      "hidden_state minus mean squared max: 211.8326873779297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.21047973632812 1.2804151773452759\n",
      "loss   382: 2.0669   grad norm: 0.8792          model param norm: 84.3555        \n",
      "\n",
      "quiet_star_policy_loss= -0.01638627052307129\n",
      "nll_loss= 2.0805675983428955\n",
      "avg_std= 0.5915129780769348\n",
      "dist std min max: 0.10859683901071548 0.5915129780769348 4.330547332763672\n",
      "hidden_states min max: -17.3172607421875 16.219030380249023\n",
      "hidden_state minus mean squared max: 280.62628173828125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.35108947753906 1.3009341955184937\n",
      "loss   383: 2.0642   grad norm: 0.8862          model param norm: 84.3615        \n",
      "\n",
      "quiet_star_policy_loss= -0.000814545142930001\n",
      "nll_loss= 2.0800511837005615\n",
      "avg_std= 0.5922540426254272\n",
      "dist std min max: 0.10681141167879105 0.5922540426254272 4.431116580963135\n",
      "hidden_states min max: -17.346641540527344 14.472494125366211\n",
      "hidden_state minus mean squared max: 296.79254150390625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.1869125366211 1.3066478967666626\n",
      "loss   384: 2.0792   grad norm: 0.8168          model param norm: 84.3674        \n",
      "\n",
      "quiet_star_policy_loss= -0.032817959785461426\n",
      "nll_loss= 2.086872100830078\n",
      "avg_std= 0.5907042622566223\n",
      "dist std min max: 0.10707967728376389 0.5907042622566223 4.213624954223633\n",
      "hidden_states min max: -14.663018226623535 17.27444839477539\n",
      "hidden_state minus mean squared max: 233.32177734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80438995361328 1.297040343284607\n",
      "loss   385: 2.0541   grad norm: 0.8622          model param norm: 84.3733        \n",
      "\n",
      "quiet_star_policy_loss= -0.05652136728167534\n",
      "nll_loss= 2.0789711475372314\n",
      "avg_std= 0.5901554226875305\n",
      "dist std min max: 0.10751394927501678 0.5901554226875305 4.327609062194824\n",
      "hidden_states min max: -17.431734085083008 16.24993324279785\n",
      "hidden_state minus mean squared max: 300.8799133300781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65995788574219 1.3052133321762085\n",
      "loss   386: 2.0224   grad norm: 0.8836          model param norm: 84.3791        \n",
      "\n",
      "quiet_star_policy_loss= -0.05478058010339737\n",
      "nll_loss= 2.0879054069519043\n",
      "avg_std= 0.5906261801719666\n",
      "dist std min max: 0.10739535838365555 0.5906261801719666 4.337980270385742\n",
      "hidden_states min max: -20.586612701416016 15.205109596252441\n",
      "hidden_state minus mean squared max: 334.0077819824219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.43817138671875 1.3069781064987183\n",
      "loss   387: 2.0331   grad norm: 0.8670          model param norm: 84.3851        \n",
      "\n",
      "quiet_star_policy_loss= -0.012997674755752087\n",
      "nll_loss= 2.0855872631073\n",
      "avg_std= 0.5900654792785645\n",
      "dist std min max: 0.10332649201154709 0.5900654792785645 4.32382345199585\n",
      "hidden_states min max: -14.822480201721191 15.729948997497559\n",
      "hidden_state minus mean squared max: 245.60195922851562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69731903076172 1.3247040510177612\n",
      "loss   388: 2.0726   grad norm: 0.8972          model param norm: 84.3909        \n",
      "\n",
      "quiet_star_policy_loss= -0.04146423563361168\n",
      "nll_loss= 2.083749294281006\n",
      "avg_std= 0.5905296802520752\n",
      "dist std min max: 0.1048690602183342 0.5905296802520752 4.24625301361084\n",
      "hidden_states min max: -14.94666862487793 15.485957145690918\n",
      "hidden_state minus mean squared max: 217.84747314453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0770492553711 1.2828007936477661\n",
      "loss   389: 2.0423   grad norm: 0.8628          model param norm: 84.3966        \n",
      "\n",
      "quiet_star_policy_loss= -0.04792366176843643\n",
      "nll_loss= 2.0804035663604736\n",
      "avg_std= 0.5878258347511292\n",
      "dist std min max: 0.10379686206579208 0.5878258347511292 4.302023887634277\n",
      "hidden_states min max: -15.399323463439941 14.131365776062012\n",
      "hidden_state minus mean squared max: 237.25079345703125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.00869750976562 1.3188382387161255\n",
      "loss   390: 2.0325   grad norm: 0.9092          model param norm: 84.4019        \n",
      "\n",
      "quiet_star_policy_loss= 0.04708557203412056\n",
      "nll_loss= 2.0804367065429688\n",
      "avg_std= 0.5950741767883301\n",
      "dist std min max: 0.11097026616334915 0.5950741767883301 4.300120830535889\n",
      "hidden_states min max: -13.326299667358398 14.772245407104492\n",
      "hidden_state minus mean squared max: 174.31985473632812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.949861526489258 1.2595382928848267\n",
      "loss   391: 2.1275   grad norm: 1.9671          model param norm: 84.4073        \n",
      "\n",
      "quiet_star_policy_loss= -0.05481128767132759\n",
      "nll_loss= 2.0833327770233154\n",
      "avg_std= 0.589286744594574\n",
      "dist std min max: 0.1081577017903328 0.589286744594574 4.326772689819336\n",
      "hidden_states min max: -13.424240112304688 16.206512451171875\n",
      "hidden_state minus mean squared max: 266.2978820800781\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.9669189453125 1.2899848222732544\n",
      "loss   392: 2.0285   grad norm: 0.8967          model param norm: 84.4123        \n",
      "\n",
      "quiet_star_policy_loss= -0.04024996981024742\n",
      "nll_loss= 2.0712785720825195\n",
      "avg_std= 0.5908807516098022\n",
      "dist std min max: 0.10844087600708008 0.5908807516098022 4.282162666320801\n",
      "hidden_states min max: -18.379243850708008 15.233726501464844\n",
      "hidden_state minus mean squared max: 419.1100769042969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5516357421875 1.280157446861267\n",
      "loss   393: 2.0310   grad norm: 0.9140          model param norm: 84.4173        \n",
      "\n",
      "quiet_star_policy_loss= 0.01979508437216282\n",
      "nll_loss= 2.0915322303771973\n",
      "avg_std= 0.5872987508773804\n",
      "dist std min max: 0.10695845633745193 0.5872987508773804 4.322659015655518\n",
      "hidden_states min max: -16.30051612854004 15.24011516571045\n",
      "hidden_state minus mean squared max: 249.51922607421875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29232025146484 1.2925015687942505\n",
      "loss   394: 2.1113   grad norm: 0.9160          model param norm: 84.4223        \n",
      "\n",
      "quiet_star_policy_loss= 0.006046867463737726\n",
      "nll_loss= 2.0690040588378906\n",
      "avg_std= 0.588483452796936\n",
      "dist std min max: 0.1068132221698761 0.588483452796936 4.212249755859375\n",
      "hidden_states min max: -12.915104866027832 16.518686294555664\n",
      "hidden_state minus mean squared max: 212.1647186279297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.58260345458984 1.292465329170227\n",
      "loss   395: 2.0751   grad norm: 0.8750          model param norm: 84.4273        \n",
      "\n",
      "quiet_star_policy_loss= -0.01881847344338894\n",
      "nll_loss= 2.074756383895874\n",
      "avg_std= 0.5884827971458435\n",
      "dist std min max: 0.10951852053403854 0.5884827971458435 4.318990707397461\n",
      "hidden_states min max: -21.226642608642578 17.036882400512695\n",
      "hidden_state minus mean squared max: 390.9249572753906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.51683044433594 1.2599843740463257\n",
      "loss   396: 2.0559   grad norm: 0.9007          model param norm: 84.4323        \n",
      "\n",
      "quiet_star_policy_loss= -0.049932193011045456\n",
      "nll_loss= 2.084160089492798\n",
      "avg_std= 0.5854143500328064\n",
      "dist std min max: 0.10939908027648926 0.5854143500328064 4.321407318115234\n",
      "hidden_states min max: -14.791498184204102 15.305042266845703\n",
      "hidden_state minus mean squared max: 235.6299591064453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.89907836914062 1.2865773439407349\n",
      "loss   397: 2.0342   grad norm: 0.9056          model param norm: 84.4374        \n",
      "\n",
      "quiet_star_policy_loss= -0.03075542487204075\n",
      "nll_loss= 2.066927433013916\n",
      "avg_std= 0.591367244720459\n",
      "dist std min max: 0.10946007817983627 0.591367244720459 4.153057098388672\n",
      "hidden_states min max: -14.711498260498047 15.462974548339844\n",
      "hidden_state minus mean squared max: 222.17787170410156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.24305725097656 1.2875605821609497\n",
      "loss   398: 2.0362   grad norm: 0.8613          model param norm: 84.4422        \n",
      "\n",
      "quiet_star_policy_loss= 0.033742524683475494\n",
      "nll_loss= 2.0702531337738037\n",
      "avg_std= 0.5882413983345032\n",
      "dist std min max: 0.1101527139544487 0.5882413983345032 4.158351421356201\n",
      "hidden_states min max: -14.910603523254395 16.50285530090332\n",
      "hidden_state minus mean squared max: 284.73504638671875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.56103515625 1.2620338201522827\n",
      "loss   399: 2.1040   grad norm: 0.9002          model param norm: 84.4471        \n",
      "eval loss 2.0782113075256348\n",
      "\n",
      "quiet_star_policy_loss= 0.006658077239990234\n",
      "nll_loss= 2.0762743949890137\n",
      "avg_std= 0.5894175171852112\n",
      "dist std min max: 0.11320299655199051 0.5894175171852112 4.108668327331543\n",
      "hidden_states min max: -15.918659210205078 17.33739471435547\n",
      "hidden_state minus mean squared max: 254.58120727539062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.185791015625 1.2452369928359985\n",
      "loss   400: 2.0829   grad norm: 0.8832          model param norm: 84.4519        \n",
      "\n",
      "quiet_star_policy_loss= -0.059609364718198776\n",
      "nll_loss= 2.0782618522644043\n",
      "avg_std= 0.5864330530166626\n",
      "dist std min max: 0.11331657320261002 0.5864330530166626 4.125129699707031\n",
      "hidden_states min max: -20.08847999572754 16.632200241088867\n",
      "hidden_state minus mean squared max: 372.1510925292969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.49223327636719 1.2319222688674927\n",
      "loss   401: 2.0187   grad norm: 0.9169          model param norm: 84.4566        \n",
      "\n",
      "quiet_star_policy_loss= -0.006218147464096546\n",
      "nll_loss= 2.065931797027588\n",
      "avg_std= 0.589244544506073\n",
      "dist std min max: 0.11389963328838348 0.589244544506073 4.076694488525391\n",
      "hidden_states min max: -27.201581954956055 17.007335662841797\n",
      "hidden_state minus mean squared max: 672.2454223632812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.78787994384766 1.231277346611023\n",
      "loss   402: 2.0597   grad norm: 0.8823          model param norm: 84.4613        \n",
      "\n",
      "quiet_star_policy_loss= -0.018922924995422363\n",
      "nll_loss= 2.0625367164611816\n",
      "avg_std= 0.5888242721557617\n",
      "dist std min max: 0.11704178154468536 0.5888242721557617 4.104780673980713\n",
      "hidden_states min max: -13.795196533203125 17.07474708557129\n",
      "hidden_state minus mean squared max: 228.75051879882812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.67872619628906 1.2050436735153198\n",
      "loss   403: 2.0436   grad norm: 0.8939          model param norm: 84.4658        \n",
      "\n",
      "quiet_star_policy_loss= -0.0814899429678917\n",
      "nll_loss= 2.065904378890991\n",
      "avg_std= 0.5916448831558228\n",
      "dist std min max: 0.11690451204776764 0.5916448831558228 4.031722068786621\n",
      "hidden_states min max: -15.114906311035156 16.239450454711914\n",
      "hidden_state minus mean squared max: 250.27700805664062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.89808654785156 1.209062933921814\n",
      "loss   404: 1.9844   grad norm: 0.8995          model param norm: 84.4705        \n",
      "\n",
      "quiet_star_policy_loss= -0.057114601135253906\n",
      "nll_loss= 2.075111150741577\n",
      "avg_std= 0.5872085690498352\n",
      "dist std min max: 0.12026430666446686 0.5872085690498352 4.017398834228516\n",
      "hidden_states min max: -23.489694595336914 18.587032318115234\n",
      "hidden_state minus mean squared max: 644.3115234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.76668548583984 1.1877347230911255\n",
      "loss   405: 2.0180   grad norm: 0.9269          model param norm: 84.4752        \n",
      "\n",
      "quiet_star_policy_loss= -0.006909895222634077\n",
      "nll_loss= 2.071777105331421\n",
      "avg_std= 0.5881509780883789\n",
      "dist std min max: 0.1202385276556015 0.5881509780883789 4.081943988800049\n",
      "hidden_states min max: -14.447563171386719 17.288455963134766\n",
      "hidden_state minus mean squared max: 272.10400390625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5195083618164 1.1739875078201294\n",
      "loss   406: 2.0649   grad norm: 0.8712          model param norm: 84.4799        \n",
      "\n",
      "quiet_star_policy_loss= -0.05923872068524361\n",
      "nll_loss= 2.0630033016204834\n",
      "avg_std= 0.5863880515098572\n",
      "dist std min max: 0.11636792868375778 0.5863880515098572 4.010496616363525\n",
      "hidden_states min max: -13.153100967407227 17.608989715576172\n",
      "hidden_state minus mean squared max: 241.6924591064453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.84630584716797 1.2269707918167114\n",
      "loss   407: 2.0038   grad norm: 0.8995          model param norm: 84.4848        \n",
      "\n",
      "quiet_star_policy_loss= -0.03105020523071289\n",
      "nll_loss= 2.076653003692627\n",
      "avg_std= 0.5845420956611633\n",
      "dist std min max: 0.11785804480314255 0.5845420956611633 4.138658046722412\n",
      "hidden_states min max: -14.716917037963867 18.16037368774414\n",
      "hidden_state minus mean squared max: 281.3244323730469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.88890075683594 1.2112418413162231\n",
      "loss   408: 2.0456   grad norm: 0.9054          model param norm: 84.4897        \n",
      "\n",
      "quiet_star_policy_loss= -0.05825846269726753\n",
      "nll_loss= 2.0734403133392334\n",
      "avg_std= 0.5860122442245483\n",
      "dist std min max: 0.12083715200424194 0.5860122442245483 4.15043830871582\n",
      "hidden_states min max: -14.196115493774414 17.27634620666504\n",
      "hidden_state minus mean squared max: 258.5243835449219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2249984741211 1.1764978170394897\n",
      "loss   409: 2.0152   grad norm: 0.9102          model param norm: 84.4949        \n",
      "\n",
      "quiet_star_policy_loss= -0.06598799675703049\n",
      "nll_loss= 2.066234588623047\n",
      "avg_std= 0.5886597037315369\n",
      "dist std min max: 0.12234190106391907 0.5886597037315369 4.013582229614258\n",
      "hidden_states min max: -13.906039237976074 17.614356994628906\n",
      "hidden_state minus mean squared max: 243.52450561523438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.19180297851562 1.1584221124649048\n",
      "loss   410: 2.0002   grad norm: 0.8186          model param norm: 84.5000        \n",
      "\n",
      "quiet_star_policy_loss= -0.05107302591204643\n",
      "nll_loss= 2.0752766132354736\n",
      "avg_std= 0.5836562514305115\n",
      "dist std min max: 0.12221735715866089 0.5836562514305115 4.098568439483643\n",
      "hidden_states min max: -21.137065887451172 17.402481079101562\n",
      "hidden_state minus mean squared max: 411.31927490234375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.5422592163086 1.1769927740097046\n",
      "loss   411: 2.0242   grad norm: 0.8959          model param norm: 84.5051        \n",
      "\n",
      "quiet_star_policy_loss= -0.060910679399967194\n",
      "nll_loss= 2.0587642192840576\n",
      "avg_std= 0.583912193775177\n",
      "dist std min max: 0.11976020038127899 0.583912193775177 4.048543930053711\n",
      "hidden_states min max: -14.406847953796387 17.304214477539062\n",
      "hidden_state minus mean squared max: 239.61802673339844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.16574096679688 1.183141827583313\n",
      "loss   412: 1.9979   grad norm: 0.9273          model param norm: 84.5101        \n",
      "\n",
      "quiet_star_policy_loss= 0.001163220382295549\n",
      "nll_loss= 2.051912784576416\n",
      "avg_std= 0.5840063691139221\n",
      "dist std min max: 0.12339993566274643 0.5840063691139221 4.083433628082275\n",
      "hidden_states min max: -14.593047142028809 21.10032081604004\n",
      "hidden_state minus mean squared max: 360.82159423828125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.82220458984375 1.1456133127212524\n",
      "loss   413: 2.0531   grad norm: 0.9436          model param norm: 84.5153        \n",
      "\n",
      "quiet_star_policy_loss= -0.04558057710528374\n",
      "nll_loss= 2.0600192546844482\n",
      "avg_std= 0.5824493765830994\n",
      "dist std min max: 0.1228194460272789 0.5824493765830994 4.100520133972168\n",
      "hidden_states min max: -15.241903305053711 17.916261672973633\n",
      "hidden_state minus mean squared max: 299.38311767578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.37084197998047 1.1613956689834595\n",
      "loss   414: 2.0144   grad norm: 0.9761          model param norm: 84.5203        \n",
      "\n",
      "quiet_star_policy_loss= -0.029962683096528053\n",
      "nll_loss= 2.057166337966919\n",
      "avg_std= 0.5818718671798706\n",
      "dist std min max: 0.12208178639411926 0.5818718671798706 4.114140510559082\n",
      "hidden_states min max: -24.668926239013672 19.64950180053711\n",
      "hidden_state minus mean squared max: 556.7234497070312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.69361114501953 1.1514242887496948\n",
      "loss   415: 2.0272   grad norm: 0.9516          model param norm: 84.5256        \n",
      "\n",
      "quiet_star_policy_loss= -0.03160219267010689\n",
      "nll_loss= 2.0628883838653564\n",
      "avg_std= 0.5789502263069153\n",
      "dist std min max: 0.12136667966842651 0.5789502263069153 4.119085788726807\n",
      "hidden_states min max: -16.435935974121094 18.7120304107666\n",
      "hidden_state minus mean squared max: 276.2464904785156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.06881713867188 1.161394715309143\n",
      "loss   416: 2.0313   grad norm: 0.9241          model param norm: 84.5311        \n",
      "\n",
      "quiet_star_policy_loss= -0.023586630821228027\n",
      "nll_loss= 2.048640012741089\n",
      "avg_std= 0.5748609304428101\n",
      "dist std min max: 0.12233198434114456 0.5748609304428101 4.140503406524658\n",
      "hidden_states min max: -15.0006103515625 17.75579071044922\n",
      "hidden_state minus mean squared max: 281.2846374511719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.70711517333984 1.1463645696640015\n",
      "loss   417: 2.0251   grad norm: 0.9243          model param norm: 84.5365        \n",
      "\n",
      "quiet_star_policy_loss= -0.004827785771340132\n",
      "nll_loss= 2.063887119293213\n",
      "avg_std= 0.5730253458023071\n",
      "dist std min max: 0.1229848861694336 0.5730253458023071 4.137874603271484\n",
      "hidden_states min max: -14.994827270507812 17.845550537109375\n",
      "hidden_state minus mean squared max: 292.5065612792969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.858394622802734 1.1550308465957642\n",
      "loss   418: 2.0591   grad norm: 0.8781          model param norm: 84.5416        \n",
      "\n",
      "quiet_star_policy_loss= -0.10477335751056671\n",
      "nll_loss= 2.055548667907715\n",
      "avg_std= 0.5823186039924622\n",
      "dist std min max: 0.12420330941677094 0.5823186039924622 4.128622531890869\n",
      "hidden_states min max: -14.17572021484375 17.28136444091797\n",
      "hidden_state minus mean squared max: 231.87164306640625\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -14.659158706665039 1.1547034978866577\n",
      "loss   419: 1.9508   grad norm: 1.8873          model param norm: 84.5465        \n",
      "\n",
      "quiet_star_policy_loss= -0.05032462999224663\n",
      "nll_loss= 2.0527195930480957\n",
      "avg_std= 0.5716060400009155\n",
      "dist std min max: 0.12150275707244873 0.5716060400009155 4.165900707244873\n",
      "hidden_states min max: -14.789200782775879 20.822818756103516\n",
      "hidden_state minus mean squared max: 355.1959228515625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.76956176757812 1.1744343042373657\n",
      "loss   420: 2.0024   grad norm: 0.8951          model param norm: 84.5520        \n",
      "\n",
      "quiet_star_policy_loss= -0.052332401275634766\n",
      "nll_loss= 2.05134654045105\n",
      "avg_std= 0.5680743455886841\n",
      "dist std min max: 0.11879082769155502 0.5680743455886841 4.182650566101074\n",
      "hidden_states min max: -17.532569885253906 17.48436737060547\n",
      "hidden_state minus mean squared max: 396.62353515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5240707397461 1.2111655473709106\n",
      "loss   421: 1.9990   grad norm: 0.9183          model param norm: 84.5573        \n",
      "\n",
      "quiet_star_policy_loss= -0.03391599655151367\n",
      "nll_loss= 2.058610200881958\n",
      "avg_std= 0.569021999835968\n",
      "dist std min max: 0.11845327913761139 0.569021999835968 4.191644668579102\n",
      "hidden_states min max: -16.503263473510742 18.96307373046875\n",
      "hidden_state minus mean squared max: 287.6734313964844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3331069946289 1.2132045030593872\n",
      "loss   422: 2.0247   grad norm: 0.9357          model param norm: 84.5628        \n",
      "\n",
      "quiet_star_policy_loss= -0.08528192341327667\n",
      "nll_loss= 2.0666966438293457\n",
      "avg_std= 0.5691580772399902\n",
      "dist std min max: 0.1172405257821083 0.5691580772399902 4.197119235992432\n",
      "hidden_states min max: -14.299287796020508 17.86418342590332\n",
      "hidden_state minus mean squared max: 269.9632263183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.47860717773438 1.210363745689392\n",
      "loss   423: 1.9814   grad norm: 0.9175          model param norm: 84.5684        \n",
      "\n",
      "quiet_star_policy_loss= -0.004090023227035999\n",
      "nll_loss= 2.057786226272583\n",
      "avg_std= 0.5645358562469482\n",
      "dist std min max: 0.1161196306347847 0.5645358562469482 4.2052998542785645\n",
      "hidden_states min max: -15.071325302124023 18.6871280670166\n",
      "hidden_state minus mean squared max: 273.8070983886719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15372467041016 1.218093752861023\n",
      "loss   424: 2.0537   grad norm: 0.8921          model param norm: 84.5739        \n",
      "\n",
      "quiet_star_policy_loss= -0.04583311080932617\n",
      "nll_loss= 2.0567116737365723\n",
      "avg_std= 0.565350353717804\n",
      "dist std min max: 0.11545711010694504 0.565350353717804 4.193376064300537\n",
      "hidden_states min max: -15.255709648132324 20.247913360595703\n",
      "hidden_state minus mean squared max: 327.2899475097656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.81594848632812 1.2354527711868286\n",
      "loss   425: 2.0109   grad norm: 0.9468          model param norm: 84.5795        \n",
      "\n",
      "quiet_star_policy_loss= -0.03401966020464897\n",
      "nll_loss= 2.051307439804077\n",
      "avg_std= 0.5645201206207275\n",
      "dist std min max: 0.11480231583118439 0.5645201206207275 4.173830032348633\n",
      "hidden_states min max: -14.794862747192383 17.13452911376953\n",
      "hidden_state minus mean squared max: 285.4623107910156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.5455093383789 1.2277005910873413\n",
      "loss   426: 2.0173   grad norm: 0.9460          model param norm: 84.5851        \n",
      "\n",
      "quiet_star_policy_loss= 0.011504746042191982\n",
      "nll_loss= 2.0355567932128906\n",
      "avg_std= 0.5622979402542114\n",
      "dist std min max: 0.11518349498510361 0.5622979402542114 4.148458003997803\n",
      "hidden_states min max: -15.07287883758545 21.96549415588379\n",
      "hidden_state minus mean squared max: 392.86376953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97067260742188 1.2276688814163208\n",
      "loss   427: 2.0471   grad norm: 0.9343          model param norm: 84.5907        \n",
      "\n",
      "quiet_star_policy_loss= -0.036705970764160156\n",
      "nll_loss= 2.0636093616485596\n",
      "avg_std= 0.5581468343734741\n",
      "dist std min max: 0.11162779480218887 0.5581468343734741 4.136751651763916\n",
      "hidden_states min max: -13.815988540649414 20.89256477355957\n",
      "hidden_state minus mean squared max: 350.24774169921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.75491333007812 1.249141812324524\n",
      "loss   428: 2.0269   grad norm: 0.9259          model param norm: 84.5963        \n",
      "\n",
      "quiet_star_policy_loss= 0.004134273622184992\n",
      "nll_loss= 2.0480666160583496\n",
      "avg_std= 0.5626707673072815\n",
      "dist std min max: 0.11256944388151169 0.5626707673072815 4.111525535583496\n",
      "hidden_states min max: -14.928814888000488 21.944515228271484\n",
      "hidden_state minus mean squared max: 390.9734802246094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.53084564208984 1.2510439157485962\n",
      "loss   429: 2.0522   grad norm: 0.9203          model param norm: 84.6019        \n",
      "\n",
      "quiet_star_policy_loss= -0.02704181708395481\n",
      "nll_loss= 2.049976348876953\n",
      "avg_std= 0.56296706199646\n",
      "dist std min max: 0.10806833207607269 0.56296706199646 4.121508598327637\n",
      "hidden_states min max: -15.25189208984375 19.356834411621094\n",
      "hidden_state minus mean squared max: 292.20111083984375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.4748764038086 1.268319010734558\n",
      "loss   430: 2.0229   grad norm: 0.9489          model param norm: 84.6074        \n",
      "\n",
      "quiet_star_policy_loss= -0.02245807647705078\n",
      "nll_loss= 2.0549614429473877\n",
      "avg_std= 0.5579464435577393\n",
      "dist std min max: 0.1094336211681366 0.5579464435577393 4.113203525543213\n",
      "hidden_states min max: -14.96035385131836 18.963932037353516\n",
      "hidden_state minus mean squared max: 297.14776611328125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15412902832031 1.2920633554458618\n",
      "loss   431: 2.0325   grad norm: 0.9580          model param norm: 84.6129        \n",
      "\n",
      "quiet_star_policy_loss= -0.05254697799682617\n",
      "nll_loss= 2.0399563312530518\n",
      "avg_std= 0.560424268245697\n",
      "dist std min max: 0.10732299834489822 0.560424268245697 4.095314979553223\n",
      "hidden_states min max: -26.082307815551758 18.333932876586914\n",
      "hidden_state minus mean squared max: 684.394287109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.79684448242188 1.2797237634658813\n",
      "loss   432: 1.9874   grad norm: 0.9844          model param norm: 84.6183        \n",
      "\n",
      "quiet_star_policy_loss= -0.0167981144040823\n",
      "nll_loss= 2.057417392730713\n",
      "avg_std= 0.558581531047821\n",
      "dist std min max: 0.10842733085155487 0.558581531047821 4.105972766876221\n",
      "hidden_states min max: -14.279732704162598 17.537988662719727\n",
      "hidden_state minus mean squared max: 271.27886962890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90019989013672 1.2782949209213257\n",
      "loss   433: 2.0406   grad norm: 0.9341          model param norm: 84.6235        \n",
      "\n",
      "quiet_star_policy_loss= -0.04274463653564453\n",
      "nll_loss= 2.043930768966675\n",
      "avg_std= 0.5574029684066772\n",
      "dist std min max: 0.10779628902673721 0.5574029684066772 4.112029552459717\n",
      "hidden_states min max: -14.294379234313965 19.205644607543945\n",
      "hidden_state minus mean squared max: 288.52020263671875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.38907623291016 1.293160319328308\n",
      "loss   434: 2.0012   grad norm: 0.9118          model param norm: 84.6288        \n",
      "\n",
      "quiet_star_policy_loss= -0.041521549224853516\n",
      "nll_loss= 2.070272445678711\n",
      "avg_std= 0.5553911328315735\n",
      "dist std min max: 0.10825389623641968 0.5553911328315735 4.084968090057373\n",
      "hidden_states min max: -13.382598876953125 17.77019500732422\n",
      "hidden_state minus mean squared max: 243.4273223876953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46707916259766 1.2991653680801392\n",
      "loss   435: 2.0288   grad norm: 0.9331          model param norm: 84.6341        \n",
      "\n",
      "quiet_star_policy_loss= -0.04032592847943306\n",
      "nll_loss= 2.050739049911499\n",
      "avg_std= 0.5548015832901001\n",
      "dist std min max: 0.10883859544992447 0.5548015832901001 4.07363224029541\n",
      "hidden_states min max: -18.191030502319336 18.945358276367188\n",
      "hidden_state minus mean squared max: 449.6859436035156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.58685302734375 1.2761825323104858\n",
      "loss   436: 2.0104   grad norm: 0.9097          model param norm: 84.6394        \n",
      "\n",
      "quiet_star_policy_loss= -0.03058807924389839\n",
      "nll_loss= 2.0608327388763428\n",
      "avg_std= 0.5560332536697388\n",
      "dist std min max: 0.10832477360963821 0.5560332536697388 4.078989505767822\n",
      "hidden_states min max: -31.739463806152344 19.036306381225586\n",
      "hidden_state minus mean squared max: 917.6289672851562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.9434585571289 1.2685569524765015\n",
      "loss   437: 2.0302   grad norm: 0.9042          model param norm: 84.6446        \n",
      "\n",
      "quiet_star_policy_loss= -0.03388021141290665\n",
      "nll_loss= 2.0328633785247803\n",
      "avg_std= 0.5571586489677429\n",
      "dist std min max: 0.10723176598548889 0.5571586489677429 4.057812213897705\n",
      "hidden_states min max: -13.42642879486084 19.705020904541016\n",
      "hidden_state minus mean squared max: 304.4082336425781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.9610366821289 1.289981484413147\n",
      "loss   438: 1.9990   grad norm: 0.8982          model param norm: 84.6497        \n",
      "\n",
      "quiet_star_policy_loss= -0.06025257334113121\n",
      "nll_loss= 2.052257776260376\n",
      "avg_std= 0.5533652901649475\n",
      "dist std min max: 0.10683523118495941 0.5533652901649475 4.062163352966309\n",
      "hidden_states min max: -24.242095947265625 18.33902931213379\n",
      "hidden_state minus mean squared max: 574.4819946289062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.7093276977539 1.2972711324691772\n",
      "loss   439: 1.9920   grad norm: 0.9466          model param norm: 84.6549        \n",
      "\n",
      "quiet_star_policy_loss= -0.041834067553281784\n",
      "nll_loss= 2.0668752193450928\n",
      "avg_std= 0.5527376532554626\n",
      "dist std min max: 0.10718253254890442 0.5527376532554626 4.08451509475708\n",
      "hidden_states min max: -21.880687713623047 18.1422119140625\n",
      "hidden_state minus mean squared max: 378.3127746582031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.50045013427734 1.3076931238174438\n",
      "loss   440: 2.0250   grad norm: 0.9833          model param norm: 84.6601        \n",
      "\n",
      "quiet_star_policy_loss= -0.05082530900835991\n",
      "nll_loss= 2.0344045162200928\n",
      "avg_std= 0.5503218173980713\n",
      "dist std min max: 0.10648714005947113 0.5503218173980713 4.088752269744873\n",
      "hidden_states min max: -23.88582420349121 20.332557678222656\n",
      "hidden_state minus mean squared max: 555.2073974609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.69224548339844 1.3042210340499878\n",
      "loss   441: 1.9836   grad norm: 0.9585          model param norm: 84.6653        \n",
      "\n",
      "quiet_star_policy_loss= -0.03798322752118111\n",
      "nll_loss= 2.044154644012451\n",
      "avg_std= 0.5516659617424011\n",
      "dist std min max: 0.10747862607240677 0.5516659617424011 4.083504676818848\n",
      "hidden_states min max: -18.250316619873047 17.76496124267578\n",
      "hidden_state minus mean squared max: 267.04278564453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.32144165039062 1.3004316091537476\n",
      "loss   442: 2.0062   grad norm: 0.9375          model param norm: 84.6702        \n",
      "\n",
      "quiet_star_policy_loss= -0.05526123195886612\n",
      "nll_loss= 2.041351556777954\n",
      "avg_std= 0.5517685413360596\n",
      "dist std min max: 0.10713552683591843 0.5517685413360596 4.100922107696533\n",
      "hidden_states min max: -12.77918815612793 18.55441665649414\n",
      "hidden_state minus mean squared max: 266.4521789550781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80711364746094 1.3114219903945923\n",
      "loss   443: 1.9861   grad norm: 0.9271          model param norm: 84.6753        \n",
      "\n",
      "quiet_star_policy_loss= -0.0316559337079525\n",
      "nll_loss= 2.0390782356262207\n",
      "avg_std= 0.5490640997886658\n",
      "dist std min max: 0.10216224938631058 0.5490640997886658 4.1344404220581055\n",
      "hidden_states min max: -14.692964553833008 18.254526138305664\n",
      "hidden_state minus mean squared max: 256.4521789550781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.03602600097656 1.3117746114730835\n",
      "loss   444: 2.0074   grad norm: 0.9027          model param norm: 84.6805        \n",
      "\n",
      "quiet_star_policy_loss= -0.06779670715332031\n",
      "nll_loss= 2.0353446006774902\n",
      "avg_std= 0.5474393963813782\n",
      "dist std min max: 0.10439762473106384 0.5474393963813782 4.101351737976074\n",
      "hidden_states min max: -13.788362503051758 19.836225509643555\n",
      "hidden_state minus mean squared max: 309.6592712402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17936706542969 1.31926429271698\n",
      "loss   445: 1.9675   grad norm: 0.9450          model param norm: 84.6855        \n",
      "\n",
      "quiet_star_policy_loss= -0.01798105239868164\n",
      "nll_loss= 2.0534160137176514\n",
      "avg_std= 0.5477613806724548\n",
      "dist std min max: 0.10307876765727997 0.5477613806724548 4.120260715484619\n",
      "hidden_states min max: -14.559144020080566 19.711261749267578\n",
      "hidden_state minus mean squared max: 306.3253173828125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.63896179199219 1.3481582403182983\n",
      "loss   446: 2.0354   grad norm: 0.9833          model param norm: 84.6905        \n",
      "\n",
      "quiet_star_policy_loss= 0.0030710857827216387\n",
      "nll_loss= 2.040790319442749\n",
      "avg_std= 0.5433218479156494\n",
      "dist std min max: 0.10378508269786835 0.5433218479156494 4.144820690155029\n",
      "hidden_states min max: -14.924396514892578 18.096843719482422\n",
      "hidden_state minus mean squared max: 294.47515869140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.02661895751953 1.3438321352005005\n",
      "loss   447: 2.0439   grad norm: 2.0892          model param norm: 84.6956        \n",
      "\n",
      "quiet_star_policy_loss= -0.06450939178466797\n",
      "nll_loss= 2.043490171432495\n",
      "avg_std= 0.5439832806587219\n",
      "dist std min max: 0.10120414942502975 0.5439832806587219 4.127269744873047\n",
      "hidden_states min max: -14.156898498535156 19.37725257873535\n",
      "hidden_state minus mean squared max: 294.6072082519531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10578155517578 1.3512331247329712\n",
      "loss   448: 1.9790   grad norm: 0.9488          model param norm: 84.7013        \n",
      "\n",
      "quiet_star_policy_loss= -0.09340820461511612\n",
      "nll_loss= 2.0397894382476807\n",
      "avg_std= 0.5434852242469788\n",
      "dist std min max: 0.10050804167985916 0.5434852242469788 4.115950584411621\n",
      "hidden_states min max: -13.599501609802246 17.822708129882812\n",
      "hidden_state minus mean squared max: 242.18524169921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65255737304688 1.3604329824447632\n",
      "loss   449: 1.9464   grad norm: 0.9217          model param norm: 84.7070        \n",
      "\n",
      "quiet_star_policy_loss= 0.010290336795151234\n",
      "nll_loss= 2.033036947250366\n",
      "avg_std= 0.5425225496292114\n",
      "dist std min max: 0.09978950768709183 0.5425225496292114 4.109271049499512\n",
      "hidden_states min max: -22.4928035736084 18.400529861450195\n",
      "hidden_state minus mean squared max: 476.75970458984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.61607360839844 1.3645590543746948\n",
      "loss   450: 2.0433   grad norm: 0.9753          model param norm: 84.7124        \n",
      "\n",
      "quiet_star_policy_loss= -0.015520679764449596\n",
      "nll_loss= 2.0377049446105957\n",
      "avg_std= 0.5416465401649475\n",
      "dist std min max: 0.09840559214353561 0.5416465401649475 4.15732479095459\n",
      "hidden_states min max: -14.33110523223877 20.197887420654297\n",
      "hidden_state minus mean squared max: 322.13372802734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30548858642578 1.3847626447677612\n",
      "loss   451: 2.0222   grad norm: 0.9060          model param norm: 84.7178        \n",
      "\n",
      "quiet_star_policy_loss= -0.021772002801299095\n",
      "nll_loss= 2.053706645965576\n",
      "avg_std= 0.5387498140335083\n",
      "dist std min max: 0.1000056266784668 0.5387498140335083 4.106713771820068\n",
      "hidden_states min max: -17.106101989746094 19.100589752197266\n",
      "hidden_state minus mean squared max: 283.9486389160156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.28832244873047 1.3695522546768188\n",
      "loss   452: 2.0319   grad norm: 1.0162          model param norm: 84.7232        \n",
      "\n",
      "quiet_star_policy_loss= -0.014911222271621227\n",
      "nll_loss= 2.0326499938964844\n",
      "avg_std= 0.5400696396827698\n",
      "dist std min max: 0.09974255412817001 0.5400696396827698 4.135026931762695\n",
      "hidden_states min max: -13.827089309692383 18.4403133392334\n",
      "hidden_state minus mean squared max: 262.7632751464844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.41411304473877 1.366920828819275\n",
      "loss   453: 2.0177   grad norm: 0.9027          model param norm: 84.7285        \n",
      "\n",
      "quiet_star_policy_loss= 0.02329702489078045\n",
      "nll_loss= 2.0375988483428955\n",
      "avg_std= 0.5372220873832703\n",
      "dist std min max: 0.10022615641355515 0.5372220873832703 4.156612873077393\n",
      "hidden_states min max: -21.02655029296875 19.42445182800293\n",
      "hidden_state minus mean squared max: 515.8897094726562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.65553283691406 1.365409255027771\n",
      "loss   454: 2.0609   grad norm: 0.9616          model param norm: 84.7339        \n",
      "\n",
      "quiet_star_policy_loss= -0.06737969070672989\n",
      "nll_loss= 2.037482738494873\n",
      "avg_std= 0.5357807278633118\n",
      "dist std min max: 0.10030612349510193 0.5357807278633118 4.200977325439453\n",
      "hidden_states min max: -18.380367279052734 19.079429626464844\n",
      "hidden_state minus mean squared max: 341.0950012207031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.44866180419922 1.3742207288742065\n",
      "loss   455: 1.9701   grad norm: 0.9535          model param norm: 84.7393        \n",
      "\n",
      "quiet_star_policy_loss= -0.02178354375064373\n",
      "nll_loss= 2.0292696952819824\n",
      "avg_std= 0.5354131460189819\n",
      "dist std min max: 0.10195651650428772 0.5354131460189819 4.210926055908203\n",
      "hidden_states min max: -15.523375511169434 20.223787307739258\n",
      "hidden_state minus mean squared max: 324.50048828125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.11307525634766 1.3636211156845093\n",
      "loss   456: 2.0075   grad norm: 0.9987          model param norm: 84.7445        \n",
      "\n",
      "quiet_star_policy_loss= -0.025029946118593216\n",
      "nll_loss= 2.03379225730896\n",
      "avg_std= 0.5355653166770935\n",
      "dist std min max: 0.1017056554555893 0.5355653166770935 4.23205041885376\n",
      "hidden_states min max: -14.824943542480469 19.644479751586914\n",
      "hidden_state minus mean squared max: 303.8983459472656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.3349838256836 1.3503392934799194\n",
      "loss   457: 2.0088   grad norm: 0.9777          model param norm: 84.7499        \n",
      "\n",
      "quiet_star_policy_loss= 0.015875864773988724\n",
      "nll_loss= 2.030233144760132\n",
      "avg_std= 0.5398348569869995\n",
      "dist std min max: 0.10112045705318451 0.5398348569869995 4.250282287597656\n",
      "hidden_states min max: -16.285140991210938 19.202861785888672\n",
      "hidden_state minus mean squared max: 339.8704528808594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.4006576538086 1.3600448369979858\n",
      "loss   458: 2.0461   grad norm: 1.0095          model param norm: 84.7552        \n",
      "\n",
      "quiet_star_policy_loss= -0.03545799478888512\n",
      "nll_loss= 2.0454983711242676\n",
      "avg_std= 0.535271942615509\n",
      "dist std min max: 0.10156069695949554 0.535271942615509 4.245243072509766\n",
      "hidden_states min max: -15.883363723754883 19.07665252685547\n",
      "hidden_state minus mean squared max: 327.03338623046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.76181030273438 1.3620833158493042\n",
      "loss   459: 2.0100   grad norm: 0.9337          model param norm: 84.7607        \n",
      "\n",
      "quiet_star_policy_loss= -0.03256940841674805\n",
      "nll_loss= 2.021498203277588\n",
      "avg_std= 0.5383466482162476\n",
      "dist std min max: 0.10027699172496796 0.5383466482162476 4.24348258972168\n",
      "hidden_states min max: -15.845857620239258 17.771427154541016\n",
      "hidden_state minus mean squared max: 257.7769470214844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.03794860839844 1.377869725227356\n",
      "loss   460: 1.9889   grad norm: 0.9473          model param norm: 84.7662        \n",
      "\n",
      "quiet_star_policy_loss= -0.031054889783263206\n",
      "nll_loss= 2.025967836380005\n",
      "avg_std= 0.5376203060150146\n",
      "dist std min max: 0.09941074997186661 0.5376203060150146 4.240633487701416\n",
      "hidden_states min max: -15.672112464904785 21.43170738220215\n",
      "hidden_state minus mean squared max: 369.3076477050781\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.75712585449219 1.3759297132492065\n",
      "loss   461: 1.9949   grad norm: 0.9965          model param norm: 84.7714        \n",
      "\n",
      "quiet_star_policy_loss= -0.032820798456668854\n",
      "nll_loss= 2.039278030395508\n",
      "avg_std= 0.534209668636322\n",
      "dist std min max: 0.10064875334501266 0.534209668636322 4.236006736755371\n",
      "hidden_states min max: -15.515034675598145 19.592082977294922\n",
      "hidden_state minus mean squared max: 309.431884765625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.23812103271484 1.3584569692611694\n",
      "loss   462: 2.0065   grad norm: 0.9630          model param norm: 84.7766        \n",
      "\n",
      "quiet_star_policy_loss= -0.03559260442852974\n",
      "nll_loss= 2.016756057739258\n",
      "avg_std= 0.5362557768821716\n",
      "dist std min max: 0.09801877290010452 0.5362557768821716 4.230963706970215\n",
      "hidden_states min max: -23.815149307250977 20.2048397064209\n",
      "hidden_state minus mean squared max: 509.18951416015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.64897918701172 1.3814328908920288\n",
      "loss   463: 1.9812   grad norm: 0.9641          model param norm: 84.7818        \n",
      "\n",
      "quiet_star_policy_loss= 0.02011861838400364\n",
      "nll_loss= 2.0224809646606445\n",
      "avg_std= 0.539044201374054\n",
      "dist std min max: 0.100440613925457 0.539044201374054 4.229248046875\n",
      "hidden_states min max: -15.43419075012207 18.70036506652832\n",
      "hidden_state minus mean squared max: 298.64813232421875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.32970428466797 1.3741346597671509\n",
      "loss   464: 2.0426   grad norm: 1.0669          model param norm: 84.7872        \n",
      "\n",
      "quiet_star_policy_loss= 0.007576561067253351\n",
      "nll_loss= 2.0215442180633545\n",
      "avg_std= 0.5394031405448914\n",
      "dist std min max: 0.09834815561771393 0.5394031405448914 4.217095851898193\n",
      "hidden_states min max: -13.830422401428223 19.062999725341797\n",
      "hidden_state minus mean squared max: 288.18560791015625\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.66645050048828 1.3571149110794067\n",
      "loss   465: 2.0291   grad norm: 0.9855          model param norm: 84.7926        \n",
      "\n",
      "quiet_star_policy_loss= -0.020599795505404472\n",
      "nll_loss= 2.0209603309631348\n",
      "avg_std= 0.5371453166007996\n",
      "dist std min max: 0.09981930255889893 0.5371453166007996 4.21931266784668\n",
      "hidden_states min max: -18.59092140197754 21.054088592529297\n",
      "hidden_state minus mean squared max: 356.6636962890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.31832885742188 1.3706153631210327\n",
      "loss   466: 2.0004   grad norm: 0.9884          model param norm: 84.7983        \n",
      "\n",
      "quiet_star_policy_loss= -0.0049453736282885075\n",
      "nll_loss= 2.022590398788452\n",
      "avg_std= 0.5370708703994751\n",
      "dist std min max: 0.0972585529088974 0.5370708703994751 4.231319427490234\n",
      "hidden_states min max: -14.628192901611328 18.173709869384766\n",
      "hidden_state minus mean squared max: 282.7218017578125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.1190414428711 1.3947571516036987\n",
      "loss   467: 2.0176   grad norm: 0.9783          model param norm: 84.8041        \n",
      "\n",
      "quiet_star_policy_loss= -0.00400466937571764\n",
      "nll_loss= 2.016737461090088\n",
      "avg_std= 0.5401175618171692\n",
      "dist std min max: 0.09678461402654648 0.5401175618171692 4.206293106079102\n",
      "hidden_states min max: -17.428712844848633 19.955610275268555\n",
      "hidden_state minus mean squared max: 385.61083984375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.48017120361328 1.4028271436691284\n",
      "loss   468: 2.0127   grad norm: 1.0233          model param norm: 84.8097        \n",
      "\n",
      "quiet_star_policy_loss= 0.018647193908691406\n",
      "nll_loss= 2.037736415863037\n",
      "avg_std= 0.5385810136795044\n",
      "dist std min max: 0.09589920192956924 0.5385810136795044 4.186822414398193\n",
      "hidden_states min max: -15.658409118652344 19.8214111328125\n",
      "hidden_state minus mean squared max: 316.8293151855469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.79485321044922 1.4149717092514038\n",
      "loss   469: 2.0564   grad norm: 0.9755          model param norm: 84.8149        \n",
      "\n",
      "quiet_star_policy_loss= -0.05595388635993004\n",
      "nll_loss= 2.009174108505249\n",
      "avg_std= 0.5359702110290527\n",
      "dist std min max: 0.09363240003585815 0.5359702110290527 4.1839494705200195\n",
      "hidden_states min max: -15.289712905883789 17.506349563598633\n",
      "hidden_state minus mean squared max: 305.2529296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.83224487304688 1.419723629951477\n",
      "loss   470: 1.9532   grad norm: 0.9754          model param norm: 84.8200        \n",
      "\n",
      "quiet_star_policy_loss= 0.014333915896713734\n",
      "nll_loss= 2.025458812713623\n",
      "avg_std= 0.5384564995765686\n",
      "dist std min max: 0.09468286484479904 0.5384564995765686 4.171169757843018\n",
      "hidden_states min max: -25.051498413085938 20.046302795410156\n",
      "hidden_state minus mean squared max: 585.3958129882812\n",
      "hidden_state minus mean divided by std max: 5.166576862335205\n",
      "log_prob min max: -104.7187271118164 1.4193567037582397\n",
      "loss   471: 2.0398   grad norm: 0.9981          model param norm: 84.8249        \n",
      "\n",
      "quiet_star_policy_loss= -0.019810914993286133\n",
      "nll_loss= 2.024266004562378\n",
      "avg_std= 0.5376924872398376\n",
      "dist std min max: 0.09371531754732132 0.5376924872398376 4.159196853637695\n",
      "hidden_states min max: -14.779874801635742 19.697725296020508\n",
      "hidden_state minus mean squared max: 306.7394104003906\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.6548080444336 1.4268704652786255\n",
      "loss   472: 2.0045   grad norm: 1.0271          model param norm: 84.8296        \n",
      "\n",
      "quiet_star_policy_loss= -0.037774182856082916\n",
      "nll_loss= 2.0365989208221436\n",
      "avg_std= 0.5369337201118469\n",
      "dist std min max: 0.09354686737060547 0.5369337201118469 4.144075393676758\n",
      "hidden_states min max: -25.111412048339844 18.008106231689453\n",
      "hidden_state minus mean squared max: 627.8497314453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.75373840332031 1.442672610282898\n",
      "loss   473: 1.9988   grad norm: 1.0228          model param norm: 84.8343        \n",
      "\n",
      "quiet_star_policy_loss= -0.027778102084994316\n",
      "nll_loss= 2.0207715034484863\n",
      "avg_std= 0.5386653542518616\n",
      "dist std min max: 0.09263107925653458 0.5386653542518616 4.148902893066406\n",
      "hidden_states min max: -15.34488296508789 18.08368492126465\n",
      "hidden_state minus mean squared max: 315.8583984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.04257202148438 1.43756902217865\n",
      "loss   474: 1.9930   grad norm: 0.9492          model param norm: 84.8389        \n",
      "\n",
      "quiet_star_policy_loss= -0.018775418400764465\n",
      "nll_loss= 2.038924217224121\n",
      "avg_std= 0.5387310981750488\n",
      "dist std min max: 0.09210268408060074 0.5387310981750488 4.187458515167236\n",
      "hidden_states min max: -14.158804893493652 16.92863655090332\n",
      "hidden_state minus mean squared max: 263.64630126953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.11135864257812 1.457445740699768\n",
      "loss   475: 2.0201   grad norm: 2.1598          model param norm: 84.8436        \n",
      "\n",
      "quiet_star_policy_loss= -0.050356175750494\n",
      "nll_loss= 2.0028586387634277\n",
      "avg_std= 0.5401825904846191\n",
      "dist std min max: 0.09284185618162155 0.5401825904846191 4.3266520500183105\n",
      "hidden_states min max: -16.22783851623535 17.521705627441406\n",
      "hidden_state minus mean squared max: 291.11492919921875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.31510925292969 1.4544330835342407\n",
      "loss   476: 1.9525   grad norm: 1.0229          model param norm: 84.8483        \n",
      "\n",
      "quiet_star_policy_loss= -0.029660796746611595\n",
      "nll_loss= 2.0340752601623535\n",
      "avg_std= 0.536749541759491\n",
      "dist std min max: 0.09183042496442795 0.536749541759491 4.3626179695129395\n",
      "hidden_states min max: -17.47428321838379 16.506908416748047\n",
      "hidden_state minus mean squared max: 292.4130859375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.37165069580078 1.4685596227645874\n",
      "loss   477: 2.0044   grad norm: 1.0155          model param norm: 84.8531        \n",
      "\n",
      "quiet_star_policy_loss= 0.0004378080484457314\n",
      "nll_loss= 2.0253372192382812\n",
      "avg_std= 0.5381865501403809\n",
      "dist std min max: 0.09128663688898087 0.5381865501403809 4.365298271179199\n",
      "hidden_states min max: -20.297454833984375 17.411922454833984\n",
      "hidden_state minus mean squared max: 378.5643005371094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.44587707519531 1.4453412294387817\n",
      "loss   478: 2.0258   grad norm: 0.9865          model param norm: 84.8579        \n",
      "\n",
      "quiet_star_policy_loss= -0.017154311761260033\n",
      "nll_loss= 2.0290355682373047\n",
      "avg_std= 0.5352618098258972\n",
      "dist std min max: 0.09067000448703766 0.5352618098258972 4.464207172393799\n",
      "hidden_states min max: -14.083555221557617 17.668869018554688\n",
      "hidden_state minus mean squared max: 242.45457458496094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.00955963134766 1.46511971950531\n",
      "loss   479: 2.0119   grad norm: 0.9560          model param norm: 84.8628        \n",
      "\n",
      "quiet_star_policy_loss= -0.010263490490615368\n",
      "nll_loss= 2.029334783554077\n",
      "avg_std= 0.5373419523239136\n",
      "dist std min max: 0.09037911891937256 0.5373419523239136 4.412863254547119\n",
      "hidden_states min max: -16.271677017211914 18.74378776550293\n",
      "hidden_state minus mean squared max: 340.3817138671875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.12549591064453 1.4674679040908813\n",
      "loss   480: 2.0191   grad norm: 0.9978          model param norm: 84.8676        \n",
      "\n",
      "quiet_star_policy_loss= -0.007901573553681374\n",
      "nll_loss= 2.0303657054901123\n",
      "avg_std= 0.5360707640647888\n",
      "dist std min max: 0.09140574187040329 0.5360707640647888 4.472171783447266\n",
      "hidden_states min max: -18.068912506103516 18.384815216064453\n",
      "hidden_state minus mean squared max: 315.8652648925781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.3089828491211 1.4500735998153687\n",
      "loss   481: 2.0225   grad norm: 0.9955          model param norm: 84.8724        \n",
      "\n",
      "quiet_star_policy_loss= -0.014782333746552467\n",
      "nll_loss= 2.0205039978027344\n",
      "avg_std= 0.5381761193275452\n",
      "dist std min max: 0.09152685850858688 0.5381761193275452 4.479689121246338\n",
      "hidden_states min max: -16.34193229675293 17.421960830688477\n",
      "hidden_state minus mean squared max: 278.1748962402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2738037109375 1.4642332792282104\n",
      "loss   482: 2.0057   grad norm: 1.0677          model param norm: 84.8773        \n",
      "\n",
      "quiet_star_policy_loss= -0.06339569389820099\n",
      "nll_loss= 2.024501085281372\n",
      "avg_std= 0.5410348176956177\n",
      "dist std min max: 0.09010439366102219 0.5410348176956177 4.482670307159424\n",
      "hidden_states min max: -19.03072738647461 18.497581481933594\n",
      "hidden_state minus mean squared max: 355.3546142578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1988296508789 1.4650996923446655\n",
      "loss   483: 1.9611   grad norm: 1.0409          model param norm: 84.8823        \n",
      "\n",
      "quiet_star_policy_loss= -0.010286855511367321\n",
      "nll_loss= 2.0210120677948\n",
      "avg_std= 0.5388048887252808\n",
      "dist std min max: 0.09031934291124344 0.5388048887252808 4.544098377227783\n",
      "hidden_states min max: -16.076738357543945 16.564847946166992\n",
      "hidden_state minus mean squared max: 242.1846466064453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15309143066406 1.4808322191238403\n",
      "loss   484: 2.0107   grad norm: 1.0816          model param norm: 84.8869        \n",
      "\n",
      "quiet_star_policy_loss= -0.03863232210278511\n",
      "nll_loss= 2.012394666671753\n",
      "avg_std= 0.5364879965782166\n",
      "dist std min max: 0.08942632377147675 0.5364879965782166 4.524322509765625\n",
      "hidden_states min max: -17.61367416381836 17.631406784057617\n",
      "hidden_state minus mean squared max: 278.02313232421875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.20670318603516 1.475669026374817\n",
      "loss   485: 1.9738   grad norm: 0.9969          model param norm: 84.8911        \n",
      "\n",
      "quiet_star_policy_loss= -0.08411719650030136\n",
      "nll_loss= 2.015923500061035\n",
      "avg_std= 0.5363801121711731\n",
      "dist std min max: 0.08891362696886063 0.5363801121711731 4.526494979858398\n",
      "hidden_states min max: -18.350296020507812 17.136001586914062\n",
      "hidden_state minus mean squared max: 328.83575439453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.26380920410156 1.4874149560928345\n",
      "loss   486: 1.9318   grad norm: 1.0467          model param norm: 84.8954        \n",
      "\n",
      "quiet_star_policy_loss= -0.018925953656435013\n",
      "nll_loss= 2.0288188457489014\n",
      "avg_std= 0.5338805317878723\n",
      "dist std min max: 0.0880623310804367 0.5338805317878723 4.558045387268066\n",
      "hidden_states min max: -16.001955032348633 17.402557373046875\n",
      "hidden_state minus mean squared max: 282.63629150390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.5423812866211 1.4926377534866333\n",
      "loss   487: 2.0099   grad norm: 1.0422          model param norm: 84.8995        \n",
      "\n",
      "quiet_star_policy_loss= -0.03859210014343262\n",
      "nll_loss= 2.012005090713501\n",
      "avg_std= 0.5330913066864014\n",
      "dist std min max: 0.08809880912303925 0.5330913066864014 4.515127182006836\n",
      "hidden_states min max: -56.23507308959961 17.432086944580078\n",
      "hidden_state minus mean squared max: 3105.807373046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.55309295654297 1.4845558404922485\n",
      "loss   488: 1.9734   grad norm: 1.0848          model param norm: 84.9035        \n",
      "\n",
      "quiet_star_policy_loss= -0.0802859291434288\n",
      "nll_loss= 2.018983840942383\n",
      "avg_std= 0.5320724248886108\n",
      "dist std min max: 0.087933748960495 0.5320724248886108 4.545452117919922\n",
      "hidden_states min max: -15.321907043457031 18.672475814819336\n",
      "hidden_state minus mean squared max: 273.08880615234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.39054870605469 1.5086654424667358\n",
      "loss   489: 1.9387   grad norm: 1.0449          model param norm: 84.9074        \n",
      "\n",
      "quiet_star_policy_loss= -0.06869735568761826\n",
      "nll_loss= 2.0280556678771973\n",
      "avg_std= 0.5301445126533508\n",
      "dist std min max: 0.08612048625946045 0.5301445126533508 4.545036792755127\n",
      "hidden_states min max: -16.131731033325195 17.168275833129883\n",
      "hidden_state minus mean squared max: 285.7125244140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.22260284423828 1.5040072202682495\n",
      "loss   490: 1.9594   grad norm: 1.0080          model param norm: 84.9113        \n",
      "\n",
      "quiet_star_policy_loss= -0.05762496218085289\n",
      "nll_loss= 2.014634370803833\n",
      "avg_std= 0.5291336178779602\n",
      "dist std min max: 0.08611194789409637 0.5291336178779602 4.514102458953857\n",
      "hidden_states min max: -15.730939865112305 16.973901748657227\n",
      "hidden_state minus mean squared max: 240.98556518554688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.51881408691406 1.5283499956130981\n",
      "loss   491: 1.9570   grad norm: 0.9632          model param norm: 84.9153        \n",
      "\n",
      "quiet_star_policy_loss= -0.015686942264437675\n",
      "nll_loss= 2.007549285888672\n",
      "avg_std= 0.5298067927360535\n",
      "dist std min max: 0.08531444519758224 0.5298067927360535 4.472764492034912\n",
      "hidden_states min max: -15.055851936340332 17.2702693939209\n",
      "hidden_state minus mean squared max: 230.6725616455078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83071899414062 1.5276068449020386\n",
      "loss   492: 1.9919   grad norm: 1.0076          model param norm: 84.9190        \n",
      "\n",
      "quiet_star_policy_loss= -0.04986915737390518\n",
      "nll_loss= 2.0136940479278564\n",
      "avg_std= 0.5257376432418823\n",
      "dist std min max: 0.08438807725906372 0.5257376432418823 4.4877238273620605\n",
      "hidden_states min max: -15.629674911499023 18.49527359008789\n",
      "hidden_state minus mean squared max: 281.4374084472656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.294921875 1.5465174913406372\n",
      "loss   493: 1.9638   grad norm: 1.0100          model param norm: 84.9228        \n",
      "\n",
      "quiet_star_policy_loss= -0.06626882404088974\n",
      "nll_loss= 2.026474952697754\n",
      "avg_std= 0.524398922920227\n",
      "dist std min max: 0.08376045525074005 0.524398922920227 4.4297990798950195\n",
      "hidden_states min max: -14.437653541564941 18.701961517333984\n",
      "hidden_state minus mean squared max: 275.5892028808594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.5974349975586 1.5457769632339478\n",
      "loss   494: 1.9602   grad norm: 1.0245          model param norm: 84.9267        \n",
      "\n",
      "quiet_star_policy_loss= -0.012862682342529297\n",
      "nll_loss= 2.000383138656616\n",
      "avg_std= 0.5237213373184204\n",
      "dist std min max: 0.08306344598531723 0.5237213373184204 4.408988952636719\n",
      "hidden_states min max: -19.95068359375 16.594154357910156\n",
      "hidden_state minus mean squared max: 388.2474060058594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.51338958740234 1.553723931312561\n",
      "loss   495: 1.9875   grad norm: 1.0391          model param norm: 84.9305        \n",
      "\n",
      "quiet_star_policy_loss= -0.059737157076597214\n",
      "nll_loss= 2.003483533859253\n",
      "avg_std= 0.5213388800621033\n",
      "dist std min max: 0.08299694955348969 0.5213388800621033 4.404767990112305\n",
      "hidden_states min max: -14.860153198242188 16.619230270385742\n",
      "hidden_state minus mean squared max: 216.12364196777344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.66606140136719 1.5534530878067017\n",
      "loss   496: 1.9437   grad norm: 1.0281          model param norm: 84.9348        \n",
      "\n",
      "quiet_star_policy_loss= -0.04349842295050621\n",
      "nll_loss= 2.0108108520507812\n",
      "avg_std= 0.5207602381706238\n",
      "dist std min max: 0.08236517757177353 0.5207602381706238 4.37648344039917\n",
      "hidden_states min max: -15.41183090209961 17.248756408691406\n",
      "hidden_state minus mean squared max: 251.9717254638672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.58268737792969 1.560520052909851\n",
      "loss   497: 1.9673   grad norm: 1.1119          model param norm: 84.9393        \n",
      "\n",
      "quiet_star_policy_loss= -0.027317523956298828\n",
      "nll_loss= 2.005624771118164\n",
      "avg_std= 0.5167169570922852\n",
      "dist std min max: 0.08259639889001846 0.5167169570922852 4.34081506729126\n",
      "hidden_states min max: -16.462970733642578 17.298744201660156\n",
      "hidden_state minus mean squared max: 249.70068359375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.95457458496094 1.5627754926681519\n",
      "loss   498: 1.9783   grad norm: 1.0024          model param norm: 84.9442        \n",
      "\n",
      "quiet_star_policy_loss= -0.011902428232133389\n",
      "nll_loss= 2.021739959716797\n",
      "avg_std= 0.5177635550498962\n",
      "dist std min max: 0.08288925141096115 0.5177635550498962 4.384350776672363\n",
      "hidden_states min max: -16.71593475341797 16.372079849243164\n",
      "hidden_state minus mean squared max: 268.35919189453125\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.00035095214844 1.5566271543502808\n",
      "loss   499: 2.0098   grad norm: 0.9554          model param norm: 84.9493        \n",
      "eval loss 2.013688564300537\n",
      "\n",
      "quiet_star_policy_loss= 0.004214477725327015\n",
      "nll_loss= 2.0011072158813477\n",
      "avg_std= 0.5165249109268188\n",
      "dist std min max: 0.08319270610809326 0.5165249109268188 4.328405380249023\n",
      "hidden_states min max: -17.40683937072754 17.90106773376465\n",
      "hidden_state minus mean squared max: 282.9856872558594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20357513427734 1.5599392652511597\n",
      "loss   500: 2.0053   grad norm: 1.0722          model param norm: 84.9541        \n",
      "\n",
      "quiet_star_policy_loss= 0.02325267903506756\n",
      "nll_loss= 1.9973328113555908\n",
      "avg_std= 0.516909658908844\n",
      "dist std min max: 0.08326832950115204 0.516909658908844 4.309686660766602\n",
      "hidden_states min max: -17.0203857421875 15.754392623901367\n",
      "hidden_state minus mean squared max: 279.5596008300781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.64788055419922 1.5573543310165405\n",
      "loss   501: 2.0206   grad norm: 1.0937          model param norm: 84.9590        \n",
      "\n",
      "quiet_star_policy_loss= -0.05784621462225914\n",
      "nll_loss= 2.0120887756347656\n",
      "avg_std= 0.5141404867172241\n",
      "dist std min max: 0.08346737176179886 0.5141404867172241 4.311639785766602\n",
      "hidden_states min max: -15.387614250183105 15.908459663391113\n",
      "hidden_state minus mean squared max: 263.2366638183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10380554199219 1.5571123361587524\n",
      "loss   502: 1.9542   grad norm: 1.0144          model param norm: 84.9637        \n",
      "\n",
      "quiet_star_policy_loss= 0.0013492838479578495\n",
      "nll_loss= 2.0072124004364014\n",
      "avg_std= 0.514998733997345\n",
      "dist std min max: 0.08460266888141632 0.514998733997345 4.319508075714111\n",
      "hidden_states min max: -13.665404319763184 14.79102611541748\n",
      "hidden_state minus mean squared max: 215.29136657714844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06204223632812 1.5347620248794556\n",
      "loss   503: 2.0086   grad norm: 2.3505          model param norm: 84.9685        \n",
      "\n",
      "quiet_star_policy_loss= -0.03234553337097168\n",
      "nll_loss= 1.9969640970230103\n",
      "avg_std= 0.5152721405029297\n",
      "dist std min max: 0.08425097912549973 0.5152721405029297 4.281365871429443\n",
      "hidden_states min max: -21.066030502319336 17.437623977661133\n",
      "hidden_state minus mean squared max: 384.3146667480469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.50831604003906 1.5474413633346558\n",
      "loss   504: 1.9646   grad norm: 1.0222          model param norm: 84.9730        \n",
      "\n",
      "quiet_star_policy_loss= -0.07468157261610031\n",
      "nll_loss= 2.0118281841278076\n",
      "avg_std= 0.5104659795761108\n",
      "dist std min max: 0.08376411348581314 0.5104659795761108 4.267605304718018\n",
      "hidden_states min max: -23.306699752807617 16.379064559936523\n",
      "hidden_state minus mean squared max: 465.736328125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.60438537597656 1.5445104837417603\n",
      "loss   505: 1.9371   grad norm: 1.1347          model param norm: 84.9776        \n",
      "\n",
      "quiet_star_policy_loss= -0.021577835083007812\n",
      "nll_loss= 2.015424966812134\n",
      "avg_std= 0.5111038088798523\n",
      "dist std min max: 0.08371907472610474 0.5111038088798523 4.331357002258301\n",
      "hidden_states min max: -15.790218353271484 17.34490394592285\n",
      "hidden_state minus mean squared max: 236.86338806152344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.21947479248047 1.5432876348495483\n",
      "loss   506: 1.9938   grad norm: 1.0563          model param norm: 84.9824        \n",
      "\n",
      "quiet_star_policy_loss= -0.022118283435702324\n",
      "nll_loss= 2.011415481567383\n",
      "avg_std= 0.5093112587928772\n",
      "dist std min max: 0.08354736864566803 0.5093112587928772 4.32144832611084\n",
      "hidden_states min max: -14.295869827270508 15.638874053955078\n",
      "hidden_state minus mean squared max: 249.5206756591797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72936248779297 1.5496619939804077\n",
      "loss   507: 1.9893   grad norm: 1.0627          model param norm: 84.9869        \n",
      "\n",
      "quiet_star_policy_loss= -0.05027742311358452\n",
      "nll_loss= 2.0028698444366455\n",
      "avg_std= 0.5072205662727356\n",
      "dist std min max: 0.08367709070444107 0.5072205662727356 4.329375267028809\n",
      "hidden_states min max: -14.930275917053223 16.2427978515625\n",
      "hidden_state minus mean squared max: 204.25189208984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.211669921875 1.5446847677230835\n",
      "loss   508: 1.9526   grad norm: 1.0173          model param norm: 84.9917        \n",
      "\n",
      "quiet_star_policy_loss= -0.02177894115447998\n",
      "nll_loss= 2.0074737071990967\n",
      "avg_std= 0.5049668550491333\n",
      "dist std min max: 0.08446229249238968 0.5049668550491333 4.339186191558838\n",
      "hidden_states min max: -14.601810455322266 15.972381591796875\n",
      "hidden_state minus mean squared max: 206.97918701171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.01589965820312 1.5435048341751099\n",
      "loss   509: 1.9857   grad norm: 1.0642          model param norm: 84.9966        \n",
      "\n",
      "quiet_star_policy_loss= -0.021846866235136986\n",
      "nll_loss= 2.005929708480835\n",
      "avg_std= 0.5053451061248779\n",
      "dist std min max: 0.08544796705245972 0.5053451061248779 4.403781890869141\n",
      "hidden_states min max: -15.958066940307617 16.049560546875\n",
      "hidden_state minus mean squared max: 241.35922241210938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.98922729492188 1.5388609170913696\n",
      "loss   510: 1.9841   grad norm: 1.0258          model param norm: 85.0016        \n",
      "\n",
      "quiet_star_policy_loss= -0.002664279891178012\n",
      "nll_loss= 1.9984979629516602\n",
      "avg_std= 0.5054476857185364\n",
      "dist std min max: 0.08580995351076126 0.5054476857185364 4.35858154296875\n",
      "hidden_states min max: -16.650758743286133 15.701181411743164\n",
      "hidden_state minus mean squared max: 247.29718017578125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.74497985839844 1.5355314016342163\n",
      "loss   511: 1.9958   grad norm: 1.0807          model param norm: 85.0066        \n",
      "\n",
      "quiet_star_policy_loss= 0.005522537510842085\n",
      "nll_loss= 2.0125648975372314\n",
      "avg_std= 0.5059717297554016\n",
      "dist std min max: 0.08518384397029877 0.5059717297554016 4.4459099769592285\n",
      "hidden_states min max: -14.984667778015137 16.518842697143555\n",
      "hidden_state minus mean squared max: 212.6901397705078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.77828216552734 1.5347999334335327\n",
      "loss   512: 2.0181   grad norm: 1.0546          model param norm: 85.0117        \n",
      "\n",
      "quiet_star_policy_loss= -0.04560675844550133\n",
      "nll_loss= 2.0093953609466553\n",
      "avg_std= 0.500766932964325\n",
      "dist std min max: 0.08473842591047287 0.500766932964325 4.512104511260986\n",
      "hidden_states min max: -15.26052474975586 15.050548553466797\n",
      "hidden_state minus mean squared max: 206.3455047607422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.13876342773438 1.5471223592758179\n",
      "loss   513: 1.9638   grad norm: 1.0111          model param norm: 85.0164        \n",
      "\n",
      "quiet_star_policy_loss= -0.04711885377764702\n",
      "nll_loss= 1.9979419708251953\n",
      "avg_std= 0.5043292045593262\n",
      "dist std min max: 0.08487140387296677 0.5043292045593262 4.4693427085876465\n",
      "hidden_states min max: -25.29098129272461 14.88316822052002\n",
      "hidden_state minus mean squared max: 531.7416381835938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.67064666748047 1.5360406637191772\n",
      "loss   514: 1.9508   grad norm: 1.0360          model param norm: 85.0208        \n",
      "\n",
      "quiet_star_policy_loss= -0.010848450474441051\n",
      "nll_loss= 1.9974972009658813\n",
      "avg_std= 0.5021853446960449\n",
      "dist std min max: 0.08488605916500092 0.5021853446960449 4.502593517303467\n",
      "hidden_states min max: -17.05433464050293 14.885262489318848\n",
      "hidden_state minus mean squared max: 284.306396484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.2334976196289 1.5394610166549683\n",
      "loss   515: 1.9866   grad norm: 1.0089          model param norm: 85.0247        \n",
      "\n",
      "quiet_star_policy_loss= -0.00996255874633789\n",
      "nll_loss= 1.9880380630493164\n",
      "avg_std= 0.5017306804656982\n",
      "dist std min max: 0.08433051407337189 0.5017306804656982 4.513667583465576\n",
      "hidden_states min max: -20.41221046447754 17.04096221923828\n",
      "hidden_state minus mean squared max: 368.65380859375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.48751068115234 1.551317572593689\n",
      "loss   516: 1.9781   grad norm: 1.0648          model param norm: 85.0284        \n",
      "\n",
      "quiet_star_policy_loss= -0.046195268630981445\n",
      "nll_loss= 2.002021312713623\n",
      "avg_std= 0.5027297139167786\n",
      "dist std min max: 0.0840451791882515 0.5027297139167786 4.530412673950195\n",
      "hidden_states min max: -16.895051956176758 15.138442993164062\n",
      "hidden_state minus mean squared max: 244.46665954589844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.23270416259766 1.539934754371643\n",
      "loss   517: 1.9558   grad norm: 1.0278          model param norm: 85.0321        \n",
      "\n",
      "quiet_star_policy_loss= 0.0006817817920818925\n",
      "nll_loss= 1.9835598468780518\n",
      "avg_std= 0.5010362863540649\n",
      "dist std min max: 0.08341087400913239 0.5010362863540649 4.51204776763916\n",
      "hidden_states min max: -19.48509407043457 16.917354583740234\n",
      "hidden_state minus mean squared max: 372.3194274902344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.48709869384766 1.5466145277023315\n",
      "loss   518: 1.9842   grad norm: 1.0026          model param norm: 85.0357        \n",
      "\n",
      "quiet_star_policy_loss= -0.02357959747314453\n",
      "nll_loss= 2.0126404762268066\n",
      "avg_std= 0.49960246682167053\n",
      "dist std min max: 0.08213844150304794 0.49960246682167053 4.559506893157959\n",
      "hidden_states min max: -16.115957260131836 16.847232818603516\n",
      "hidden_state minus mean squared max: 280.3978271484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.34136962890625 1.5514153242111206\n",
      "loss   519: 1.9891   grad norm: 1.0692          model param norm: 85.0394        \n",
      "\n",
      "quiet_star_policy_loss= 0.01077957171946764\n",
      "nll_loss= 1.9940948486328125\n",
      "avg_std= 0.49975839257240295\n",
      "dist std min max: 0.08248922973871231 0.49975839257240295 4.623383522033691\n",
      "hidden_states min max: -16.057865142822266 16.035167694091797\n",
      "hidden_state minus mean squared max: 258.9794006347656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.75550842285156 1.552167534828186\n",
      "loss   520: 2.0049   grad norm: 1.0640          model param norm: 85.0430        \n",
      "\n",
      "quiet_star_policy_loss= -0.0612308494746685\n",
      "nll_loss= 1.9888925552368164\n",
      "avg_std= 0.5017435550689697\n",
      "dist std min max: 0.08106759935617447 0.5017435550689697 4.605003356933594\n",
      "hidden_states min max: -18.230724334716797 16.626380920410156\n",
      "hidden_state minus mean squared max: 325.2634582519531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.77967071533203 1.5696252584457397\n",
      "loss   521: 1.9277   grad norm: 1.0633          model param norm: 85.0470        \n",
      "\n",
      "quiet_star_policy_loss= -0.020468616858124733\n",
      "nll_loss= 1.9990507364273071\n",
      "avg_std= 0.49776095151901245\n",
      "dist std min max: 0.0802612155675888 0.49776095151901245 4.587796211242676\n",
      "hidden_states min max: -18.00482177734375 17.16132354736328\n",
      "hidden_state minus mean squared max: 281.64190673828125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.23149871826172 1.5894376039505005\n",
      "loss   522: 1.9786   grad norm: 1.0875          model param norm: 85.0511        \n",
      "\n",
      "quiet_star_policy_loss= 0.03491048887372017\n",
      "nll_loss= 2.000844717025757\n",
      "avg_std= 0.49952882528305054\n",
      "dist std min max: 0.0787530317902565 0.49952882528305054 4.548087120056152\n",
      "hidden_states min max: -21.988454818725586 15.762478828430176\n",
      "hidden_state minus mean squared max: 591.431884765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.7238540649414 1.6098719835281372\n",
      "loss   523: 2.0358   grad norm: 1.0729          model param norm: 85.0551        \n",
      "\n",
      "quiet_star_policy_loss= 0.023617316037416458\n",
      "nll_loss= 2.006598711013794\n",
      "avg_std= 0.5015280842781067\n",
      "dist std min max: 0.07825697213411331 0.5015280842781067 4.594376087188721\n",
      "hidden_states min max: -18.148956298828125 16.520488739013672\n",
      "hidden_state minus mean squared max: 276.1197204589844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.03043365478516 1.6061047315597534\n",
      "loss   524: 2.0302   grad norm: 1.0270          model param norm: 85.0592        \n",
      "\n",
      "quiet_star_policy_loss= -0.026885438710451126\n",
      "nll_loss= 2.010335922241211\n",
      "avg_std= 0.49806010723114014\n",
      "dist std min max: 0.07784775644540787 0.49806010723114014 4.573381423950195\n",
      "hidden_states min max: -32.75103759765625 15.44237232208252\n",
      "hidden_state minus mean squared max: 1080.8494873046875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -105.02533721923828 1.6018568277359009\n",
      "loss   525: 1.9835   grad norm: 1.1511          model param norm: 85.0632        \n",
      "\n",
      "quiet_star_policy_loss= -0.014796162024140358\n",
      "nll_loss= 1.9809261560440063\n",
      "avg_std= 0.4997923672199249\n",
      "dist std min max: 0.07904105633497238 0.4997923672199249 4.63502836227417\n",
      "hidden_states min max: -15.868722915649414 17.658428192138672\n",
      "hidden_state minus mean squared max: 307.34466552734375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.66993713378906 1.5893605947494507\n",
      "loss   526: 1.9661   grad norm: 0.9898          model param norm: 85.0675        \n",
      "\n",
      "quiet_star_policy_loss= -0.0013798713916912675\n",
      "nll_loss= 2.0041401386260986\n",
      "avg_std= 0.4996222257614136\n",
      "dist std min max: 0.07860367000102997 0.4996222257614136 4.685574531555176\n",
      "hidden_states min max: -18.006500244140625 19.163259506225586\n",
      "hidden_state minus mean squared max: 359.1184997558594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3374252319336 1.60615074634552\n",
      "loss   527: 2.0028   grad norm: 1.0324          model param norm: 85.0718        \n",
      "\n",
      "quiet_star_policy_loss= -0.04918179661035538\n",
      "nll_loss= 2.00114369392395\n",
      "avg_std= 0.5009745359420776\n",
      "dist std min max: 0.07768035680055618 0.5009745359420776 4.676563262939453\n",
      "hidden_states min max: -16.948759078979492 16.643627166748047\n",
      "hidden_state minus mean squared max: 282.22100830078125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.18062591552734 1.6320527791976929\n",
      "loss   528: 1.9520   grad norm: 1.0396          model param norm: 85.0759        \n",
      "\n",
      "quiet_star_policy_loss= -0.02343587949872017\n",
      "nll_loss= 1.9858436584472656\n",
      "avg_std= 0.4970893859863281\n",
      "dist std min max: 0.07787751406431198 0.4970893859863281 4.70187520980835\n",
      "hidden_states min max: -18.652719497680664 18.535579681396484\n",
      "hidden_state minus mean squared max: 380.99066162109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.840087890625 1.6310874223709106\n",
      "loss   529: 1.9624   grad norm: 1.0051          model param norm: 85.0797        \n",
      "\n",
      "quiet_star_policy_loss= -0.04870929941534996\n",
      "nll_loss= 2.0020203590393066\n",
      "avg_std= 0.49853670597076416\n",
      "dist std min max: 0.07654011249542236 0.49853670597076416 4.74200963973999\n",
      "hidden_states min max: -21.0792179107666 17.696956634521484\n",
      "hidden_state minus mean squared max: 512.7677612304688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.6524887084961 1.638053059577942\n",
      "loss   530: 1.9533   grad norm: 1.1623          model param norm: 85.0836        \n",
      "\n",
      "quiet_star_policy_loss= -0.17894481122493744\n",
      "nll_loss= 2.0038163661956787\n",
      "avg_std= 0.4988299012184143\n",
      "dist std min max: 0.07708390802145004 0.4988299012184143 4.738224029541016\n",
      "hidden_states min max: -15.387928009033203 15.780386924743652\n",
      "hidden_state minus mean squared max: 243.83056640625\n",
      "hidden_state minus mean divided by std max: 4.957175254821777\n",
      "log_prob min max: -102.81351470947266 1.6302412748336792\n",
      "loss   531: 1.8249   grad norm: 2.1276          model param norm: 85.0874        \n",
      "\n",
      "quiet_star_policy_loss= -0.011336780153214931\n",
      "nll_loss= 1.9978694915771484\n",
      "avg_std= 0.4959016442298889\n",
      "dist std min max: 0.07547497749328613 0.4959016442298889 4.767999172210693\n",
      "hidden_states min max: -16.27554702758789 16.578710556030273\n",
      "hidden_state minus mean squared max: 273.85552978515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.26070404052734 1.6429890394210815\n",
      "loss   532: 1.9865   grad norm: 1.0912          model param norm: 85.0918        \n",
      "\n",
      "quiet_star_policy_loss= -0.009246349334716797\n",
      "nll_loss= 1.9875891208648682\n",
      "avg_std= 0.49528396129608154\n",
      "dist std min max: 0.07544032484292984 0.49528396129608154 4.791718482971191\n",
      "hidden_states min max: -22.165800094604492 19.584383010864258\n",
      "hidden_state minus mean squared max: 396.384033203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.45111083984375 1.6458410024642944\n",
      "loss   533: 1.9783   grad norm: 1.1354          model param norm: 85.0962        \n",
      "\n",
      "quiet_star_policy_loss= -0.00972528476268053\n",
      "nll_loss= 1.9907115697860718\n",
      "avg_std= 0.4972253739833832\n",
      "dist std min max: 0.07442474365234375 0.4972253739833832 4.839016437530518\n",
      "hidden_states min max: -16.450775146484375 19.39214515686035\n",
      "hidden_state minus mean squared max: 363.7581787109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.64508056640625 1.6566189527511597\n",
      "loss   534: 1.9810   grad norm: 1.1165          model param norm: 85.1009        \n",
      "\n",
      "quiet_star_policy_loss= -0.03908712789416313\n",
      "nll_loss= 2.0026795864105225\n",
      "avg_std= 0.4973534345626831\n",
      "dist std min max: 0.07324253022670746 0.4973534345626831 4.856645107269287\n",
      "hidden_states min max: -15.98668098449707 16.115238189697266\n",
      "hidden_state minus mean squared max: 298.8644104003906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80288696289062 1.6827963590621948\n",
      "loss   535: 1.9636   grad norm: 1.1055          model param norm: 85.1057        \n",
      "\n",
      "quiet_star_policy_loss= -0.03044109418988228\n",
      "nll_loss= 2.0013914108276367\n",
      "avg_std= 0.4932175576686859\n",
      "dist std min max: 0.07350943237543106 0.4932175576686859 4.853478908538818\n",
      "hidden_states min max: -15.45022201538086 16.1317081451416\n",
      "hidden_state minus mean squared max: 247.0630645751953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10466003417969 1.6814395189285278\n",
      "loss   536: 1.9710   grad norm: 1.0447          model param norm: 85.1103        \n",
      "\n",
      "quiet_star_policy_loss= -0.03419056162238121\n",
      "nll_loss= 1.9835201501846313\n",
      "avg_std= 0.4946204721927643\n",
      "dist std min max: 0.07402068376541138 0.4946204721927643 4.997486114501953\n",
      "hidden_states min max: -17.755229949951172 17.545516967773438\n",
      "hidden_state minus mean squared max: 278.5589904785156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.51383209228516 1.6683036088943481\n",
      "loss   537: 1.9493   grad norm: 1.0459          model param norm: 85.1151        \n",
      "\n",
      "quiet_star_policy_loss= -0.007058334536850452\n",
      "nll_loss= 1.9875993728637695\n",
      "avg_std= 0.49322885274887085\n",
      "dist std min max: 0.07268573343753815 0.49322885274887085 5.032604694366455\n",
      "hidden_states min max: -16.95730972290039 16.41716957092285\n",
      "hidden_state minus mean squared max: 273.5379943847656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.27252197265625 1.6729878187179565\n",
      "loss   538: 1.9805   grad norm: 1.0995          model param norm: 85.1197        \n",
      "\n",
      "quiet_star_policy_loss= -0.03543491289019585\n",
      "nll_loss= 1.9806740283966064\n",
      "avg_std= 0.49274468421936035\n",
      "dist std min max: 0.07329150289297104 0.49274468421936035 5.027575969696045\n",
      "hidden_states min max: -16.42125701904297 17.246938705444336\n",
      "hidden_state minus mean squared max: 297.6339416503906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.23908996582031 1.6727973222732544\n",
      "loss   539: 1.9452   grad norm: 1.1782          model param norm: 85.1246        \n",
      "\n",
      "quiet_star_policy_loss= -0.011636543087661266\n",
      "nll_loss= 1.9949826002120972\n",
      "avg_std= 0.4928709864616394\n",
      "dist std min max: 0.07321913540363312 0.4928709864616394 4.94469690322876\n",
      "hidden_states min max: -15.274611473083496 18.585365295410156\n",
      "hidden_state minus mean squared max: 386.0063171386719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.76420593261719 1.6765085458755493\n",
      "loss   540: 1.9833   grad norm: 1.0470          model param norm: 85.1290        \n",
      "\n",
      "quiet_star_policy_loss= -0.05153846740722656\n",
      "nll_loss= 1.9948053359985352\n",
      "avg_std= 0.4903661608695984\n",
      "dist std min max: 0.07310562580823898 0.4903661608695984 4.958942890167236\n",
      "hidden_states min max: -19.930898666381836 17.0137996673584\n",
      "hidden_state minus mean squared max: 393.46380615234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.94953155517578 1.6835123300552368\n",
      "loss   541: 1.9433   grad norm: 1.0847          model param norm: 85.1333        \n",
      "\n",
      "quiet_star_policy_loss= -0.025521064177155495\n",
      "nll_loss= 1.9880622625350952\n",
      "avg_std= 0.4900803864002228\n",
      "dist std min max: 0.07305295765399933 0.4900803864002228 4.899755954742432\n",
      "hidden_states min max: -17.190778732299805 17.99812126159668\n",
      "hidden_state minus mean squared max: 322.14764404296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0051040649414 1.6968387365341187\n",
      "loss   542: 1.9625   grad norm: 1.0677          model param norm: 85.1378        \n",
      "\n",
      "quiet_star_policy_loss= 0.0004062652587890625\n",
      "nll_loss= 1.9965404272079468\n",
      "avg_std= 0.486576646566391\n",
      "dist std min max: 0.07233277708292007 0.486576646566391 4.906737804412842\n",
      "hidden_states min max: -18.48345184326172 17.75161361694336\n",
      "hidden_state minus mean squared max: 340.3572082519531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.89939880371094 1.6992906332015991\n",
      "loss   543: 1.9969   grad norm: 1.1583          model param norm: 85.1423        \n",
      "\n",
      "quiet_star_policy_loss= -0.0014090538024902344\n",
      "nll_loss= 1.9933761358261108\n",
      "avg_std= 0.4906213879585266\n",
      "dist std min max: 0.07119958102703094 0.4906213879585266 4.949746608734131\n",
      "hidden_states min max: -18.0101261138916 18.33000946044922\n",
      "hidden_state minus mean squared max: 331.5825500488281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.43451690673828 1.7194634675979614\n",
      "loss   544: 1.9920   grad norm: 1.1347          model param norm: 85.1463        \n",
      "\n",
      "quiet_star_policy_loss= -0.015984846279025078\n",
      "nll_loss= 1.9871116876602173\n",
      "avg_std= 0.48677071928977966\n",
      "dist std min max: 0.06897743046283722 0.48677071928977966 4.978751182556152\n",
      "hidden_states min max: -16.88578987121582 19.852039337158203\n",
      "hidden_state minus mean squared max: 379.9063415527344\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -104.2555923461914 1.7262314558029175\n",
      "loss   545: 1.9711   grad norm: 1.0568          model param norm: 85.1503        \n",
      "\n",
      "quiet_star_policy_loss= -0.004226064775139093\n",
      "nll_loss= 1.9718306064605713\n",
      "avg_std= 0.48737141489982605\n",
      "dist std min max: 0.06893077492713928 0.48737141489982605 5.02172327041626\n",
      "hidden_states min max: -16.401052474975586 17.49204444885254\n",
      "hidden_state minus mean squared max: 305.9755554199219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.17410278320312 1.7414647340774536\n",
      "loss   546: 1.9676   grad norm: 1.0650          model param norm: 85.1544        \n",
      "\n",
      "quiet_star_policy_loss= -0.019203854724764824\n",
      "nll_loss= 1.9833040237426758\n",
      "avg_std= 0.4883189797401428\n",
      "dist std min max: 0.06712093204259872 0.4883189797401428 4.979557037353516\n",
      "hidden_states min max: -18.2791805267334 18.16581153869629\n",
      "hidden_state minus mean squared max: 333.5272521972656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.95381927490234 1.7711063623428345\n",
      "loss   547: 1.9641   grad norm: 1.2195          model param norm: 85.1585        \n",
      "\n",
      "quiet_star_policy_loss= -0.025153731927275658\n",
      "nll_loss= 1.9838908910751343\n",
      "avg_std= 0.48721712827682495\n",
      "dist std min max: 0.06562046706676483 0.48721712827682495 4.89993143081665\n",
      "hidden_states min max: -16.68304443359375 18.07549285888672\n",
      "hidden_state minus mean squared max: 312.77764892578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.45355224609375 1.7969228029251099\n",
      "loss   548: 1.9587   grad norm: 1.0787          model param norm: 85.1630        \n",
      "\n",
      "quiet_star_policy_loss= 0.022082870826125145\n",
      "nll_loss= 1.997536540031433\n",
      "avg_std= 0.4856252372264862\n",
      "dist std min max: 0.06573109328746796 0.4856252372264862 4.966443061828613\n",
      "hidden_states min max: -17.683853149414062 19.536357879638672\n",
      "hidden_state minus mean squared max: 375.7882385253906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.40792083740234 1.7760549783706665\n",
      "loss   549: 2.0196   grad norm: 1.1036          model param norm: 85.1671        \n",
      "\n",
      "quiet_star_policy_loss= -0.036527037620544434\n",
      "nll_loss= 1.9957313537597656\n",
      "avg_std= 0.48442915081977844\n",
      "dist std min max: 0.06597545742988586 0.48442915081977844 4.951948165893555\n",
      "hidden_states min max: -19.22744369506836 17.0410099029541\n",
      "hidden_state minus mean squared max: 376.03814697265625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.91475677490234 1.7759724855422974\n",
      "loss   550: 1.9592   grad norm: 1.1196          model param norm: 85.1710        \n",
      "\n",
      "quiet_star_policy_loss= -0.03328314051032066\n",
      "nll_loss= 2.0008139610290527\n",
      "avg_std= 0.48124217987060547\n",
      "dist std min max: 0.06521221250295639 0.48124217987060547 4.980161190032959\n",
      "hidden_states min max: -17.194406509399414 17.207128524780273\n",
      "hidden_state minus mean squared max: 293.8036804199219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.82284545898438 1.7982045412063599\n",
      "loss   551: 1.9675   grad norm: 1.1607          model param norm: 85.1745        \n",
      "\n",
      "quiet_star_policy_loss= 0.00027158259763382375\n",
      "nll_loss= 1.9720051288604736\n",
      "avg_std= 0.4835778474807739\n",
      "dist std min max: 0.06573536992073059 0.4835778474807739 5.027916431427002\n",
      "hidden_states min max: -17.56661605834961 20.05367088317871\n",
      "hidden_state minus mean squared max: 400.61187744140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.53507232666016 1.7875703573226929\n",
      "loss   552: 1.9723   grad norm: 1.1108          model param norm: 85.1777        \n",
      "\n",
      "quiet_star_policy_loss= -0.019094323739409447\n",
      "nll_loss= 1.973797082901001\n",
      "avg_std= 0.4838010370731354\n",
      "dist std min max: 0.0651244968175888 0.4838010370731354 4.98671817779541\n",
      "hidden_states min max: -17.38016700744629 19.856773376464844\n",
      "hidden_state minus mean squared max: 318.8583679199219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.76940155029297 1.785589575767517\n",
      "loss   553: 1.9547   grad norm: 1.1769          model param norm: 85.1809        \n",
      "\n",
      "quiet_star_policy_loss= -0.06753778457641602\n",
      "nll_loss= 1.9844303131103516\n",
      "avg_std= 0.47994258999824524\n",
      "dist std min max: 0.06542535126209259 0.47994258999824524 5.10350227355957\n",
      "hidden_states min max: -17.4093017578125 17.478374481201172\n",
      "hidden_state minus mean squared max: 310.26800537109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.18315124511719 1.7826098203659058\n",
      "loss   554: 1.9169   grad norm: 1.1391          model param norm: 85.1844        \n",
      "\n",
      "quiet_star_policy_loss= -0.03527989611029625\n",
      "nll_loss= 1.9862898588180542\n",
      "avg_std= 0.4802446663379669\n",
      "dist std min max: 0.06540940701961517 0.4802446663379669 5.108865737915039\n",
      "hidden_states min max: -20.40257453918457 19.290212631225586\n",
      "hidden_state minus mean squared max: 420.19927978515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.73319244384766 1.793967366218567\n",
      "loss   555: 1.9510   grad norm: 1.1114          model param norm: 85.1885        \n",
      "\n",
      "quiet_star_policy_loss= -0.05543839931488037\n",
      "nll_loss= 1.9852818250656128\n",
      "avg_std= 0.4793282449245453\n",
      "dist std min max: 0.06587885320186615 0.4793282449245453 5.0064592361450195\n",
      "hidden_states min max: -22.708721160888672 19.30681800842285\n",
      "hidden_state minus mean squared max: 469.1941833496094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.30611419677734 1.798993706703186\n",
      "loss   556: 1.9298   grad norm: 1.0720          model param norm: 85.1927        \n",
      "\n",
      "quiet_star_policy_loss= 0.003302621887996793\n",
      "nll_loss= 1.9943571090698242\n",
      "avg_std= 0.4759458303451538\n",
      "dist std min max: 0.06672316789627075 0.4759458303451538 4.979150295257568\n",
      "hidden_states min max: -17.418012619018555 17.612993240356445\n",
      "hidden_state minus mean squared max: 303.2047424316406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.47444152832031 1.7685507535934448\n",
      "loss   557: 1.9977   grad norm: 1.0892          model param norm: 85.1968        \n",
      "\n",
      "quiet_star_policy_loss= -0.009132814593613148\n",
      "nll_loss= 1.9956943988800049\n",
      "avg_std= 0.4763779640197754\n",
      "dist std min max: 0.06632345169782639 0.4763779640197754 4.981391906738281\n",
      "hidden_states min max: -18.453657150268555 17.111312866210938\n",
      "hidden_state minus mean squared max: 263.52117919921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30015563964844 1.7814422845840454\n",
      "loss   558: 1.9866   grad norm: 1.0716          model param norm: 85.2008        \n",
      "\n",
      "quiet_star_policy_loss= -0.059972330927848816\n",
      "nll_loss= 1.9813206195831299\n",
      "avg_std= 0.4755493104457855\n",
      "dist std min max: 0.06569303572177887 0.4755493104457855 4.925117492675781\n",
      "hidden_states min max: -18.18208122253418 15.60981559753418\n",
      "hidden_state minus mean squared max: 337.0516052246094\n",
      "hidden_state minus mean divided by std max: 4.957176208496094\n",
      "log_prob min max: -103.8896713256836 1.7727545499801636\n",
      "loss   559: 1.9213   grad norm: 2.2325          model param norm: 85.2046        \n",
      "\n",
      "quiet_star_policy_loss= 0.03492782264947891\n",
      "nll_loss= 1.9780044555664062\n",
      "avg_std= 0.4723738431930542\n",
      "dist std min max: 0.06528537720441818 0.4723738431930542 4.92657470703125\n",
      "hidden_states min max: -24.790802001953125 17.45722770690918\n",
      "hidden_state minus mean squared max: 524.970703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.66424560546875 1.8022195100784302\n",
      "loss   560: 2.0129   grad norm: 1.1191          model param norm: 85.2081        \n",
      "\n",
      "quiet_star_policy_loss= -0.06370539963245392\n",
      "nll_loss= 1.9838060140609741\n",
      "avg_std= 0.4753240644931793\n",
      "dist std min max: 0.06364285945892334 0.4753240644931793 4.8682403564453125\n",
      "hidden_states min max: -15.896611213684082 16.930143356323242\n",
      "hidden_state minus mean squared max: 264.2163391113281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.62505340576172 1.815388560295105\n",
      "loss   561: 1.9201   grad norm: 1.0493          model param norm: 85.2119        \n",
      "\n",
      "quiet_star_policy_loss= -0.03177366405725479\n",
      "nll_loss= 1.9784713983535767\n",
      "avg_std= 0.4739810526371002\n",
      "dist std min max: 0.06299612671136856 0.4739810526371002 4.906504154205322\n",
      "hidden_states min max: -15.833881378173828 17.889022827148438\n",
      "hidden_state minus mean squared max: 259.2550354003906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.2158203125 1.8401926755905151\n",
      "loss   562: 1.9467   grad norm: 1.1383          model param norm: 85.2157        \n",
      "\n",
      "quiet_star_policy_loss= -0.05668528005480766\n",
      "nll_loss= 1.9737821817398071\n",
      "avg_std= 0.473410427570343\n",
      "dist std min max: 0.062540203332901 0.473410427570343 4.829924583435059\n",
      "hidden_states min max: -20.897750854492188 22.1083927154541\n",
      "hidden_state minus mean squared max: 396.2053527832031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.91957092285156 1.852678656578064\n",
      "loss   563: 1.9171   grad norm: 1.1259          model param norm: 85.2196        \n",
      "\n",
      "quiet_star_policy_loss= -0.009841203689575195\n",
      "nll_loss= 1.97923743724823\n",
      "avg_std= 0.4714629352092743\n",
      "dist std min max: 0.06259232759475708 0.4714629352092743 4.897456645965576\n",
      "hidden_states min max: -20.799333572387695 19.667444229125977\n",
      "hidden_state minus mean squared max: 389.5436706542969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.51508331298828 1.8473790884017944\n",
      "loss   564: 1.9694   grad norm: 1.1471          model param norm: 85.2236        \n",
      "\n",
      "quiet_star_policy_loss= -0.006166744511574507\n",
      "nll_loss= 1.9883053302764893\n",
      "avg_std= 0.4714643359184265\n",
      "dist std min max: 0.06134099140763283 0.4714643359184265 4.8990912437438965\n",
      "hidden_states min max: -17.80267333984375 17.878618240356445\n",
      "hidden_state minus mean squared max: 325.0694580078125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9687728881836 1.865967869758606\n",
      "loss   565: 1.9821   grad norm: 1.1179          model param norm: 85.2275        \n",
      "\n",
      "quiet_star_policy_loss= -0.004178774543106556\n",
      "nll_loss= 1.9745311737060547\n",
      "avg_std= 0.47150465846061707\n",
      "dist std min max: 0.06219325587153435 0.47150465846061707 4.853069305419922\n",
      "hidden_states min max: -17.151453018188477 18.405282974243164\n",
      "hidden_state minus mean squared max: 294.4727478027344\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.77597045898438 1.8297351598739624\n",
      "loss   566: 1.9704   grad norm: 1.1204          model param norm: 85.2313        \n",
      "\n",
      "quiet_star_policy_loss= 0.01685314252972603\n",
      "nll_loss= 1.9653240442276\n",
      "avg_std= 0.7580967545509338\n",
      "dist std min max: 0.06267669796943665 0.4742434024810791 4.891965389251709\n",
      "hidden_states min max: -19.569211959838867 16.5439453125\n",
      "hidden_state minus mean squared max: 379.1570739746094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90840911865234 1.8309487104415894\n",
      "loss   567: 1.9822   grad norm: 1.0418          model param norm: 85.2353        \n",
      "\n",
      "quiet_star_policy_loss= -0.0072644236497581005\n",
      "nll_loss= 1.9772106409072876\n",
      "avg_std= 0.4699849784374237\n",
      "dist std min max: 0.06251309812068939 0.4699849784374237 4.893158912658691\n",
      "hidden_states min max: -17.0391845703125 17.313825607299805\n",
      "hidden_state minus mean squared max: 290.6369323730469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.85450744628906 1.8483408689498901\n",
      "loss   568: 1.9699   grad norm: 1.0988          model param norm: 85.2395        \n",
      "\n",
      "quiet_star_policy_loss= -0.051213838160037994\n",
      "nll_loss= 1.9830513000488281\n",
      "avg_std= 0.47174394130706787\n",
      "dist std min max: 0.06254126131534576 0.47174394130706787 4.9402689933776855\n",
      "hidden_states min max: -15.313718795776367 16.42645263671875\n",
      "hidden_state minus mean squared max: 267.9720153808594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.47807312011719 1.8266855478286743\n",
      "loss   569: 1.9318   grad norm: 1.1515          model param norm: 85.2435        \n",
      "\n",
      "quiet_star_policy_loss= -0.057567693293094635\n",
      "nll_loss= 1.9733301401138306\n",
      "avg_std= 0.4719991683959961\n",
      "dist std min max: 0.0623922236263752 0.4719991683959961 4.88912296295166\n",
      "hidden_states min max: -18.72726058959961 19.648832321166992\n",
      "hidden_state minus mean squared max: 384.7895202636719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -14.417630195617676 1.8290075063705444\n",
      "loss   570: 1.9158   grad norm: 1.1374          model param norm: 85.2472        \n",
      "\n",
      "quiet_star_policy_loss= -0.044361114501953125\n",
      "nll_loss= 1.9600502252578735\n",
      "avg_std= 0.4729550778865814\n",
      "dist std min max: 0.06254246830940247 0.4729550778865814 4.865396499633789\n",
      "hidden_states min max: -20.678565979003906 16.8190975189209\n",
      "hidden_state minus mean squared max: 393.0357360839844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.72307586669922 1.8306421041488647\n",
      "loss   571: 1.9157   grad norm: 1.1580          model param norm: 85.2512        \n",
      "\n",
      "quiet_star_policy_loss= 0.013851821422576904\n",
      "nll_loss= 1.9710140228271484\n",
      "avg_std= 0.4659471809864044\n",
      "dist std min max: 0.06158284842967987 0.4659471809864044 4.954315185546875\n",
      "hidden_states min max: -16.083219528198242 16.52284049987793\n",
      "hidden_state minus mean squared max: 281.7497253417969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.167835235595703 1.8516031503677368\n",
      "loss   572: 1.9849   grad norm: 1.1289          model param norm: 85.2556        \n",
      "\n",
      "quiet_star_policy_loss= -0.02113199234008789\n",
      "nll_loss= 1.9799152612686157\n",
      "avg_std= 0.4667065143585205\n",
      "dist std min max: 0.060743845999240875 0.4667065143585205 4.83718204498291\n",
      "hidden_states min max: -16.78450584411621 16.16236114501953\n",
      "hidden_state minus mean squared max: 287.50726318359375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9911880493164 1.8820830583572388\n",
      "loss   573: 1.9588   grad norm: 1.1765          model param norm: 85.2597        \n",
      "\n",
      "quiet_star_policy_loss= -0.01881094090640545\n",
      "nll_loss= 1.9676111936569214\n",
      "avg_std= 0.4671834707260132\n",
      "dist std min max: 0.06031768023967743 0.4671834707260132 4.904254913330078\n",
      "hidden_states min max: -15.627946853637695 17.37900161743164\n",
      "hidden_state minus mean squared max: 247.5643310546875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.54325866699219 1.8770533800125122\n",
      "loss   574: 1.9488   grad norm: 1.0650          model param norm: 85.2639        \n",
      "\n",
      "quiet_star_policy_loss= 0.006923485081642866\n",
      "nll_loss= 1.9849776029586792\n",
      "avg_std= 0.46556612849235535\n",
      "dist std min max: 0.05993720144033432 0.46556612849235535 4.876993656158447\n",
      "hidden_states min max: -16.955127716064453 18.06949234008789\n",
      "hidden_state minus mean squared max: 291.3349609375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.99333953857422 1.8911651372909546\n",
      "loss   575: 1.9919   grad norm: 1.1461          model param norm: 85.2683        \n",
      "\n",
      "quiet_star_policy_loss= -0.015706254169344902\n",
      "nll_loss= 1.9783868789672852\n",
      "avg_std= 0.46952715516090393\n",
      "dist std min max: 0.059354230761528015 0.46952715516090393 4.86998987197876\n",
      "hidden_states min max: -17.993738174438477 17.451032638549805\n",
      "hidden_state minus mean squared max: 329.260498046875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.20154571533203 1.8927603960037231\n",
      "loss   576: 1.9627   grad norm: 1.1786          model param norm: 85.2727        \n",
      "\n",
      "quiet_star_policy_loss= 0.006087017245590687\n",
      "nll_loss= 1.9693092107772827\n",
      "avg_std= 0.4631946384906769\n",
      "dist std min max: 0.0589037649333477 0.4631946384906769 4.842658519744873\n",
      "hidden_states min max: -20.554065704345703 17.698036193847656\n",
      "hidden_state minus mean squared max: 356.0082092285156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.47007751464844 1.9064639806747437\n",
      "loss   577: 1.9754   grad norm: 1.1318          model param norm: 85.2773        \n",
      "\n",
      "quiet_star_policy_loss= 0.0038585185538977385\n",
      "nll_loss= 1.974617600440979\n",
      "avg_std= 0.46560096740722656\n",
      "dist std min max: 0.0579427033662796 0.46560096740722656 4.878354549407959\n",
      "hidden_states min max: -16.663190841674805 18.22528839111328\n",
      "hidden_state minus mean squared max: 326.0666809082031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.2678451538086 1.9119340181350708\n",
      "loss   578: 1.9785   grad norm: 1.1268          model param norm: 85.2819        \n",
      "\n",
      "quiet_star_policy_loss= -0.026639461517333984\n",
      "nll_loss= 1.985591173171997\n",
      "avg_std= 0.46289175748825073\n",
      "dist std min max: 0.05743785947561264 0.46289175748825073 4.821820259094238\n",
      "hidden_states min max: -16.67755126953125 19.461774826049805\n",
      "hidden_state minus mean squared max: 381.25592041015625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.20048522949219 1.928171992301941\n",
      "loss   579: 1.9590   grad norm: 1.1492          model param norm: 85.2865        \n",
      "\n",
      "quiet_star_policy_loss= -0.02061023749411106\n",
      "nll_loss= 1.9752811193466187\n",
      "avg_std= 0.4645530581474304\n",
      "dist std min max: 0.056916315108537674 0.4645530581474304 4.833888053894043\n",
      "hidden_states min max: -15.998067855834961 17.084497451782227\n",
      "hidden_state minus mean squared max: 351.2868957519531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.46338653564453 1.9395116567611694\n",
      "loss   580: 1.9547   grad norm: 1.1494          model param norm: 85.2910        \n",
      "\n",
      "quiet_star_policy_loss= 0.006099069025367498\n",
      "nll_loss= 1.974172592163086\n",
      "avg_std= 0.4620334506034851\n",
      "dist std min max: 0.05628257244825363 0.4620334506034851 4.8721394538879395\n",
      "hidden_states min max: -16.236326217651367 17.864566802978516\n",
      "hidden_state minus mean squared max: 259.0115661621094\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.59759521484375 1.9578784704208374\n",
      "loss   581: 1.9803   grad norm: 1.1632          model param norm: 85.2954        \n",
      "\n",
      "quiet_star_policy_loss= -0.008436441421508789\n",
      "nll_loss= 1.9892864227294922\n",
      "avg_std= 0.4615878760814667\n",
      "dist std min max: 0.05627087131142616 0.4615878760814667 4.876134872436523\n",
      "hidden_states min max: -15.900797843933105 15.760430335998535\n",
      "hidden_state minus mean squared max: 251.1427764892578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.74452209472656 1.9385203123092651\n",
      "loss   582: 1.9808   grad norm: 1.1626          model param norm: 85.2996        \n",
      "\n",
      "quiet_star_policy_loss= -0.020042896270751953\n",
      "nll_loss= 1.976724624633789\n",
      "avg_std= 0.4598102867603302\n",
      "dist std min max: 0.055858999490737915 0.4598102867603302 4.902562141418457\n",
      "hidden_states min max: -17.255859375 22.541481018066406\n",
      "hidden_state minus mean squared max: 406.75836181640625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.75546264648438 1.9591716527938843\n",
      "loss   583: 1.9567   grad norm: 1.2172          model param norm: 85.3040        \n",
      "\n",
      "quiet_star_policy_loss= -0.00084009172860533\n",
      "nll_loss= 1.968939185142517\n",
      "avg_std= 0.4653489589691162\n",
      "dist std min max: 0.0548151433467865 0.4653489589691162 4.868294715881348\n",
      "hidden_states min max: -19.699626922607422 17.25955581665039\n",
      "hidden_state minus mean squared max: 380.65509033203125\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.97750091552734 1.976575493812561\n",
      "loss   584: 1.9681   grad norm: 1.1898          model param norm: 85.3082        \n",
      "\n",
      "quiet_star_policy_loss= -0.040691424161195755\n",
      "nll_loss= 1.9663361310958862\n",
      "avg_std= 0.4608685076236725\n",
      "dist std min max: 0.054432764649391174 0.4608685076236725 4.867425441741943\n",
      "hidden_states min max: -17.247316360473633 17.434423446655273\n",
      "hidden_state minus mean squared max: 300.27557373046875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.31748962402344 1.9779988527297974\n",
      "loss   585: 1.9256   grad norm: 1.1260          model param norm: 85.3126        \n",
      "\n",
      "quiet_star_policy_loss= -0.05193357542157173\n",
      "nll_loss= 1.9715232849121094\n",
      "avg_std= 0.46243131160736084\n",
      "dist std min max: 0.053387727588415146 0.46243131160736084 4.821205139160156\n",
      "hidden_states min max: -29.689855575561523 16.589542388916016\n",
      "hidden_state minus mean squared max: 881.1640014648438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.92320251464844 2.0012879371643066\n",
      "loss   586: 1.9196   grad norm: 1.1476          model param norm: 85.3172        \n",
      "\n",
      "quiet_star_policy_loss= 0.045670777559280396\n",
      "nll_loss= 2.0177502632141113\n",
      "avg_std= 0.45823249220848083\n",
      "dist std min max: 0.0542093850672245 0.45823249220848083 4.850485324859619\n",
      "hidden_states min max: -15.311826705932617 16.234172821044922\n",
      "hidden_state minus mean squared max: 232.49258422851562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.5877685546875 1.9629756212234497\n",
      "loss   587: 2.0634   grad norm: 2.1821          model param norm: 85.3217        \n",
      "\n",
      "quiet_star_policy_loss= -0.027100468054413795\n",
      "nll_loss= 1.9557186365127563\n",
      "avg_std= 0.4617152512073517\n",
      "dist std min max: 0.05326497554779053 0.4617152512073517 4.867631435394287\n",
      "hidden_states min max: -17.058822631835938 17.676441192626953\n",
      "hidden_state minus mean squared max: 316.5692443847656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.22335815429688 2.0045623779296875\n",
      "loss   588: 1.9286   grad norm: 1.1409          model param norm: 85.3258        \n",
      "\n",
      "quiet_star_policy_loss= -0.01795678213238716\n",
      "nll_loss= 1.9585663080215454\n",
      "avg_std= 0.4607142210006714\n",
      "dist std min max: 0.05382988601922989 0.4607142210006714 4.811466693878174\n",
      "hidden_states min max: -17.469417572021484 16.87946891784668\n",
      "hidden_state minus mean squared max: 304.2780456542969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.28614807128906 1.9816149473190308\n",
      "loss   589: 1.9406   grad norm: 1.1002          model param norm: 85.3300        \n",
      "\n",
      "quiet_star_policy_loss= -0.03427600860595703\n",
      "nll_loss= 1.9687756299972534\n",
      "avg_std= 0.46274009346961975\n",
      "dist std min max: 0.05393247306346893 0.46274009346961975 4.819096088409424\n",
      "hidden_states min max: -19.010391235351562 17.84309959411621\n",
      "hidden_state minus mean squared max: 367.99066162109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.03076934814453 1.9958165884017944\n",
      "loss   590: 1.9345   grad norm: 1.2142          model param norm: 85.3344        \n",
      "\n",
      "quiet_star_policy_loss= 0.009201842360198498\n",
      "nll_loss= 1.953999400138855\n",
      "avg_std= 0.4619925320148468\n",
      "dist std min max: 0.05424285680055618 0.4619925320148468 4.8498125076293945\n",
      "hidden_states min max: -18.07723617553711 16.084453582763672\n",
      "hidden_state minus mean squared max: 332.24102783203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80501556396484 1.9705299139022827\n",
      "loss   591: 1.9632   grad norm: 1.2553          model param norm: 85.3386        \n",
      "\n",
      "quiet_star_policy_loss= -0.03126673772931099\n",
      "nll_loss= 1.9723255634307861\n",
      "avg_std= 0.46050772070884705\n",
      "dist std min max: 0.05444273352622986 0.46050772070884705 4.944986820220947\n",
      "hidden_states min max: -15.198322296142578 17.76324462890625\n",
      "hidden_state minus mean squared max: 273.47625732421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.84416198730469 1.977626919746399\n",
      "loss   592: 1.9411   grad norm: 1.1741          model param norm: 85.3428        \n",
      "\n",
      "quiet_star_policy_loss= -0.03974122926592827\n",
      "nll_loss= 1.9821884632110596\n",
      "avg_std= 0.4619470238685608\n",
      "dist std min max: 0.055530183017253876 0.4619470238685608 4.899996280670166\n",
      "hidden_states min max: -17.461109161376953 16.843034744262695\n",
      "hidden_state minus mean squared max: 303.7900390625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.6316909790039 1.967366337776184\n",
      "loss   593: 1.9424   grad norm: 1.1202          model param norm: 85.3470        \n",
      "\n",
      "quiet_star_policy_loss= -0.06741690635681152\n",
      "nll_loss= 1.971085786819458\n",
      "avg_std= 0.4595717191696167\n",
      "dist std min max: 0.055781289935112 0.4595717191696167 4.986876010894775\n",
      "hidden_states min max: -17.079872131347656 18.358627319335938\n",
      "hidden_state minus mean squared max: 342.4163818359375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83428192138672 1.9537042379379272\n",
      "loss   594: 1.9037   grad norm: 1.1585          model param norm: 85.3509        \n",
      "\n",
      "quiet_star_policy_loss= -0.0007050514104776084\n",
      "nll_loss= 1.96376633644104\n",
      "avg_std= 0.460646390914917\n",
      "dist std min max: 0.056516651064157486 0.460646390914917 4.876161575317383\n",
      "hidden_states min max: -15.900370597839355 16.261037826538086\n",
      "hidden_state minus mean squared max: 306.43896484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.46378326416016 1.9504138231277466\n",
      "loss   595: 1.9631   grad norm: 1.2378          model param norm: 85.3544        \n",
      "\n",
      "quiet_star_policy_loss= 0.004602718632668257\n",
      "nll_loss= 1.9775558710098267\n",
      "avg_std= 0.45848503708839417\n",
      "dist std min max: 0.05686981603503227 0.45848503708839417 4.930251121520996\n",
      "hidden_states min max: -17.698999404907227 18.1863956451416\n",
      "hidden_state minus mean squared max: 331.2578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.27336883544922 1.9324744939804077\n",
      "loss   596: 1.9822   grad norm: 1.1786          model param norm: 85.3581        \n",
      "\n",
      "quiet_star_policy_loss= -0.01074439287185669\n",
      "nll_loss= 1.9709417819976807\n",
      "avg_std= 0.4583292305469513\n",
      "dist std min max: 0.05767461284995079 0.4583292305469513 4.9701409339904785\n",
      "hidden_states min max: -16.78958511352539 18.29956817626953\n",
      "hidden_state minus mean squared max: 335.9673156738281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.35111236572266 1.9155217409133911\n",
      "loss   597: 1.9602   grad norm: 1.1313          model param norm: 85.3616        \n",
      "\n",
      "quiet_star_policy_loss= -0.0019961358048021793\n",
      "nll_loss= 1.9674996137619019\n",
      "avg_std= 0.45930224657058716\n",
      "dist std min max: 0.058561090379953384 0.45930224657058716 4.927117347717285\n",
      "hidden_states min max: -16.819242477416992 17.18282699584961\n",
      "hidden_state minus mean squared max: 291.6897277832031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.64400482177734 1.9100064039230347\n",
      "loss   598: 1.9655   grad norm: 1.2343          model param norm: 85.3652        \n",
      "\n",
      "quiet_star_policy_loss= -0.02835564687848091\n",
      "nll_loss= 1.9728816747665405\n",
      "avg_std= 0.45811691880226135\n",
      "dist std min max: 0.05879040062427521 0.45811691880226135 4.894092082977295\n",
      "hidden_states min max: -16.808420181274414 18.11432456970215\n",
      "hidden_state minus mean squared max: 345.9859313964844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.02021789550781 1.9066299200057983\n",
      "loss   599: 1.9445   grad norm: 1.1433          model param norm: 85.3688        \n",
      "eval loss 1.970141053199768\n",
      "\n",
      "quiet_star_policy_loss= -0.03188939020037651\n",
      "nll_loss= 1.9523652791976929\n",
      "avg_std= 0.45946675539016724\n",
      "dist std min max: 0.05838199332356453 0.45946675539016724 4.878635406494141\n",
      "hidden_states min max: -17.889684677124023 17.724639892578125\n",
      "hidden_state minus mean squared max: 320.0062255859375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.70372009277344 1.8951088190078735\n",
      "loss   600: 1.9205   grad norm: 1.2023          model param norm: 85.3726        \n",
      "\n",
      "quiet_star_policy_loss= -0.049065280705690384\n",
      "nll_loss= 1.9762042760849\n",
      "avg_std= 0.45551151037216187\n",
      "dist std min max: 0.05869948863983154 0.45551151037216187 4.952124118804932\n",
      "hidden_states min max: -42.132877349853516 17.267414093017578\n",
      "hidden_state minus mean squared max: 1989.08984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.33029174804688 1.8956729173660278\n",
      "loss   601: 1.9271   grad norm: 1.1498          model param norm: 85.3764        \n",
      "\n",
      "quiet_star_policy_loss= -0.052023746073246\n",
      "nll_loss= 1.962755560874939\n",
      "avg_std= 0.45536354184150696\n",
      "dist std min max: 0.05915946513414383 0.45536354184150696 4.914948463439941\n",
      "hidden_states min max: -16.537076950073242 17.549821853637695\n",
      "hidden_state minus mean squared max: 284.0297546386719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.73417663574219 1.8908888101577759\n",
      "loss   602: 1.9107   grad norm: 1.2528          model param norm: 85.3803        \n",
      "\n",
      "quiet_star_policy_loss= -0.02509925328195095\n",
      "nll_loss= 1.9728666543960571\n",
      "avg_std= 0.45291608572006226\n",
      "dist std min max: 0.05918186530470848 0.45291608572006226 4.883086204528809\n",
      "hidden_states min max: -20.477493286132812 16.501375198364258\n",
      "hidden_state minus mean squared max: 409.7781982421875\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.97994995117188 1.9072235822677612\n",
      "loss   603: 1.9478   grad norm: 1.1658          model param norm: 85.3844        \n",
      "\n",
      "quiet_star_policy_loss= -0.03139999136328697\n",
      "nll_loss= 1.9721224308013916\n",
      "avg_std= 0.4563139081001282\n",
      "dist std min max: 0.05898405238986015 0.4563139081001282 4.87059211730957\n",
      "hidden_states min max: -17.836713790893555 17.44695281982422\n",
      "hidden_state minus mean squared max: 318.7174072265625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.65213775634766 1.8982709646224976\n",
      "loss   604: 1.9407   grad norm: 1.2230          model param norm: 85.3886        \n",
      "\n",
      "quiet_star_policy_loss= -0.06155004724860191\n",
      "nll_loss= 1.957648515701294\n",
      "avg_std= 0.455020546913147\n",
      "dist std min max: 0.0591529905796051 0.455020546913147 4.911424160003662\n",
      "hidden_states min max: -17.134092330932617 18.03553581237793\n",
      "hidden_state minus mean squared max: 330.8832092285156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.84490966796875 1.8958359956741333\n",
      "loss   605: 1.8961   grad norm: 1.1525          model param norm: 85.3926        \n",
      "\n",
      "quiet_star_policy_loss= 0.0026180564891546965\n",
      "nll_loss= 1.9445762634277344\n",
      "avg_std= 0.4552173912525177\n",
      "dist std min max: 0.059257518500089645 0.4552173912525177 4.846574306488037\n",
      "hidden_states min max: -18.042821884155273 17.32659912109375\n",
      "hidden_state minus mean squared max: 324.7351379394531\n",
      "hidden_state minus mean divided by std max: 5.166576385498047\n",
      "log_prob min max: -104.21569061279297 1.887074589729309\n",
      "loss   606: 1.9472   grad norm: 1.1591          model param norm: 85.3965        \n",
      "\n",
      "quiet_star_policy_loss= -0.01182336825877428\n",
      "nll_loss= 1.955186128616333\n",
      "avg_std= 0.4514482319355011\n",
      "dist std min max: 0.06016765907406807 0.4514482319355011 4.805558204650879\n",
      "hidden_states min max: -16.11359214782715 17.443716049194336\n",
      "hidden_state minus mean squared max: 264.2196350097656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.5755844116211 1.8734627962112427\n",
      "loss   607: 1.9434   grad norm: 1.1512          model param norm: 85.4004        \n",
      "\n",
      "quiet_star_policy_loss= -0.029767990112304688\n",
      "nll_loss= 1.9791433811187744\n",
      "avg_std= 0.4507211148738861\n",
      "dist std min max: 0.060094185173511505 0.4507211148738861 4.821747303009033\n",
      "hidden_states min max: -27.340150833129883 19.995746612548828\n",
      "hidden_state minus mean squared max: 709.4273071289062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.8147964477539 1.8863362073898315\n",
      "loss   608: 1.9494   grad norm: 1.1883          model param norm: 85.4045        \n",
      "\n",
      "quiet_star_policy_loss= -0.04664568975567818\n",
      "nll_loss= 1.9665132761001587\n",
      "avg_std= 0.45079168677330017\n",
      "dist std min max: 0.0597904808819294 0.45079168677330017 4.755643844604492\n",
      "hidden_states min max: -17.942970275878906 17.061384201049805\n",
      "hidden_state minus mean squared max: 318.7372131347656\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.97530364990234 1.8824504613876343\n",
      "loss   609: 1.9199   grad norm: 1.1971          model param norm: 85.4087        \n",
      "\n",
      "quiet_star_policy_loss= -0.02483987808227539\n",
      "nll_loss= 1.9605255126953125\n",
      "avg_std= 0.44924014806747437\n",
      "dist std min max: 0.059782084077596664 0.44924014806747437 4.744326591491699\n",
      "hidden_states min max: -17.893272399902344 20.17160987854004\n",
      "hidden_state minus mean squared max: 324.7421569824219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.96633911132812 1.8870216608047485\n",
      "loss   610: 1.9357   grad norm: 1.1268          model param norm: 85.4128        \n",
      "\n",
      "quiet_star_policy_loss= -0.007490062620490789\n",
      "nll_loss= 1.9678287506103516\n",
      "avg_std= 0.4497690200805664\n",
      "dist std min max: 0.05981594696640968 0.4497690200805664 4.809444904327393\n",
      "hidden_states min max: -18.319711685180664 18.14011573791504\n",
      "hidden_state minus mean squared max: 288.3945007324219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.20177459716797 1.894293189048767\n",
      "loss   611: 1.9603   grad norm: 1.0914          model param norm: 85.4170        \n",
      "\n",
      "quiet_star_policy_loss= -0.01051566656678915\n",
      "nll_loss= 1.9627383947372437\n",
      "avg_std= 0.4470829963684082\n",
      "dist std min max: 0.05992313101887703 0.4470829963684082 4.745776653289795\n",
      "hidden_states min max: -17.244884490966797 17.331195831298828\n",
      "hidden_state minus mean squared max: 298.1603698730469\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -14.04452133178711 1.8758448362350464\n",
      "loss   612: 1.9522   grad norm: 1.1445          model param norm: 85.4213        \n",
      "\n",
      "quiet_star_policy_loss= -0.020946383476257324\n",
      "nll_loss= 1.9757565259933472\n",
      "avg_std= 0.44717979431152344\n",
      "dist std min max: 0.059443339705467224 0.44717979431152344 4.75550651550293\n",
      "hidden_states min max: -17.104351043701172 20.398136138916016\n",
      "hidden_state minus mean squared max: 341.75848388671875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.53738403320312 1.9031964540481567\n",
      "loss   613: 1.9548   grad norm: 1.1954          model param norm: 85.4257        \n",
      "\n",
      "quiet_star_policy_loss= -0.01786346547305584\n",
      "nll_loss= 1.9538204669952393\n",
      "avg_std= 0.446230947971344\n",
      "dist std min max: 0.05944259092211723 0.446230947971344 4.708110332489014\n",
      "hidden_states min max: -19.823776245117188 19.861940383911133\n",
      "hidden_state minus mean squared max: 396.6688232421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3619155883789 1.8753701448440552\n",
      "loss   614: 1.9360   grad norm: 1.1266          model param norm: 85.4298        \n",
      "\n",
      "quiet_star_policy_loss= 0.00211702985689044\n",
      "nll_loss= 1.9674537181854248\n",
      "avg_std= 0.4429270029067993\n",
      "dist std min max: 0.061310842633247375 0.4429270029067993 4.713534355163574\n",
      "hidden_states min max: -15.224334716796875 17.904865264892578\n",
      "hidden_state minus mean squared max: 246.57615661621094\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -14.06725025177002 1.8351856470108032\n",
      "loss   615: 1.9696   grad norm: 2.4447          model param norm: 85.4338        \n",
      "\n",
      "quiet_star_policy_loss= -0.007574200630187988\n",
      "nll_loss= 1.9588730335235596\n",
      "avg_std= 0.4440087676048279\n",
      "dist std min max: 0.05976583808660507 0.4440087676048279 4.6967902183532715\n",
      "hidden_states min max: -22.340566635131836 18.414520263671875\n",
      "hidden_state minus mean squared max: 413.97509765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.54434204101562 1.8747931718826294\n",
      "loss   616: 1.9513   grad norm: 1.1607          model param norm: 85.4378        \n",
      "\n",
      "quiet_star_policy_loss= -0.041048526763916016\n",
      "nll_loss= 1.9604949951171875\n",
      "avg_std= 0.44379723072052\n",
      "dist std min max: 0.05783747509121895 0.44379723072052 4.704354763031006\n",
      "hidden_states min max: -20.813474655151367 20.624006271362305\n",
      "hidden_state minus mean squared max: 376.8351745605469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.49848937988281 1.9085558652877808\n",
      "loss   617: 1.9194   grad norm: 1.2127          model param norm: 85.4417        \n",
      "\n",
      "quiet_star_policy_loss= -0.030388975515961647\n",
      "nll_loss= 1.9524439573287964\n",
      "avg_std= 0.44615638256073\n",
      "dist std min max: 0.0598064623773098 0.44615638256073 4.775533199310303\n",
      "hidden_states min max: -30.205495834350586 19.865802764892578\n",
      "hidden_state minus mean squared max: 1042.382568359375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -105.00721740722656 1.867050051689148\n",
      "loss   618: 1.9221   grad norm: 1.2045          model param norm: 85.4455        \n",
      "\n",
      "quiet_star_policy_loss= -0.02028508298099041\n",
      "nll_loss= 1.9516327381134033\n",
      "avg_std= 0.4416991174221039\n",
      "dist std min max: 0.060259219259023666 0.4416991174221039 4.76254415512085\n",
      "hidden_states min max: -16.528217315673828 19.61480712890625\n",
      "hidden_state minus mean squared max: 377.59710693359375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.40373229980469 1.859397292137146\n",
      "loss   619: 1.9313   grad norm: 1.1369          model param norm: 85.4495        \n",
      "\n",
      "quiet_star_policy_loss= -0.013125133700668812\n",
      "nll_loss= 1.959093451499939\n",
      "avg_std= 0.44161149859428406\n",
      "dist std min max: 0.06071808934211731 0.44161149859428406 4.707632064819336\n",
      "hidden_states min max: -17.229909896850586 16.509218215942383\n",
      "hidden_state minus mean squared max: 302.75048828125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.35919952392578 1.872259259223938\n",
      "loss   620: 1.9460   grad norm: 1.1202          model param norm: 85.4538        \n",
      "\n",
      "quiet_star_policy_loss= 0.009569001384079456\n",
      "nll_loss= 1.9566211700439453\n",
      "avg_std= 0.44217896461486816\n",
      "dist std min max: 0.059787046164274216 0.44217896461486816 4.63701868057251\n",
      "hidden_states min max: -16.335912704467773 17.112302780151367\n",
      "hidden_state minus mean squared max: 272.9856262207031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5506591796875 1.8886560201644897\n",
      "loss   621: 1.9662   grad norm: 1.1727          model param norm: 85.4577        \n",
      "\n",
      "quiet_star_policy_loss= -0.020686686038970947\n",
      "nll_loss= 1.96533203125\n",
      "avg_std= 0.44341135025024414\n",
      "dist std min max: 0.06021544709801674 0.44341135025024414 4.675972938537598\n",
      "hidden_states min max: -14.4426908493042 17.123062133789062\n",
      "hidden_state minus mean squared max: 216.4793243408203\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.30789947509766 1.890786051750183\n",
      "loss   622: 1.9446   grad norm: 1.1201          model param norm: 85.4616        \n",
      "\n",
      "quiet_star_policy_loss= -0.0376247875392437\n",
      "nll_loss= 1.9582659006118774\n",
      "avg_std= 0.44206300377845764\n",
      "dist std min max: 0.061057694256305695 0.44206300377845764 4.54703950881958\n",
      "hidden_states min max: -16.05246353149414 16.421552658081055\n",
      "hidden_state minus mean squared max: 313.2164306640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.57357025146484 1.8557389974594116\n",
      "loss   623: 1.9206   grad norm: 1.1425          model param norm: 85.4655        \n",
      "\n",
      "quiet_star_policy_loss= -0.008331132121384144\n",
      "nll_loss= 1.9523422718048096\n",
      "avg_std= 0.44457557797431946\n",
      "dist std min max: 0.06141192093491554 0.44457557797431946 4.541491508483887\n",
      "hidden_states min max: -16.570415496826172 17.90298080444336\n",
      "hidden_state minus mean squared max: 272.05780029296875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.54502868652344 1.8506349325180054\n",
      "loss   624: 1.9440   grad norm: 1.1922          model param norm: 85.4692        \n",
      "\n",
      "quiet_star_policy_loss= -0.035336684435606\n",
      "nll_loss= 1.9394352436065674\n",
      "avg_std= 0.4420488774776459\n",
      "dist std min max: 0.061049334704875946 0.4420488774776459 4.472054958343506\n",
      "hidden_states min max: -21.972705841064453 20.21758460998535\n",
      "hidden_state minus mean squared max: 467.3111877441406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.60606384277344 1.8575295209884644\n",
      "loss   625: 1.9041   grad norm: 1.2108          model param norm: 85.4726        \n",
      "\n",
      "quiet_star_policy_loss= -0.00487287063151598\n",
      "nll_loss= 1.9491599798202515\n",
      "avg_std= 0.4416554570198059\n",
      "dist std min max: 0.060085635632276535 0.4416554570198059 4.462887287139893\n",
      "hidden_states min max: -17.144672393798828 17.17206382751465\n",
      "hidden_state minus mean squared max: 297.4008483886719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.51204681396484 1.8739656209945679\n",
      "loss   626: 1.9443   grad norm: 1.1811          model param norm: 85.4759        \n",
      "\n",
      "quiet_star_policy_loss= 0.006846690084785223\n",
      "nll_loss= 1.9538230895996094\n",
      "avg_std= 0.44226592779159546\n",
      "dist std min max: 0.06029506400227547 0.44226592779159546 4.429370403289795\n",
      "hidden_states min max: -18.92889976501465 18.953086853027344\n",
      "hidden_state minus mean squared max: 292.81744384765625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.37234497070312 1.8739465475082397\n",
      "loss   627: 1.9607   grad norm: 1.1735          model param norm: 85.4791        \n",
      "\n",
      "quiet_star_policy_loss= -0.0019488573307171464\n",
      "nll_loss= 1.955196738243103\n",
      "avg_std= 0.43887579441070557\n",
      "dist std min max: 0.06023460254073143 0.43887579441070557 4.427384376525879\n",
      "hidden_states min max: -15.618721961975098 17.750131607055664\n",
      "hidden_state minus mean squared max: 312.36749267578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.11381530761719 1.8733967542648315\n",
      "loss   628: 1.9532   grad norm: 1.2495          model param norm: 85.4825        \n",
      "\n",
      "quiet_star_policy_loss= 0.009465194307267666\n",
      "nll_loss= 1.9623855352401733\n",
      "avg_std= 0.4384901821613312\n",
      "dist std min max: 0.05891561880707741 0.4384901821613312 4.399062633514404\n",
      "hidden_states min max: -14.440327644348145 16.248239517211914\n",
      "hidden_state minus mean squared max: 220.18492126464844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69770812988281 1.8801668882369995\n",
      "loss   629: 1.9719   grad norm: 1.1210          model param norm: 85.4858        \n",
      "\n",
      "quiet_star_policy_loss= -0.02509927749633789\n",
      "nll_loss= 1.9421013593673706\n",
      "avg_std= 0.43992945551872253\n",
      "dist std min max: 0.05771606042981148 0.43992945551872253 4.405549049377441\n",
      "hidden_states min max: -15.098285675048828 16.326374053955078\n",
      "hidden_state minus mean squared max: 242.03208923339844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.11659240722656 1.8879247903823853\n",
      "loss   630: 1.9170   grad norm: 1.1210          model param norm: 85.4891        \n",
      "\n",
      "quiet_star_policy_loss= -0.03252911567687988\n",
      "nll_loss= 1.952484130859375\n",
      "avg_std= 0.4391529858112335\n",
      "dist std min max: 0.060303907841444016 0.4391529858112335 4.3681230545043945\n",
      "hidden_states min max: -15.188018798828125 16.94432830810547\n",
      "hidden_state minus mean squared max: 230.5121307373047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.61030578613281 1.8712636232376099\n",
      "loss   631: 1.9200   grad norm: 1.1679          model param norm: 85.4925        \n",
      "\n",
      "quiet_star_policy_loss= -0.00921630859375\n",
      "nll_loss= 1.9648637771606445\n",
      "avg_std= 0.43654245138168335\n",
      "dist std min max: 0.06049567833542824 0.43654245138168335 4.366469383239746\n",
      "hidden_states min max: -20.791736602783203 17.23952865600586\n",
      "hidden_state minus mean squared max: 321.3157958984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4188003540039 1.8649159669876099\n",
      "loss   632: 1.9556   grad norm: 1.1309          model param norm: 85.4957        \n",
      "\n",
      "quiet_star_policy_loss= -0.011759924702346325\n",
      "nll_loss= 1.9595943689346313\n",
      "avg_std= 0.4361538589000702\n",
      "dist std min max: 0.06091693788766861 0.4361538589000702 4.39231538772583\n",
      "hidden_states min max: -14.964027404785156 16.91410255432129\n",
      "hidden_state minus mean squared max: 238.35594177246094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.85920715332031 1.8636003732681274\n",
      "loss   633: 1.9478   grad norm: 1.2261          model param norm: 85.4987        \n",
      "\n",
      "quiet_star_policy_loss= -0.01011812686920166\n",
      "nll_loss= 1.945919394493103\n",
      "avg_std= 0.437221884727478\n",
      "dist std min max: 0.061048902571201324 0.437221884727478 4.372791767120361\n",
      "hidden_states min max: -30.687335968017578 17.58315658569336\n",
      "hidden_state minus mean squared max: 902.4937744140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.93515014648438 1.8745602369308472\n",
      "loss   634: 1.9358   grad norm: 1.2937          model param norm: 85.5017        \n",
      "\n",
      "quiet_star_policy_loss= -0.021679116412997246\n",
      "nll_loss= 1.9546453952789307\n",
      "avg_std= 0.4388939142227173\n",
      "dist std min max: 0.061598461121320724 0.4388939142227173 4.375130653381348\n",
      "hidden_states min max: -14.368917465209961 17.35763168334961\n",
      "hidden_state minus mean squared max: 252.383056640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.63001251220703 1.860906720161438\n",
      "loss   635: 1.9330   grad norm: 1.2433          model param norm: 85.5049        \n",
      "\n",
      "quiet_star_policy_loss= -0.02603936195373535\n",
      "nll_loss= 1.949156641960144\n",
      "avg_std= 0.4395850896835327\n",
      "dist std min max: 0.06070958822965622 0.4395850896835327 4.361557483673096\n",
      "hidden_states min max: -16.053674697875977 20.526514053344727\n",
      "hidden_state minus mean squared max: 324.1510925292969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.55255126953125 1.8549476861953735\n",
      "loss   636: 1.9231   grad norm: 1.2093          model param norm: 85.5083        \n",
      "\n",
      "quiet_star_policy_loss= -0.023964906111359596\n",
      "nll_loss= 1.955020546913147\n",
      "avg_std= 0.43730050325393677\n",
      "dist std min max: 0.061998337507247925 0.43730050325393677 4.376946449279785\n",
      "hidden_states min max: -14.086885452270508 16.335113525390625\n",
      "hidden_state minus mean squared max: 226.51809692382812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.35903930664062 1.8385993242263794\n",
      "loss   637: 1.9311   grad norm: 1.1227          model param norm: 85.5119        \n",
      "\n",
      "quiet_star_policy_loss= -0.05036621168255806\n",
      "nll_loss= 1.9612705707550049\n",
      "avg_std= 0.43801364302635193\n",
      "dist std min max: 0.062044575810432434 0.43801364302635193 4.358767509460449\n",
      "hidden_states min max: -24.992843627929688 16.26300621032715\n",
      "hidden_state minus mean squared max: 584.5392456054688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.71800231933594 1.8388854265213013\n",
      "loss   638: 1.9109   grad norm: 1.2491          model param norm: 85.5151        \n",
      "\n",
      "quiet_star_policy_loss= -0.041132308542728424\n",
      "nll_loss= 1.964309573173523\n",
      "avg_std= 0.43776455521583557\n",
      "dist std min max: 0.060811445116996765 0.43776455521583557 4.370668411254883\n",
      "hidden_states min max: -14.499354362487793 20.023427963256836\n",
      "hidden_state minus mean squared max: 316.30853271484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.52459716796875 1.858028769493103\n",
      "loss   639: 1.9232   grad norm: 1.2285          model param norm: 85.5182        \n",
      "\n",
      "quiet_star_policy_loss= -0.014061677269637585\n",
      "nll_loss= 1.930096983909607\n",
      "avg_std= 0.43819868564605713\n",
      "dist std min max: 0.06119058281183243 0.43819868564605713 4.36011266708374\n",
      "hidden_states min max: -14.987706184387207 17.0410099029541\n",
      "hidden_state minus mean squared max: 235.46034240722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8616714477539 1.8652130365371704\n",
      "loss   640: 1.9160   grad norm: 1.1813          model param norm: 85.5213        \n",
      "\n",
      "quiet_star_policy_loss= -0.005625343415886164\n",
      "nll_loss= 1.9601033926010132\n",
      "avg_std= 0.4343167841434479\n",
      "dist std min max: 0.0617746077477932 0.4343167841434479 4.369629859924316\n",
      "hidden_states min max: -14.047487258911133 17.248287200927734\n",
      "hidden_state minus mean squared max: 288.7319030761719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.74415588378906 1.8519355058670044\n",
      "loss   641: 1.9545   grad norm: 1.2108          model param norm: 85.5244        \n",
      "\n",
      "quiet_star_policy_loss= -0.01867385022342205\n",
      "nll_loss= 1.9574545621871948\n",
      "avg_std= 0.43647342920303345\n",
      "dist std min max: 0.06108827143907547 0.43647342920303345 4.397218704223633\n",
      "hidden_states min max: -13.579967498779297 17.398326873779297\n",
      "hidden_state minus mean squared max: 280.7048034667969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.079833984375 1.8581899404525757\n",
      "loss   642: 1.9388   grad norm: 1.1792          model param norm: 85.5275        \n",
      "\n",
      "quiet_star_policy_loss= 0.013704681769013405\n",
      "nll_loss= 1.921838402748108\n",
      "avg_std= 0.43277472257614136\n",
      "dist std min max: 0.06234145909547806 0.43277472257614136 4.40347957611084\n",
      "hidden_states min max: -14.363980293273926 16.644569396972656\n",
      "hidden_state minus mean squared max: 209.27659606933594\n",
      "hidden_state minus mean divided by std max: 5.035405158996582\n",
      "log_prob min max: -12.882190704345703 1.829849123954773\n",
      "loss   643: 1.9355   grad norm: 2.4708          model param norm: 85.5308        \n",
      "\n",
      "quiet_star_policy_loss= -0.04111599922180176\n",
      "nll_loss= 1.9404773712158203\n",
      "avg_std= 0.43728107213974\n",
      "dist std min max: 0.061229508370161057 0.43728107213974 4.366458892822266\n",
      "hidden_states min max: -13.750378608703613 18.373565673828125\n",
      "hidden_state minus mean squared max: 259.77130126953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.890130996704102 1.8585339784622192\n",
      "loss   644: 1.8994   grad norm: 1.1963          model param norm: 85.5345        \n",
      "\n",
      "quiet_star_policy_loss= -0.004839754197746515\n",
      "nll_loss= 1.9550228118896484\n",
      "avg_std= 0.5006903409957886\n",
      "dist std min max: 0.06228689104318619 0.4361463487148285 4.338719367980957\n",
      "hidden_states min max: -14.205113410949707 16.4838924407959\n",
      "hidden_state minus mean squared max: 262.0683288574219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.63768005371094 1.8436723947525024\n",
      "loss   645: 1.9502   grad norm: 1.2236          model param norm: 85.5382        \n",
      "\n",
      "quiet_star_policy_loss= -0.04039612039923668\n",
      "nll_loss= 1.9462753534317017\n",
      "avg_std= 0.4373702108860016\n",
      "dist std min max: 0.06184498965740204 0.4373702108860016 4.310748100280762\n",
      "hidden_states min max: -14.20637321472168 19.065156936645508\n",
      "hidden_state minus mean squared max: 353.0887451171875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.75135040283203 1.8557730913162231\n",
      "loss   646: 1.9059   grad norm: 1.1422          model param norm: 85.5418        \n",
      "\n",
      "quiet_star_policy_loss= -0.029631232842803\n",
      "nll_loss= 1.9595355987548828\n",
      "avg_std= 0.43484485149383545\n",
      "dist std min max: 0.06083322688937187 0.43484485149383545 4.2745490074157715\n",
      "hidden_states min max: -17.606468200683594 15.707565307617188\n",
      "hidden_state minus mean squared max: 315.4030456542969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.747802734375 1.8553334474563599\n",
      "loss   647: 1.9299   grad norm: 1.2021          model param norm: 85.5452        \n",
      "\n",
      "quiet_star_policy_loss= 0.00595436105504632\n",
      "nll_loss= 1.957204818725586\n",
      "avg_std= 0.43768376111984253\n",
      "dist std min max: 0.062267009168863297 0.43768376111984253 4.2521281242370605\n",
      "hidden_states min max: -14.262118339538574 15.71223258972168\n",
      "hidden_state minus mean squared max: 239.56199645996094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.51740264892578 1.8393627405166626\n",
      "loss   648: 1.9632   grad norm: 1.1430          model param norm: 85.5485        \n",
      "\n",
      "quiet_star_policy_loss= 0.014253235422074795\n",
      "nll_loss= 1.9411019086837769\n",
      "avg_std= 0.4365186095237732\n",
      "dist std min max: 0.0622655414044857 0.4365186095237732 4.243902206420898\n",
      "hidden_states min max: -14.336169242858887 16.697187423706055\n",
      "hidden_state minus mean squared max: 271.3419494628906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80620574951172 1.838138461112976\n",
      "loss   649: 1.9554   grad norm: 1.3002          model param norm: 85.5519        \n",
      "\n",
      "quiet_star_policy_loss= -0.04205737262964249\n",
      "nll_loss= 1.948076605796814\n",
      "avg_std= 0.43507108092308044\n",
      "dist std min max: 0.0602928064763546 0.43507108092308044 4.229672908782959\n",
      "hidden_states min max: -14.520092964172363 16.919889450073242\n",
      "hidden_state minus mean squared max: 285.541748046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.35977172851562 1.866059422492981\n",
      "loss   650: 1.9060   grad norm: 1.1145          model param norm: 85.5554        \n",
      "\n",
      "quiet_star_policy_loss= 0.010916328988969326\n",
      "nll_loss= 1.947974443435669\n",
      "avg_std= 0.438128262758255\n",
      "dist std min max: 0.061432186514139175 0.438128262758255 4.183835983276367\n",
      "hidden_states min max: -16.92314910888672 16.820068359375\n",
      "hidden_state minus mean squared max: 294.73974609375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.30533599853516 1.8591097593307495\n",
      "loss   651: 1.9589   grad norm: 1.1217          model param norm: 85.5588        \n",
      "\n",
      "quiet_star_policy_loss= -0.0154122831299901\n",
      "nll_loss= 1.9304832220077515\n",
      "avg_std= 0.43559563159942627\n",
      "dist std min max: 0.062158405780792236 0.43559563159942627 4.185146331787109\n",
      "hidden_states min max: -14.76643180847168 15.966489791870117\n",
      "hidden_state minus mean squared max: 223.82388305664062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.6385726928711 1.8387786149978638\n",
      "loss   652: 1.9151   grad norm: 1.1902          model param norm: 85.5621        \n",
      "\n",
      "quiet_star_policy_loss= -0.03051314316689968\n",
      "nll_loss= 1.9461567401885986\n",
      "avg_std= 0.4369274973869324\n",
      "dist std min max: 0.0613943375647068 0.4369274973869324 4.149763107299805\n",
      "hidden_states min max: -13.834228515625 15.655144691467285\n",
      "hidden_state minus mean squared max: 212.072265625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.56668090820312 1.8697110414505005\n",
      "loss   653: 1.9156   grad norm: 1.2319          model param norm: 85.5653        \n",
      "\n",
      "quiet_star_policy_loss= -0.06218719482421875\n",
      "nll_loss= 1.9312461614608765\n",
      "avg_std= 0.4404037892818451\n",
      "dist std min max: 0.06075892597436905 0.4404037892818451 4.14540958404541\n",
      "hidden_states min max: -13.33222484588623 16.489612579345703\n",
      "hidden_state minus mean squared max: 185.4036407470703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72181701660156 1.856850504875183\n",
      "loss   654: 1.8691   grad norm: 1.2355          model param norm: 85.5685        \n",
      "\n",
      "quiet_star_policy_loss= -0.005635214038193226\n",
      "nll_loss= 1.950853705406189\n",
      "avg_std= 0.4349331557750702\n",
      "dist std min max: 0.06110679730772972 0.4349331557750702 4.145481109619141\n",
      "hidden_states min max: -14.039505004882812 16.55155372619629\n",
      "hidden_state minus mean squared max: 208.5686492919922\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.10226440429688 1.8567274808883667\n",
      "loss   655: 1.9452   grad norm: 1.1927          model param norm: 85.5720        \n",
      "\n",
      "quiet_star_policy_loss= 0.030477583408355713\n",
      "nll_loss= 1.9553453922271729\n",
      "avg_std= 0.43467772006988525\n",
      "dist std min max: 0.061924710869789124 0.43467772006988525 4.0658392906188965\n",
      "hidden_states min max: -13.731019973754883 17.200693130493164\n",
      "hidden_state minus mean squared max: 251.7314453125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.93235778808594 1.8614434003829956\n",
      "loss   656: 1.9858   grad norm: 1.1707          model param norm: 85.5759        \n",
      "\n",
      "quiet_star_policy_loss= 0.008160972967743874\n",
      "nll_loss= 1.9566681385040283\n",
      "avg_std= 0.4343353807926178\n",
      "dist std min max: 0.06060153618454933 0.4343353807926178 4.028477191925049\n",
      "hidden_states min max: -12.952223777770996 18.806228637695312\n",
      "hidden_state minus mean squared max: 280.4598388671875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1199722290039 1.8686705827713013\n",
      "loss   657: 1.9648   grad norm: 1.2563          model param norm: 85.5797        \n",
      "\n",
      "quiet_star_policy_loss= -0.029772615060210228\n",
      "nll_loss= 1.9610016345977783\n",
      "avg_std= 0.4365417957305908\n",
      "dist std min max: 0.060290683060884476 0.4365417957305908 4.062745571136475\n",
      "hidden_states min max: -15.16968822479248 15.718473434448242\n",
      "hidden_state minus mean squared max: 239.37496948242188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.39955139160156 1.8727139234542847\n",
      "loss   658: 1.9312   grad norm: 1.1540          model param norm: 85.5834        \n",
      "\n",
      "quiet_star_policy_loss= -0.034117795526981354\n",
      "nll_loss= 1.934485673904419\n",
      "avg_std= 0.43836572766304016\n",
      "dist std min max: 0.06095071882009506 0.43836572766304016 4.0277838706970215\n",
      "hidden_states min max: -13.62645149230957 17.160003662109375\n",
      "hidden_state minus mean squared max: 214.14402770996094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.42040252685547 1.856685757637024\n",
      "loss   659: 1.9004   grad norm: 1.2372          model param norm: 85.5871        \n",
      "\n",
      "quiet_star_policy_loss= -0.02328622341156006\n",
      "nll_loss= 1.9623886346817017\n",
      "avg_std= 0.43606996536254883\n",
      "dist std min max: 0.05989726260304451 0.43606996536254883 3.9672353267669678\n",
      "hidden_states min max: -13.657679557800293 17.879064559936523\n",
      "hidden_state minus mean squared max: 227.2949981689453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.86195373535156 1.8784819841384888\n",
      "loss   660: 1.9391   grad norm: 1.2550          model param norm: 85.5908        \n",
      "\n",
      "quiet_star_policy_loss= -0.014012753963470459\n",
      "nll_loss= 1.9459532499313354\n",
      "avg_std= 0.43652039766311646\n",
      "dist std min max: 0.060218892991542816 0.7627662420272827 3.9744784832000732\n",
      "hidden_states min max: -13.875249862670898 14.827851295471191\n",
      "hidden_state minus mean squared max: 210.19212341308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.26317596435547 1.8890060186386108\n",
      "loss   661: 1.9319   grad norm: 1.1643          model param norm: 85.5947        \n",
      "\n",
      "quiet_star_policy_loss= -0.041703224182128906\n",
      "nll_loss= 1.952657699584961\n",
      "avg_std= 0.4346570372581482\n",
      "dist std min max: 0.06109615042805672 0.4346570372581482 3.9481916427612305\n",
      "hidden_states min max: -15.363300323486328 17.34490394592285\n",
      "hidden_state minus mean squared max: 224.26121520996094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.23898315429688 1.8648306131362915\n",
      "loss   662: 1.9110   grad norm: 1.2274          model param norm: 85.5984        \n",
      "\n",
      "quiet_star_policy_loss= -0.037297118455171585\n",
      "nll_loss= 1.951182246208191\n",
      "avg_std= 0.43367111682891846\n",
      "dist std min max: 0.06030022352933884 0.43367111682891846 3.950688362121582\n",
      "hidden_states min max: -13.20567512512207 15.034862518310547\n",
      "hidden_state minus mean squared max: 185.8897247314453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99799346923828 1.8823317289352417\n",
      "loss   663: 1.9139   grad norm: 1.2243          model param norm: 85.6017        \n",
      "\n",
      "quiet_star_policy_loss= 0.009373629465699196\n",
      "nll_loss= 1.942081332206726\n",
      "avg_std= 0.4362556040287018\n",
      "dist std min max: 0.06007934734225273 0.4362556040287018 3.953057050704956\n",
      "hidden_states min max: -13.20706844329834 15.282064437866211\n",
      "hidden_state minus mean squared max: 177.79319763183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.45282745361328 1.8841995000839233\n",
      "loss   664: 1.9515   grad norm: 1.1464          model param norm: 85.6049        \n",
      "\n",
      "quiet_star_policy_loss= -0.014474964700639248\n",
      "nll_loss= 1.9588268995285034\n",
      "avg_std= 0.43265339732170105\n",
      "dist std min max: 0.05998563766479492 0.43265339732170105 3.9504129886627197\n",
      "hidden_states min max: -13.135464668273926 14.02859878540039\n",
      "hidden_state minus mean squared max: 202.96670532226562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.42704010009766 1.8845857381820679\n",
      "loss   665: 1.9444   grad norm: 1.1748          model param norm: 85.6084        \n",
      "\n",
      "quiet_star_policy_loss= -0.05528426170349121\n",
      "nll_loss= 1.9457305669784546\n",
      "avg_std= 0.4344555139541626\n",
      "dist std min max: 0.058688823133707047 0.4344555139541626 3.9267759323120117\n",
      "hidden_states min max: -13.86196231842041 17.160499572753906\n",
      "hidden_state minus mean squared max: 219.07411193847656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.95256805419922 1.9161397218704224\n",
      "loss   666: 1.8904   grad norm: 1.2070          model param norm: 85.6116        \n",
      "\n",
      "quiet_star_policy_loss= -0.046640824526548386\n",
      "nll_loss= 1.9383544921875\n",
      "avg_std= 0.43244174122810364\n",
      "dist std min max: 0.057766035199165344 0.43244174122810364 3.905282497406006\n",
      "hidden_states min max: -13.656888961791992 13.935418128967285\n",
      "hidden_state minus mean squared max: 190.37217712402344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.53347778320312 1.9124985933303833\n",
      "loss   667: 1.8917   grad norm: 1.1263          model param norm: 85.6151        \n",
      "\n",
      "quiet_star_policy_loss= -0.008476543240249157\n",
      "nll_loss= 1.9372490644454956\n",
      "avg_std= 0.432625949382782\n",
      "dist std min max: 0.057852812111377716 0.432625949382782 3.8963146209716797\n",
      "hidden_states min max: -34.24384307861328 15.721710205078125\n",
      "hidden_state minus mean squared max: 1135.3978271484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.04994201660156 1.9173966646194458\n",
      "loss   668: 1.9288   grad norm: 1.2046          model param norm: 85.6184        \n",
      "\n",
      "quiet_star_policy_loss= -0.04143228754401207\n",
      "nll_loss= 1.9411228895187378\n",
      "avg_std= 0.4292892515659332\n",
      "dist std min max: 0.05619804188609123 0.4292892515659332 3.9177262783050537\n",
      "hidden_states min max: -13.91066837310791 13.949886322021484\n",
      "hidden_state minus mean squared max: 200.9351806640625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.35839080810547 1.9321259260177612\n",
      "loss   669: 1.8997   grad norm: 1.1149          model param norm: 85.6217        \n",
      "\n",
      "quiet_star_policy_loss= -0.019899774342775345\n",
      "nll_loss= 1.9642237424850464\n",
      "avg_std= 0.42720240354537964\n",
      "dist std min max: 0.05716877058148384 0.42720240354537964 3.923790216445923\n",
      "hidden_states min max: -13.233243942260742 15.291297912597656\n",
      "hidden_state minus mean squared max: 186.36956787109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.54895782470703 1.9258848428726196\n",
      "loss   670: 1.9443   grad norm: 1.1505          model param norm: 85.6250        \n",
      "\n",
      "quiet_star_policy_loss= 0.034281592816114426\n",
      "nll_loss= 1.9289536476135254\n",
      "avg_std= 0.4310237169265747\n",
      "dist std min max: 0.056069858372211456 0.4310237169265747 3.928034543991089\n",
      "hidden_states min max: -11.97071647644043 13.63137435913086\n",
      "hidden_state minus mean squared max: 165.49070739746094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.370274543762207 1.9260679483413696\n",
      "loss   671: 1.9632   grad norm: 2.5080          model param norm: 85.6284        \n",
      "\n",
      "quiet_star_policy_loss= 0.013696789741516113\n",
      "nll_loss= 1.9465636014938354\n",
      "avg_std= 0.42745241522789\n",
      "dist std min max: 0.053975146263837814 0.42745241522789 3.9237444400787354\n",
      "hidden_states min max: -13.670514106750488 14.93330192565918\n",
      "hidden_state minus mean squared max: 192.3600311279297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72651672363281 1.9951359033584595\n",
      "loss   672: 1.9603   grad norm: 1.1692          model param norm: 85.6317        \n",
      "\n",
      "quiet_star_policy_loss= -0.03279430791735649\n",
      "nll_loss= 1.9488691091537476\n",
      "avg_std= 0.42644575238227844\n",
      "dist std min max: 0.054216042160987854 0.42644575238227844 3.9252545833587646\n",
      "hidden_states min max: -14.253212928771973 16.04721450805664\n",
      "hidden_state minus mean squared max: 247.14466857910156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.00636291503906 1.9791027307510376\n",
      "loss   673: 1.9161   grad norm: 1.1919          model param norm: 85.6349        \n",
      "\n",
      "quiet_star_policy_loss= 0.00136823661159724\n",
      "nll_loss= 1.9460166692733765\n",
      "avg_std= 0.426684707403183\n",
      "dist std min max: 0.052435655146837234 0.426684707403183 3.932403326034546\n",
      "hidden_states min max: -14.547292709350586 17.24368667602539\n",
      "hidden_state minus mean squared max: 285.7509460449219\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.63909912109375 2.0021634101867676\n",
      "loss   674: 1.9474   grad norm: 1.1803          model param norm: 85.6379        \n",
      "\n",
      "quiet_star_policy_loss= -0.014570212922990322\n",
      "nll_loss= 1.9436839818954468\n",
      "avg_std= 0.4266572892665863\n",
      "dist std min max: 0.052461039274930954 0.4266572892665863 3.954367160797119\n",
      "hidden_states min max: -14.69943904876709 15.052258491516113\n",
      "hidden_state minus mean squared max: 226.58401489257812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.48028564453125 2.0104355812072754\n",
      "loss   675: 1.9291   grad norm: 1.1960          model param norm: 85.6409        \n",
      "\n",
      "quiet_star_policy_loss= -0.012781977653503418\n",
      "nll_loss= 1.9327373504638672\n",
      "avg_std= 0.42810896039009094\n",
      "dist std min max: 0.05109179764986038 0.42810896039009094 3.9677038192749023\n",
      "hidden_states min max: -14.819183349609375 15.177906036376953\n",
      "hidden_state minus mean squared max: 227.9062042236328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.50960540771484 1.9977713823318481\n",
      "loss   676: 1.9200   grad norm: 1.2014          model param norm: 85.6438        \n",
      "\n",
      "quiet_star_policy_loss= -0.017419815063476562\n",
      "nll_loss= 1.937882661819458\n",
      "avg_std= 0.4298524260520935\n",
      "dist std min max: 0.05149770900607109 0.4298524260520935 3.988001823425293\n",
      "hidden_states min max: -12.67707633972168 13.767659187316895\n",
      "hidden_state minus mean squared max: 179.2510223388672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.979736328125 2.0234766006469727\n",
      "loss   677: 1.9205   grad norm: 1.2254          model param norm: 85.6470        \n",
      "\n",
      "quiet_star_policy_loss= -0.03871555253863335\n",
      "nll_loss= 1.9517837762832642\n",
      "avg_std= 0.4261021316051483\n",
      "dist std min max: 0.05223434045910835 0.4261021316051483 3.9872853755950928\n",
      "hidden_states min max: -14.591226577758789 15.375972747802734\n",
      "hidden_state minus mean squared max: 222.74624633789062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.15035247802734 2.0167531967163086\n",
      "loss   678: 1.9131   grad norm: 1.2724          model param norm: 85.6499        \n",
      "\n",
      "quiet_star_policy_loss= -0.03538640961050987\n",
      "nll_loss= 1.9453150033950806\n",
      "avg_std= 0.42730239033699036\n",
      "dist std min max: 0.05158842355012894 0.42730239033699036 3.993877649307251\n",
      "hidden_states min max: -14.975980758666992 15.719661712646484\n",
      "hidden_state minus mean squared max: 225.8782501220703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.09085845947266 2.0294294357299805\n",
      "loss   679: 1.9099   grad norm: 1.1611          model param norm: 85.6529        \n",
      "\n",
      "quiet_star_policy_loss= -0.007092714309692383\n",
      "nll_loss= 1.9321240186691284\n",
      "avg_std= 0.4305664002895355\n",
      "dist std min max: 0.05135465785861015 0.4305664002895355 3.977565288543701\n",
      "hidden_states min max: -13.115453720092773 15.894753456115723\n",
      "hidden_state minus mean squared max: 188.0819549560547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.8070297241211 2.0221147537231445\n",
      "loss   680: 1.9250   grad norm: 1.2258          model param norm: 85.6559        \n",
      "\n",
      "quiet_star_policy_loss= -0.01637127436697483\n",
      "nll_loss= 1.9443076848983765\n",
      "avg_std= 0.4289495050907135\n",
      "dist std min max: 0.05151689052581787 0.4289495050907135 3.9951398372650146\n",
      "hidden_states min max: -13.117522239685059 15.155537605285645\n",
      "hidden_state minus mean squared max: 195.36280822753906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69988250732422 2.031407356262207\n",
      "loss   681: 1.9279   grad norm: 1.2317          model param norm: 85.6590        \n",
      "\n",
      "quiet_star_policy_loss= -0.029513467103242874\n",
      "nll_loss= 1.9474514722824097\n",
      "avg_std= 0.428947776556015\n",
      "dist std min max: 0.05095154047012329 0.428947776556015 3.9747960567474365\n",
      "hidden_states min max: -16.722349166870117 15.375457763671875\n",
      "hidden_state minus mean squared max: 321.485595703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.41903686523438 2.0450611114501953\n",
      "loss   682: 1.9179   grad norm: 1.2552          model param norm: 85.6618        \n",
      "\n",
      "quiet_star_policy_loss= -0.027691412717103958\n",
      "nll_loss= 1.938605546951294\n",
      "avg_std= 0.4307665228843689\n",
      "dist std min max: 0.050788763910532 0.4307665228843689 4.010325908660889\n",
      "hidden_states min max: -16.096147537231445 16.34813117980957\n",
      "hidden_state minus mean squared max: 221.19241333007812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97052001953125 2.048844814300537\n",
      "loss   683: 1.9109   grad norm: 1.1720          model param norm: 85.6647        \n",
      "\n",
      "quiet_star_policy_loss= -0.016207648441195488\n",
      "nll_loss= 1.9373903274536133\n",
      "avg_std= 0.42951342463493347\n",
      "dist std min max: 0.05017215013504028 0.42951342463493347 3.9935169219970703\n",
      "hidden_states min max: -14.211213111877441 16.248659133911133\n",
      "hidden_state minus mean squared max: 211.15318298339844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.31098937988281 2.065723419189453\n",
      "loss   684: 1.9212   grad norm: 1.1636          model param norm: 85.6673        \n",
      "\n",
      "quiet_star_policy_loss= -0.009294605813920498\n",
      "nll_loss= 1.9356319904327393\n",
      "avg_std= 0.4319927990436554\n",
      "dist std min max: 0.050165604799985886 0.4319927990436554 3.9964873790740967\n",
      "hidden_states min max: -17.190074920654297 14.902042388916016\n",
      "hidden_state minus mean squared max: 263.2581787109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.31914520263672 2.067593574523926\n",
      "loss   685: 1.9263   grad norm: 1.1391          model param norm: 85.6701        \n",
      "\n",
      "quiet_star_policy_loss= -0.012084389105439186\n",
      "nll_loss= 1.941048502922058\n",
      "avg_std= 0.4295603632926941\n",
      "dist std min max: 0.04981487616896629 0.4295603632926941 3.977125644683838\n",
      "hidden_states min max: -16.669662475585938 15.722692489624023\n",
      "hidden_state minus mean squared max: 325.59259033203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.42539978027344 2.064634323120117\n",
      "loss   686: 1.9290   grad norm: 1.1697          model param norm: 85.6729        \n",
      "\n",
      "quiet_star_policy_loss= -0.013823747634887695\n",
      "nll_loss= 1.940489411354065\n",
      "avg_std= 0.43201568722724915\n",
      "dist std min max: 0.04962377995252609 0.43201568722724915 3.9659299850463867\n",
      "hidden_states min max: -22.262283325195312 15.760354995727539\n",
      "hidden_state minus mean squared max: 578.4968872070312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.71278381347656 2.075296401977539\n",
      "loss   687: 1.9267   grad norm: 1.3157          model param norm: 85.6759        \n",
      "\n",
      "quiet_star_policy_loss= -0.015339309349656105\n",
      "nll_loss= 1.934504508972168\n",
      "avg_std= 0.43208515644073486\n",
      "dist std min max: 0.04949894919991493 0.43208515644073486 3.96608567237854\n",
      "hidden_states min max: -13.356611251831055 15.581435203552246\n",
      "hidden_state minus mean squared max: 187.837646484375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.75444793701172 2.0865001678466797\n",
      "loss   688: 1.9192   grad norm: 1.1555          model param norm: 85.6788        \n",
      "\n",
      "quiet_star_policy_loss= -0.031049681827425957\n",
      "nll_loss= 1.9279606342315674\n",
      "avg_std= 0.43054813146591187\n",
      "dist std min max: 0.049395233392715454 0.43054813146591187 3.9260590076446533\n",
      "hidden_states min max: -12.53138256072998 15.022963523864746\n",
      "hidden_state minus mean squared max: 183.41110229492188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.90585327148438 2.0867419242858887\n",
      "loss   689: 1.8969   grad norm: 1.3015          model param norm: 85.6817        \n",
      "\n",
      "quiet_star_policy_loss= -0.03279175981879234\n",
      "nll_loss= 1.930145263671875\n",
      "avg_std= 0.4284190237522125\n",
      "dist std min max: 0.04877524822950363 0.4284190237522125 3.9200265407562256\n",
      "hidden_states min max: -13.3850679397583 15.872774124145508\n",
      "hidden_state minus mean squared max: 190.53883361816406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.42503356933594 2.073263645172119\n",
      "loss   690: 1.8974   grad norm: 1.1884          model param norm: 85.6847        \n",
      "\n",
      "quiet_star_policy_loss= -0.0004127979336772114\n",
      "nll_loss= 1.9309967756271362\n",
      "avg_std= 0.42987996339797974\n",
      "dist std min max: 0.04906174913048744 0.42987996339797974 3.876699209213257\n",
      "hidden_states min max: -13.921363830566406 15.3186674118042\n",
      "hidden_state minus mean squared max: 218.85670471191406\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.46276092529297 2.0877370834350586\n",
      "loss   691: 1.9306   grad norm: 1.2410          model param norm: 85.6877        \n",
      "\n",
      "quiet_star_policy_loss= 0.005490374751389027\n",
      "nll_loss= 1.9168720245361328\n",
      "avg_std= 0.43028590083122253\n",
      "dist std min max: 0.048150770366191864 0.43028590083122253 4.08875036239624\n",
      "hidden_states min max: -12.611479759216309 14.331304550170898\n",
      "hidden_state minus mean squared max: 190.54808044433594\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.51338195800781 2.1141161918640137\n",
      "loss   692: 1.9224   grad norm: 1.2540          model param norm: 85.6907        \n",
      "\n",
      "quiet_star_policy_loss= -0.017420291900634766\n",
      "nll_loss= 1.9467443227767944\n",
      "avg_std= 0.4284849464893341\n",
      "dist std min max: 0.04916227236390114 0.4284849464893341 4.023902416229248\n",
      "hidden_states min max: -17.35625457763672 15.228289604187012\n",
      "hidden_state minus mean squared max: 270.15899658203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.33208465576172 2.076992988586426\n",
      "loss   693: 1.9293   grad norm: 1.1779          model param norm: 85.6937        \n",
      "\n",
      "quiet_star_policy_loss= -0.010487544350326061\n",
      "nll_loss= 1.9316701889038086\n",
      "avg_std= 0.42777374386787415\n",
      "dist std min max: 0.04968728497624397 0.42777374386787415 3.8114712238311768\n",
      "hidden_states min max: -13.45421314239502 14.198935508728027\n",
      "hidden_state minus mean squared max: 195.46055603027344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.52015686035156 2.0732593536376953\n",
      "loss   694: 1.9212   grad norm: 1.1480          model param norm: 85.6967        \n",
      "\n",
      "quiet_star_policy_loss= -0.03145179897546768\n",
      "nll_loss= 1.9601287841796875\n",
      "avg_std= 0.6806327700614929\n",
      "dist std min max: 0.049130719155073166 0.4260613024234772 3.791120767593384\n",
      "hidden_states min max: -12.962908744812012 13.869129180908203\n",
      "hidden_state minus mean squared max: 178.08639526367188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.48416900634766 2.079808235168457\n",
      "loss   695: 1.9287   grad norm: 1.2218          model param norm: 85.6997        \n",
      "\n",
      "quiet_star_policy_loss= -0.04029369354248047\n",
      "nll_loss= 1.9378193616867065\n",
      "avg_std= 0.4250214397907257\n",
      "dist std min max: 0.04802077263593674 0.4250214397907257 3.8052401542663574\n",
      "hidden_states min max: -11.188471794128418 13.754980087280273\n",
      "hidden_state minus mean squared max: 169.78289794921875\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.1970443725586 2.0833945274353027\n",
      "loss   696: 1.8975   grad norm: 1.2299          model param norm: 85.7026        \n",
      "\n",
      "quiet_star_policy_loss= -0.015200949274003506\n",
      "nll_loss= 1.9353126287460327\n",
      "avg_std= 0.42479389905929565\n",
      "dist std min max: 0.04875679686665535 0.42479389905929565 3.8612940311431885\n",
      "hidden_states min max: -12.741971969604492 14.306757926940918\n",
      "hidden_state minus mean squared max: 175.86119079589844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.63231658935547 2.0981192588806152\n",
      "loss   697: 1.9201   grad norm: 1.2192          model param norm: 85.7058        \n",
      "\n",
      "quiet_star_policy_loss= 0.0055811405181884766\n",
      "nll_loss= 1.929951548576355\n",
      "avg_std= 0.42376062273979187\n",
      "dist std min max: 0.048273175954818726 0.42376062273979187 3.7904038429260254\n",
      "hidden_states min max: -12.842167854309082 12.925044059753418\n",
      "hidden_state minus mean squared max: 176.81028747558594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.1732406616211 2.1015095710754395\n",
      "loss   698: 1.9355   grad norm: 1.2109          model param norm: 85.7091        \n",
      "\n",
      "quiet_star_policy_loss= -0.046646323055028915\n",
      "nll_loss= 1.9546942710876465\n",
      "avg_std= 0.4263561964035034\n",
      "dist std min max: 0.04902559146285057 0.4263561964035034 3.789165496826172\n",
      "hidden_states min max: -10.95106029510498 12.26130485534668\n",
      "hidden_state minus mean squared max: 142.24447631835938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.149275779724121 2.084549903869629\n",
      "loss   699: 1.9080   grad norm: 2.5921          model param norm: 85.7124        \n",
      "eval loss 1.938738465309143\n",
      "\n",
      "quiet_star_policy_loss= -0.0007073879241943359\n",
      "nll_loss= 1.924910545349121\n",
      "avg_std= 0.4237724244594574\n",
      "dist std min max: 0.04629708081483841 0.4237724244594574 3.7955780029296875\n",
      "hidden_states min max: -12.821568489074707 13.2830810546875\n",
      "hidden_state minus mean squared max: 175.34033203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.81068420410156 2.1311984062194824\n",
      "loss   700: 1.9242   grad norm: 1.2401          model param norm: 85.7157        \n",
      "\n",
      "quiet_star_policy_loss= -0.009171390905976295\n",
      "nll_loss= 1.9358290433883667\n",
      "avg_std= 0.4228350818157196\n",
      "dist std min max: 0.04671666398644447 0.4228350818157196 3.8095178604125977\n",
      "hidden_states min max: -12.992115020751953 14.899651527404785\n",
      "hidden_state minus mean squared max: 205.9091796875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.71296691894531 2.137669563293457\n",
      "loss   701: 1.9267   grad norm: 1.2179          model param norm: 85.7187        \n",
      "\n",
      "quiet_star_policy_loss= -0.003347110701724887\n",
      "nll_loss= 1.945942759513855\n",
      "avg_std= 0.4218195080757141\n",
      "dist std min max: 0.04775237292051315 0.4218195080757141 4.12347936630249\n",
      "hidden_states min max: -23.978347778320312 13.699180603027344\n",
      "hidden_state minus mean squared max: 550.3871459960938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.6878890991211 2.113434314727783\n",
      "loss   702: 1.9426   grad norm: 1.1957          model param norm: 85.7218        \n",
      "\n",
      "quiet_star_policy_loss= -0.041387151926755905\n",
      "nll_loss= 1.9396322965621948\n",
      "avg_std= 0.421697199344635\n",
      "dist std min max: 0.04693874716758728 0.421697199344635 3.796182632446289\n",
      "hidden_states min max: -13.006725311279297 13.460569381713867\n",
      "hidden_state minus mean squared max: 170.12789916992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8140640258789 2.118422031402588\n",
      "loss   703: 1.8982   grad norm: 1.2282          model param norm: 85.7251        \n",
      "\n",
      "quiet_star_policy_loss= -0.04438796266913414\n",
      "nll_loss= 1.934580683708191\n",
      "avg_std= 0.42177191376686096\n",
      "dist std min max: 0.0462113693356514 0.42177191376686096 3.817185401916504\n",
      "hidden_states min max: -13.17833423614502 14.919699668884277\n",
      "hidden_state minus mean squared max: 225.6007080078125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.24195098876953 2.1371569633483887\n",
      "loss   704: 1.8902   grad norm: 1.2144          model param norm: 85.7284        \n",
      "\n",
      "quiet_star_policy_loss= -5.7220458984375e-06\n",
      "nll_loss= 1.9256771802902222\n",
      "avg_std= 0.4209040701389313\n",
      "dist std min max: 0.046319492161273956 0.4209040701389313 3.7620632648468018\n",
      "hidden_states min max: -13.757678031921387 14.980207443237305\n",
      "hidden_state minus mean squared max: 195.0052032470703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.41902923583984 2.144583225250244\n",
      "loss   705: 1.9257   grad norm: 1.2037          model param norm: 85.7319        \n",
      "\n",
      "quiet_star_policy_loss= -0.03288979455828667\n",
      "nll_loss= 1.9187217950820923\n",
      "avg_std= 0.4203896224498749\n",
      "dist std min max: 0.04580424353480339 0.4203896224498749 3.7766501903533936\n",
      "hidden_states min max: -12.557815551757812 14.553194046020508\n",
      "hidden_state minus mean squared max: 143.4728546142578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.67964172363281 2.146129608154297\n",
      "loss   706: 1.8858   grad norm: 1.1529          model param norm: 85.7358        \n",
      "\n",
      "quiet_star_policy_loss= -0.016959859058260918\n",
      "nll_loss= 1.953525424003601\n",
      "avg_std= 0.4180094301700592\n",
      "dist std min max: 0.04573675990104675 0.4180094301700592 3.7629261016845703\n",
      "hidden_states min max: -11.536714553833008 14.595251083374023\n",
      "hidden_state minus mean squared max: 156.20904541015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.13882446289062 2.162707805633545\n",
      "loss   707: 1.9366   grad norm: 1.1703          model param norm: 85.7392        \n",
      "\n",
      "quiet_star_policy_loss= -0.043730925768613815\n",
      "nll_loss= 1.9191502332687378\n",
      "avg_std= 0.4202190637588501\n",
      "dist std min max: 0.04549647122621536 0.4202190637588501 3.750744342803955\n",
      "hidden_states min max: -13.61557674407959 17.029050827026367\n",
      "hidden_state minus mean squared max: 220.7725372314453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.39764404296875 2.1589784622192383\n",
      "loss   708: 1.8754   grad norm: 1.2158          model param norm: 85.7428        \n",
      "\n",
      "quiet_star_policy_loss= -0.03263969346880913\n",
      "nll_loss= 1.937410593032837\n",
      "avg_std= 0.4185188114643097\n",
      "dist std min max: 0.045081689953804016 0.4185188114643097 3.7318451404571533\n",
      "hidden_states min max: -15.31038761138916 14.762611389160156\n",
      "hidden_state minus mean squared max: 186.20855712890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1460189819336 2.168199062347412\n",
      "loss   709: 1.9048   grad norm: 1.2059          model param norm: 85.7464        \n",
      "\n",
      "quiet_star_policy_loss= -0.026684237644076347\n",
      "nll_loss= 1.9513663053512573\n",
      "avg_std= 0.4168735444545746\n",
      "dist std min max: 0.044195756316185 0.4168735444545746 3.7217912673950195\n",
      "hidden_states min max: -13.511113166809082 15.48266315460205\n",
      "hidden_state minus mean squared max: 194.49957275390625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.50203704833984 2.1910672187805176\n",
      "loss   710: 1.9247   grad norm: 1.1995          model param norm: 85.7498        \n",
      "\n",
      "quiet_star_policy_loss= -0.027691150084137917\n",
      "nll_loss= 1.9483412504196167\n",
      "avg_std= 0.4146348834037781\n",
      "dist std min max: 0.04433467239141464 0.4146348834037781 3.732024908065796\n",
      "hidden_states min max: -12.264930725097656 14.569148063659668\n",
      "hidden_state minus mean squared max: 197.00808715820312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.18219757080078 2.1837239265441895\n",
      "loss   711: 1.9207   grad norm: 1.1900          model param norm: 85.7530        \n",
      "\n",
      "quiet_star_policy_loss= -0.03146810457110405\n",
      "nll_loss= 1.9346904754638672\n",
      "avg_std= 0.4175106883049011\n",
      "dist std min max: 0.04369816184043884 0.4175106883049011 3.7439441680908203\n",
      "hidden_states min max: -12.742191314697266 14.566822052001953\n",
      "hidden_state minus mean squared max: 168.57479858398438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.05402374267578 2.188199043273926\n",
      "loss   712: 1.9032   grad norm: 1.1671          model param norm: 85.7561        \n",
      "\n",
      "quiet_star_policy_loss= -0.046959590166807175\n",
      "nll_loss= 1.9345921277999878\n",
      "avg_std= 0.4160102307796478\n",
      "dist std min max: 0.04420461505651474 0.4160102307796478 3.732147693634033\n",
      "hidden_states min max: -14.174060821533203 15.345235824584961\n",
      "hidden_state minus mean squared max: 214.35897827148438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.53227233886719 2.1909031867980957\n",
      "loss   713: 1.8876   grad norm: 1.2460          model param norm: 85.7593        \n",
      "\n",
      "quiet_star_policy_loss= 0.011116886511445045\n",
      "nll_loss= 1.9269325733184814\n",
      "avg_std= 0.4169997274875641\n",
      "dist std min max: 0.04422752559185028 0.4169997274875641 3.7334437370300293\n",
      "hidden_states min max: -13.807674407958984 15.73807430267334\n",
      "hidden_state minus mean squared max: 186.35150146484375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.1715316772461 2.19734525680542\n",
      "loss   714: 1.9380   grad norm: 1.2382          model param norm: 85.7624        \n",
      "\n",
      "quiet_star_policy_loss= -0.048470497131347656\n",
      "nll_loss= 1.9321237802505493\n",
      "avg_std= 0.4136488139629364\n",
      "dist std min max: 0.044033415615558624 0.4136488139629364 3.9168899059295654\n",
      "hidden_states min max: -13.284846305847168 13.88421630859375\n",
      "hidden_state minus mean squared max: 144.35552978515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.450613021850586 2.1876063346862793\n",
      "loss   715: 1.8837   grad norm: 1.2166          model param norm: 85.7657        \n",
      "\n",
      "quiet_star_policy_loss= -0.0719112753868103\n",
      "nll_loss= 1.9231303930282593\n",
      "avg_std= 0.41584640741348267\n",
      "dist std min max: 0.044761039316654205 0.41584640741348267 3.759601354598999\n",
      "hidden_states min max: -12.384817123413086 13.966718673706055\n",
      "hidden_state minus mean squared max: 165.00347900390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.5135498046875 2.182365894317627\n",
      "loss   716: 1.8512   grad norm: 1.1684          model param norm: 85.7689        \n",
      "\n",
      "quiet_star_policy_loss= -0.0037257433868944645\n",
      "nll_loss= 1.9331587553024292\n",
      "avg_std= 0.409772664308548\n",
      "dist std min max: 0.04441548138856888 0.409772664308548 3.7529048919677734\n",
      "hidden_states min max: -14.928573608398438 16.17875099182129\n",
      "hidden_state minus mean squared max: 198.48597717285156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17522430419922 2.169498920440674\n",
      "loss   717: 1.9294   grad norm: 1.2853          model param norm: 85.7726        \n",
      "\n",
      "quiet_star_policy_loss= -0.018597936257719994\n",
      "nll_loss= 1.9196672439575195\n",
      "avg_std= 0.40966102480888367\n",
      "dist std min max: 0.04458330199122429 0.40966102480888367 3.775634288787842\n",
      "hidden_states min max: -13.048495292663574 15.781845092773438\n",
      "hidden_state minus mean squared max: 196.9659881591797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24266052246094 2.1864938735961914\n",
      "loss   718: 1.9011   grad norm: 1.2397          model param norm: 85.7763        \n",
      "\n",
      "quiet_star_policy_loss= -0.0383165143430233\n",
      "nll_loss= 1.919316291809082\n",
      "avg_std= 0.408444881439209\n",
      "dist std min max: 0.04450017213821411 0.408444881439209 3.774423599243164\n",
      "hidden_states min max: -12.911795616149902 15.299056053161621\n",
      "hidden_state minus mean squared max: 178.351806640625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.83262634277344 2.186159610748291\n",
      "loss   719: 1.8810   grad norm: 1.1607          model param norm: 85.7800        \n",
      "\n",
      "quiet_star_policy_loss= -0.0026853084564208984\n",
      "nll_loss= 1.9180015325546265\n",
      "avg_std= 0.40884891152381897\n",
      "dist std min max: 0.04375065863132477 0.40884891152381897 3.7777769565582275\n",
      "hidden_states min max: -12.415657997131348 13.969045639038086\n",
      "hidden_state minus mean squared max: 162.07522583007812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78394317626953 2.180847644805908\n",
      "loss   720: 1.9153   grad norm: 1.1491          model param norm: 85.7837        \n",
      "\n",
      "quiet_star_policy_loss= -0.05192136764526367\n",
      "nll_loss= 1.926287293434143\n",
      "avg_std= 0.40615522861480713\n",
      "dist std min max: 0.04366612434387207 0.40615522861480713 3.7807366847991943\n",
      "hidden_states min max: -12.19149398803711 13.381892204284668\n",
      "hidden_state minus mean squared max: 162.8896484375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.18186950683594 2.183365821838379\n",
      "loss   721: 1.8744   grad norm: 1.2968          model param norm: 85.7874        \n",
      "\n",
      "quiet_star_policy_loss= -0.02450261078774929\n",
      "nll_loss= 1.9284626245498657\n",
      "avg_std= 0.40502792596817017\n",
      "dist std min max: 0.04401044175028801 0.40502792596817017 3.810152292251587\n",
      "hidden_states min max: -12.801156997680664 14.034297943115234\n",
      "hidden_state minus mean squared max: 184.23805236816406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.72999572753906 2.185081958770752\n",
      "loss   722: 1.9040   grad norm: 1.1655          model param norm: 85.7912        \n",
      "\n",
      "quiet_star_policy_loss= -0.02705535851418972\n",
      "nll_loss= 1.913777232170105\n",
      "avg_std= 0.4067751169204712\n",
      "dist std min max: 0.044216010719537735 0.4067751169204712 3.8262200355529785\n",
      "hidden_states min max: -13.0977201461792 14.772159576416016\n",
      "hidden_state minus mean squared max: 176.54457092285156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.64533996582031 2.1855711936950684\n",
      "loss   723: 1.8867   grad norm: 1.2531          model param norm: 85.7946        \n",
      "\n",
      "quiet_star_policy_loss= 0.022053146734833717\n",
      "nll_loss= 1.9243555068969727\n",
      "avg_std= 0.4057154357433319\n",
      "dist std min max: 0.04415004700422287 0.4057154357433319 3.8182663917541504\n",
      "hidden_states min max: -11.82394790649414 15.44871997833252\n",
      "hidden_state minus mean squared max: 178.39833068847656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0974349975586 2.1945137977600098\n",
      "loss   724: 1.9464   grad norm: 1.2182          model param norm: 85.7979        \n",
      "\n",
      "quiet_star_policy_loss= 0.01652994193136692\n",
      "nll_loss= 1.9279518127441406\n",
      "avg_std= 0.404328316450119\n",
      "dist std min max: 0.04374255612492561 0.404328316450119 3.828594446182251\n",
      "hidden_states min max: -11.960580825805664 13.444036483764648\n",
      "hidden_state minus mean squared max: 146.2198944091797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.9078140258789 2.210446834564209\n",
      "loss   725: 1.9445   grad norm: 1.2510          model param norm: 85.8012        \n",
      "\n",
      "quiet_star_policy_loss= -0.016205906867980957\n",
      "nll_loss= 1.9194921255111694\n",
      "avg_std= 0.40404337644577026\n",
      "dist std min max: 0.04313083365559578 0.40404337644577026 3.8102731704711914\n",
      "hidden_states min max: -12.253676414489746 15.423648834228516\n",
      "hidden_state minus mean squared max: 175.57102966308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.96725463867188 2.2088308334350586\n",
      "loss   726: 1.9033   grad norm: 1.2365          model param norm: 85.8045        \n",
      "\n",
      "quiet_star_policy_loss= -0.032600607722997665\n",
      "nll_loss= 1.9250869750976562\n",
      "avg_std= 0.40553194284439087\n",
      "dist std min max: 0.042904455214738846 0.40553194284439087 3.8421437740325928\n",
      "hidden_states min max: -11.553788185119629 12.567264556884766\n",
      "hidden_state minus mean squared max: 154.6590576171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.9869384765625 2.2137746810913086\n",
      "loss   727: 1.8925   grad norm: 2.5578          model param norm: 85.8080        \n",
      "\n",
      "quiet_star_policy_loss= -0.01199185848236084\n",
      "nll_loss= 1.9419063329696655\n",
      "avg_std= 0.4047628939151764\n",
      "dist std min max: 0.0429462194442749 0.4047628939151764 3.8120100498199463\n",
      "hidden_states min max: -21.232135772705078 15.447196960449219\n",
      "hidden_state minus mean squared max: 307.3281555175781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.39653778076172 2.219517707824707\n",
      "loss   728: 1.9299   grad norm: 1.3067          model param norm: 85.8115        \n",
      "\n",
      "quiet_star_policy_loss= -0.0029578865505754948\n",
      "nll_loss= 1.9318097829818726\n",
      "avg_std= 0.40446197986602783\n",
      "dist std min max: 0.04169017821550369 0.40446197986602783 3.831346035003662\n",
      "hidden_states min max: -15.621740341186523 15.769817352294922\n",
      "hidden_state minus mean squared max: 191.52923583984375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.16008758544922 2.221162796020508\n",
      "loss   729: 1.9289   grad norm: 1.2300          model param norm: 85.8152        \n",
      "\n",
      "quiet_star_policy_loss= 0.02078700065612793\n",
      "nll_loss= 1.9264219999313354\n",
      "avg_std= 0.4046894311904907\n",
      "dist std min max: 0.042532965540885925 0.4046894311904907 3.77182936668396\n",
      "hidden_states min max: -12.895501136779785 14.332015991210938\n",
      "hidden_state minus mean squared max: 146.55873107910156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8250732421875 2.2255783081054688\n",
      "loss   730: 1.9472   grad norm: 1.2663          model param norm: 85.8190        \n",
      "\n",
      "quiet_star_policy_loss= -0.0060038091614842415\n",
      "nll_loss= 1.9289894104003906\n",
      "avg_std= 0.406790167093277\n",
      "dist std min max: 0.042361002415418625 0.406790167093277 3.7919182777404785\n",
      "hidden_states min max: -13.213249206542969 14.626836776733398\n",
      "hidden_state minus mean squared max: 178.98606872558594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46839141845703 2.238344669342041\n",
      "loss   731: 1.9230   grad norm: 1.1985          model param norm: 85.8226        \n",
      "\n",
      "quiet_star_policy_loss= -0.031537868082523346\n",
      "nll_loss= 1.9078056812286377\n",
      "avg_std= 0.4086301624774933\n",
      "dist std min max: 0.040394533425569534 0.4086301624774933 3.800474166870117\n",
      "hidden_states min max: -36.361907958984375 16.719566345214844\n",
      "hidden_state minus mean squared max: 1165.4161376953125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -105.06298065185547 2.242368221282959\n",
      "loss   732: 1.8763   grad norm: 1.1732          model param norm: 85.8265        \n",
      "\n",
      "quiet_star_policy_loss= 0.02063446119427681\n",
      "nll_loss= 1.9244621992111206\n",
      "avg_std= 0.40794700384140015\n",
      "dist std min max: 0.041878532618284225 0.40794700384140015 3.8060643672943115\n",
      "hidden_states min max: -12.795583724975586 14.316722869873047\n",
      "hidden_state minus mean squared max: 165.7843780517578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.32500457763672 2.249213695526123\n",
      "loss   733: 1.9451   grad norm: 1.3071          model param norm: 85.8302        \n",
      "\n",
      "quiet_star_policy_loss= -0.0222033504396677\n",
      "nll_loss= 1.919684648513794\n",
      "avg_std= 0.40921565890312195\n",
      "dist std min max: 0.04193606972694397 0.40921565890312195 3.785372734069824\n",
      "hidden_states min max: -14.306811332702637 14.383142471313477\n",
      "hidden_state minus mean squared max: 175.50491333007812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.11641693115234 2.2429723739624023\n",
      "loss   734: 1.8975   grad norm: 1.1726          model param norm: 85.8338        \n",
      "\n",
      "quiet_star_policy_loss= -0.023868752643465996\n",
      "nll_loss= 1.9218648672103882\n",
      "avg_std= 0.4056526720523834\n",
      "dist std min max: 0.042051952332258224 0.4056526720523834 3.8217389583587646\n",
      "hidden_states min max: -20.324020385742188 14.514876365661621\n",
      "hidden_state minus mean squared max: 304.4591369628906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.39185333251953 2.2405505180358887\n",
      "loss   735: 1.8980   grad norm: 1.2356          model param norm: 85.8374        \n",
      "\n",
      "quiet_star_policy_loss= -0.010353040881454945\n",
      "nll_loss= 1.9102916717529297\n",
      "avg_std= 0.407469242811203\n",
      "dist std min max: 0.04172496125102043 0.407469242811203 3.8177008628845215\n",
      "hidden_states min max: -15.335281372070312 16.441665649414062\n",
      "hidden_state minus mean squared max: 209.60157775878906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.12370300292969 2.247621536254883\n",
      "loss   736: 1.8999   grad norm: 1.2314          model param norm: 85.8411        \n",
      "\n",
      "quiet_star_policy_loss= -0.018651986494660378\n",
      "nll_loss= 1.9229000806808472\n",
      "avg_std= 0.41101792454719543\n",
      "dist std min max: 0.04194941371679306 0.41101792454719543 3.8099629878997803\n",
      "hidden_states min max: -15.022215843200684 14.172977447509766\n",
      "hidden_state minus mean squared max: 169.9291534423828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.10026550292969 2.2352547645568848\n",
      "loss   737: 1.9042   grad norm: 1.2962          model param norm: 85.8447        \n",
      "\n",
      "quiet_star_policy_loss= -0.004450416658073664\n",
      "nll_loss= 1.9373245239257812\n",
      "avg_std= 0.4081869125366211\n",
      "dist std min max: 0.042021434754133224 0.4081869125366211 3.7769131660461426\n",
      "hidden_states min max: -14.054189682006836 13.381620407104492\n",
      "hidden_state minus mean squared max: 238.1804962158203\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.26910400390625 2.2411346435546875\n",
      "loss   738: 1.9329   grad norm: 1.2683          model param norm: 85.8484        \n",
      "\n",
      "quiet_star_policy_loss= -0.058374740183353424\n",
      "nll_loss= 1.9164003133773804\n",
      "avg_std= 0.41125357151031494\n",
      "dist std min max: 0.041918061673641205 0.41125357151031494 3.822265148162842\n",
      "hidden_states min max: -13.971259117126465 13.672511100769043\n",
      "hidden_state minus mean squared max: 189.17919921875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.45680236816406 2.2529330253601074\n",
      "loss   739: 1.8580   grad norm: 1.2498          model param norm: 85.8522        \n",
      "\n",
      "quiet_star_policy_loss= -0.02339010313153267\n",
      "nll_loss= 1.9331976175308228\n",
      "avg_std= 0.41002070903778076\n",
      "dist std min max: 0.041934944689273834 0.41002070903778076 3.7859575748443604\n",
      "hidden_states min max: -12.41338062286377 15.175848960876465\n",
      "hidden_state minus mean squared max: 173.90042114257812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.42440795898438 2.246131420135498\n",
      "loss   740: 1.9098   grad norm: 1.2101          model param norm: 85.8558        \n",
      "\n",
      "quiet_star_policy_loss= -0.03569931909441948\n",
      "nll_loss= 1.9446521997451782\n",
      "avg_std= 0.40786126255989075\n",
      "dist std min max: 0.042055319994688034 0.40786126255989075 3.809666872024536\n",
      "hidden_states min max: -12.49510669708252 15.081071853637695\n",
      "hidden_state minus mean squared max: 165.0567169189453\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -14.05388355255127 2.241342067718506\n",
      "loss   741: 1.9090   grad norm: 1.2763          model param norm: 85.8598        \n",
      "\n",
      "quiet_star_policy_loss= -0.029537225142121315\n",
      "nll_loss= 1.929774522781372\n",
      "avg_std= 0.407997190952301\n",
      "dist std min max: 0.042058512568473816 0.407997190952301 3.829681873321533\n",
      "hidden_states min max: -12.772916793823242 13.846479415893555\n",
      "hidden_state minus mean squared max: 142.43234252929688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.80252075195312 2.248638153076172\n",
      "loss   742: 1.9002   grad norm: 1.1977          model param norm: 85.8636        \n",
      "\n",
      "quiet_star_policy_loss= -0.009974658489227295\n",
      "nll_loss= 1.9237432479858398\n",
      "avg_std= 0.4075429439544678\n",
      "dist std min max: 0.042311571538448334 0.4075429439544678 3.79394268989563\n",
      "hidden_states min max: -12.92950439453125 13.9501953125\n",
      "hidden_state minus mean squared max: 139.31161499023438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.75231170654297 2.2330446243286133\n",
      "loss   743: 1.9138   grad norm: 1.2917          model param norm: 85.8673        \n",
      "\n",
      "quiet_star_policy_loss= -0.045777320861816406\n",
      "nll_loss= 1.9160279035568237\n",
      "avg_std= 0.4089932143688202\n",
      "dist std min max: 0.042489632964134216 0.4089932143688202 4.318282127380371\n",
      "hidden_states min max: -12.558218002319336 15.386186599731445\n",
      "hidden_state minus mean squared max: 200.3565216064453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.39920806884766 2.239088535308838\n",
      "loss   744: 1.8703   grad norm: 1.2896          model param norm: 85.8708        \n",
      "\n",
      "quiet_star_policy_loss= -0.04808476194739342\n",
      "nll_loss= 1.9154773950576782\n",
      "avg_std= 0.4078887104988098\n",
      "dist std min max: 0.04269639775156975 0.4078887104988098 3.8438050746917725\n",
      "hidden_states min max: -13.300296783447266 14.535822868347168\n",
      "hidden_state minus mean squared max: 151.26019287109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29666137695312 2.2336082458496094\n",
      "loss   745: 1.8674   grad norm: 1.2202          model param norm: 85.8744        \n",
      "\n",
      "quiet_star_policy_loss= -0.03952253982424736\n",
      "nll_loss= 1.9237384796142578\n",
      "avg_std= 0.40745919942855835\n",
      "dist std min max: 0.04283204302191734 0.40745919942855835 3.7958078384399414\n",
      "hidden_states min max: -33.460899353027344 15.214959144592285\n",
      "hidden_state minus mean squared max: 949.038330078125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.96027374267578 2.219602108001709\n",
      "loss   746: 1.8842   grad norm: 1.2244          model param norm: 85.8777        \n",
      "\n",
      "quiet_star_policy_loss= -0.035127878189086914\n",
      "nll_loss= 1.9144747257232666\n",
      "avg_std= 0.407939076423645\n",
      "dist std min max: 0.042798325419425964 0.407939076423645 3.818132162094116\n",
      "hidden_states min max: -12.93863582611084 14.4459228515625\n",
      "hidden_state minus mean squared max: 159.1659698486328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.02847290039062 2.2188515663146973\n",
      "loss   747: 1.8793   grad norm: 1.3215          model param norm: 85.8807        \n",
      "\n",
      "quiet_star_policy_loss= -0.012943649664521217\n",
      "nll_loss= 1.929064154624939\n",
      "avg_std= 0.4056228697299957\n",
      "dist std min max: 0.04267014190554619 0.4056228697299957 3.848088502883911\n",
      "hidden_states min max: -12.418020248413086 14.540699005126953\n",
      "hidden_state minus mean squared max: 146.726318359375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8175048828125 2.228118419647217\n",
      "loss   748: 1.9161   grad norm: 1.3568          model param norm: 85.8837        \n",
      "\n",
      "quiet_star_policy_loss= -0.027008509263396263\n",
      "nll_loss= 1.9257882833480835\n",
      "avg_std= 0.40421777963638306\n",
      "dist std min max: 0.04266994073987007 0.40421777963638306 3.8307600021362305\n",
      "hidden_states min max: -13.020659446716309 14.63746166229248\n",
      "hidden_state minus mean squared max: 155.5457000732422\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.47901916503906 2.219926357269287\n",
      "loss   749: 1.8988   grad norm: 1.2477          model param norm: 85.8865        \n",
      "\n",
      "quiet_star_policy_loss= -0.02901277504861355\n",
      "nll_loss= 1.9372482299804688\n",
      "avg_std= 0.40459829568862915\n",
      "dist std min max: 0.04271098971366882 0.40459829568862915 3.854203462600708\n",
      "hidden_states min max: -12.252888679504395 14.73529052734375\n",
      "hidden_state minus mean squared max: 164.94151306152344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.9642333984375 2.2313308715820312\n",
      "loss   750: 1.9082   grad norm: 1.2290          model param norm: 85.8891        \n",
      "\n",
      "quiet_star_policy_loss= -0.0033997774589806795\n",
      "nll_loss= 1.9266655445098877\n",
      "avg_std= 0.40438055992126465\n",
      "dist std min max: 0.042411405593156815 0.40438055992126465 3.822463035583496\n",
      "hidden_states min max: -13.510201454162598 15.227672576904297\n",
      "hidden_state minus mean squared max: 170.6532745361328\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.44642639160156 2.216489315032959\n",
      "loss   751: 1.9233   grad norm: 1.3389          model param norm: 85.8915        \n",
      "\n",
      "quiet_star_policy_loss= -0.020316505804657936\n",
      "nll_loss= 1.921449065208435\n",
      "avg_std= 0.4037337005138397\n",
      "dist std min max: 0.042986758053302765 0.4037337005138397 3.7919540405273438\n",
      "hidden_states min max: -17.945966720581055 15.086524963378906\n",
      "hidden_state minus mean squared max: 307.4045104980469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.39666748046875 2.2185144424438477\n",
      "loss   752: 1.9011   grad norm: 1.3438          model param norm: 85.8940        \n",
      "\n",
      "quiet_star_policy_loss= -0.018771637231111526\n",
      "nll_loss= 1.9079378843307495\n",
      "avg_std= 0.4056929051876068\n",
      "dist std min max: 0.0422060564160347 0.4056929051876068 3.952688455581665\n",
      "hidden_states min max: -11.998234748840332 14.572688102722168\n",
      "hidden_state minus mean squared max: 146.37155151367188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.67693328857422 2.233377456665039\n",
      "loss   753: 1.8892   grad norm: 1.1985          model param norm: 85.8964        \n",
      "\n",
      "quiet_star_policy_loss= -0.055914558470249176\n",
      "nll_loss= 1.9032424688339233\n",
      "avg_std= 0.4049408733844757\n",
      "dist std min max: 0.042317330837249756 0.4049408733844757 3.7576465606689453\n",
      "hidden_states min max: -13.363853454589844 13.928354263305664\n",
      "hidden_state minus mean squared max: 189.6139678955078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83411407470703 2.2340455055236816\n",
      "loss   754: 1.8473   grad norm: 1.2343          model param norm: 85.8987        \n",
      "\n",
      "quiet_star_policy_loss= -0.010769043117761612\n",
      "nll_loss= 1.9113163948059082\n",
      "avg_std= 0.40804922580718994\n",
      "dist std min max: 0.04269815981388092 0.40804922580718994 3.704191207885742\n",
      "hidden_states min max: -12.729169845581055 12.831273078918457\n",
      "hidden_state minus mean squared max: 127.15155792236328\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.45275115966797 2.217759609222412\n",
      "loss   755: 1.9005   grad norm: 2.6062          model param norm: 85.9012        \n",
      "\n",
      "quiet_star_policy_loss= -0.026000583544373512\n",
      "nll_loss= 1.929332971572876\n",
      "avg_std= 0.4032757878303528\n",
      "dist std min max: 0.04267047718167305 0.4032757878303528 3.790999412536621\n",
      "hidden_states min max: -12.724559783935547 16.25969886779785\n",
      "hidden_state minus mean squared max: 202.06240844726562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.105232238769531 2.226059913635254\n",
      "loss   756: 1.9033   grad norm: 1.2423          model param norm: 85.9040        \n",
      "\n",
      "quiet_star_policy_loss= -0.022641373798251152\n",
      "nll_loss= 1.915453314781189\n",
      "avg_std= 0.40249836444854736\n",
      "dist std min max: 0.04234384745359421 0.40249836444854736 3.746121406555176\n",
      "hidden_states min max: -13.222688674926758 14.381957054138184\n",
      "hidden_state minus mean squared max: 150.07693481445312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.91471099853516 2.231510639190674\n",
      "loss   757: 1.8928   grad norm: 1.3554          model param norm: 85.9070        \n",
      "\n",
      "quiet_star_policy_loss= -0.016298294067382812\n",
      "nll_loss= 1.9267629384994507\n",
      "avg_std= 0.4045380651950836\n",
      "dist std min max: 0.04238968342542648 0.4045380651950836 3.7577836513519287\n",
      "hidden_states min max: -13.047515869140625 14.779195785522461\n",
      "hidden_state minus mean squared max: 193.58895874023438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.1411361694336 2.2206473350524902\n",
      "loss   758: 1.9105   grad norm: 1.2920          model param norm: 85.9101        \n",
      "\n",
      "quiet_star_policy_loss= -0.04509386047720909\n",
      "nll_loss= 1.9096630811691284\n",
      "avg_std= 0.4013383090496063\n",
      "dist std min max: 0.042499836534261703 0.4013383090496063 3.734116792678833\n",
      "hidden_states min max: -14.295218467712402 20.29292869567871\n",
      "hidden_state minus mean squared max: 325.89825439453125\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.98516082763672 2.2269067764282227\n",
      "loss   759: 1.8646   grad norm: 1.3171          model param norm: 85.9134        \n",
      "\n",
      "quiet_star_policy_loss= -0.00547099718824029\n",
      "nll_loss= 1.9008400440216064\n",
      "avg_std= 0.4013347029685974\n",
      "dist std min max: 0.04208022728562355 0.4013347029685974 3.730963945388794\n",
      "hidden_states min max: -14.136373519897461 14.734437942504883\n",
      "hidden_state minus mean squared max: 135.76321411132812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46585083007812 2.23941707611084\n",
      "loss   760: 1.8954   grad norm: 1.2185          model param norm: 85.9166        \n",
      "\n",
      "quiet_star_policy_loss= -0.02785496786236763\n",
      "nll_loss= 1.9107176065444946\n",
      "avg_std= 0.4008307456970215\n",
      "dist std min max: 0.04199673607945442 0.4008307456970215 3.712711811065674\n",
      "hidden_states min max: -13.42233943939209 14.399374008178711\n",
      "hidden_state minus mean squared max: 150.971923828125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.09835815429688 2.2324185371398926\n",
      "loss   761: 1.8829   grad norm: 1.2219          model param norm: 85.9201        \n",
      "\n",
      "quiet_star_policy_loss= -0.03645988181233406\n",
      "nll_loss= 1.9117473363876343\n",
      "avg_std= 0.40296670794487\n",
      "dist std min max: 0.04221769794821739 0.40296670794487 3.6890792846679688\n",
      "hidden_states min max: -12.814455032348633 15.018399238586426\n",
      "hidden_state minus mean squared max: 161.8196258544922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.89567565917969 2.228793144226074\n",
      "loss   762: 1.8753   grad norm: 1.2393          model param norm: 85.9238        \n",
      "\n",
      "quiet_star_policy_loss= -0.028354978188872337\n",
      "nll_loss= 1.9418267011642456\n",
      "avg_std= 0.40088343620300293\n",
      "dist std min max: 0.042139530181884766 0.40088343620300293 3.688076972961426\n",
      "hidden_states min max: -13.131828308105469 15.101235389709473\n",
      "hidden_state minus mean squared max: 167.5467987060547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90093231201172 2.2337560653686523\n",
      "loss   763: 1.9135   grad norm: 1.2830          model param norm: 85.9272        \n",
      "\n",
      "quiet_star_policy_loss= -0.02297215536236763\n",
      "nll_loss= 1.916774034500122\n",
      "avg_std= 0.4013631045818329\n",
      "dist std min max: 0.042065780609846115 0.4013631045818329 3.677046537399292\n",
      "hidden_states min max: -12.515332221984863 14.135173797607422\n",
      "hidden_state minus mean squared max: 151.86192321777344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99170684814453 2.2367024421691895\n",
      "loss   764: 1.8938   grad norm: 1.3200          model param norm: 85.9301        \n",
      "\n",
      "quiet_star_policy_loss= -0.04587812349200249\n",
      "nll_loss= 1.9266372919082642\n",
      "avg_std= 0.40326592326164246\n",
      "dist std min max: 0.04196421802043915 0.40326592326164246 3.627713680267334\n",
      "hidden_states min max: -16.08224105834961 15.808048248291016\n",
      "hidden_state minus mean squared max: 204.79803466796875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.19358825683594 2.244999885559082\n",
      "loss   765: 1.8808   grad norm: 1.3128          model param norm: 85.9328        \n",
      "\n",
      "quiet_star_policy_loss= -0.014004075899720192\n",
      "nll_loss= 1.9131027460098267\n",
      "avg_std= 0.3998178243637085\n",
      "dist std min max: 0.04192955419421196 0.3998178243637085 3.6606616973876953\n",
      "hidden_states min max: -14.103813171386719 15.537029266357422\n",
      "hidden_state minus mean squared max: 171.2309112548828\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.96792602539062 2.2400732040405273\n",
      "loss   766: 1.8991   grad norm: 1.2848          model param norm: 85.9356        \n",
      "\n",
      "quiet_star_policy_loss= -0.03780844435095787\n",
      "nll_loss= 1.9056339263916016\n",
      "avg_std= 0.4006962180137634\n",
      "dist std min max: 0.041914790868759155 0.4006962180137634 3.63091778755188\n",
      "hidden_states min max: -13.062662124633789 14.112481117248535\n",
      "hidden_state minus mean squared max: 150.33367919921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9503402709961 2.246445655822754\n",
      "loss   767: 1.8678   grad norm: 1.3089          model param norm: 85.9384        \n",
      "\n",
      "quiet_star_policy_loss= -0.012446070089936256\n",
      "nll_loss= 1.9028781652450562\n",
      "avg_std= 0.39986252784729004\n",
      "dist std min max: 0.04167301207780838 0.39986252784729004 3.636481285095215\n",
      "hidden_states min max: -12.080548286437988 14.825385093688965\n",
      "hidden_state minus mean squared max: 155.1501922607422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.99848175048828 2.2381081581115723\n",
      "loss   768: 1.8904   grad norm: 1.3175          model param norm: 85.9411        \n",
      "\n",
      "quiet_star_policy_loss= -0.001975202700123191\n",
      "nll_loss= 1.9181387424468994\n",
      "avg_std= 0.3991181552410126\n",
      "dist std min max: 0.040913958102464676 0.3991181552410126 3.6468310356140137\n",
      "hidden_states min max: -19.523250579833984 14.35668659210205\n",
      "hidden_state minus mean squared max: 458.53936767578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.59661102294922 2.250544548034668\n",
      "loss   769: 1.9162   grad norm: 1.2981          model param norm: 85.9439        \n",
      "\n",
      "quiet_star_policy_loss= -0.024107515811920166\n",
      "nll_loss= 1.900336503982544\n",
      "avg_std= 0.40099841356277466\n",
      "dist std min max: 0.041599150747060776 0.40099841356277466 3.6443521976470947\n",
      "hidden_states min max: -14.229503631591797 13.384395599365234\n",
      "hidden_state minus mean squared max: 179.7046356201172\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -102.73479461669922 2.2492613792419434\n",
      "loss   770: 1.8762   grad norm: 1.2685          model param norm: 85.9466        \n",
      "\n",
      "quiet_star_policy_loss= 0.029051637277007103\n",
      "nll_loss= 1.9274075031280518\n",
      "avg_std= 0.39835888147354126\n",
      "dist std min max: 0.04091180860996246 0.39835888147354126 3.6266019344329834\n",
      "hidden_states min max: -13.26039981842041 13.891246795654297\n",
      "hidden_state minus mean squared max: 138.74887084960938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.25152587890625 2.262453556060791\n",
      "loss   771: 1.9565   grad norm: 1.3324          model param norm: 85.9495        \n",
      "\n",
      "quiet_star_policy_loss= -0.025431180372834206\n",
      "nll_loss= 1.9079383611679077\n",
      "avg_std= 0.39968404173851013\n",
      "dist std min max: 0.04127287119626999 0.39968404173851013 3.629528760910034\n",
      "hidden_states min max: -13.417835235595703 16.841190338134766\n",
      "hidden_state minus mean squared max: 185.6088409423828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.9051742553711 2.254962921142578\n",
      "loss   772: 1.8825   grad norm: 1.2908          model param norm: 85.9524        \n",
      "\n",
      "quiet_star_policy_loss= -0.0035283328033983707\n",
      "nll_loss= 1.9004278182983398\n",
      "avg_std= 0.40223824977874756\n",
      "dist std min max: 0.04106380045413971 0.40223824977874756 3.64874267578125\n",
      "hidden_states min max: -12.557439804077148 14.432690620422363\n",
      "hidden_state minus mean squared max: 182.5607147216797\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.36729431152344 2.268458843231201\n",
      "loss   773: 1.8969   grad norm: 1.2723          model param norm: 85.9555        \n",
      "\n",
      "quiet_star_policy_loss= -0.02185986004769802\n",
      "nll_loss= 1.9308624267578125\n",
      "avg_std= 0.3996286690235138\n",
      "dist std min max: 0.04153582453727722 0.3996286690235138 4.049282073974609\n",
      "hidden_states min max: -13.56131362915039 14.075901985168457\n",
      "hidden_state minus mean squared max: 139.99281311035156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9662857055664 2.2493934631347656\n",
      "loss   774: 1.9090   grad norm: 1.2861          model param norm: 85.9584        \n",
      "\n",
      "quiet_star_policy_loss= -0.010129463858902454\n",
      "nll_loss= 1.9193801879882812\n",
      "avg_std= 0.4010629653930664\n",
      "dist std min max: 0.041143205016851425 0.4010629653930664 3.6545896530151367\n",
      "hidden_states min max: -14.90406608581543 13.995891571044922\n",
      "hidden_state minus mean squared max: 160.75576782226562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.82557678222656 2.2612648010253906\n",
      "loss   775: 1.9093   grad norm: 1.2742          model param norm: 85.9609        \n",
      "\n",
      "quiet_star_policy_loss= -0.05106537416577339\n",
      "nll_loss= 1.905840516090393\n",
      "avg_std= 0.3985820412635803\n",
      "dist std min max: 0.04176799952983856 0.3985820412635803 3.659574031829834\n",
      "hidden_states min max: -12.86231803894043 14.702584266662598\n",
      "hidden_state minus mean squared max: 158.47512817382812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.06536865234375 2.2429933547973633\n",
      "loss   776: 1.8548   grad norm: 1.3448          model param norm: 85.9638        \n",
      "\n",
      "quiet_star_policy_loss= -0.050881411880254745\n",
      "nll_loss= 1.9159259796142578\n",
      "avg_std= 0.39845919609069824\n",
      "dist std min max: 0.04080362617969513 0.39845919609069824 3.6624274253845215\n",
      "hidden_states min max: -12.733712196350098 14.203892707824707\n",
      "hidden_state minus mean squared max: 154.7860565185547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78724670410156 2.269658088684082\n",
      "loss   777: 1.8650   grad norm: 1.2865          model param norm: 85.9665        \n",
      "\n",
      "quiet_star_policy_loss= -0.03557705879211426\n",
      "nll_loss= 1.907922625541687\n",
      "avg_std= 0.3976869583129883\n",
      "dist std min max: 0.04232625290751457 0.3976869583129883 3.647463798522949\n",
      "hidden_states min max: -11.998990058898926 14.772760391235352\n",
      "hidden_state minus mean squared max: 154.9320831298828\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.66649627685547 2.239452838897705\n",
      "loss   778: 1.8723   grad norm: 1.3459          model param norm: 85.9693        \n",
      "\n",
      "quiet_star_policy_loss= -0.004928481765091419\n",
      "nll_loss= 1.9096873998641968\n",
      "avg_std= 0.3944172263145447\n",
      "dist std min max: 0.041183844208717346 0.3944172263145447 3.791351556777954\n",
      "hidden_states min max: -14.0210599899292 14.724472045898438\n",
      "hidden_state minus mean squared max: 146.28012084960938\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.83163452148438 2.2574658393859863\n",
      "loss   779: 1.9048   grad norm: 1.2828          model param norm: 85.9722        \n",
      "\n",
      "quiet_star_policy_loss= 0.0016374588012695312\n",
      "nll_loss= 1.9265336990356445\n",
      "avg_std= 0.39346635341644287\n",
      "dist std min max: 0.04214052855968475 0.39346635341644287 3.64471173286438\n",
      "hidden_states min max: -14.031378746032715 15.010286331176758\n",
      "hidden_state minus mean squared max: 183.81996154785156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.15849304199219 2.242171287536621\n",
      "loss   780: 1.9282   grad norm: 1.2609          model param norm: 85.9750        \n",
      "\n",
      "quiet_star_policy_loss= -0.03418276831507683\n",
      "nll_loss= 1.904022216796875\n",
      "avg_std= 0.3964095115661621\n",
      "dist std min max: 0.04140583053231239 0.3964095115661621 3.642913579940796\n",
      "hidden_states min max: -45.18538284301758 15.012428283691406\n",
      "hidden_state minus mean squared max: 2260.61279296875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.3942642211914 2.2652549743652344\n",
      "loss   781: 1.8698   grad norm: 1.2276          model param norm: 85.9778        \n",
      "\n",
      "quiet_star_policy_loss= -0.04140353202819824\n",
      "nll_loss= 1.9112358093261719\n",
      "avg_std= 0.39573952555656433\n",
      "dist std min max: 0.041867490857839584 0.39573952555656433 3.714768886566162\n",
      "hidden_states min max: -15.370513916015625 15.68206787109375\n",
      "hidden_state minus mean squared max: 178.2017364501953\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.12403869628906 2.2529611587524414\n",
      "loss   782: 1.8698   grad norm: 1.2444          model param norm: 85.9809        \n",
      "\n",
      "quiet_star_policy_loss= -0.033292848616838455\n",
      "nll_loss= 1.9307938814163208\n",
      "avg_std= 0.39350852370262146\n",
      "dist std min max: 0.04241270199418068 0.39350852370262146 4.0148162841796875\n",
      "hidden_states min max: -12.626575469970703 14.684710502624512\n",
      "hidden_state minus mean squared max: 152.48391723632812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.82384490966797 2.2290897369384766\n",
      "loss   783: 1.8975   grad norm: 2.6480          model param norm: 85.9838        \n",
      "\n",
      "quiet_star_policy_loss= -0.02932601049542427\n",
      "nll_loss= 1.909981608390808\n",
      "avg_std= 0.3971438407897949\n",
      "dist std min max: 0.04191145300865173 0.3971438407897949 3.681270122528076\n",
      "hidden_states min max: -13.495841979980469 14.744003295898438\n",
      "hidden_state minus mean squared max: 163.56900024414062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.97539520263672 2.2452502250671387\n",
      "loss   784: 1.8807   grad norm: 1.2195          model param norm: 85.9874        \n",
      "\n",
      "quiet_star_policy_loss= -0.04997548088431358\n",
      "nll_loss= 1.9148975610733032\n",
      "avg_std= 0.39453738927841187\n",
      "dist std min max: 0.041726771742105484 0.39453738927841187 3.6657514572143555\n",
      "hidden_states min max: -17.951004028320312 15.993073463439941\n",
      "hidden_state minus mean squared max: 268.028076171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.328125 2.2394027709960938\n",
      "loss   785: 1.8649   grad norm: 1.3119          model param norm: 85.9911        \n",
      "\n",
      "quiet_star_policy_loss= -0.009882974438369274\n",
      "nll_loss= 1.8973537683486938\n",
      "avg_std= 0.39543500542640686\n",
      "dist std min max: 0.04235655814409256 0.39543500542640686 3.6788523197174072\n",
      "hidden_states min max: -13.698995590209961 16.005346298217773\n",
      "hidden_state minus mean squared max: 216.02151489257812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.89541625976562 2.234325408935547\n",
      "loss   786: 1.8875   grad norm: 1.2885          model param norm: 85.9948        \n",
      "\n",
      "quiet_star_policy_loss= -0.0005772590520791709\n",
      "nll_loss= 1.912149429321289\n",
      "avg_std= 0.3906349837779999\n",
      "dist std min max: 0.04161449521780014 0.3906349837779999 3.702817440032959\n",
      "hidden_states min max: -18.23422622680664 17.02429962158203\n",
      "hidden_state minus mean squared max: 410.4920349121094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.54126739501953 2.2442550659179688\n",
      "loss   787: 1.9116   grad norm: 1.3055          model param norm: 85.9983        \n",
      "\n",
      "quiet_star_policy_loss= 0.004662621300667524\n",
      "nll_loss= 1.9064152240753174\n",
      "avg_std= 0.3947964608669281\n",
      "dist std min max: 0.042450033128261566 0.3947964608669281 4.280741214752197\n",
      "hidden_states min max: -12.982975006103516 16.085786819458008\n",
      "hidden_state minus mean squared max: 182.26144409179688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.45755767822266 2.2266507148742676\n",
      "loss   788: 1.9111   grad norm: 1.3094          model param norm: 86.0017        \n",
      "\n",
      "quiet_star_policy_loss= -0.01390001829713583\n",
      "nll_loss= 1.9177563190460205\n",
      "avg_std= 0.39380916953086853\n",
      "dist std min max: 0.042044393718242645 0.39380916953086853 3.6946799755096436\n",
      "hidden_states min max: -13.628175735473633 15.060247421264648\n",
      "hidden_state minus mean squared max: 170.88499450683594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.5457534790039 2.2397780418395996\n",
      "loss   789: 1.9039   grad norm: 1.2937          model param norm: 86.0052        \n",
      "\n",
      "quiet_star_policy_loss= -0.018797779455780983\n",
      "nll_loss= 1.9091453552246094\n",
      "avg_std= 0.3909137547016144\n",
      "dist std min max: 0.04213961213827133 0.3909137547016144 3.6717145442962646\n",
      "hidden_states min max: -14.461906433105469 14.675418853759766\n",
      "hidden_state minus mean squared max: 182.95269775390625\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -13.445391654968262 2.2468342781066895\n",
      "loss   790: 1.8903   grad norm: 1.2907          model param norm: 86.0088        \n",
      "\n",
      "quiet_star_policy_loss= -0.0078573701903224\n",
      "nll_loss= 1.912645697593689\n",
      "avg_std= 0.39050161838531494\n",
      "dist std min max: 0.042123522609472275 0.39050161838531494 3.698962450027466\n",
      "hidden_states min max: -14.150636672973633 15.49482250213623\n",
      "hidden_state minus mean squared max: 162.69020080566406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.76526641845703 2.2261338233947754\n",
      "loss   791: 1.9048   grad norm: 1.2280          model param norm: 86.0119        \n",
      "\n",
      "quiet_star_policy_loss= 0.006811571307480335\n",
      "nll_loss= 1.9087028503417969\n",
      "avg_std= 0.3934139311313629\n",
      "dist std min max: 0.04168035835027695 0.3934139311313629 3.680105209350586\n",
      "hidden_states min max: -13.657304763793945 15.917600631713867\n",
      "hidden_state minus mean squared max: 177.76617431640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.68647003173828 2.2357287406921387\n",
      "loss   792: 1.9155   grad norm: 1.3436          model param norm: 86.0149        \n",
      "\n",
      "quiet_star_policy_loss= -0.011056840419769287\n",
      "nll_loss= 1.9082005023956299\n",
      "avg_std= 0.39133578538894653\n",
      "dist std min max: 0.04047512635588646 0.39133578538894653 3.676280975341797\n",
      "hidden_states min max: -12.56702709197998 14.901554107666016\n",
      "hidden_state minus mean squared max: 176.98492431640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.18853759765625 2.274967670440674\n",
      "loss   793: 1.8971   grad norm: 1.3299          model param norm: 86.0176        \n",
      "\n",
      "quiet_star_policy_loss= -0.014384818263351917\n",
      "nll_loss= 1.8790403604507446\n",
      "avg_std= 0.3922160863876343\n",
      "dist std min max: 0.04180792346596718 0.3922160863876343 4.331973075866699\n",
      "hidden_states min max: -12.63709831237793 14.924012184143066\n",
      "hidden_state minus mean squared max: 166.2049560546875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.59060668945312 2.252619743347168\n",
      "loss   794: 1.8647   grad norm: 1.2462          model param norm: 86.0204        \n",
      "\n",
      "quiet_star_policy_loss= -0.03826932981610298\n",
      "nll_loss= 1.9127944707870483\n",
      "avg_std= 0.3920443058013916\n",
      "dist std min max: 0.04098629206418991 0.3920443058013916 3.707723379135132\n",
      "hidden_states min max: -13.294121742248535 15.572237968444824\n",
      "hidden_state minus mean squared max: 173.5302734375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.27017211914062 2.2613582611083984\n",
      "loss   795: 1.8745   grad norm: 1.2556          model param norm: 86.0234        \n",
      "\n",
      "quiet_star_policy_loss= -0.02938518486917019\n",
      "nll_loss= 1.9281023740768433\n",
      "avg_std= 0.3904038071632385\n",
      "dist std min max: 0.040123164653778076 0.3904038071632385 3.865208625793457\n",
      "hidden_states min max: -13.672764778137207 14.488369941711426\n",
      "hidden_state minus mean squared max: 181.45632934570312\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.07292175292969 2.287053108215332\n",
      "loss   796: 1.8987   grad norm: 1.2750          model param norm: 86.0263        \n",
      "\n",
      "quiet_star_policy_loss= -0.008572995662689209\n",
      "nll_loss= 1.8924354314804077\n",
      "avg_std= 0.3901839554309845\n",
      "dist std min max: 0.04050225019454956 0.3901839554309845 3.645318031311035\n",
      "hidden_states min max: -12.475851058959961 13.914955139160156\n",
      "hidden_state minus mean squared max: 140.8211669921875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.96018981933594 2.272573947906494\n",
      "loss   797: 1.8839   grad norm: 1.3952          model param norm: 86.0291        \n",
      "\n",
      "quiet_star_policy_loss= -0.024060536175966263\n",
      "nll_loss= 1.9103659391403198\n",
      "avg_std= 0.38717901706695557\n",
      "dist std min max: 0.03945464268326759 0.38717901706695557 3.6626875400543213\n",
      "hidden_states min max: -13.151956558227539 14.070535659790039\n",
      "hidden_state minus mean squared max: 248.6764373779297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.71463012695312 2.2877964973449707\n",
      "loss   798: 1.8863   grad norm: 1.3450          model param norm: 86.0321        \n",
      "\n",
      "quiet_star_policy_loss= -0.030737513676285744\n",
      "nll_loss= 1.8962208032608032\n",
      "avg_std= 0.39060088992118835\n",
      "dist std min max: 0.04044526070356369 0.39060088992118835 3.6297402381896973\n",
      "hidden_states min max: -11.738351821899414 15.498346328735352\n",
      "hidden_state minus mean squared max: 168.88070678710938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.18245697021484 2.28560209274292\n",
      "loss   799: 1.8655   grad norm: 1.3159          model param norm: 86.0350        \n",
      "eval loss 1.9124953746795654\n",
      "\n",
      "quiet_star_policy_loss= 0.006236433982849121\n",
      "nll_loss= 1.9013713598251343\n",
      "avg_std= 0.3866419792175293\n",
      "dist std min max: 0.039343368262052536 0.3866419792175293 3.5932579040527344\n",
      "hidden_states min max: -11.72262191772461 14.096734046936035\n",
      "hidden_state minus mean squared max: 137.51641845703125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.20777893066406 2.3112568855285645\n",
      "loss   800: 1.9076   grad norm: 1.2547          model param norm: 86.0378        \n",
      "\n",
      "quiet_star_policy_loss= -0.05756426975131035\n",
      "nll_loss= 1.8888105154037476\n",
      "avg_std= 0.3883955180644989\n",
      "dist std min max: 0.039601027965545654 0.3883955180644989 3.588432550430298\n",
      "hidden_states min max: -12.803102493286133 15.349392890930176\n",
      "hidden_state minus mean squared max: 173.16065979003906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.86649322509766 2.3082380294799805\n",
      "loss   801: 1.8312   grad norm: 1.3532          model param norm: 86.0405        \n",
      "\n",
      "quiet_star_policy_loss= -0.022586394101381302\n",
      "nll_loss= 1.9131726026535034\n",
      "avg_std= 0.3850632905960083\n",
      "dist std min max: 0.03909846022725105 0.3850632905960083 3.5803635120391846\n",
      "hidden_states min max: -13.091923713684082 14.189627647399902\n",
      "hidden_state minus mean squared max: 167.70602416992188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.60575103759766 2.300222873687744\n",
      "loss   802: 1.8906   grad norm: 1.2843          model param norm: 86.0430        \n",
      "\n",
      "quiet_star_policy_loss= 0.0021744847763329744\n",
      "nll_loss= 1.9143589735031128\n",
      "avg_std= 0.38459983468055725\n",
      "dist std min max: 0.03862663358449936 0.38459983468055725 3.5621542930603027\n",
      "hidden_states min max: -11.749954223632812 14.9573335647583\n",
      "hidden_state minus mean squared max: 193.0914764404297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7187271118164 2.312774181365967\n",
      "loss   803: 1.9165   grad norm: 1.4157          model param norm: 86.0458        \n",
      "\n",
      "quiet_star_policy_loss= -0.05131260305643082\n",
      "nll_loss= 1.9138504266738892\n",
      "avg_std= 0.38477346301078796\n",
      "dist std min max: 0.03868371993303299 0.38477346301078796 3.5637776851654053\n",
      "hidden_states min max: -12.36678695678711 13.886547088623047\n",
      "hidden_state minus mean squared max: 150.78683471679688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80342864990234 2.3225464820861816\n",
      "loss   804: 1.8625   grad norm: 1.2892          model param norm: 86.0482        \n",
      "\n",
      "quiet_star_policy_loss= -0.017231512814760208\n",
      "nll_loss= 1.9005874395370483\n",
      "avg_std= 0.38377997279167175\n",
      "dist std min max: 0.03809839487075806 0.38377997279167175 3.5340466499328613\n",
      "hidden_states min max: -13.048541069030762 15.559860229492188\n",
      "hidden_state minus mean squared max: 147.63880920410156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06258392333984 2.333588123321533\n",
      "loss   805: 1.8834   grad norm: 1.2335          model param norm: 86.0507        \n",
      "\n",
      "quiet_star_policy_loss= -0.0020265341736376286\n",
      "nll_loss= 1.9091930389404297\n",
      "avg_std= 0.38287264108657837\n",
      "dist std min max: 0.03759096562862396 0.38287264108657837 3.5147056579589844\n",
      "hidden_states min max: -11.51260757446289 14.916608810424805\n",
      "hidden_state minus mean squared max: 144.35267639160156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.05216217041016 2.3411073684692383\n",
      "loss   806: 1.9072   grad norm: 1.3355          model param norm: 86.0531        \n",
      "\n",
      "quiet_star_policy_loss= -0.04179667308926582\n",
      "nll_loss= 1.9278348684310913\n",
      "avg_std= 0.38092097640037537\n",
      "dist std min max: 0.036955103278160095 0.38092097640037537 3.5337302684783936\n",
      "hidden_states min max: -23.36916732788086 14.346016883850098\n",
      "hidden_state minus mean squared max: 401.20147705078125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.52983093261719 2.366835594177246\n",
      "loss   807: 1.8860   grad norm: 1.2581          model param norm: 86.0556        \n",
      "\n",
      "quiet_star_policy_loss= -0.028732633218169212\n",
      "nll_loss= 1.9096416234970093\n",
      "avg_std= 0.3827318251132965\n",
      "dist std min max: 0.036755163222551346 0.3827318251132965 3.534424066543579\n",
      "hidden_states min max: -12.353513717651367 13.659272193908691\n",
      "hidden_state minus mean squared max: 147.8434295654297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.78665161132812 2.3776636123657227\n",
      "loss   808: 1.8809   grad norm: 1.4055          model param norm: 86.0582        \n",
      "\n",
      "quiet_star_policy_loss= -0.002014255616813898\n",
      "nll_loss= 1.9116142988204956\n",
      "avg_std= 0.380252480506897\n",
      "dist std min max: 0.035987503826618195 0.380252480506897 3.5345497131347656\n",
      "hidden_states min max: -12.616499900817871 14.496905326843262\n",
      "hidden_state minus mean squared max: 137.51133728027344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8205795288086 2.386564254760742\n",
      "loss   809: 1.9096   grad norm: 1.2992          model param norm: 86.0608        \n",
      "\n",
      "quiet_star_policy_loss= 0.0035181045532226562\n",
      "nll_loss= 1.9118757247924805\n",
      "avg_std= 0.3799454867839813\n",
      "dist std min max: 0.03562914952635765 0.3799454867839813 3.5073931217193604\n",
      "hidden_states min max: -11.755221366882324 16.116308212280273\n",
      "hidden_state minus mean squared max: 183.69212341308594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.308349609375 2.4085307121276855\n",
      "loss   810: 1.9154   grad norm: 1.3157          model param norm: 86.0637        \n",
      "\n",
      "quiet_star_policy_loss= -0.0052093504928052425\n",
      "nll_loss= 1.943457841873169\n",
      "avg_std= 0.37566614151000977\n",
      "dist std min max: 0.03551146760582924 0.37566614151000977 3.516075849533081\n",
      "hidden_states min max: -10.765408515930176 12.800812721252441\n",
      "hidden_state minus mean squared max: 103.19721221923828\n",
      "hidden_state minus mean divided by std max: 4.9571757316589355\n",
      "log_prob min max: -102.80602264404297 2.418539524078369\n",
      "loss   811: 1.9382   grad norm: 2.7976          model param norm: 86.0666        \n",
      "\n",
      "quiet_star_policy_loss= -0.02602078951895237\n",
      "nll_loss= 1.9104397296905518\n",
      "avg_std= 0.37999945878982544\n",
      "dist std min max: 0.03486572578549385 0.37999945878982544 3.5140113830566406\n",
      "hidden_states min max: -12.744632720947266 14.810440063476562\n",
      "hidden_state minus mean squared max: 144.3514862060547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.71699523925781 2.4214229583740234\n",
      "loss   812: 1.8844   grad norm: 1.2935          model param norm: 86.0694        \n",
      "\n",
      "quiet_star_policy_loss= -0.021324682980775833\n",
      "nll_loss= 1.9137977361679077\n",
      "avg_std= 0.3787635862827301\n",
      "dist std min max: 0.03460349887609482 0.3787635862827301 3.5497004985809326\n",
      "hidden_states min max: -12.422234535217285 14.741941452026367\n",
      "hidden_state minus mean squared max: 187.8650360107422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8698501586914 2.4312477111816406\n",
      "loss   813: 1.8925   grad norm: 1.3068          model param norm: 86.0724        \n",
      "\n",
      "quiet_star_policy_loss= -0.018741726875305176\n",
      "nll_loss= 1.9095312356948853\n",
      "avg_std= 0.3785092234611511\n",
      "dist std min max: 0.03452930971980095 0.3785092234611511 3.636133909225464\n",
      "hidden_states min max: -11.677749633789062 14.674759864807129\n",
      "hidden_state minus mean squared max: 140.11172485351562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.21116638183594 2.4461569786071777\n",
      "loss   814: 1.8908   grad norm: 1.3984          model param norm: 86.0751        \n",
      "\n",
      "quiet_star_policy_loss= 0.007150006480515003\n",
      "nll_loss= 1.8959375619888306\n",
      "avg_std= 0.37654703855514526\n",
      "dist std min max: 0.03398310765624046 0.37654703855514526 3.533966541290283\n",
      "hidden_states min max: -12.331672668457031 15.079955101013184\n",
      "hidden_state minus mean squared max: 154.89442443847656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.19786834716797 2.4548816680908203\n",
      "loss   815: 1.9031   grad norm: 1.2424          model param norm: 86.0782        \n",
      "\n",
      "quiet_star_policy_loss= -0.01351079996675253\n",
      "nll_loss= 1.8948811292648315\n",
      "avg_std= 0.3750426769256592\n",
      "dist std min max: 0.03357125446200371 0.3750426769256592 3.5093793869018555\n",
      "hidden_states min max: -11.453462600708008 15.388688087463379\n",
      "hidden_state minus mean squared max: 157.84136962890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.43385314941406 2.460350513458252\n",
      "loss   816: 1.8814   grad norm: 1.3071          model param norm: 86.0811        \n",
      "\n",
      "quiet_star_policy_loss= -0.004393267910927534\n",
      "nll_loss= 1.9135971069335938\n",
      "avg_std= 0.3756901025772095\n",
      "dist std min max: 0.03362376615405083 0.3756901025772095 3.5390126705169678\n",
      "hidden_states min max: -11.369200706481934 14.311997413635254\n",
      "hidden_state minus mean squared max: 130.0379180908203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.77205657958984 2.469498634338379\n",
      "loss   817: 1.9092   grad norm: 1.3415          model param norm: 86.0840        \n",
      "\n",
      "quiet_star_policy_loss= -0.017976118251681328\n",
      "nll_loss= 1.9007848501205444\n",
      "avg_std= 0.37672293186187744\n",
      "dist std min max: 0.033124737441539764 0.37672293186187744 3.5135624408721924\n",
      "hidden_states min max: -12.156632423400879 14.247201919555664\n",
      "hidden_state minus mean squared max: 137.75286865234375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.54544830322266 2.4762744903564453\n",
      "loss   818: 1.8828   grad norm: 1.2969          model param norm: 86.0871        \n",
      "\n",
      "quiet_star_policy_loss= -0.0006801605341024697\n",
      "nll_loss= 1.911077857017517\n",
      "avg_std= 0.3760341703891754\n",
      "dist std min max: 0.03272407129406929 0.3760341703891754 3.501575469970703\n",
      "hidden_states min max: -17.32097816467285 15.098987579345703\n",
      "hidden_state minus mean squared max: 272.44256591796875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.33628845214844 2.4926705360412598\n",
      "loss   819: 1.9104   grad norm: 1.2990          model param norm: 86.0900        \n",
      "\n",
      "quiet_star_policy_loss= -0.020963121205568314\n",
      "nll_loss= 1.889726996421814\n",
      "avg_std= 0.3757067620754242\n",
      "dist std min max: 0.0326816588640213 0.3757067620754242 3.526956796646118\n",
      "hidden_states min max: -11.734599113464355 14.043007850646973\n",
      "hidden_state minus mean squared max: 149.4280242919922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.6757583618164 2.500373363494873\n",
      "loss   820: 1.8688   grad norm: 1.3591          model param norm: 86.0926        \n",
      "\n",
      "quiet_star_policy_loss= -0.017401600256562233\n",
      "nll_loss= 1.8983898162841797\n",
      "avg_std= 0.3769861161708832\n",
      "dist std min max: 0.032044362276792526 0.3769861161708832 3.5051987171173096\n",
      "hidden_states min max: -12.521529197692871 16.013486862182617\n",
      "hidden_state minus mean squared max: 170.91831970214844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.59587860107422 2.5045127868652344\n",
      "loss   821: 1.8810   grad norm: 1.2730          model param norm: 86.0954        \n",
      "\n",
      "quiet_star_policy_loss= 0.008889570832252502\n",
      "nll_loss= 1.9019542932510376\n",
      "avg_std= 0.37347936630249023\n",
      "dist std min max: 0.03198741748929024 0.37347936630249023 3.5026168823242188\n",
      "hidden_states min max: -13.4813232421875 14.728740692138672\n",
      "hidden_state minus mean squared max: 126.94293212890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.56099700927734 2.5085511207580566\n",
      "loss   822: 1.9108   grad norm: 1.2586          model param norm: 86.0985        \n",
      "\n",
      "quiet_star_policy_loss= -0.01279308833181858\n",
      "nll_loss= 1.9146947860717773\n",
      "avg_std= 0.3738095462322235\n",
      "dist std min max: 0.031945519149303436 0.3738095462322235 3.518707513809204\n",
      "hidden_states min max: -11.639925003051758 16.912935256958008\n",
      "hidden_state minus mean squared max: 202.0959930419922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.21332550048828 2.5135879516601562\n",
      "loss   823: 1.9019   grad norm: 1.2939          model param norm: 86.1014        \n",
      "\n",
      "quiet_star_policy_loss= -0.02807796001434326\n",
      "nll_loss= 1.9156162738800049\n",
      "avg_std= 0.37448957562446594\n",
      "dist std min max: 0.031813982874155045 0.37448957562446594 3.490384578704834\n",
      "hidden_states min max: -11.601371765136719 16.802656173706055\n",
      "hidden_state minus mean squared max: 189.52459716796875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.96546173095703 2.521247386932373\n",
      "loss   824: 1.8875   grad norm: 1.4079          model param norm: 86.1041        \n",
      "\n",
      "quiet_star_policy_loss= -0.034347742795944214\n",
      "nll_loss= 1.8926868438720703\n",
      "avg_std= 0.37445348501205444\n",
      "dist std min max: 0.031248467043042183 0.37445348501205444 3.5021402835845947\n",
      "hidden_states min max: -11.62098217010498 15.504849433898926\n",
      "hidden_state minus mean squared max: 174.0288543701172\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.34268951416016 2.542037010192871\n",
      "loss   825: 1.8583   grad norm: 1.3791          model param norm: 86.1067        \n",
      "\n",
      "quiet_star_policy_loss= -0.0008359432104043663\n",
      "nll_loss= 1.9055683612823486\n",
      "avg_std= 0.370544970035553\n",
      "dist std min max: 0.030564744025468826 0.370544970035553 3.5139901638031006\n",
      "hidden_states min max: -11.430789947509766 15.494829177856445\n",
      "hidden_state minus mean squared max: 158.1182403564453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.42018127441406 2.554159641265869\n",
      "loss   826: 1.9047   grad norm: 1.2351          model param norm: 86.1092        \n",
      "\n",
      "quiet_star_policy_loss= -0.014651322737336159\n",
      "nll_loss= 1.8990081548690796\n",
      "avg_std= 0.37280669808387756\n",
      "dist std min max: 0.031154243275523186 0.37280669808387756 3.4923925399780273\n",
      "hidden_states min max: -11.057609558105469 14.302268981933594\n",
      "hidden_state minus mean squared max: 165.38916015625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08673095703125 2.5399250984191895\n",
      "loss   827: 1.8844   grad norm: 1.4172          model param norm: 86.1116        \n",
      "\n",
      "quiet_star_policy_loss= -0.053784169256687164\n",
      "nll_loss= 1.8854343891143799\n",
      "avg_std= 0.37405097484588623\n",
      "dist std min max: 0.030714696273207664 0.37405097484588623 3.7218339443206787\n",
      "hidden_states min max: -11.879446983337402 17.145231246948242\n",
      "hidden_state minus mean squared max: 197.9818115234375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.0933837890625 2.554659843444824\n",
      "loss   828: 1.8317   grad norm: 1.2255          model param norm: 86.1140        \n",
      "\n",
      "quiet_star_policy_loss= -0.007751781027764082\n",
      "nll_loss= 1.8938177824020386\n",
      "avg_std= 0.37182557582855225\n",
      "dist std min max: 0.030435776337981224 0.37182557582855225 3.4839978218078613\n",
      "hidden_states min max: -11.647859573364258 13.979086875915527\n",
      "hidden_state minus mean squared max: 195.18763732910156\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.1695556640625 2.556142807006836\n",
      "loss   829: 1.8861   grad norm: 1.3500          model param norm: 86.1166        \n",
      "\n",
      "quiet_star_policy_loss= -0.03000054322183132\n",
      "nll_loss= 1.9030125141143799\n",
      "avg_std= 0.37343496084213257\n",
      "dist std min max: 0.030408672988414764 0.37343496084213257 3.4691810607910156\n",
      "hidden_states min max: -16.210590362548828 14.088351249694824\n",
      "hidden_state minus mean squared max: 261.28326416015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.31538391113281 2.559455394744873\n",
      "loss   830: 1.8730   grad norm: 1.3082          model param norm: 86.1189        \n",
      "\n",
      "quiet_star_policy_loss= -0.05513883754611015\n",
      "nll_loss= 1.9027206897735596\n",
      "avg_std= 0.3747696280479431\n",
      "dist std min max: 0.030059030279517174 0.3747696280479431 3.5747904777526855\n",
      "hidden_states min max: -11.197318077087402 15.404986381530762\n",
      "hidden_state minus mean squared max: 150.89723205566406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.76206970214844 2.569049835205078\n",
      "loss   831: 1.8476   grad norm: 1.2989          model param norm: 86.1213        \n",
      "\n",
      "quiet_star_policy_loss= -0.030105531215667725\n",
      "nll_loss= 1.897396445274353\n",
      "avg_std= 0.3720594644546509\n",
      "dist std min max: 0.02971494570374489 0.3720594644546509 3.659980058670044\n",
      "hidden_states min max: -17.867788314819336 14.682117462158203\n",
      "hidden_state minus mean squared max: 328.442626953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.42974853515625 2.5908069610595703\n",
      "loss   832: 1.8673   grad norm: 1.3193          model param norm: 86.1237        \n",
      "\n",
      "quiet_star_policy_loss= -0.01578707806766033\n",
      "nll_loss= 1.89922034740448\n",
      "avg_std= 0.37141937017440796\n",
      "dist std min max: 0.029926925897598267 0.37141937017440796 3.4689817428588867\n",
      "hidden_states min max: -11.233596801757812 16.97258949279785\n",
      "hidden_state minus mean squared max: 197.7937774658203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.75459289550781 2.5855369567871094\n",
      "loss   833: 1.8834   grad norm: 1.2633          model param norm: 86.1267        \n",
      "\n",
      "quiet_star_policy_loss= -0.05772064998745918\n",
      "nll_loss= 1.8940309286117554\n",
      "avg_std= 0.3726326525211334\n",
      "dist std min max: 0.030113961547613144 0.3726326525211334 3.479231834411621\n",
      "hidden_states min max: -11.005753517150879 14.272857666015625\n",
      "hidden_state minus mean squared max: 127.63105010986328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.01350402832031 2.5824050903320312\n",
      "loss   834: 1.8363   grad norm: 1.3484          model param norm: 86.1295        \n",
      "\n",
      "quiet_star_policy_loss= -0.002326536225154996\n",
      "nll_loss= 1.899073600769043\n",
      "avg_std= 0.3685629963874817\n",
      "dist std min max: 0.030202634632587433 0.3685629963874817 3.4518210887908936\n",
      "hidden_states min max: -11.905014991760254 13.895099639892578\n",
      "hidden_state minus mean squared max: 112.21941375732422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.20489501953125 2.576106071472168\n",
      "loss   835: 1.8967   grad norm: 1.3745          model param norm: 86.1322        \n",
      "\n",
      "quiet_star_policy_loss= 0.02209451235830784\n",
      "nll_loss= 1.902962565422058\n",
      "avg_std= 0.3669582009315491\n",
      "dist std min max: 0.029829731211066246 0.3669582009315491 3.449777603149414\n",
      "hidden_states min max: -11.599125862121582 14.183692932128906\n",
      "hidden_state minus mean squared max: 133.9531707763672\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.37393951416016 2.5864086151123047\n",
      "loss   836: 1.9251   grad norm: 1.3002          model param norm: 86.1353        \n",
      "\n",
      "quiet_star_policy_loss= 0.026679469272494316\n",
      "nll_loss= 1.8954445123672485\n",
      "avg_std= 0.369726300239563\n",
      "dist std min max: 0.029710298404097557 0.369726300239563 3.6152424812316895\n",
      "hidden_states min max: -31.760160446166992 14.372797012329102\n",
      "hidden_state minus mean squared max: 791.3668212890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.86946868896484 2.5794453620910645\n",
      "loss   837: 1.9221   grad norm: 1.2753          model param norm: 86.1383        \n",
      "\n",
      "quiet_star_policy_loss= -0.003119993256404996\n",
      "nll_loss= 1.9078865051269531\n",
      "avg_std= 0.36871394515037537\n",
      "dist std min max: 0.030042186379432678 0.36871394515037537 3.4284884929656982\n",
      "hidden_states min max: -11.35676383972168 15.478736877441406\n",
      "hidden_state minus mean squared max: 155.76651000976562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.00888061523438 2.5813751220703125\n",
      "loss   838: 1.9048   grad norm: 1.4384          model param norm: 86.1415        \n",
      "\n",
      "quiet_star_policy_loss= -0.05147819593548775\n",
      "nll_loss= 1.9151455163955688\n",
      "avg_std= 0.36576399207115173\n",
      "dist std min max: 0.02943088859319687 0.36576399207115173 3.38503098487854\n",
      "hidden_states min max: -10.94690227508545 11.984583854675293\n",
      "hidden_state minus mean squared max: 121.64543151855469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.6883544921875 2.5745010375976562\n",
      "loss   839: 1.8637   grad norm: 2.8315          model param norm: 86.1446        \n",
      "\n",
      "quiet_star_policy_loss= -0.013387799263000488\n",
      "nll_loss= 1.9169129133224487\n",
      "avg_std= 0.367656946182251\n",
      "dist std min max: 0.029608119279146194 0.367656946182251 3.3912160396575928\n",
      "hidden_states min max: -12.800254821777344 13.27665901184082\n",
      "hidden_state minus mean squared max: 144.30938720703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.41490936279297 2.591555595397949\n",
      "loss   840: 1.9035   grad norm: 1.4481          model param norm: 86.1476        \n",
      "\n",
      "quiet_star_policy_loss= -0.014532983303070068\n",
      "nll_loss= 1.8956031799316406\n",
      "avg_std= 0.3676338493824005\n",
      "dist std min max: 0.02904234081506729 0.3676338493824005 3.382591724395752\n",
      "hidden_states min max: -11.683526039123535 14.550233840942383\n",
      "hidden_state minus mean squared max: 133.087158203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.33460235595703 2.615959644317627\n",
      "loss   841: 1.8811   grad norm: 1.4175          model param norm: 86.1505        \n",
      "\n",
      "quiet_star_policy_loss= -0.040314387530088425\n",
      "nll_loss= 1.8925474882125854\n",
      "avg_std= 0.369070440530777\n",
      "dist std min max: 0.028706641867756844 0.369070440530777 3.341520309448242\n",
      "hidden_states min max: -12.445151329040527 13.476621627807617\n",
      "hidden_state minus mean squared max: 156.36422729492188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.59486389160156 2.6143317222595215\n",
      "loss   842: 1.8522   grad norm: 1.3568          model param norm: 86.1534        \n",
      "\n",
      "quiet_star_policy_loss= -0.024412168189883232\n",
      "nll_loss= 1.898866891860962\n",
      "avg_std= 0.36573201417922974\n",
      "dist std min max: 0.028541626408696175 0.36573201417922974 3.3178062438964844\n",
      "hidden_states min max: -12.706406593322754 13.893633842468262\n",
      "hidden_state minus mean squared max: 198.6366424560547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.41455078125 2.6229724884033203\n",
      "loss   843: 1.8745   grad norm: 1.4071          model param norm: 86.1565        \n",
      "\n",
      "quiet_star_policy_loss= -0.022125685587525368\n",
      "nll_loss= 1.8856890201568604\n",
      "avg_std= 0.3669358491897583\n",
      "dist std min max: 0.028439274057745934 0.3669358491897583 3.3129429817199707\n",
      "hidden_states min max: -11.200887680053711 14.958691596984863\n",
      "hidden_state minus mean squared max: 149.13525390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.73664093017578 2.6334877014160156\n",
      "loss   844: 1.8636   grad norm: 1.4256          model param norm: 86.1596        \n",
      "\n",
      "quiet_star_policy_loss= -0.02886483445763588\n",
      "nll_loss= 1.8885616064071655\n",
      "avg_std= 0.36573946475982666\n",
      "dist std min max: 0.02797522395849228 0.36573946475982666 3.267239809036255\n",
      "hidden_states min max: -12.075150489807129 15.149452209472656\n",
      "hidden_state minus mean squared max: 148.3562774658203\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.27668762207031 2.629669666290283\n",
      "loss   845: 1.8597   grad norm: 1.3169          model param norm: 86.1629        \n",
      "\n",
      "quiet_star_policy_loss= -0.012242746539413929\n",
      "nll_loss= 1.8910000324249268\n",
      "avg_std= 0.3667282164096832\n",
      "dist std min max: 0.027901047840714455 0.3667282164096832 3.2391562461853027\n",
      "hidden_states min max: -13.076314926147461 13.39987850189209\n",
      "hidden_state minus mean squared max: 182.05096435546875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.13471221923828 2.6300859451293945\n",
      "loss   846: 1.8788   grad norm: 1.3357          model param norm: 86.1662        \n",
      "\n",
      "quiet_star_policy_loss= -0.01949482038617134\n",
      "nll_loss= 1.9163925647735596\n",
      "avg_std= 0.36254361271858215\n",
      "dist std min max: 0.027776451781392097 0.36254361271858215 3.859334707260132\n",
      "hidden_states min max: -23.47484588623047 15.663110733032227\n",
      "hidden_state minus mean squared max: 505.48699951171875\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -104.64533233642578 2.647124767303467\n",
      "loss   847: 1.8969   grad norm: 1.3803          model param norm: 86.1694        \n",
      "\n",
      "quiet_star_policy_loss= -0.0012156993616372347\n",
      "nll_loss= 1.9137020111083984\n",
      "avg_std= 0.3638206422328949\n",
      "dist std min max: 0.027502713724970818 0.3638206422328949 3.2122325897216797\n",
      "hidden_states min max: -13.77822208404541 14.410152435302734\n",
      "hidden_state minus mean squared max: 237.2047882080078\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.26703643798828 2.6504759788513184\n",
      "loss   848: 1.9125   grad norm: 1.3110          model param norm: 86.1723        \n",
      "\n",
      "quiet_star_policy_loss= -0.011106467805802822\n",
      "nll_loss= 1.887633204460144\n",
      "avg_std= 0.36719492077827454\n",
      "dist std min max: 0.027602525427937508 0.36719492077827454 3.1979925632476807\n",
      "hidden_states min max: -15.86302375793457 13.521289825439453\n",
      "hidden_state minus mean squared max: 254.34898376464844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.30193328857422 2.660795211791992\n",
      "loss   849: 1.8765   grad norm: 1.3760          model param norm: 86.1751        \n",
      "\n",
      "quiet_star_policy_loss= -0.03280053287744522\n",
      "nll_loss= 1.909460425376892\n",
      "avg_std= 0.36122751235961914\n",
      "dist std min max: 0.0274173766374588 0.36122751235961914 3.1862971782684326\n",
      "hidden_states min max: -12.94191837310791 13.983535766601562\n",
      "hidden_state minus mean squared max: 154.30999755859375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.92642211914062 2.6682000160217285\n",
      "loss   850: 1.8767   grad norm: 1.3225          model param norm: 86.1775        \n",
      "\n",
      "quiet_star_policy_loss= -0.0164076816290617\n",
      "nll_loss= 1.9090744256973267\n",
      "avg_std= 0.3645468056201935\n",
      "dist std min max: 0.02703901194036007 0.3645468056201935 3.1766417026519775\n",
      "hidden_states min max: -14.793502807617188 13.428487777709961\n",
      "hidden_state minus mean squared max: 200.16246032714844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.18215942382812 2.6783742904663086\n",
      "loss   851: 1.8927   grad norm: 1.3708          model param norm: 86.1799        \n",
      "\n",
      "quiet_star_policy_loss= -0.02984314039349556\n",
      "nll_loss= 1.893130898475647\n",
      "avg_std= 0.36516377329826355\n",
      "dist std min max: 0.026891376823186874 0.36516377329826355 3.135232448577881\n",
      "hidden_states min max: -11.224348068237305 14.177445411682129\n",
      "hidden_state minus mean squared max: 132.21493530273438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.32061767578125 2.681331157684326\n",
      "loss   852: 1.8633   grad norm: 1.4425          model param norm: 86.1823        \n",
      "\n",
      "quiet_star_policy_loss= -0.024680042639374733\n",
      "nll_loss= 1.8973430395126343\n",
      "avg_std= 0.3652452826499939\n",
      "dist std min max: 0.027227086946368217 0.3652452826499939 3.1187736988067627\n",
      "hidden_states min max: -12.825187683105469 15.559721946716309\n",
      "hidden_state minus mean squared max: 165.59117126464844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.07528686523438 2.670722007751465\n",
      "loss   853: 1.8727   grad norm: 1.3163          model param norm: 86.1846        \n",
      "\n",
      "quiet_star_policy_loss= 0.021100236102938652\n",
      "nll_loss= 1.8990367650985718\n",
      "avg_std= 0.3634506165981293\n",
      "dist std min max: 0.026568148285150528 0.3634506165981293 3.1098453998565674\n",
      "hidden_states min max: -11.088327407836914 14.328096389770508\n",
      "hidden_state minus mean squared max: 116.73591613769531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.3318099975586 2.689077377319336\n",
      "loss   854: 1.9201   grad norm: 1.3955          model param norm: 86.1871        \n",
      "\n",
      "quiet_star_policy_loss= 0.021356476470828056\n",
      "nll_loss= 1.9017174243927002\n",
      "avg_std= 0.36517611145973206\n",
      "dist std min max: 0.02669951692223549 0.36517611145973206 3.376481771469116\n",
      "hidden_states min max: -16.430782318115234 12.961738586425781\n",
      "hidden_state minus mean squared max: 345.04766845703125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.45442199707031 2.6879959106445312\n",
      "loss   855: 1.9231   grad norm: 1.3110          model param norm: 86.1897        \n",
      "\n",
      "quiet_star_policy_loss= 0.011859714984893799\n",
      "nll_loss= 1.883368730545044\n",
      "avg_std= 0.36542096734046936\n",
      "dist std min max: 0.026682818308472633 0.36542096734046936 3.185821771621704\n",
      "hidden_states min max: -11.969886779785156 12.4655122756958\n",
      "hidden_state minus mean squared max: 172.2843017578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.25630187988281 2.689309597015381\n",
      "loss   856: 1.8952   grad norm: 1.3550          model param norm: 86.1924        \n",
      "\n",
      "quiet_star_policy_loss= -0.00931356567889452\n",
      "nll_loss= 1.8990296125411987\n",
      "avg_std= 0.3634747564792633\n",
      "dist std min max: 0.026425477117300034 0.3634747564792633 3.2473394870758057\n",
      "hidden_states min max: -12.30560302734375 13.474721908569336\n",
      "hidden_state minus mean squared max: 138.2698516845703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.83622741699219 2.7134623527526855\n",
      "loss   857: 1.8897   grad norm: 1.3347          model param norm: 86.1954        \n",
      "\n",
      "quiet_star_policy_loss= -0.02411130629479885\n",
      "nll_loss= 1.8875350952148438\n",
      "avg_std= 0.3631141781806946\n",
      "dist std min max: 0.026522492989897728 0.3631141781806946 3.080725908279419\n",
      "hidden_states min max: -11.599543571472168 13.091508865356445\n",
      "hidden_state minus mean squared max: 134.94290161132812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.28166961669922 2.702259063720703\n",
      "loss   858: 1.8634   grad norm: 1.3448          model param norm: 86.1985        \n",
      "\n",
      "quiet_star_policy_loss= -0.0106758838519454\n",
      "nll_loss= 1.9009268283843994\n",
      "avg_std= 0.3646298944950104\n",
      "dist std min max: 0.026586176827549934 0.3646298944950104 3.491579055786133\n",
      "hidden_states min max: -12.096732139587402 12.838556289672852\n",
      "hidden_state minus mean squared max: 99.06974792480469\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.67546081542969 2.692925453186035\n",
      "loss   859: 1.8903   grad norm: 1.4879          model param norm: 86.2014        \n",
      "\n",
      "quiet_star_policy_loss= -0.005563855171203613\n",
      "nll_loss= 1.907731294631958\n",
      "avg_std= 0.36371082067489624\n",
      "dist std min max: 0.025833558291196823 0.36371082067489624 3.094944953918457\n",
      "hidden_states min max: -16.898588180541992 13.500017166137695\n",
      "hidden_state minus mean squared max: 361.4681701660156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.47766876220703 2.706676483154297\n",
      "loss   860: 1.9022   grad norm: 1.3840          model param norm: 86.2042        \n",
      "\n",
      "quiet_star_policy_loss= -0.0398530475795269\n",
      "nll_loss= 1.89630126953125\n",
      "avg_std= 0.3602854907512665\n",
      "dist std min max: 0.02685832791030407 0.3602854907512665 3.06954288482666\n",
      "hidden_states min max: -12.290292739868164 13.987373352050781\n",
      "hidden_state minus mean squared max: 120.362060546875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.87859344482422 2.686408042907715\n",
      "loss   861: 1.8564   grad norm: 1.4676          model param norm: 86.2068        \n",
      "\n",
      "quiet_star_policy_loss= -0.009044894948601723\n",
      "nll_loss= 1.880682349205017\n",
      "avg_std= 0.3628697395324707\n",
      "dist std min max: 0.026554666459560394 0.3628697395324707 3.058012008666992\n",
      "hidden_states min max: -11.632684707641602 13.316814422607422\n",
      "hidden_state minus mean squared max: 96.67390441894531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.3384017944336 2.6936898231506348\n",
      "loss   862: 1.8716   grad norm: 1.4102          model param norm: 86.2094        \n",
      "\n",
      "quiet_star_policy_loss= -0.022623658180236816\n",
      "nll_loss= 1.890650749206543\n",
      "avg_std= 0.3652701675891876\n",
      "dist std min max: 0.02623455971479416 0.3652701675891876 3.0329880714416504\n",
      "hidden_states min max: -12.453536033630371 13.124727249145508\n",
      "hidden_state minus mean squared max: 102.51168823242188\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.31136322021484 2.7070541381835938\n",
      "loss   863: 1.8680   grad norm: 1.3685          model param norm: 86.2121        \n",
      "\n",
      "quiet_star_policy_loss= -0.02009361982345581\n",
      "nll_loss= 1.8846025466918945\n",
      "avg_std= 0.3612666428089142\n",
      "dist std min max: 0.02642950415611267 0.3612666428089142 3.02052903175354\n",
      "hidden_states min max: -12.804304122924805 13.531147956848145\n",
      "hidden_state minus mean squared max: 109.3048324584961\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1735610961914 2.7098617553710938\n",
      "loss   864: 1.8645   grad norm: 1.3631          model param norm: 86.2147        \n",
      "\n",
      "quiet_star_policy_loss= 0.006937352009117603\n",
      "nll_loss= 1.8832905292510986\n",
      "avg_std= 0.3610275983810425\n",
      "dist std min max: 0.026733864098787308 0.3610275983810425 3.006531000137329\n",
      "hidden_states min max: -25.038698196411133 12.447310447692871\n",
      "hidden_state minus mean squared max: 547.0151977539062\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -104.684814453125 2.692089080810547\n",
      "loss   865: 1.8902   grad norm: 1.4401          model param norm: 86.2176        \n",
      "\n",
      "quiet_star_policy_loss= -0.03861963748931885\n",
      "nll_loss= 1.8977736234664917\n",
      "avg_std= 0.3605838119983673\n",
      "dist std min max: 0.02672620117664337 0.3605838119983673 2.99056077003479\n",
      "hidden_states min max: -12.319246292114258 13.414750099182129\n",
      "hidden_state minus mean squared max: 97.80013275146484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.58592987060547 2.687662124633789\n",
      "loss   866: 1.8592   grad norm: 1.4401          model param norm: 86.2206        \n",
      "\n",
      "quiet_star_policy_loss= -0.034800734370946884\n",
      "nll_loss= 1.8753414154052734\n",
      "avg_std= 0.36255955696105957\n",
      "dist std min max: 0.026643138378858566 0.36255955696105957 2.9359853267669678\n",
      "hidden_states min max: -10.688961029052734 11.863920211791992\n",
      "hidden_state minus mean squared max: 91.7548599243164\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.7921371459961 2.6966328620910645\n",
      "loss   867: 1.8405   grad norm: 3.0104          model param norm: 86.2233        \n",
      "\n",
      "quiet_star_policy_loss= 0.014878511428833008\n",
      "nll_loss= 1.8936653137207031\n",
      "avg_std= 0.3596186339855194\n",
      "dist std min max: 0.026344241574406624 0.3596186339855194 2.9358468055725098\n",
      "hidden_states min max: -13.476727485656738 13.69147777557373\n",
      "hidden_state minus mean squared max: 103.42799377441406\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -103.11492919921875 2.68380069732666\n",
      "loss   868: 1.9085   grad norm: 1.4001          model param norm: 86.2264        \n",
      "\n",
      "quiet_star_policy_loss= -0.016878342255949974\n",
      "nll_loss= 1.8906246423721313\n",
      "avg_std= 0.35981228947639465\n",
      "dist std min max: 0.02629738301038742 0.35981228947639465 3.266392230987549\n",
      "hidden_states min max: -11.38545036315918 12.477999687194824\n",
      "hidden_state minus mean squared max: 137.65476989746094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.97010803222656 2.703518867492676\n",
      "loss   869: 1.8737   grad norm: 1.4923          model param norm: 86.2296        \n",
      "\n",
      "quiet_star_policy_loss= -0.006246304605156183\n",
      "nll_loss= 1.8852436542510986\n",
      "avg_std= 0.35877346992492676\n",
      "dist std min max: 0.026076622307300568 0.35877346992492676 2.910719156265259\n",
      "hidden_states min max: -11.677416801452637 11.935990333557129\n",
      "hidden_state minus mean squared max: 97.69788360595703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.82351684570312 2.7092690467834473\n",
      "loss   870: 1.8790   grad norm: 1.2902          model param norm: 86.2328        \n",
      "\n",
      "quiet_star_policy_loss= -0.028899556025862694\n",
      "nll_loss= 1.9053890705108643\n",
      "avg_std= 0.35502180457115173\n",
      "dist std min max: 0.026078151538968086 0.35502180457115173 2.8976967334747314\n",
      "hidden_states min max: -12.585051536560059 12.353903770446777\n",
      "hidden_state minus mean squared max: 131.1595916748047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.64525604248047 2.71539306640625\n",
      "loss   871: 1.8765   grad norm: 1.5132          model param norm: 86.2365        \n",
      "\n",
      "quiet_star_policy_loss= -0.014170423150062561\n",
      "nll_loss= 1.8858076333999634\n",
      "avg_std= 0.3567974269390106\n",
      "dist std min max: 0.0257930438965559 0.3567974269390106 2.8813281059265137\n",
      "hidden_states min max: -11.240997314453125 12.785463333129883\n",
      "hidden_state minus mean squared max: 96.77777099609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.32453155517578 2.729733467102051\n",
      "loss   872: 1.8716   grad norm: 1.4060          model param norm: 86.2401        \n",
      "\n",
      "quiet_star_policy_loss= 0.02346985973417759\n",
      "nll_loss= 1.8827975988388062\n",
      "avg_std= 0.35808229446411133\n",
      "dist std min max: 0.025888994336128235 0.35808229446411133 2.8643131256103516\n",
      "hidden_states min max: -11.275224685668945 13.286688804626465\n",
      "hidden_state minus mean squared max: 131.7217254638672\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.86565399169922 2.7170324325561523\n",
      "loss   873: 1.9063   grad norm: 1.4721          model param norm: 86.2437        \n",
      "\n",
      "quiet_star_policy_loss= -0.01909637451171875\n",
      "nll_loss= 1.893882393836975\n",
      "avg_std= 0.3540816307067871\n",
      "dist std min max: 0.025899719446897507 0.3540816307067871 2.852245807647705\n",
      "hidden_states min max: -11.309236526489258 12.589194297790527\n",
      "hidden_state minus mean squared max: 116.22752380371094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.93634033203125 2.6929173469543457\n",
      "loss   874: 1.8748   grad norm: 1.4255          model param norm: 86.2471        \n",
      "\n",
      "quiet_star_policy_loss= -0.01688096486032009\n",
      "nll_loss= 1.8999757766723633\n",
      "avg_std= 0.35650870203971863\n",
      "dist std min max: 0.026122083887457848 0.35650870203971863 2.8334264755249023\n",
      "hidden_states min max: -12.157195091247559 11.666543960571289\n",
      "hidden_state minus mean squared max: 106.04054260253906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.209511756896973 2.7215523719787598\n",
      "loss   875: 1.8831   grad norm: 1.4443          model param norm: 86.2505        \n",
      "\n",
      "quiet_star_policy_loss= -0.03361297771334648\n",
      "nll_loss= 1.9085644483566284\n",
      "avg_std= 0.35482993721961975\n",
      "dist std min max: 0.025668742135167122 0.35482993721961975 3.197127342224121\n",
      "hidden_states min max: -11.52059555053711 12.650861740112305\n",
      "hidden_state minus mean squared max: 96.85135650634766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.00962829589844 2.7325620651245117\n",
      "loss   876: 1.8750   grad norm: 1.3921          model param norm: 86.2539        \n",
      "\n",
      "quiet_star_policy_loss= 0.003318357514217496\n",
      "nll_loss= 1.8872225284576416\n",
      "avg_std= 0.3548508882522583\n",
      "dist std min max: 0.025806736201047897 0.3548508882522583 2.8695456981658936\n",
      "hidden_states min max: -11.18596363067627 12.83462905883789\n",
      "hidden_state minus mean squared max: 103.0741958618164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.09095001220703 2.728435516357422\n",
      "loss   877: 1.8905   grad norm: 1.3872          model param norm: 86.2573        \n",
      "\n",
      "quiet_star_policy_loss= 0.017866158857941628\n",
      "nll_loss= 1.8943462371826172\n",
      "avg_std= 0.3528839647769928\n",
      "dist std min max: 0.025417540222406387 0.3528839647769928 2.8046207427978516\n",
      "hidden_states min max: -12.688980102539062 12.432132720947266\n",
      "hidden_state minus mean squared max: 106.91641235351562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.6319580078125 2.7105045318603516\n",
      "loss   878: 1.9122   grad norm: 1.3821          model param norm: 86.2606        \n",
      "\n",
      "quiet_star_policy_loss= -0.02582113817334175\n",
      "nll_loss= 1.9095267057418823\n",
      "avg_std= 0.3515452444553375\n",
      "dist std min max: 0.025594981387257576 0.3515452444553375 2.793579339981079\n",
      "hidden_states min max: -16.56438636779785 12.084086418151855\n",
      "hidden_state minus mean squared max: 266.5815734863281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.32540893554688 2.731386184692383\n",
      "loss   879: 1.8837   grad norm: 1.4090          model param norm: 86.2637        \n",
      "\n",
      "quiet_star_policy_loss= -0.027137357741594315\n",
      "nll_loss= 1.8887052536010742\n",
      "avg_std= 0.3511834442615509\n",
      "dist std min max: 0.02594633214175701 0.3511834442615509 3.0722856521606445\n",
      "hidden_states min max: -11.59998893737793 13.211780548095703\n",
      "hidden_state minus mean squared max: 101.87612915039062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0625 2.723876953125\n",
      "loss   880: 1.8616   grad norm: 1.4455          model param norm: 86.2666        \n",
      "\n",
      "quiet_star_policy_loss= -0.006779402494430542\n",
      "nll_loss= 1.878673791885376\n",
      "avg_std= 0.35491979122161865\n",
      "dist std min max: 0.025401048362255096 0.35491979122161865 2.9046664237976074\n",
      "hidden_states min max: -11.41113567352295 13.608464241027832\n",
      "hidden_state minus mean squared max: 117.73439025878906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.10763549804688 2.7393999099731445\n",
      "loss   881: 1.8719   grad norm: 1.4809          model param norm: 86.2694        \n",
      "\n",
      "quiet_star_policy_loss= -0.013977480120956898\n",
      "nll_loss= 1.8773698806762695\n",
      "avg_std= 0.3508218228816986\n",
      "dist std min max: 0.025824524462223053 0.3508218228816986 3.253046751022339\n",
      "hidden_states min max: -15.107956886291504 13.10588550567627\n",
      "hidden_state minus mean squared max: 207.60450744628906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.20040130615234 2.722505569458008\n",
      "loss   882: 1.8634   grad norm: 1.3527          model param norm: 86.2723        \n",
      "\n",
      "quiet_star_policy_loss= -0.022878075018525124\n",
      "nll_loss= 1.8919719457626343\n",
      "avg_std= 0.3500157594680786\n",
      "dist std min max: 0.025227941572666168 0.3500157594680786 2.9165701866149902\n",
      "hidden_states min max: -11.475992202758789 12.954492568969727\n",
      "hidden_state minus mean squared max: 110.04283905029297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.61038970947266 2.756420135498047\n",
      "loss   883: 1.8691   grad norm: 1.3259          model param norm: 86.2749        \n",
      "\n",
      "quiet_star_policy_loss= -0.033997997641563416\n",
      "nll_loss= 1.8931894302368164\n",
      "avg_std= 0.3509664237499237\n",
      "dist std min max: 0.025117110460996628 0.3509664237499237 2.932847738265991\n",
      "hidden_states min max: -11.88360595703125 11.497396469116211\n",
      "hidden_state minus mean squared max: 102.88809204101562\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.47879028320312 2.7416224479675293\n",
      "loss   884: 1.8592   grad norm: 1.3499          model param norm: 86.2773        \n",
      "\n",
      "quiet_star_policy_loss= -0.010283231735229492\n",
      "nll_loss= 1.890428900718689\n",
      "avg_std= 0.3492545485496521\n",
      "dist std min max: 0.024996690452098846 0.3492545485496521 2.9538936614990234\n",
      "hidden_states min max: -12.821964263916016 11.996491432189941\n",
      "hidden_state minus mean squared max: 83.86136627197266\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.14630889892578 2.7342796325683594\n",
      "loss   885: 1.8801   grad norm: 1.3527          model param norm: 86.2797        \n",
      "\n",
      "quiet_star_policy_loss= -0.001532912254333496\n",
      "nll_loss= 1.8919070959091187\n",
      "avg_std= 0.34840449690818787\n",
      "dist std min max: 0.024858383461833 0.34840449690818787 3.1260476112365723\n",
      "hidden_states min max: -11.762594223022461 11.516026496887207\n",
      "hidden_state minus mean squared max: 95.81013488769531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.44778442382812 2.7738494873046875\n",
      "loss   886: 1.8904   grad norm: 1.4089          model param norm: 86.2821        \n",
      "\n",
      "quiet_star_policy_loss= -0.04618554189801216\n",
      "nll_loss= 1.8778985738754272\n",
      "avg_std= 0.34844130277633667\n",
      "dist std min max: 0.024947019293904305 0.34844130277633667 2.9529967308044434\n",
      "hidden_states min max: -15.469449996948242 11.115769386291504\n",
      "hidden_state minus mean squared max: 182.35972595214844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.13558197021484 2.744597911834717\n",
      "loss   887: 1.8317   grad norm: 1.4760          model param norm: 86.2844        \n",
      "\n",
      "quiet_star_policy_loss= 0.0416053906083107\n",
      "nll_loss= 1.897781252861023\n",
      "avg_std= 0.34634435176849365\n",
      "dist std min max: 0.024568146094679832 0.34634435176849365 2.9726035594940186\n",
      "hidden_states min max: -11.397923469543457 12.74122142791748\n",
      "hidden_state minus mean squared max: 113.86494445800781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1495590209961 2.785280704498291\n",
      "loss   888: 1.9394   grad norm: 1.4583          model param norm: 86.2866        \n",
      "\n",
      "quiet_star_policy_loss= -0.024057520553469658\n",
      "nll_loss= 1.8864973783493042\n",
      "avg_std= 0.345441073179245\n",
      "dist std min max: 0.024554917588829994 0.345441073179245 3.0575246810913086\n",
      "hidden_states min max: -24.876760482788086 11.03408145904541\n",
      "hidden_state minus mean squared max: 532.7324829101562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.67160034179688 2.781132698059082\n",
      "loss   889: 1.8624   grad norm: 1.4668          model param norm: 86.2890        \n",
      "\n",
      "quiet_star_policy_loss= -0.020388467237353325\n",
      "nll_loss= 1.8820246458053589\n",
      "avg_std= 0.34554973244667053\n",
      "dist std min max: 0.025039512664079666 0.34554973244667053 2.9939253330230713\n",
      "hidden_states min max: -12.379674911499023 12.007438659667969\n",
      "hidden_state minus mean squared max: 100.86881256103516\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.53438568115234 2.7620229721069336\n",
      "loss   890: 1.8616   grad norm: 1.3337          model param norm: 86.2915        \n",
      "\n",
      "quiet_star_policy_loss= -0.008870959281921387\n",
      "nll_loss= 1.9024959802627563\n",
      "avg_std= 0.34470051527023315\n",
      "dist std min max: 0.024871032685041428 0.34470051527023315 3.150542974472046\n",
      "hidden_states min max: -14.369134902954102 12.181888580322266\n",
      "hidden_state minus mean squared max: 129.0171661376953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.96255493164062 2.769028663635254\n",
      "loss   891: 1.8936   grad norm: 1.4352          model param norm: 86.2941        \n",
      "\n",
      "quiet_star_policy_loss= -0.01919487677514553\n",
      "nll_loss= 1.8949863910675049\n",
      "avg_std= 0.34412577748298645\n",
      "dist std min max: 0.024711020290851593 0.34412577748298645 2.9817445278167725\n",
      "hidden_states min max: -11.438203811645508 11.733732223510742\n",
      "hidden_state minus mean squared max: 131.2720947265625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.25775909423828 2.7524805068969727\n",
      "loss   892: 1.8758   grad norm: 1.4188          model param norm: 86.2969        \n",
      "\n",
      "quiet_star_policy_loss= -0.037318456918001175\n",
      "nll_loss= 1.906274676322937\n",
      "avg_std= 0.3404943645000458\n",
      "dist std min max: 0.025085123255848885 0.3404943645000458 2.9716835021972656\n",
      "hidden_states min max: -12.852673530578613 11.52662467956543\n",
      "hidden_state minus mean squared max: 93.29586791992188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.53791809082031 2.742027759552002\n",
      "loss   893: 1.8690   grad norm: 1.4212          model param norm: 86.2998        \n",
      "\n",
      "quiet_star_policy_loss= -0.003768444061279297\n",
      "nll_loss= 1.8885525465011597\n",
      "avg_std= 0.34391653537750244\n",
      "dist std min max: 0.025402242317795753 0.34391653537750244 2.9741294384002686\n",
      "hidden_states min max: -11.456188201904297 11.401643753051758\n",
      "hidden_state minus mean squared max: 93.05091094970703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72830200195312 2.7538061141967773\n",
      "loss   894: 1.8848   grad norm: 1.5022          model param norm: 86.3030        \n",
      "\n",
      "quiet_star_policy_loss= -0.0321502685546875\n",
      "nll_loss= 1.8836801052093506\n",
      "avg_std= 0.34119322896003723\n",
      "dist std min max: 0.0265030674636364 0.34119322896003723 2.965562343597412\n",
      "hidden_states min max: -11.140510559082031 10.863666534423828\n",
      "hidden_state minus mean squared max: 77.21473693847656\n",
      "hidden_state minus mean divided by std max: 4.957175254821777\n",
      "log_prob min max: -102.72626495361328 2.7075295448303223\n",
      "loss   895: 1.8515   grad norm: 3.0023          model param norm: 86.3063        \n",
      "\n",
      "quiet_star_policy_loss= -0.00809873640537262\n",
      "nll_loss= 1.877907633781433\n",
      "avg_std= 0.34244683384895325\n",
      "dist std min max: 0.02583211660385132 0.34244683384895325 3.1425395011901855\n",
      "hidden_states min max: -22.50263214111328 11.955361366271973\n",
      "hidden_state minus mean squared max: 414.6501159667969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.54627990722656 2.718202590942383\n",
      "loss   896: 1.8698   grad norm: 1.4681          model param norm: 86.3102        \n",
      "\n",
      "quiet_star_policy_loss= -0.03224387392401695\n",
      "nll_loss= 1.904344916343689\n",
      "avg_std= 0.33820265531539917\n",
      "dist std min max: 0.0264987014234066 0.33820265531539917 2.9520578384399414\n",
      "hidden_states min max: -12.055207252502441 11.726350784301758\n",
      "hidden_state minus mean squared max: 89.95567321777344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.2883071899414 2.7116684913635254\n",
      "loss   897: 1.8721   grad norm: 1.5080          model param norm: 86.3137        \n",
      "\n",
      "quiet_star_policy_loss= -0.025751600041985512\n",
      "nll_loss= 1.8966515064239502\n",
      "avg_std= 0.3392680883407593\n",
      "dist std min max: 0.026362815871834755 0.3392680883407593 2.9769809246063232\n",
      "hidden_states min max: -11.632424354553223 11.23982048034668\n",
      "hidden_state minus mean squared max: 82.21158599853516\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.43130493164062 2.68416690826416\n",
      "loss   898: 1.8709   grad norm: 1.4542          model param norm: 86.3171        \n",
      "\n",
      "quiet_star_policy_loss= -0.05771473050117493\n",
      "nll_loss= 1.8958675861358643\n",
      "avg_std= 0.33810678124427795\n",
      "dist std min max: 0.025773709639906883 0.33810678124427795 3.022745370864868\n",
      "hidden_states min max: -16.71213150024414 11.86082935333252\n",
      "hidden_state minus mean squared max: 249.90838623046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.29312896728516 2.7368197441101074\n",
      "loss   899: 1.8382   grad norm: 1.3768          model param norm: 86.3210        \n",
      "eval loss 1.8943814039230347\n",
      "\n",
      "quiet_star_policy_loss= 0.01090421061962843\n",
      "nll_loss= 1.9136743545532227\n",
      "avg_std= 0.33774662017822266\n",
      "dist std min max: 0.026110908016562462 0.33774662017822266 3.1273112297058105\n",
      "hidden_states min max: -21.129596710205078 11.508272171020508\n",
      "hidden_state minus mean squared max: 577.492431640625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.71192932128906 2.7078423500061035\n",
      "loss   900: 1.9246   grad norm: 1.3157          model param norm: 86.3250        \n",
      "\n",
      "quiet_star_policy_loss= -0.015803098678588867\n",
      "nll_loss= 1.889700174331665\n",
      "avg_std= 0.33695536851882935\n",
      "dist std min max: 0.02665836364030838 0.33695536851882935 2.9552927017211914\n",
      "hidden_states min max: -22.292861938476562 12.827712059020996\n",
      "hidden_state minus mean squared max: 651.4459838867188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.77217102050781 2.6927990913391113\n",
      "loss   901: 1.8739   grad norm: 1.4025          model param norm: 86.3288        \n",
      "\n",
      "quiet_star_policy_loss= -0.007782801985740662\n",
      "nll_loss= 1.8947267532348633\n",
      "avg_std= 0.3372489809989929\n",
      "dist std min max: 0.02737809158861637 0.3372489809989929 2.9994237422943115\n",
      "hidden_states min max: -10.91408920288086 11.683046340942383\n",
      "hidden_state minus mean squared max: 90.39712524414062\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.21401977539062 2.6754283905029297\n",
      "loss   902: 1.8869   grad norm: 1.4235          model param norm: 86.3326        \n",
      "\n",
      "quiet_star_policy_loss= -0.008684515953063965\n",
      "nll_loss= 1.887586236000061\n",
      "avg_std= 0.3373691737651825\n",
      "dist std min max: 0.02725069224834442 0.3373691737651825 2.9574267864227295\n",
      "hidden_states min max: -15.286386489868164 10.493058204650879\n",
      "hidden_state minus mean squared max: 160.2959442138672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07110595703125 2.6753578186035156\n",
      "loss   903: 1.8789   grad norm: 1.3396          model param norm: 86.3363        \n",
      "\n",
      "quiet_star_policy_loss= 0.0014242709148675203\n",
      "nll_loss= 1.8916265964508057\n",
      "avg_std= 0.3370423913002014\n",
      "dist std min max: 0.0270578283816576 0.3370423913002014 2.9634575843811035\n",
      "hidden_states min max: -23.820741653442383 11.531512260437012\n",
      "hidden_state minus mean squared max: 621.5333862304688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.7486801147461 2.6846814155578613\n",
      "loss   904: 1.8931   grad norm: 1.4035          model param norm: 86.3399        \n",
      "\n",
      "quiet_star_policy_loss= -0.05301539972424507\n",
      "nll_loss= 1.8950378894805908\n",
      "avg_std= 0.33634933829307556\n",
      "dist std min max: 0.028226738795638084 0.33634933829307556 3.0849344730377197\n",
      "hidden_states min max: -14.80312442779541 11.259248733520508\n",
      "hidden_state minus mean squared max: 191.13429260253906\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.15907287597656 2.6481552124023438\n",
      "loss   905: 1.8420   grad norm: 1.4218          model param norm: 86.3436        \n",
      "\n",
      "quiet_star_policy_loss= -0.018627166748046875\n",
      "nll_loss= 1.8711636066436768\n",
      "avg_std= 0.33590465784072876\n",
      "dist std min max: 0.027548396959900856 0.33590465784072876 2.972106456756592\n",
      "hidden_states min max: -11.63692855834961 11.965902328491211\n",
      "hidden_state minus mean squared max: 130.36117553710938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.20529174804688 2.661090850830078\n",
      "loss   906: 1.8525   grad norm: 1.3046          model param norm: 86.3473        \n",
      "\n",
      "quiet_star_policy_loss= -0.004716551396995783\n",
      "nll_loss= 1.8883336782455444\n",
      "avg_std= 0.3339313864707947\n",
      "dist std min max: 0.028035270050168037 0.3339313864707947 2.96372389793396\n",
      "hidden_states min max: -12.868396759033203 10.95545768737793\n",
      "hidden_state minus mean squared max: 108.02039337158203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0801010131836 2.63720703125\n",
      "loss   907: 1.8836   grad norm: 1.3330          model param norm: 86.3511        \n",
      "\n",
      "quiet_star_policy_loss= 0.008191585540771484\n",
      "nll_loss= 1.8756945133209229\n",
      "avg_std= 0.33448636531829834\n",
      "dist std min max: 0.027376746758818626 0.33448636531829834 2.9467620849609375\n",
      "hidden_states min max: -11.480040550231934 11.085888862609863\n",
      "hidden_state minus mean squared max: 155.38914489746094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.0555419921875 2.6791224479675293\n",
      "loss   908: 1.8839   grad norm: 1.3863          model param norm: 86.3549        \n",
      "\n",
      "quiet_star_policy_loss= 0.007834047079086304\n",
      "nll_loss= 1.8880856037139893\n",
      "avg_std= 0.3344273269176483\n",
      "dist std min max: 0.02809624932706356 0.3344273269176483 3.0073490142822266\n",
      "hidden_states min max: -12.179912567138672 12.300012588500977\n",
      "hidden_state minus mean squared max: 101.70584869384766\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.76014709472656 2.63839054107666\n",
      "loss   909: 1.8959   grad norm: 1.4623          model param norm: 86.3586        \n",
      "\n",
      "quiet_star_policy_loss= -0.0189825426787138\n",
      "nll_loss= 1.8856124877929688\n",
      "avg_std= 0.33466872572898865\n",
      "dist std min max: 0.028262348845601082 0.33466872572898865 2.9654037952423096\n",
      "hidden_states min max: -11.276726722717285 11.364575386047363\n",
      "hidden_state minus mean squared max: 87.74263000488281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.18238830566406 2.62860107421875\n",
      "loss   910: 1.8666   grad norm: 1.3886          model param norm: 86.3623        \n",
      "\n",
      "quiet_star_policy_loss= -0.0028645514976233244\n",
      "nll_loss= 1.8691929578781128\n",
      "avg_std= 0.3341017961502075\n",
      "dist std min max: 0.02739839255809784 0.3341017961502075 3.0620274543762207\n",
      "hidden_states min max: -11.01620101928711 11.969869613647461\n",
      "hidden_state minus mean squared max: 96.37139892578125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.67569732666016 2.6498923301696777\n",
      "loss   911: 1.8663   grad norm: 1.4211          model param norm: 86.3658        \n",
      "\n",
      "quiet_star_policy_loss= -0.011493620462715626\n",
      "nll_loss= 1.9058655500411987\n",
      "avg_std= 0.33323436975479126\n",
      "dist std min max: 0.02871672622859478 0.33323436975479126 2.9933741092681885\n",
      "hidden_states min max: -12.175930976867676 11.919130325317383\n",
      "hidden_state minus mean squared max: 161.04403686523438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07341766357422 2.614410400390625\n",
      "loss   912: 1.8944   grad norm: 1.3129          model param norm: 86.3695        \n",
      "\n",
      "quiet_star_policy_loss= -0.011897012591362\n",
      "nll_loss= 1.899367332458496\n",
      "avg_std= 0.33510833978652954\n",
      "dist std min max: 0.02918711118400097 0.33510833978652954 3.114863872528076\n",
      "hidden_states min max: -14.022270202636719 11.876723289489746\n",
      "hidden_state minus mean squared max: 137.24789428710938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.99346923828125 2.5773086547851562\n",
      "loss   913: 1.8875   grad norm: 1.3638          model param norm: 86.3732        \n",
      "\n",
      "quiet_star_policy_loss= -0.023479808121919632\n",
      "nll_loss= 1.910559892654419\n",
      "avg_std= 0.3313996195793152\n",
      "dist std min max: 0.02937215194106102 0.3313996195793152 3.0075342655181885\n",
      "hidden_states min max: -12.295641899108887 11.079139709472656\n",
      "hidden_state minus mean squared max: 90.56837463378906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5936050415039 2.590092182159424\n",
      "loss   914: 1.8871   grad norm: 1.4418          model param norm: 86.3768        \n",
      "\n",
      "quiet_star_policy_loss= 0.0029746205545961857\n",
      "nll_loss= 1.8716459274291992\n",
      "avg_std= 0.33688947558403015\n",
      "dist std min max: 0.029156723991036415 0.33688947558403015 3.0610511302948\n",
      "hidden_states min max: -11.82085132598877 11.844992637634277\n",
      "hidden_state minus mean squared max: 142.38710021972656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0118408203125 2.611250400543213\n",
      "loss   915: 1.8746   grad norm: 1.4356          model param norm: 86.3807        \n",
      "\n",
      "quiet_star_policy_loss= -0.012774121947586536\n",
      "nll_loss= 1.8796342611312866\n",
      "avg_std= 0.3348352611064911\n",
      "dist std min max: 0.030134374275803566 0.3348352611064911 3.018333911895752\n",
      "hidden_states min max: -12.094898223876953 12.170830726623535\n",
      "hidden_state minus mean squared max: 110.43758392333984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.88481903076172 2.553889274597168\n",
      "loss   916: 1.8669   grad norm: 1.3276          model param norm: 86.3845        \n",
      "\n",
      "quiet_star_policy_loss= -0.04230917617678642\n",
      "nll_loss= 1.881734848022461\n",
      "avg_std= 0.3353019654750824\n",
      "dist std min max: 0.029874470084905624 0.3353019654750824 2.9933741092681885\n",
      "hidden_states min max: -17.36661148071289 11.432188987731934\n",
      "hidden_state minus mean squared max: 220.08563232421875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.22958374023438 2.5717358589172363\n",
      "loss   917: 1.8394   grad norm: 1.3562          model param norm: 86.3881        \n",
      "\n",
      "quiet_star_policy_loss= 0.016800547018647194\n",
      "nll_loss= 1.895232081413269\n",
      "avg_std= 0.33514976501464844\n",
      "dist std min max: 0.029877491295337677 0.33514976501464844 3.096769332885742\n",
      "hidden_states min max: -15.948331832885742 12.147263526916504\n",
      "hidden_state minus mean squared max: 161.51278686523438\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.07486724853516 2.5802149772644043\n",
      "loss   918: 1.9120   grad norm: 1.4245          model param norm: 86.3916        \n",
      "\n",
      "quiet_star_policy_loss= -0.023918358609080315\n",
      "nll_loss= 1.889041543006897\n",
      "avg_std= 0.3365101218223572\n",
      "dist std min max: 0.031106572598218918 0.3365101218223572 3.0265214443206787\n",
      "hidden_states min max: -11.988409042358398 11.000242233276367\n",
      "hidden_state minus mean squared max: 97.21092987060547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.821044921875 2.5289182662963867\n",
      "loss   919: 1.8651   grad norm: 1.3412          model param norm: 86.3946        \n",
      "\n",
      "quiet_star_policy_loss= -0.03530867025256157\n",
      "nll_loss= 1.8794702291488647\n",
      "avg_std= 0.3366868495941162\n",
      "dist std min max: 0.030396372079849243 0.3366868495941162 3.076932668685913\n",
      "hidden_states min max: -11.932580947875977 10.927629470825195\n",
      "hidden_state minus mean squared max: 94.63058471679688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.43682098388672 2.5589241981506348\n",
      "loss   920: 1.8442   grad norm: 1.3905          model param norm: 86.3973        \n",
      "\n",
      "quiet_star_policy_loss= -0.03503212332725525\n",
      "nll_loss= 1.8935412168502808\n",
      "avg_std= 0.33458200097084045\n",
      "dist std min max: 0.030705587938427925 0.33458200097084045 3.062976837158203\n",
      "hidden_states min max: -12.053454399108887 11.622313499450684\n",
      "hidden_state minus mean squared max: 81.54549407958984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.89495849609375 2.5207700729370117\n",
      "loss   921: 1.8585   grad norm: 1.4211          model param norm: 86.4003        \n",
      "\n",
      "quiet_star_policy_loss= -0.028032077476382256\n",
      "nll_loss= 1.8683483600616455\n",
      "avg_std= 0.33646875619888306\n",
      "dist std min max: 0.03183995559811592 0.33646875619888306 3.0030107498168945\n",
      "hidden_states min max: -19.550790786743164 13.156188011169434\n",
      "hidden_state minus mean squared max: 425.676513671875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.55940246582031 2.507718086242676\n",
      "loss   922: 1.8403   grad norm: 1.3562          model param norm: 86.4031        \n",
      "\n",
      "quiet_star_policy_loss= -0.05480748787522316\n",
      "nll_loss= 1.8936337232589722\n",
      "avg_std= 0.33444273471832275\n",
      "dist std min max: 0.03162785619497299 0.33444273471832275 2.958696126937866\n",
      "hidden_states min max: -10.406760215759277 11.140609741210938\n",
      "hidden_state minus mean squared max: 107.87604522705078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24542999267578 2.5070581436157227\n",
      "loss   923: 1.8388   grad norm: 3.0207          model param norm: 86.4060        \n",
      "\n",
      "quiet_star_policy_loss= 0.004012858960777521\n",
      "nll_loss= 1.8814318180084229\n",
      "avg_std= 0.3356018364429474\n",
      "dist std min max: 0.029725801199674606 0.3356018364429474 3.0175986289978027\n",
      "hidden_states min max: -11.420160293579102 11.559286117553711\n",
      "hidden_state minus mean squared max: 80.57574462890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.48545837402344 2.596653461456299\n",
      "loss   924: 1.8854   grad norm: 1.5356          model param norm: 86.4084        \n",
      "\n",
      "quiet_star_policy_loss= 0.014914864674210548\n",
      "nll_loss= 1.892861008644104\n",
      "avg_std= 0.33485519886016846\n",
      "dist std min max: 0.03156554698944092 0.33485519886016846 2.916473627090454\n",
      "hidden_states min max: -20.506147384643555 11.078526496887207\n",
      "hidden_state minus mean squared max: 206.4824676513672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19770812988281 2.517303943634033\n",
      "loss   925: 1.9078   grad norm: 1.4782          model param norm: 86.4111        \n",
      "\n",
      "quiet_star_policy_loss= -0.0016056090826168656\n",
      "nll_loss= 1.892663598060608\n",
      "avg_std= 0.33449891209602356\n",
      "dist std min max: 0.030966414138674736 0.33449891209602356 2.8990275859832764\n",
      "hidden_states min max: -11.616837501525879 11.517353057861328\n",
      "hidden_state minus mean squared max: 161.31753540039062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.07427215576172 2.5487594604492188\n",
      "loss   926: 1.8911   grad norm: 1.4584          model param norm: 86.4135        \n",
      "\n",
      "quiet_star_policy_loss= -0.010181236080825329\n",
      "nll_loss= 1.8806476593017578\n",
      "avg_std= 0.33684828877449036\n",
      "dist std min max: 0.0314132459461689 0.33684828877449036 2.9207873344421387\n",
      "hidden_states min max: -11.374218940734863 10.469062805175781\n",
      "hidden_state minus mean squared max: 90.64155578613281\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.64764404296875 2.5208702087402344\n",
      "loss   927: 1.8705   grad norm: 1.4823          model param norm: 86.4160        \n",
      "\n",
      "quiet_star_policy_loss= 0.0174536295235157\n",
      "nll_loss= 1.8665717840194702\n",
      "avg_std= 0.3374791145324707\n",
      "dist std min max: 0.030481478199362755 0.3374791145324707 2.943375587463379\n",
      "hidden_states min max: -13.812466621398926 10.888981819152832\n",
      "hidden_state minus mean squared max: 130.4951171875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.29723358154297 2.5701088905334473\n",
      "loss   928: 1.8840   grad norm: 1.3899          model param norm: 86.4188        \n",
      "\n",
      "quiet_star_policy_loss= -0.011273539625108242\n",
      "nll_loss= 1.889461874961853\n",
      "avg_std= 0.33587419986724854\n",
      "dist std min max: 0.030891884118318558 0.33587419986724854 2.912724733352661\n",
      "hidden_states min max: -11.4866304397583 10.903247833251953\n",
      "hidden_state minus mean squared max: 92.41152954101562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.25279998779297 2.5535945892333984\n",
      "loss   929: 1.8782   grad norm: 1.3922          model param norm: 86.4218        \n",
      "\n",
      "quiet_star_policy_loss= -0.02780756913125515\n",
      "nll_loss= 1.9010928869247437\n",
      "avg_std= 0.3350794315338135\n",
      "dist std min max: 0.030422037467360497 0.3350794315338135 2.8481528759002686\n",
      "hidden_states min max: -11.357009887695312 11.011533737182617\n",
      "hidden_state minus mean squared max: 77.57504272460938\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.6611557006836 2.552668571472168\n",
      "loss   930: 1.8733   grad norm: 1.4284          model param norm: 86.4246        \n",
      "\n",
      "quiet_star_policy_loss= -0.04942521080374718\n",
      "nll_loss= 1.8823658227920532\n",
      "avg_std= 0.33657458424568176\n",
      "dist std min max: 0.030110180377960205 0.33657458424568176 2.8580374717712402\n",
      "hidden_states min max: -10.76123332977295 10.173733711242676\n",
      "hidden_state minus mean squared max: 84.02648162841797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.47065734863281 2.568742275238037\n",
      "loss   931: 1.8329   grad norm: 1.3997          model param norm: 86.4274        \n",
      "\n",
      "quiet_star_policy_loss= -0.000631052243988961\n",
      "nll_loss= 1.8863146305084229\n",
      "avg_std= 0.3356002867221832\n",
      "dist std min max: 0.030354663729667664 0.3356002867221832 2.9438726902008057\n",
      "hidden_states min max: -18.01409339904785 10.99635124206543\n",
      "hidden_state minus mean squared max: 253.01902770996094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29930877685547 2.5375070571899414\n",
      "loss   932: 1.8857   grad norm: 1.4068          model param norm: 86.4303        \n",
      "\n",
      "quiet_star_policy_loss= -0.03393196687102318\n",
      "nll_loss= 1.8981351852416992\n",
      "avg_std= 0.3345433473587036\n",
      "dist std min max: 0.03052440471947193 0.3345433473587036 2.943404197692871\n",
      "hidden_states min max: -11.661788940429688 10.123247146606445\n",
      "hidden_state minus mean squared max: 78.52717590332031\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.31621551513672 2.558138370513916\n",
      "loss   933: 1.8642   grad norm: 1.5144          model param norm: 86.4329        \n",
      "\n",
      "quiet_star_policy_loss= -0.01883871667087078\n",
      "nll_loss= 1.8790680170059204\n",
      "avg_std= 0.3350304961204529\n",
      "dist std min max: 0.030629806220531464 0.3350304961204529 2.890223741531372\n",
      "hidden_states min max: -17.765583038330078 12.717586517333984\n",
      "hidden_state minus mean squared max: 239.12071228027344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.27107238769531 2.5537567138671875\n",
      "loss   934: 1.8602   grad norm: 1.4357          model param norm: 86.4353        \n",
      "\n",
      "quiet_star_policy_loss= 0.005822604987770319\n",
      "nll_loss= 1.8681942224502563\n",
      "avg_std= 0.3350760340690613\n",
      "dist std min max: 0.030016569420695305 0.3350760340690613 2.893646001815796\n",
      "hidden_states min max: -12.627989768981934 10.843362808227539\n",
      "hidden_state minus mean squared max: 87.50331115722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.76840209960938 2.558863639831543\n",
      "loss   935: 1.8740   grad norm: 1.3492          model param norm: 86.4376        \n",
      "\n",
      "quiet_star_policy_loss= -0.029014581814408302\n",
      "nll_loss= 1.8984222412109375\n",
      "avg_std= 0.33482176065444946\n",
      "dist std min max: 0.030241260305047035 0.33482176065444946 2.8536627292633057\n",
      "hidden_states min max: -10.957014083862305 10.175872802734375\n",
      "hidden_state minus mean squared max: 88.4358139038086\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29446411132812 2.5523929595947266\n",
      "loss   936: 1.8694   grad norm: 1.4454          model param norm: 86.4404        \n",
      "\n",
      "quiet_star_policy_loss= -0.00038852691068314016\n",
      "nll_loss= 1.8836593627929688\n",
      "avg_std= 0.3357638716697693\n",
      "dist std min max: 0.030170952901244164 0.3357638716697693 2.8870179653167725\n",
      "hidden_states min max: -10.825032234191895 11.971259117126465\n",
      "hidden_state minus mean squared max: 121.56558990478516\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.66865539550781 2.5560622215270996\n",
      "loss   937: 1.8833   grad norm: 1.4333          model param norm: 86.4434        \n",
      "\n",
      "quiet_star_policy_loss= -0.008361530490219593\n",
      "nll_loss= 1.8831405639648438\n",
      "avg_std= 0.33423912525177\n",
      "dist std min max: 0.030386727303266525 0.33423912525177 2.9135842323303223\n",
      "hidden_states min max: -10.790725708007812 11.244041442871094\n",
      "hidden_state minus mean squared max: 83.80237579345703\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.74417877197266 2.571826934814453\n",
      "loss   938: 1.8748   grad norm: 1.5016          model param norm: 86.4463        \n",
      "\n",
      "quiet_star_policy_loss= 0.010346484370529652\n",
      "nll_loss= 1.8999910354614258\n",
      "avg_std= 0.3321754038333893\n",
      "dist std min max: 0.02951204404234886 0.3321754038333893 2.8742353916168213\n",
      "hidden_states min max: -22.7272891998291 10.674156188964844\n",
      "hidden_state minus mean squared max: 568.8268432617188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.7043685913086 2.592796802520752\n",
      "loss   939: 1.9103   grad norm: 1.3034          model param norm: 86.4492        \n",
      "\n",
      "quiet_star_policy_loss= 0.007962805218994617\n",
      "nll_loss= 1.8928571939468384\n",
      "avg_std= 0.3321426808834076\n",
      "dist std min max: 0.029332829639315605 0.3321426808834076 2.89615535736084\n",
      "hidden_states min max: -11.219355583190918 10.760772705078125\n",
      "hidden_state minus mean squared max: 80.1639633178711\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.56024169921875 2.580578327178955\n",
      "loss   940: 1.9008   grad norm: 1.4468          model param norm: 86.4523        \n",
      "\n",
      "quiet_star_policy_loss= 0.02354658767580986\n",
      "nll_loss= 1.8959496021270752\n",
      "avg_std= 0.3303925693035126\n",
      "dist std min max: 0.029324067756533623 0.3303925693035126 2.8629889488220215\n",
      "hidden_states min max: -15.683828353881836 11.65954875946045\n",
      "hidden_state minus mean squared max: 160.02056884765625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.07023620605469 2.5844359397888184\n",
      "loss   941: 1.9195   grad norm: 1.4523          model param norm: 86.4555        \n",
      "\n",
      "quiet_star_policy_loss= -0.0007053613662719727\n",
      "nll_loss= 1.891903281211853\n",
      "avg_std= 0.33199945092201233\n",
      "dist std min max: 0.029257522895932198 0.33199945092201233 2.842662811279297\n",
      "hidden_states min max: -10.67530632019043 10.556997299194336\n",
      "hidden_state minus mean squared max: 74.99421691894531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.00628662109375 2.6025795936584473\n",
      "loss   942: 1.8912   grad norm: 1.3816          model param norm: 86.4585        \n",
      "\n",
      "quiet_star_policy_loss= -0.01799309253692627\n",
      "nll_loss= 1.8761032819747925\n",
      "avg_std= 0.33269399404525757\n",
      "dist std min max: 0.02952563390135765 0.33269399404525757 2.8388783931732178\n",
      "hidden_states min max: -10.852405548095703 9.89948844909668\n",
      "hidden_state minus mean squared max: 73.07716369628906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.64961242675781 2.5859522819519043\n",
      "loss   943: 1.8581   grad norm: 1.3237          model param norm: 86.4616        \n",
      "\n",
      "quiet_star_policy_loss= -0.006900429725646973\n",
      "nll_loss= 1.8640998601913452\n",
      "avg_std= 0.3327493369579315\n",
      "dist std min max: 0.02916010282933712 0.3327493369579315 2.834643840789795\n",
      "hidden_states min max: -11.428552627563477 10.746201515197754\n",
      "hidden_state minus mean squared max: 90.0654525756836\n",
      "hidden_state minus mean divided by std max: 5.035408020019531\n",
      "log_prob min max: -103.11127471923828 2.591738700866699\n",
      "loss   944: 1.8572   grad norm: 1.4771          model param norm: 86.4645        \n",
      "\n",
      "quiet_star_policy_loss= -0.006153989117592573\n",
      "nll_loss= 1.885748267173767\n",
      "avg_std= 0.33352240920066833\n",
      "dist std min max: 0.029139623045921326 0.33352240920066833 2.865727186203003\n",
      "hidden_states min max: -11.667447090148926 10.748262405395508\n",
      "hidden_state minus mean squared max: 81.21781158447266\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.01314544677734 2.592048168182373\n",
      "loss   945: 1.8796   grad norm: 1.4145          model param norm: 86.4673        \n",
      "\n",
      "quiet_star_policy_loss= -0.003908950369805098\n",
      "nll_loss= 1.8655956983566284\n",
      "avg_std= 0.3358152210712433\n",
      "dist std min max: 0.029205314815044403 0.3358152210712433 2.8839633464813232\n",
      "hidden_states min max: -12.477157592773438 10.722142219543457\n",
      "hidden_state minus mean squared max: 147.7505340576172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.03033447265625 2.604022979736328\n",
      "loss   946: 1.8617   grad norm: 1.4618          model param norm: 86.4701        \n",
      "\n",
      "quiet_star_policy_loss= -0.0009652316803112626\n",
      "nll_loss= 1.8762050867080688\n",
      "avg_std= 0.33536288142204285\n",
      "dist std min max: 0.02897895686328411 0.33536288142204285 2.9233412742614746\n",
      "hidden_states min max: -11.117730140686035 10.97472095489502\n",
      "hidden_state minus mean squared max: 82.3958740234375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.68428802490234 2.6026902198791504\n",
      "loss   947: 1.8752   grad norm: 1.3452          model param norm: 86.4730        \n",
      "\n",
      "quiet_star_policy_loss= -0.012104558758437634\n",
      "nll_loss= 1.8586931228637695\n",
      "avg_std= 0.33372753858566284\n",
      "dist std min max: 0.029241615906357765 0.33372753858566284 2.838958978652954\n",
      "hidden_states min max: -23.57796859741211 10.01512336730957\n",
      "hidden_state minus mean squared max: 469.9434509277344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.60889434814453 2.5946903228759766\n",
      "loss   948: 1.8466   grad norm: 1.4668          model param norm: 86.4759        \n",
      "\n",
      "quiet_star_policy_loss= -0.011872529983520508\n",
      "nll_loss= 1.897558569908142\n",
      "avg_std= 0.33541950583457947\n",
      "dist std min max: 0.02890479937195778 0.33541950583457947 2.884854555130005\n",
      "hidden_states min max: -11.78242015838623 10.457733154296875\n",
      "hidden_state minus mean squared max: 91.08385467529297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.68621063232422 2.6062183380126953\n",
      "loss   949: 1.8857   grad norm: 1.4353          model param norm: 86.4788        \n",
      "\n",
      "quiet_star_policy_loss= 0.0077498555183410645\n",
      "nll_loss= 1.8700488805770874\n",
      "avg_std= 0.33580511808395386\n",
      "dist std min max: 0.028957519680261612 0.33580511808395386 3.0138046741485596\n",
      "hidden_states min max: -11.722665786743164 10.895730972290039\n",
      "hidden_state minus mean squared max: 80.67916107177734\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.30919647216797 2.594313144683838\n",
      "loss   950: 1.8778   grad norm: 1.3515          model param norm: 86.4815        \n",
      "\n",
      "quiet_star_policy_loss= -0.0444006621837616\n",
      "nll_loss= 1.8786942958831787\n",
      "avg_std= 0.3394339978694916\n",
      "dist std min max: 0.029365941882133484 0.3394339978694916 2.9218392372131348\n",
      "hidden_states min max: -11.635167121887207 10.060243606567383\n",
      "hidden_state minus mean squared max: 132.5091552734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97591400146484 2.59678316116333\n",
      "loss   951: 1.8343   grad norm: 3.1302          model param norm: 86.4842        \n",
      "\n",
      "quiet_star_policy_loss= -0.01298800390213728\n",
      "nll_loss= 1.8800820112228394\n",
      "avg_std= 0.3343754708766937\n",
      "dist std min max: 0.029927924275398254 0.3343754708766937 2.8936820030212402\n",
      "hidden_states min max: -11.741640090942383 11.018571853637695\n",
      "hidden_state minus mean squared max: 113.58477783203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.89884948730469 2.587460517883301\n",
      "loss   952: 1.8671   grad norm: 1.5025          model param norm: 86.4876        \n",
      "\n",
      "quiet_star_policy_loss= -0.02557540498673916\n",
      "nll_loss= 1.862221121788025\n",
      "avg_std= 0.3379387855529785\n",
      "dist std min max: 0.029115281999111176 0.3379387855529785 2.9074840545654297\n",
      "hidden_states min max: -13.01530933380127 11.150810241699219\n",
      "hidden_state minus mean squared max: 116.44577026367188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.91129302978516 2.6055006980895996\n",
      "loss   953: 1.8366   grad norm: 1.5263          model param norm: 86.4906        \n",
      "\n",
      "quiet_star_policy_loss= -0.030907154083251953\n",
      "nll_loss= 1.8803281784057617\n",
      "avg_std= 0.33507323265075684\n",
      "dist std min max: 0.029049869626760483 0.33507323265075684 2.798985004425049\n",
      "hidden_states min max: -11.447041511535645 10.804850578308105\n",
      "hidden_state minus mean squared max: 72.26258850097656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.11988067626953 2.605067729949951\n",
      "loss   954: 1.8494   grad norm: 1.4607          model param norm: 86.4934        \n",
      "\n",
      "quiet_star_policy_loss= -0.048190873116254807\n",
      "nll_loss= 1.8908863067626953\n",
      "avg_std= 0.33416062593460083\n",
      "dist std min max: 0.029210703447461128 0.33416062593460083 2.864854335784912\n",
      "hidden_states min max: -11.425443649291992 11.476835250854492\n",
      "hidden_state minus mean squared max: 90.08927917480469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.6688232421875 2.6058268547058105\n",
      "loss   955: 1.8427   grad norm: 1.4055          model param norm: 86.4962        \n",
      "\n",
      "quiet_star_policy_loss= -0.0307051669806242\n",
      "nll_loss= 1.870693564414978\n",
      "avg_std= 0.33437323570251465\n",
      "dist std min max: 0.029136065393686295 0.33437323570251465 2.8197021484375\n",
      "hidden_states min max: -11.333921432495117 11.120634078979492\n",
      "hidden_state minus mean squared max: 125.8792724609375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.95024108886719 2.6013565063476562\n",
      "loss   956: 1.8400   grad norm: 1.4779          model param norm: 86.4988        \n",
      "\n",
      "quiet_star_policy_loss= -0.00804829876869917\n",
      "nll_loss= 1.8667789697647095\n",
      "avg_std= 0.3352015018463135\n",
      "dist std min max: 0.028839321807026863 0.3352015018463135 2.7862138748168945\n",
      "hidden_states min max: -13.197576522827148 10.603525161743164\n",
      "hidden_state minus mean squared max: 107.08700561523438\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.8694076538086 2.620929718017578\n",
      "loss   957: 1.8587   grad norm: 1.4619          model param norm: 86.5014        \n",
      "\n",
      "quiet_star_policy_loss= 0.001100993133150041\n",
      "nll_loss= 1.8682540655136108\n",
      "avg_std= 0.3353704512119293\n",
      "dist std min max: 0.029025934636592865 0.3353704512119293 2.7783043384552\n",
      "hidden_states min max: -11.222956657409668 10.07458782196045\n",
      "hidden_state minus mean squared max: 78.8280029296875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.09613800048828 2.6165575981140137\n",
      "loss   958: 1.8694   grad norm: 1.4933          model param norm: 86.5044        \n",
      "\n",
      "quiet_star_policy_loss= 0.0002656459982972592\n",
      "nll_loss= 1.8724077939987183\n",
      "avg_std= 0.33642005920410156\n",
      "dist std min max: 0.029048213735222816 0.33642005920410156 2.7872304916381836\n",
      "hidden_states min max: -11.455678939819336 10.477752685546875\n",
      "hidden_state minus mean squared max: 77.04508209228516\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.68929290771484 2.5955605506896973\n",
      "loss   959: 1.8727   grad norm: 1.4959          model param norm: 86.5077        \n",
      "\n",
      "quiet_star_policy_loss= -0.027472078800201416\n",
      "nll_loss= 1.886988639831543\n",
      "avg_std= 0.33419254422187805\n",
      "dist std min max: 0.029611533507704735 0.33419254422187805 3.236713171005249\n",
      "hidden_states min max: -12.178558349609375 10.216791152954102\n",
      "hidden_state minus mean squared max: 87.04840087890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.3432846069336 2.5898590087890625\n",
      "loss   960: 1.8595   grad norm: 1.2866          model param norm: 86.5108        \n",
      "\n",
      "quiet_star_policy_loss= -0.03625204786658287\n",
      "nll_loss= 1.8987442255020142\n",
      "avg_std= 0.3332784175872803\n",
      "dist std min max: 0.02929983101785183 0.3332784175872803 2.7683725357055664\n",
      "hidden_states min max: -13.412946701049805 10.72256088256836\n",
      "hidden_state minus mean squared max: 171.24630737304688\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.1041259765625 2.6057868003845215\n",
      "loss   961: 1.8625   grad norm: 1.3823          model param norm: 86.5138        \n",
      "\n",
      "quiet_star_policy_loss= -0.01948823407292366\n",
      "nll_loss= 1.8948805332183838\n",
      "avg_std= 0.33339923620224\n",
      "dist std min max: 0.02859829179942608 0.33339923620224 2.809814691543579\n",
      "hidden_states min max: -11.442158699035645 10.304278373718262\n",
      "hidden_state minus mean squared max: 81.05180358886719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29611206054688 2.6176438331604004\n",
      "loss   962: 1.8754   grad norm: 1.3798          model param norm: 86.5169        \n",
      "\n",
      "quiet_star_policy_loss= -0.02747436799108982\n",
      "nll_loss= 1.8762203454971313\n",
      "avg_std= 0.33569711446762085\n",
      "dist std min max: 0.02924421615898609 0.33569711446762085 2.8774209022521973\n",
      "hidden_states min max: -12.610907554626465 9.8646879196167\n",
      "hidden_state minus mean squared max: 99.38182067871094\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.83206176757812 2.5995569229125977\n",
      "loss   963: 1.8487   grad norm: 1.4035          model param norm: 86.5200        \n",
      "\n",
      "quiet_star_policy_loss= -0.02508164383471012\n",
      "nll_loss= 1.8961496353149414\n",
      "avg_std= 0.33352500200271606\n",
      "dist std min max: 0.02896925061941147 0.33352500200271606 2.699977397918701\n",
      "hidden_states min max: -17.608488082885742 10.479604721069336\n",
      "hidden_state minus mean squared max: 246.70578002929688\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.28668212890625 2.62147855758667\n",
      "loss   964: 1.8711   grad norm: 1.4285          model param norm: 86.5232        \n",
      "\n",
      "quiet_star_policy_loss= -0.024401307106018066\n",
      "nll_loss= 1.8757232427597046\n",
      "avg_std= 0.3332980275154114\n",
      "dist std min max: 0.0288497693836689 0.3332980275154114 2.6607918739318848\n",
      "hidden_states min max: -12.268011093139648 10.216266632080078\n",
      "hidden_state minus mean squared max: 75.16554260253906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1953353881836 2.617340087890625\n",
      "loss   965: 1.8513   grad norm: 1.5038          model param norm: 86.5264        \n",
      "\n",
      "quiet_star_policy_loss= -0.012236452661454678\n",
      "nll_loss= 1.8731063604354858\n",
      "avg_std= 0.3332683742046356\n",
      "dist std min max: 0.028298402205109596 0.3332683742046356 2.615123987197876\n",
      "hidden_states min max: -12.074899673461914 12.4284029006958\n",
      "hidden_state minus mean squared max: 117.11067962646484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.317500114440918 2.62685489654541\n",
      "loss   966: 1.8609   grad norm: 1.4536          model param norm: 86.5297        \n",
      "\n",
      "quiet_star_policy_loss= -0.03419208526611328\n",
      "nll_loss= 1.8671283721923828\n",
      "avg_std= 0.3343884348869324\n",
      "dist std min max: 0.028245408087968826 0.3343884348869324 2.789976119995117\n",
      "hidden_states min max: -11.323518753051758 11.282706260681152\n",
      "hidden_state minus mean squared max: 91.37613677978516\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.79006958007812 2.6154861450195312\n",
      "loss   967: 1.8329   grad norm: 1.4519          model param norm: 86.5329        \n",
      "\n",
      "quiet_star_policy_loss= -0.01034913957118988\n",
      "nll_loss= 1.8794071674346924\n",
      "avg_std= 0.33148255944252014\n",
      "dist std min max: 0.028317386284470558 0.33148255944252014 2.632026433944702\n",
      "hidden_states min max: -11.00898551940918 10.779424667358398\n",
      "hidden_state minus mean squared max: 87.44808197021484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.64464569091797 2.6221084594726562\n",
      "loss   968: 1.8691   grad norm: 1.4801          model param norm: 86.5362        \n",
      "\n",
      "quiet_star_policy_loss= 0.0012817264068871737\n",
      "nll_loss= 1.8781455755233765\n",
      "avg_std= 0.33262667059898376\n",
      "dist std min max: 0.028331946581602097 0.33262667059898376 2.6054654121398926\n",
      "hidden_states min max: -11.121688842773438 11.08411979675293\n",
      "hidden_state minus mean squared max: 78.47123718261719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46217346191406 2.6344704627990723\n",
      "loss   969: 1.8794   grad norm: 1.3785          model param norm: 86.5396        \n",
      "\n",
      "quiet_star_policy_loss= -0.017775660380721092\n",
      "nll_loss= 1.8656119108200073\n",
      "avg_std= 0.3329800069332123\n",
      "dist std min max: 0.028561489656567574 0.3329800069332123 2.6679491996765137\n",
      "hidden_states min max: -11.004448890686035 9.704512596130371\n",
      "hidden_state minus mean squared max: 99.34278869628906\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.54339599609375 2.635477066040039\n",
      "loss   970: 1.8478   grad norm: 1.5009          model param norm: 86.5430        \n",
      "\n",
      "quiet_star_policy_loss= -0.010060161352157593\n",
      "nll_loss= 1.905028223991394\n",
      "avg_std= 0.3316701352596283\n",
      "dist std min max: 0.028457803651690483 0.3316701352596283 2.8390519618988037\n",
      "hidden_states min max: -10.235448837280273 10.546725273132324\n",
      "hidden_state minus mean squared max: 80.06687927246094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.59329986572266 2.613839626312256\n",
      "loss   971: 1.8950   grad norm: 1.5354          model param norm: 86.5462        \n",
      "\n",
      "quiet_star_policy_loss= 0.002106630941852927\n",
      "nll_loss= 1.8769524097442627\n",
      "avg_std= 0.3323913812637329\n",
      "dist std min max: 0.027724694460630417 0.3323913812637329 3.0086803436279297\n",
      "hidden_states min max: -10.914554595947266 9.533740997314453\n",
      "hidden_state minus mean squared max: 80.78130340576172\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.46092224121094 2.6622605323791504\n",
      "loss   972: 1.8791   grad norm: 1.4643          model param norm: 86.5491        \n",
      "\n",
      "quiet_star_policy_loss= -0.010434413328766823\n",
      "nll_loss= 1.8615236282348633\n",
      "avg_std= 0.3336856961250305\n",
      "dist std min max: 0.028475375846028328 0.3336856961250305 2.790971517562866\n",
      "hidden_states min max: -12.608749389648438 10.139537811279297\n",
      "hidden_state minus mean squared max: 109.52235412597656\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.8806381225586 2.635556697845459\n",
      "loss   973: 1.8511   grad norm: 1.4670          model param norm: 86.5517        \n",
      "\n",
      "quiet_star_policy_loss= -0.008734099566936493\n",
      "nll_loss= 1.8628185987472534\n",
      "avg_std= 0.33360421657562256\n",
      "dist std min max: 0.027730213478207588 0.33360421657562256 2.5775041580200195\n",
      "hidden_states min max: -11.909659385681152 10.869427680969238\n",
      "hidden_state minus mean squared max: 103.90067291259766\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.32823944091797 2.6559314727783203\n",
      "loss   974: 1.8541   grad norm: 1.3997          model param norm: 86.5543        \n",
      "\n",
      "quiet_star_policy_loss= -0.026495790109038353\n",
      "nll_loss= 1.8548650741577148\n",
      "avg_std= 0.33417943120002747\n",
      "dist std min max: 0.028602376580238342 0.33417943120002747 2.6325414180755615\n",
      "hidden_states min max: -10.59111499786377 10.313268661499023\n",
      "hidden_state minus mean squared max: 70.49993133544922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.75575256347656 2.613053798675537\n",
      "loss   975: 1.8284   grad norm: 1.4871          model param norm: 86.5570        \n",
      "\n",
      "quiet_star_policy_loss= -0.0029649853240698576\n",
      "nll_loss= 1.8853167295455933\n",
      "avg_std= 0.33427053689956665\n",
      "dist std min max: 0.028756868094205856 0.33427053689956665 2.8472914695739746\n",
      "hidden_states min max: -10.817625045776367 11.59399700164795\n",
      "hidden_state minus mean squared max: 97.7226791381836\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.82364654541016 2.624790668487549\n",
      "loss   976: 1.8824   grad norm: 1.6164          model param norm: 86.5597        \n",
      "\n",
      "quiet_star_policy_loss= 0.005065512843430042\n",
      "nll_loss= 1.8760277032852173\n",
      "avg_std= 0.33283352851867676\n",
      "dist std min max: 0.028232473880052567 0.33283352851867676 2.631791353225708\n",
      "hidden_states min max: -10.92066764831543 10.69009017944336\n",
      "hidden_state minus mean squared max: 100.51555633544922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.83773040771484 2.6414146423339844\n",
      "loss   977: 1.8811   grad norm: 1.4989          model param norm: 86.5622        \n",
      "\n",
      "quiet_star_policy_loss= -0.017206525430083275\n",
      "nll_loss= 1.867621660232544\n",
      "avg_std= 0.3342379629611969\n",
      "dist std min max: 0.028472531586885452 0.3342379629611969 2.6191282272338867\n",
      "hidden_states min max: -10.809370040893555 10.05836296081543\n",
      "hidden_state minus mean squared max: 81.62567138671875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.58649444580078 2.6265769004821777\n",
      "loss   978: 1.8504   grad norm: 1.4559          model param norm: 86.5650        \n",
      "\n",
      "quiet_star_policy_loss= -0.11979545652866364\n",
      "nll_loss= 1.878232479095459\n",
      "avg_std= 0.33108821511268616\n",
      "dist std min max: 0.028836164623498917 0.33108821511268616 2.729252815246582\n",
      "hidden_states min max: -10.489355087280273 10.74312686920166\n",
      "hidden_state minus mean squared max: 78.73942565917969\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -13.455698013305664 2.6109843254089355\n",
      "loss   979: 1.7584   grad norm: 3.2306          model param norm: 86.5677        \n",
      "\n",
      "quiet_star_policy_loss= -0.03281267359852791\n",
      "nll_loss= 1.8692657947540283\n",
      "avg_std= 0.33250606060028076\n",
      "dist std min max: 0.028261823579669 0.33250606060028076 3.034250259399414\n",
      "hidden_states min max: -10.94101333618164 10.931358337402344\n",
      "hidden_state minus mean squared max: 106.60301971435547\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.97845458984375 2.6295275688171387\n",
      "loss   980: 1.8365   grad norm: 1.4268          model param norm: 86.5703        \n",
      "\n",
      "quiet_star_policy_loss= -0.029539400711655617\n",
      "nll_loss= 1.8895057439804077\n",
      "avg_std= 0.3316629230976105\n",
      "dist std min max: 0.028178123757243156 0.3316629230976105 2.7319765090942383\n",
      "hidden_states min max: -13.516775131225586 10.498977661132812\n",
      "hidden_state minus mean squared max: 173.0950927734375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.1094970703125 2.6397175788879395\n",
      "loss   981: 1.8600   grad norm: 1.4511          model param norm: 86.5728        \n",
      "\n",
      "quiet_star_policy_loss= -0.053313303738832474\n",
      "nll_loss= 1.8707937002182007\n",
      "avg_std= 0.3322058618068695\n",
      "dist std min max: 0.02811797335743904 0.3322058618068695 2.683400869369507\n",
      "hidden_states min max: -10.813749313354492 10.307207107543945\n",
      "hidden_state minus mean squared max: 106.53498077392578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.289306640625 2.6374125480651855\n",
      "loss   982: 1.8175   grad norm: 1.5723          model param norm: 86.5753        \n",
      "\n",
      "quiet_star_policy_loss= -0.03980712965130806\n",
      "nll_loss= 1.8760117292404175\n",
      "avg_std= 0.33278191089630127\n",
      "dist std min max: 0.028076013550162315 0.33278191089630127 2.7684521675109863\n",
      "hidden_states min max: -11.737110137939453 10.3783540725708\n",
      "hidden_state minus mean squared max: 78.98160552978516\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.1501693725586 2.633150577545166\n",
      "loss   983: 1.8362   grad norm: 1.4727          model param norm: 86.5778        \n",
      "\n",
      "quiet_star_policy_loss= -0.03399166092276573\n",
      "nll_loss= 1.866045355796814\n",
      "avg_std= 0.332833468914032\n",
      "dist std min max: 0.02811860479414463 0.332833468914032 2.726644277572632\n",
      "hidden_states min max: -10.331132888793945 11.668121337890625\n",
      "hidden_state minus mean squared max: 88.69249725341797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.93505859375 2.6458353996276855\n",
      "loss   984: 1.8321   grad norm: 1.5401          model param norm: 86.5800        \n",
      "\n",
      "quiet_star_policy_loss= -0.036026086658239365\n",
      "nll_loss= 1.8898266553878784\n",
      "avg_std= 0.33120134472846985\n",
      "dist std min max: 0.027166826650500298 0.33120134472846985 2.7685718536376953\n",
      "hidden_states min max: -10.335079193115234 10.797013282775879\n",
      "hidden_state minus mean squared max: 95.78034210205078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.65789794921875 2.676112174987793\n",
      "loss   985: 1.8538   grad norm: 1.5889          model param norm: 86.5826        \n",
      "\n",
      "quiet_star_policy_loss= -0.007809555623680353\n",
      "nll_loss= 1.890913963317871\n",
      "avg_std= 0.3301370441913605\n",
      "dist std min max: 0.027883317321538925 0.3301370441913605 2.689178228378296\n",
      "hidden_states min max: -10.254895210266113 10.290529251098633\n",
      "hidden_state minus mean squared max: 100.37010192871094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.54446411132812 2.6535258293151855\n",
      "loss   986: 1.8831   grad norm: 1.5279          model param norm: 86.5851        \n",
      "\n",
      "quiet_star_policy_loss= -0.042210061103105545\n",
      "nll_loss= 1.8930797576904297\n",
      "avg_std= 0.3292447328567505\n",
      "dist std min max: 0.027928754687309265 0.3292447328567505 3.518212080001831\n",
      "hidden_states min max: -10.065448760986328 10.74180793762207\n",
      "hidden_state minus mean squared max: 91.74190521240234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.6053237915039 2.6483516693115234\n",
      "loss   987: 1.8509   grad norm: 1.5150          model param norm: 86.5880        \n",
      "\n",
      "quiet_star_policy_loss= 0.00438761105760932\n",
      "nll_loss= 1.8666441440582275\n",
      "avg_std= 0.3300057053565979\n",
      "dist std min max: 0.0273361187428236 0.3300057053565979 2.744110345840454\n",
      "hidden_states min max: -10.396257400512695 12.728546142578125\n",
      "hidden_state minus mean squared max: 122.81493377685547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.11576080322266 2.6714305877685547\n",
      "loss   988: 1.8710   grad norm: 1.4429          model param norm: 86.5906        \n",
      "\n",
      "quiet_star_policy_loss= -0.01756921410560608\n",
      "nll_loss= 1.8611332178115845\n",
      "avg_std= 0.330573171377182\n",
      "dist std min max: 0.02731516771018505 0.330573171377182 2.690786361694336\n",
      "hidden_states min max: -10.233901977539062 10.319990158081055\n",
      "hidden_state minus mean squared max: 97.6937026977539\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.90074920654297 2.6722970008850098\n",
      "loss   989: 1.8436   grad norm: 1.6121          model param norm: 86.5931        \n",
      "\n",
      "quiet_star_policy_loss= -0.049703072756528854\n",
      "nll_loss= 1.8696621656417847\n",
      "avg_std= 0.32852083444595337\n",
      "dist std min max: 0.0272989422082901 0.32852083444595337 2.8839175701141357\n",
      "hidden_states min max: -10.596755027770996 11.394288063049316\n",
      "hidden_state minus mean squared max: 76.3536148071289\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.43470001220703 2.6733078956604004\n",
      "loss   990: 1.8200   grad norm: 1.4935          model param norm: 86.5953        \n",
      "\n",
      "quiet_star_policy_loss= -0.025895554572343826\n",
      "nll_loss= 1.8656638860702515\n",
      "avg_std= 0.32787013053894043\n",
      "dist std min max: 0.026971682906150818 0.32787013053894043 2.8472089767456055\n",
      "hidden_states min max: -10.730690956115723 10.344042778015137\n",
      "hidden_state minus mean squared max: 104.30726623535156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.85623931884766 2.690772533416748\n",
      "loss   991: 1.8398   grad norm: 1.4752          model param norm: 86.5977        \n",
      "\n",
      "quiet_star_policy_loss= 0.013011956587433815\n",
      "nll_loss= 1.8675651550292969\n",
      "avg_std= 0.32654640078544617\n",
      "dist std min max: 0.026923755183815956 0.32654640078544617 2.796640396118164\n",
      "hidden_states min max: -10.399131774902344 10.67041015625\n",
      "hidden_state minus mean squared max: 90.09231567382812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.4749526977539 2.677988052368164\n",
      "loss   992: 1.8806   grad norm: 1.4744          model param norm: 86.6000        \n",
      "\n",
      "quiet_star_policy_loss= -0.015546155162155628\n",
      "nll_loss= 1.88301682472229\n",
      "avg_std= 0.32680827379226685\n",
      "dist std min max: 0.026716506108641624 0.32680827379226685 2.8400139808654785\n",
      "hidden_states min max: -10.19097900390625 13.073263168334961\n",
      "hidden_state minus mean squared max: 124.04173278808594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.41812133789062 2.685257911682129\n",
      "loss   993: 1.8675   grad norm: 1.6331          model param norm: 86.6025        \n",
      "\n",
      "quiet_star_policy_loss= -0.01075274683535099\n",
      "nll_loss= 1.8647812604904175\n",
      "avg_std= 0.325143039226532\n",
      "dist std min max: 0.026312710717320442 0.325143039226532 2.811354160308838\n",
      "hidden_states min max: -10.266092300415039 12.29630184173584\n",
      "hidden_state minus mean squared max: 111.3001937866211\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.66388702392578 2.7091293334960938\n",
      "loss   994: 1.8540   grad norm: 1.6583          model param norm: 86.6051        \n",
      "\n",
      "quiet_star_policy_loss= -0.010440505109727383\n",
      "nll_loss= 1.8727829456329346\n",
      "avg_std= 0.3252566158771515\n",
      "dist std min max: 0.025880947709083557 0.3252566158771515 2.8307273387908936\n",
      "hidden_states min max: -10.059732437133789 11.411650657653809\n",
      "hidden_state minus mean squared max: 95.59687042236328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.62506103515625 2.7142014503479004\n",
      "loss   995: 1.8623   grad norm: 1.4295          model param norm: 86.6076        \n",
      "\n",
      "quiet_star_policy_loss= -0.014038753695786\n",
      "nll_loss= 1.8684927225112915\n",
      "avg_std= 0.3238743841648102\n",
      "dist std min max: 0.02604580856859684 0.3238743841648102 2.7813031673431396\n",
      "hidden_states min max: -17.092256546020508 11.831625938415527\n",
      "hidden_state minus mean squared max: 268.6063232421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3292007446289 2.7156424522399902\n",
      "loss   996: 1.8545   grad norm: 1.5942          model param norm: 86.6100        \n",
      "\n",
      "quiet_star_policy_loss= 0.01411476731300354\n",
      "nll_loss= 1.8766558170318604\n",
      "avg_std= 0.32456207275390625\n",
      "dist std min max: 0.02592434361577034 0.32456207275390625 2.9424448013305664\n",
      "hidden_states min max: -10.525480270385742 12.295130729675293\n",
      "hidden_state minus mean squared max: 112.89260864257812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.58814239501953 2.7127132415771484\n",
      "loss   997: 1.8908   grad norm: 1.5032          model param norm: 86.6122        \n",
      "\n",
      "quiet_star_policy_loss= -0.0018092200625687838\n",
      "nll_loss= 1.866226077079773\n",
      "avg_std= 0.3223152160644531\n",
      "dist std min max: 0.02538224123418331 0.3223152160644531 2.925901174545288\n",
      "hidden_states min max: -10.147212982177734 11.99098014831543\n",
      "hidden_state minus mean squared max: 107.37162780761719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7308578491211 2.736443519592285\n",
      "loss   998: 1.8644   grad norm: 1.4055          model param norm: 86.6144        \n",
      "\n",
      "quiet_star_policy_loss= -0.015091336332261562\n",
      "nll_loss= 1.8723524808883667\n",
      "avg_std= 0.32241395115852356\n",
      "dist std min max: 0.025818441063165665 0.32241395115852356 2.7954633235931396\n",
      "hidden_states min max: -10.1195650100708 11.021193504333496\n",
      "hidden_state minus mean squared max: 80.38945770263672\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.60343170166016 2.729262351989746\n",
      "loss   999: 1.8573   grad norm: 1.5254          model param norm: 86.6169        \n",
      "eval loss 1.8774818181991577\n",
      "\n",
      "quiet_star_policy_loss= -0.022032488137483597\n",
      "nll_loss= 1.8773812055587769\n",
      "avg_std= 0.3222615718841553\n",
      "dist std min max: 0.025665413588285446 0.3222615718841553 3.005046844482422\n",
      "hidden_states min max: -11.072362899780273 11.910820960998535\n",
      "hidden_state minus mean squared max: 140.3016815185547\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.69257354736328 2.7410154342651367\n",
      "loss  1000: 1.8553   grad norm: 1.4220          model param norm: 86.6194        \n",
      "\n",
      "quiet_star_policy_loss= -0.020195437595248222\n",
      "nll_loss= 1.8655925989151\n",
      "avg_std= 0.3234102427959442\n",
      "dist std min max: 0.024968346580863 0.3234102427959442 2.8896827697753906\n",
      "hidden_states min max: -9.936264038085938 11.343362808227539\n",
      "hidden_state minus mean squared max: 91.68634033203125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.12633514404297 2.7667922973632812\n",
      "loss  1001: 1.8454   grad norm: 1.6127          model param norm: 86.6218        \n",
      "\n",
      "quiet_star_policy_loss= -0.007465147878974676\n",
      "nll_loss= 1.8618038892745972\n",
      "avg_std= 0.3236677348613739\n",
      "dist std min max: 0.025788001716136932 0.3236677348613739 3.2747979164123535\n",
      "hidden_states min max: -10.258466720581055 10.499483108520508\n",
      "hidden_state minus mean squared max: 88.64041137695312\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.5131607055664 2.718674659729004\n",
      "loss  1002: 1.8543   grad norm: 1.5068          model param norm: 86.6241        \n",
      "\n",
      "quiet_star_policy_loss= 0.003668123623356223\n",
      "nll_loss= 1.8520034551620483\n",
      "avg_std= 0.3228946924209595\n",
      "dist std min max: 0.025158829987049103 0.3228946924209595 2.8524227142333984\n",
      "hidden_states min max: -10.133228302001953 11.682273864746094\n",
      "hidden_state minus mean squared max: 101.93512725830078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.41341400146484 2.732428550720215\n",
      "loss  1003: 1.8557   grad norm: 1.5270          model param norm: 86.6262        \n",
      "\n",
      "quiet_star_policy_loss= -0.02729007974267006\n",
      "nll_loss= 1.874288558959961\n",
      "avg_std= 0.3218916952610016\n",
      "dist std min max: 0.026183128356933594 0.3218916952610016 2.9529731273651123\n",
      "hidden_states min max: -10.023078918457031 12.146222114562988\n",
      "hidden_state minus mean squared max: 113.58018493652344\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.55896759033203 2.7043371200561523\n",
      "loss  1004: 1.8470   grad norm: 1.5962          model param norm: 86.6280        \n",
      "\n",
      "quiet_star_policy_loss= -0.02459160052239895\n",
      "nll_loss= 1.8742427825927734\n",
      "avg_std= 0.3217936158180237\n",
      "dist std min max: 0.02634253166615963 0.3217936158180237 2.8393678665161133\n",
      "hidden_states min max: -10.937727928161621 10.883893013000488\n",
      "hidden_state minus mean squared max: 136.46292114257812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.99060821533203 2.707509994506836\n",
      "loss  1005: 1.8497   grad norm: 1.5247          model param norm: 86.6299        \n",
      "\n",
      "quiet_star_policy_loss= -0.022421563044190407\n",
      "nll_loss= 1.880165934562683\n",
      "avg_std= 0.3204658329486847\n",
      "dist std min max: 0.025873098522424698 0.3204658329486847 2.902034282684326\n",
      "hidden_states min max: -10.406852722167969 13.374666213989258\n",
      "hidden_state minus mean squared max: 138.0269775390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79012298583984 2.701842784881592\n",
      "loss  1006: 1.8577   grad norm: 1.4764          model param norm: 86.6317        \n",
      "\n",
      "quiet_star_policy_loss= -0.007445602677762508\n",
      "nll_loss= 1.8183484077453613\n",
      "avg_std= 0.32262668013572693\n",
      "dist std min max: 0.027733955532312393 0.32262668013572693 2.8169071674346924\n",
      "hidden_states min max: -9.669668197631836 10.550331115722656\n",
      "hidden_state minus mean squared max: 98.60063171386719\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -101.8553695678711 2.663577079772949\n",
      "loss  1007: 1.8109   grad norm: 3.1485          model param norm: 86.6335        \n",
      "\n",
      "quiet_star_policy_loss= -0.012279259972274303\n",
      "nll_loss= 1.8665913343429565\n",
      "avg_std= 0.3200218677520752\n",
      "dist std min max: 0.02682904712855816 0.3200218677520752 3.244108200073242\n",
      "hidden_states min max: -10.138874053955078 13.01486587524414\n",
      "hidden_state minus mean squared max: 121.92692565917969\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.85039520263672 2.686966896057129\n",
      "loss  1008: 1.8543   grad norm: 1.4377          model param norm: 86.6352        \n",
      "\n",
      "quiet_star_policy_loss= -0.003056144807487726\n",
      "nll_loss= 1.872415542602539\n",
      "avg_std= 0.3198612630367279\n",
      "dist std min max: 0.025710612535476685 0.3198612630367279 3.206083059310913\n",
      "hidden_states min max: -10.176765441894531 11.49410343170166\n",
      "hidden_state minus mean squared max: 95.52849578857422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.05448913574219 2.7277069091796875\n",
      "loss  1009: 1.8694   grad norm: 1.5433          model param norm: 86.6367        \n",
      "\n",
      "quiet_star_policy_loss= 0.0035733759868890047\n",
      "nll_loss= 1.85358464717865\n",
      "avg_std= 0.3193359971046448\n",
      "dist std min max: 0.026998242363333702 0.3193359971046448 2.8389132022857666\n",
      "hidden_states min max: -9.964016914367676 11.838479995727539\n",
      "hidden_state minus mean squared max: 104.2755355834961\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.78230285644531 2.6819205284118652\n",
      "loss  1010: 1.8572   grad norm: 1.5195          model param norm: 86.6384        \n",
      "\n",
      "quiet_star_policy_loss= -0.027192136272788048\n",
      "nll_loss= 1.8603858947753906\n",
      "avg_std= 0.3200785517692566\n",
      "dist std min max: 0.0267439354211092 0.3200785517692566 2.8056516647338867\n",
      "hidden_states min max: -10.144501686096191 11.921466827392578\n",
      "hidden_state minus mean squared max: 127.03523254394531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.95482635498047 2.6912894248962402\n",
      "loss  1011: 1.8332   grad norm: 1.5462          model param norm: 86.6404        \n",
      "\n",
      "quiet_star_policy_loss= -0.02423030696809292\n",
      "nll_loss= 1.867889404296875\n",
      "avg_std= 0.3168657720088959\n",
      "dist std min max: 0.02681906335055828 0.3168657720088959 2.919990062713623\n",
      "hidden_states min max: -14.41639518737793 12.257317543029785\n",
      "hidden_state minus mean squared max: 153.280029296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.04872131347656 2.6898345947265625\n",
      "loss  1012: 1.8437   grad norm: 1.5245          model param norm: 86.6424        \n",
      "\n",
      "quiet_star_policy_loss= -0.02409467101097107\n",
      "nll_loss= 1.8756316900253296\n",
      "avg_std= 0.3169528841972351\n",
      "dist std min max: 0.026006493717432022 0.3169528841972351 2.93186354637146\n",
      "hidden_states min max: -9.621453285217285 12.476177215576172\n",
      "hidden_state minus mean squared max: 116.28250885009766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.32858276367188 2.7224411964416504\n",
      "loss  1013: 1.8515   grad norm: 1.4808          model param norm: 86.6445        \n",
      "\n",
      "quiet_star_policy_loss= -0.01545854564756155\n",
      "nll_loss= 1.8681087493896484\n",
      "avg_std= 0.3163553476333618\n",
      "dist std min max: 0.025741752237081528 0.3163553476333618 3.0186312198638916\n",
      "hidden_states min max: -10.006221771240234 11.511110305786133\n",
      "hidden_state minus mean squared max: 95.48041534423828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.08551025390625 2.7121925354003906\n",
      "loss  1014: 1.8527   grad norm: 1.5257          model param norm: 86.6465        \n",
      "\n",
      "quiet_star_policy_loss= -0.00924605131149292\n",
      "nll_loss= 1.8657524585723877\n",
      "avg_std= 0.31563350558280945\n",
      "dist std min max: 0.026491083204746246 0.31563350558280945 3.0739428997039795\n",
      "hidden_states min max: -10.4930419921875 11.02314567565918\n",
      "hidden_state minus mean squared max: 96.0999755859375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77311706542969 2.707167148590088\n",
      "loss  1015: 1.8565   grad norm: 1.6150          model param norm: 86.6484        \n",
      "\n",
      "quiet_star_policy_loss= 0.006663656327873468\n",
      "nll_loss= 1.9082330465316772\n",
      "avg_std= 0.3143469989299774\n",
      "dist std min max: 0.026419134810566902 0.3143469989299774 2.8482022285461426\n",
      "hidden_states min max: -9.724706649780273 12.128226280212402\n",
      "hidden_state minus mean squared max: 92.11753845214844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.57584381103516 2.7142529487609863\n",
      "loss  1016: 1.9149   grad norm: 1.4986          model param norm: 86.6506        \n",
      "\n",
      "quiet_star_policy_loss= -0.024526918306946754\n",
      "nll_loss= 1.8769387006759644\n",
      "avg_std= 0.31389322876930237\n",
      "dist std min max: 0.026631681248545647 0.31389322876930237 2.9414844512939453\n",
      "hidden_states min max: -10.353020668029785 11.783767700195312\n",
      "hidden_state minus mean squared max: 122.92562866210938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.93834686279297 2.7053070068359375\n",
      "loss  1017: 1.8524   grad norm: 1.4713          model param norm: 86.6525        \n",
      "\n",
      "quiet_star_policy_loss= 0.008289644494652748\n",
      "nll_loss= 1.8740026950836182\n",
      "avg_std= 0.3127764165401459\n",
      "dist std min max: 0.02629248797893524 0.3127764165401459 3.0564136505126953\n",
      "hidden_states min max: -23.96004295349121 11.155996322631836\n",
      "hidden_state minus mean squared max: 435.4522399902344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.57077026367188 2.7132110595703125\n",
      "loss  1018: 1.8823   grad norm: 1.5398          model param norm: 86.6545        \n",
      "\n",
      "quiet_star_policy_loss= -0.006672588177025318\n",
      "nll_loss= 1.8700764179229736\n",
      "avg_std= 0.313270628452301\n",
      "dist std min max: 0.0248306542634964 0.313270628452301 3.0772669315338135\n",
      "hidden_states min max: -15.60335922241211 11.24494743347168\n",
      "hidden_state minus mean squared max: 175.23739624023438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.11563110351562 2.7476463317871094\n",
      "loss  1019: 1.8634   grad norm: 1.4210          model param norm: 86.6567        \n",
      "\n",
      "quiet_star_policy_loss= -0.02798759937286377\n",
      "nll_loss= 1.8700125217437744\n",
      "avg_std= 0.31420737504959106\n",
      "dist std min max: 0.0254575964063406 0.31420737504959106 3.0587611198425293\n",
      "hidden_states min max: -9.997320175170898 11.534411430358887\n",
      "hidden_state minus mean squared max: 120.85494232177734\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.92986297607422 2.7325239181518555\n",
      "loss  1020: 1.8420   grad norm: 1.5123          model param norm: 86.6590        \n",
      "\n",
      "quiet_star_policy_loss= -0.014937621541321278\n",
      "nll_loss= 1.8753353357315063\n",
      "avg_std= 0.31191280484199524\n",
      "dist std min max: 0.025540174916386604 0.31191280484199524 2.9937126636505127\n",
      "hidden_states min max: -10.181854248046875 11.506942749023438\n",
      "hidden_state minus mean squared max: 104.53624725341797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.08849334716797 2.7355618476867676\n",
      "loss  1021: 1.8604   grad norm: 1.5403          model param norm: 86.6612        \n",
      "\n",
      "quiet_star_policy_loss= 0.01835099421441555\n",
      "nll_loss= 1.881250023841858\n",
      "avg_std= 0.31392574310302734\n",
      "dist std min max: 0.024760104715824127 0.31392574310302734 3.289745330810547\n",
      "hidden_states min max: -13.163952827453613 13.121201515197754\n",
      "hidden_state minus mean squared max: 159.31716918945312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0680160522461 2.7462525367736816\n",
      "loss  1022: 1.8996   grad norm: 1.6051          model param norm: 86.6634        \n",
      "\n",
      "quiet_star_policy_loss= -0.007943940348923206\n",
      "nll_loss= 1.8673065900802612\n",
      "avg_std= 0.3097979426383972\n",
      "dist std min max: 0.024482594802975655 0.3097979426383972 3.0558695793151855\n",
      "hidden_states min max: -11.546072006225586 11.459132194519043\n",
      "hidden_state minus mean squared max: 96.46334075927734\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.81715393066406 2.7829666137695312\n",
      "loss  1023: 1.8594   grad norm: 1.5714          model param norm: 86.6658        \n",
      "\n",
      "quiet_star_policy_loss= -0.014103841967880726\n",
      "nll_loss= 1.8844263553619385\n",
      "avg_std= 0.31159454584121704\n",
      "dist std min max: 0.024073239415884018 0.31159454584121704 3.1375038623809814\n",
      "hidden_states min max: -15.368317604064941 12.586685180664062\n",
      "hidden_state minus mean squared max: 217.2441864013672\n",
      "hidden_state minus mean divided by std max: 5.166576862335205\n",
      "log_prob min max: -104.22309875488281 2.8026204109191895\n",
      "loss  1024: 1.8703   grad norm: 1.6513          model param norm: 86.6682        \n",
      "\n",
      "quiet_star_policy_loss= -0.023608243092894554\n",
      "nll_loss= 1.8690309524536133\n",
      "avg_std= 0.3080270290374756\n",
      "dist std min max: 0.024899950250983238 0.3080270290374756 3.092043876647949\n",
      "hidden_states min max: -9.406509399414062 12.097601890563965\n",
      "hidden_state minus mean squared max: 119.09540557861328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.45988464355469 2.7570180892944336\n",
      "loss  1025: 1.8454   grad norm: 1.4135          model param norm: 86.6704        \n",
      "\n",
      "quiet_star_policy_loss= -0.020530838519334793\n",
      "nll_loss= 1.8633610010147095\n",
      "avg_std= 0.3101567327976227\n",
      "dist std min max: 0.02434435486793518 0.3101567327976227 3.243412494659424\n",
      "hidden_states min max: -9.43211555480957 11.818845748901367\n",
      "hidden_state minus mean squared max: 92.05130004882812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.65036010742188 2.7789502143859863\n",
      "loss  1026: 1.8428   grad norm: 1.6159          model param norm: 86.6725        \n",
      "\n",
      "quiet_star_policy_loss= -0.027855707332491875\n",
      "nll_loss= 1.8662140369415283\n",
      "avg_std= 0.3099287450313568\n",
      "dist std min max: 0.02342045307159424 0.3099287450313568 3.1811251640319824\n",
      "hidden_states min max: -11.178360939025879 12.285894393920898\n",
      "hidden_state minus mean squared max: 148.59234619140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.97198486328125 2.821418285369873\n",
      "loss  1027: 1.8384   grad norm: 1.6809          model param norm: 86.6745        \n",
      "\n",
      "quiet_star_policy_loss= -0.018811548128724098\n",
      "nll_loss= 1.8613258600234985\n",
      "avg_std= 0.31270381808280945\n",
      "dist std min max: 0.023929424583911896 0.31270381808280945 3.1146132946014404\n",
      "hidden_states min max: -10.613919258117676 11.344091415405273\n",
      "hidden_state minus mean squared max: 144.27252197265625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85929107666016 2.796830177307129\n",
      "loss  1028: 1.8425   grad norm: 1.5363          model param norm: 86.6764        \n",
      "\n",
      "quiet_star_policy_loss= -0.009877800941467285\n",
      "nll_loss= 1.8540661334991455\n",
      "avg_std= 0.312852680683136\n",
      "dist std min max: 0.02283807098865509 0.312852680683136 3.0792388916015625\n",
      "hidden_states min max: -9.598783493041992 14.723067283630371\n",
      "hidden_state minus mean squared max: 172.5838165283203\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.90852355957031 2.8210272789001465\n",
      "loss  1029: 1.8442   grad norm: 1.5185          model param norm: 86.6783        \n",
      "\n",
      "quiet_star_policy_loss= -0.045947469770908356\n",
      "nll_loss= 1.8745616674423218\n",
      "avg_std= 0.3120027482509613\n",
      "dist std min max: 0.022557219490408897 0.3120027482509613 3.3030343055725098\n",
      "hidden_states min max: -9.354774475097656 13.006657600402832\n",
      "hidden_state minus mean squared max: 127.41050720214844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.9308853149414 2.8655009269714355\n",
      "loss  1030: 1.8286   grad norm: 1.5128          model param norm: 86.6803        \n",
      "\n",
      "quiet_star_policy_loss= -0.01463203877210617\n",
      "nll_loss= 1.8604179620742798\n",
      "avg_std= 0.30943799018859863\n",
      "dist std min max: 0.02255335822701454 0.30943799018859863 3.2148218154907227\n",
      "hidden_states min max: -10.018203735351562 12.13901138305664\n",
      "hidden_state minus mean squared max: 123.52474212646484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.53904724121094 2.872929096221924\n",
      "loss  1031: 1.8458   grad norm: 1.5785          model param norm: 86.6823        \n",
      "\n",
      "quiet_star_policy_loss= -0.011387109756469727\n",
      "nll_loss= 1.8661842346191406\n",
      "avg_std= 0.30637192726135254\n",
      "dist std min max: 0.022235020995140076 0.30637192726135254 3.145571708679199\n",
      "hidden_states min max: -10.254646301269531 11.269704818725586\n",
      "hidden_state minus mean squared max: 112.39169311523438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.68090057373047 2.8479151725769043\n",
      "loss  1032: 1.8548   grad norm: 1.4769          model param norm: 86.6842        \n",
      "\n",
      "quiet_star_policy_loss= -0.019483143463730812\n",
      "nll_loss= 1.8610948324203491\n",
      "avg_std= 0.3085184395313263\n",
      "dist std min max: 0.023600466549396515 0.3085184395313263 3.228126287460327\n",
      "hidden_states min max: -9.631961822509766 12.233936309814453\n",
      "hidden_state minus mean squared max: 112.79544067382812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.45341491699219 2.8248815536499023\n",
      "loss  1033: 1.8416   grad norm: 1.5215          model param norm: 86.6861        \n",
      "\n",
      "quiet_star_policy_loss= -0.029424358159303665\n",
      "nll_loss= 1.8665951490402222\n",
      "avg_std= 0.3058469593524933\n",
      "dist std min max: 0.021199045702815056 0.3058469593524933 3.156489849090576\n",
      "hidden_states min max: -10.408160209655762 13.794607162475586\n",
      "hidden_state minus mean squared max: 157.66831970214844\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -104.06282806396484 2.916900634765625\n",
      "loss  1034: 1.8372   grad norm: 1.4512          model param norm: 86.6879        \n",
      "\n",
      "quiet_star_policy_loss= -0.013146311976015568\n",
      "nll_loss= 1.8793892860412598\n",
      "avg_std= 0.30407294631004333\n",
      "dist std min max: 0.022731855511665344 0.30407294631004333 3.207139253616333\n",
      "hidden_states min max: -8.861906051635742 11.919902801513672\n",
      "hidden_state minus mean squared max: 103.07395935058594\n",
      "hidden_state minus mean divided by std max: 4.957176208496094\n",
      "log_prob min max: -13.390168190002441 2.8549365997314453\n",
      "loss  1035: 1.8662   grad norm: 3.0106          model param norm: 86.6896        \n",
      "\n",
      "quiet_star_policy_loss= -0.020812256261706352\n",
      "nll_loss= 1.8637570142745972\n",
      "avg_std= 0.3071597218513489\n",
      "dist std min max: 0.020533863455057144 0.3071597218513489 3.258145570755005\n",
      "hidden_states min max: -10.264183044433594 12.15057373046875\n",
      "hidden_state minus mean squared max: 106.46090698242188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.10394287109375 2.9195265769958496\n",
      "loss  1036: 1.8429   grad norm: 1.4003          model param norm: 86.6908        \n",
      "\n",
      "quiet_star_policy_loss= 0.00977675337344408\n",
      "nll_loss= 1.8573211431503296\n",
      "avg_std= 0.3062417507171631\n",
      "dist std min max: 0.02105138823390007 0.3062417507171631 3.165740728378296\n",
      "hidden_states min max: -9.74174690246582 13.847379684448242\n",
      "hidden_state minus mean squared max: 152.29269409179688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.18782043457031 2.9275693893432617\n",
      "loss  1037: 1.8671   grad norm: 1.5639          model param norm: 86.6919        \n",
      "\n",
      "quiet_star_policy_loss= -0.05551663041114807\n",
      "nll_loss= 1.8853397369384766\n",
      "avg_std= 0.30596137046813965\n",
      "dist std min max: 0.022123217582702637 0.30596137046813965 3.2268271446228027\n",
      "hidden_states min max: -14.573290824890137 14.148141860961914\n",
      "hidden_state minus mean squared max: 177.74415588378906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12275695800781 2.8904061317443848\n",
      "loss  1038: 1.8298   grad norm: 1.4419          model param norm: 86.6935        \n",
      "\n",
      "quiet_star_policy_loss= -0.018271947279572487\n",
      "nll_loss= 1.8707551956176758\n",
      "avg_std= 0.30438682436943054\n",
      "dist std min max: 0.022319497540593147 0.30438682436943054 3.2275583744049072\n",
      "hidden_states min max: -9.834329605102539 11.626405715942383\n",
      "hidden_state minus mean squared max: 127.52027130126953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.96546936035156 2.8696537017822266\n",
      "loss  1039: 1.8525   grad norm: 1.5902          model param norm: 86.6956        \n",
      "\n",
      "quiet_star_policy_loss= -0.02634011022746563\n",
      "nll_loss= 1.870748519897461\n",
      "avg_std= 0.3049117624759674\n",
      "dist std min max: 0.022532140836119652 0.3049117624759674 3.225050449371338\n",
      "hidden_states min max: -9.442300796508789 12.553400993347168\n",
      "hidden_state minus mean squared max: 118.965576171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.10575103759766 2.8510384559631348\n",
      "loss  1040: 1.8444   grad norm: 1.6101          model param norm: 86.6977        \n",
      "\n",
      "quiet_star_policy_loss= -0.025529325008392334\n",
      "nll_loss= 1.8753137588500977\n",
      "avg_std= 0.3029579818248749\n",
      "dist std min max: 0.0210433192551136 0.3029579818248749 3.2344348430633545\n",
      "hidden_states min max: -9.43134880065918 13.824151992797852\n",
      "hidden_state minus mean squared max: 153.41355895996094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.41956329345703 2.914400577545166\n",
      "loss  1041: 1.8498   grad norm: 1.5358          model param norm: 86.7001        \n",
      "\n",
      "quiet_star_policy_loss= -0.013236177153885365\n",
      "nll_loss= 1.8540012836456299\n",
      "avg_std= 0.30504918098449707\n",
      "dist std min max: 0.020673729479312897 0.30504918098449707 3.249016046524048\n",
      "hidden_states min max: -9.71410846710205 11.946832656860352\n",
      "hidden_state minus mean squared max: 124.62434387207031\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.72313690185547 2.946117877960205\n",
      "loss  1042: 1.8408   grad norm: 1.4673          model param norm: 86.7025        \n",
      "\n",
      "quiet_star_policy_loss= -0.01701420545578003\n",
      "nll_loss= 1.8612871170043945\n",
      "avg_std= 0.3031773269176483\n",
      "dist std min max: 0.021344758570194244 0.3031773269176483 3.268617868423462\n",
      "hidden_states min max: -17.692197799682617 12.177749633789062\n",
      "hidden_state minus mean squared max: 223.85545349121094\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.23808288574219 2.8901448249816895\n",
      "loss  1043: 1.8443   grad norm: 1.6298          model param norm: 86.7050        \n",
      "\n",
      "quiet_star_policy_loss= -0.02801818586885929\n",
      "nll_loss= 1.8637211322784424\n",
      "avg_std= 0.30473753809928894\n",
      "dist std min max: 0.022165164351463318 0.30473753809928894 3.2526307106018066\n",
      "hidden_states min max: -16.389453887939453 11.967574119567871\n",
      "hidden_state minus mean squared max: 241.75076293945312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.27654266357422 2.8768553733825684\n",
      "loss  1044: 1.8357   grad norm: 1.4943          model param norm: 86.7072        \n",
      "\n",
      "quiet_star_policy_loss= -0.007065677549690008\n",
      "nll_loss= 1.854334831237793\n",
      "avg_std= 0.302743524312973\n",
      "dist std min max: 0.022444995120167732 0.302743524312973 3.2835845947265625\n",
      "hidden_states min max: -10.407788276672363 12.264578819274902\n",
      "hidden_state minus mean squared max: 143.37924194335938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.01532745361328 2.8481154441833496\n",
      "loss  1045: 1.8473   grad norm: 1.6289          model param norm: 86.7093        \n",
      "\n",
      "quiet_star_policy_loss= -0.0005715012666769326\n",
      "nll_loss= 1.8603134155273438\n",
      "avg_std= 0.3024921417236328\n",
      "dist std min max: 0.02181677520275116 0.3024921417236328 3.2572689056396484\n",
      "hidden_states min max: -9.464079856872559 12.279975891113281\n",
      "hidden_state minus mean squared max: 114.25997161865234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.55371856689453 2.8998990058898926\n",
      "loss  1046: 1.8597   grad norm: 1.5411          model param norm: 86.7113        \n",
      "\n",
      "quiet_star_policy_loss= -0.006811103317886591\n",
      "nll_loss= 1.8767067193984985\n",
      "avg_std= 0.29942014813423157\n",
      "dist std min max: 0.02121741697192192 0.29942014813423157 3.2838993072509766\n",
      "hidden_states min max: -12.203847885131836 12.823084831237793\n",
      "hidden_state minus mean squared max: 119.1041259765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.84635925292969 2.917361259460449\n",
      "loss  1047: 1.8699   grad norm: 1.6328          model param norm: 86.7133        \n",
      "\n",
      "quiet_star_policy_loss= -0.017294866964221\n",
      "nll_loss= 1.869214653968811\n",
      "avg_std= 0.29961416125297546\n",
      "dist std min max: 0.022183150053024292 0.29961416125297546 3.2806012630462646\n",
      "hidden_states min max: -9.535417556762695 12.631715774536133\n",
      "hidden_state minus mean squared max: 113.91033172607422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.20828247070312 2.8709468841552734\n",
      "loss  1048: 1.8519   grad norm: 1.4565          model param norm: 86.7155        \n",
      "\n",
      "quiet_star_policy_loss= 0.003179070306941867\n",
      "nll_loss= 1.8718022108078003\n",
      "avg_std= 0.30282333493232727\n",
      "dist std min max: 0.02228090725839138 0.30282333493232727 3.2901079654693604\n",
      "hidden_states min max: -9.878612518310547 12.25228500366211\n",
      "hidden_state minus mean squared max: 130.83753967285156\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.2715835571289 2.875082492828369\n",
      "loss  1049: 1.8750   grad norm: 1.6027          model param norm: 86.7179        \n",
      "\n",
      "quiet_star_policy_loss= -0.012276706285774708\n",
      "nll_loss= 1.8689762353897095\n",
      "avg_std= 0.30013954639434814\n",
      "dist std min max: 0.020995143800973892 0.30013954639434814 3.2572011947631836\n",
      "hidden_states min max: -9.732505798339844 12.526350975036621\n",
      "hidden_state minus mean squared max: 123.5278091430664\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.19636535644531 2.9341835975646973\n",
      "loss  1050: 1.8567   grad norm: 1.5447          model param norm: 86.7201        \n",
      "\n",
      "quiet_star_policy_loss= -0.025948507711291313\n",
      "nll_loss= 1.871015191078186\n",
      "avg_std= 0.30134454369544983\n",
      "dist std min max: 0.02252391166985035 0.30134454369544983 3.267241954803467\n",
      "hidden_states min max: -9.39840030670166 12.260180473327637\n",
      "hidden_state minus mean squared max: 117.11848449707031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8412857055664 2.853231906890869\n",
      "loss  1051: 1.8451   grad norm: 1.6559          model param norm: 86.7224        \n",
      "\n",
      "quiet_star_policy_loss= 0.009984351694583893\n",
      "nll_loss= 1.8653267621994019\n",
      "avg_std= 0.29925602674484253\n",
      "dist std min max: 0.02198912389576435 0.29925602674484253 3.2755253314971924\n",
      "hidden_states min max: -41.485328674316406 13.23419189453125\n",
      "hidden_state minus mean squared max: 1847.54833984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.29338073730469 2.8871588706970215\n",
      "loss  1052: 1.8753   grad norm: 1.5548          model param norm: 86.7248        \n",
      "\n",
      "quiet_star_policy_loss= 0.014568418264389038\n",
      "nll_loss= 1.8756431341171265\n",
      "avg_std= 0.3010346293449402\n",
      "dist std min max: 0.021320661529898643 0.3010346293449402 3.2510335445404053\n",
      "hidden_states min max: -14.36190128326416 13.438081741333008\n",
      "hidden_state minus mean squared max: 201.7991485595703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.18622589111328 2.9194746017456055\n",
      "loss  1053: 1.8902   grad norm: 1.4799          model param norm: 86.7273        \n",
      "\n",
      "quiet_star_policy_loss= -0.019956180825829506\n",
      "nll_loss= 1.8519632816314697\n",
      "avg_std= 0.2997874915599823\n",
      "dist std min max: 0.021033959463238716 0.2997874915599823 3.2472338676452637\n",
      "hidden_states min max: -9.246416091918945 12.381211280822754\n",
      "hidden_state minus mean squared max: 119.24736022949219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.13372802734375 2.9334588050842285\n",
      "loss  1054: 1.8320   grad norm: 1.5602          model param norm: 86.7301        \n",
      "\n",
      "quiet_star_policy_loss= -0.05474643036723137\n",
      "nll_loss= 1.8478199243545532\n",
      "avg_std= 0.3011561334133148\n",
      "dist std min max: 0.021220138296484947 0.3011561334133148 3.259326219558716\n",
      "hidden_states min max: -9.644023895263672 14.134465217590332\n",
      "hidden_state minus mean squared max: 160.602294921875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.716713905334473 2.906381130218506\n",
      "loss  1055: 1.7931   grad norm: 1.4933          model param norm: 86.7328        \n",
      "\n",
      "quiet_star_policy_loss= 0.01020579319447279\n",
      "nll_loss= 1.8646183013916016\n",
      "avg_std= 0.30232974886894226\n",
      "dist std min max: 0.019841047003865242 0.30232974886894226 3.2284295558929443\n",
      "hidden_states min max: -18.150434494018555 12.315502166748047\n",
      "hidden_state minus mean squared max: 331.88519287109375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.43498229980469 2.9605202674865723\n",
      "loss  1056: 1.8748   grad norm: 1.4914          model param norm: 86.7353        \n",
      "\n",
      "quiet_star_policy_loss= -0.005925810430198908\n",
      "nll_loss= 1.8704051971435547\n",
      "avg_std= 0.2997387945652008\n",
      "dist std min max: 0.02248760126531124 0.2997387945652008 3.1896514892578125\n",
      "hidden_states min max: -9.52975845336914 13.021753311157227\n",
      "hidden_state minus mean squared max: 138.654296875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.618896484375 2.857398509979248\n",
      "loss  1057: 1.8645   grad norm: 1.6345          model param norm: 86.7379        \n",
      "\n",
      "quiet_star_policy_loss= 0.0001502156228525564\n",
      "nll_loss= 1.8506481647491455\n",
      "avg_std= 0.3012858033180237\n",
      "dist std min max: 0.0224041435867548 0.3012858033180237 3.191019058227539\n",
      "hidden_states min max: -9.83838939666748 11.933622360229492\n",
      "hidden_state minus mean squared max: 126.12398529052734\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.45530700683594 2.8623733520507812\n",
      "loss  1058: 1.8508   grad norm: 1.6048          model param norm: 86.7404        \n",
      "\n",
      "quiet_star_policy_loss= -0.011946439743041992\n",
      "nll_loss= 1.869532585144043\n",
      "avg_std= 0.3002914488315582\n",
      "dist std min max: 0.022256039083003998 0.3002914488315582 3.1022443771362305\n",
      "hidden_states min max: -9.553893089294434 12.253029823303223\n",
      "hidden_state minus mean squared max: 110.14130401611328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.93939971923828 2.8740291595458984\n",
      "loss  1059: 1.8576   grad norm: 1.5110          model param norm: 86.7429        \n",
      "\n",
      "quiet_star_policy_loss= -0.049087174236774445\n",
      "nll_loss= 1.8757305145263672\n",
      "avg_std= 0.29858800768852234\n",
      "dist std min max: 0.02205696329474449 0.29858800768852234 3.0524227619171143\n",
      "hidden_states min max: -12.608895301818848 12.51266098022461\n",
      "hidden_state minus mean squared max: 122.52165222167969\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.86539459228516 2.866283416748047\n",
      "loss  1060: 1.8266   grad norm: 1.5355          model param norm: 86.7455        \n",
      "\n",
      "quiet_star_policy_loss= -0.018245166167616844\n",
      "nll_loss= 1.8857067823410034\n",
      "avg_std= 0.2958316504955292\n",
      "dist std min max: 0.02206348069012165 0.2958316504955292 3.0374138355255127\n",
      "hidden_states min max: -10.144384384155273 11.205733299255371\n",
      "hidden_state minus mean squared max: 132.9010009765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8062515258789 2.878284454345703\n",
      "loss  1061: 1.8675   grad norm: 1.5455          model param norm: 86.7479        \n",
      "\n",
      "quiet_star_policy_loss= -0.01875411346554756\n",
      "nll_loss= 1.8552868366241455\n",
      "avg_std= 0.29845476150512695\n",
      "dist std min max: 0.02231019176542759 0.29845476150512695 3.059555768966675\n",
      "hidden_states min max: -10.866829872131348 14.022466659545898\n",
      "hidden_state minus mean squared max: 160.80938720703125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.15019226074219 2.878763198852539\n",
      "loss  1062: 1.8365   grad norm: 1.4865          model param norm: 86.7500        \n",
      "\n",
      "quiet_star_policy_loss= -0.023278236389160156\n",
      "nll_loss= 1.833263635635376\n",
      "avg_std= 0.2968074679374695\n",
      "dist std min max: 0.023305978626012802 0.2968074679374695 2.9415221214294434\n",
      "hidden_states min max: -9.328380584716797 10.418352127075195\n",
      "hidden_state minus mean squared max: 91.56687927246094\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -102.96312713623047 2.815798759460449\n",
      "loss  1063: 1.8100   grad norm: 3.0562          model param norm: 86.7519        \n",
      "\n",
      "quiet_star_policy_loss= 0.006196081638336182\n",
      "nll_loss= 1.8596361875534058\n",
      "avg_std= 0.2950967252254486\n",
      "dist std min max: 0.02227337658405304 0.2950967252254486 3.0321502685546875\n",
      "hidden_states min max: -10.805330276489258 11.412249565124512\n",
      "hidden_state minus mean squared max: 144.75294494628906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.02009582519531 2.8853206634521484\n",
      "loss  1064: 1.8658   grad norm: 1.5806          model param norm: 86.7541        \n",
      "\n",
      "quiet_star_policy_loss= -0.02571495808660984\n",
      "nll_loss= 1.8628921508789062\n",
      "avg_std= 0.29353806376457214\n",
      "dist std min max: 0.020273230969905853 0.29353806376457214 2.9133846759796143\n",
      "hidden_states min max: -9.459393501281738 11.704190254211426\n",
      "hidden_state minus mean squared max: 102.43777465820312\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -103.0245132446289 2.967806816101074\n",
      "loss  1065: 1.8372   grad norm: 1.6206          model param norm: 86.7565        \n",
      "\n",
      "quiet_star_policy_loss= -0.028484761714935303\n",
      "nll_loss= 1.870226502418518\n",
      "avg_std= 0.2949349582195282\n",
      "dist std min max: 0.021599359810352325 0.2949349582195282 2.9165945053100586\n",
      "hidden_states min max: -15.582219123840332 11.855232238769531\n",
      "hidden_state minus mean squared max: 219.09922790527344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.22734069824219 2.9021058082580566\n",
      "loss  1066: 1.8417   grad norm: 1.6745          model param norm: 86.7589        \n",
      "\n",
      "quiet_star_policy_loss= -0.021146977320313454\n",
      "nll_loss= 1.8624536991119385\n",
      "avg_std= 0.2934541404247284\n",
      "dist std min max: 0.02197839505970478 0.2934541404247284 2.8921940326690674\n",
      "hidden_states min max: -10.821914672851562 12.867993354797363\n",
      "hidden_state minus mean squared max: 143.15480041503906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.46919250488281 2.8266820907592773\n",
      "loss  1067: 1.8413   grad norm: 1.4134          model param norm: 86.7616        \n",
      "\n",
      "quiet_star_policy_loss= -0.0017152189975604415\n",
      "nll_loss= 1.8798134326934814\n",
      "avg_std= 0.29388687014579773\n",
      "dist std min max: 0.021965662017464638 0.29388687014579773 2.8968989849090576\n",
      "hidden_states min max: -11.087783813476562 11.040314674377441\n",
      "hidden_state minus mean squared max: 118.6763687133789\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.92076873779297 2.86679744720459\n",
      "loss  1068: 1.8781   grad norm: 1.5739          model param norm: 86.7644        \n",
      "\n",
      "quiet_star_policy_loss= -0.04126894474029541\n",
      "nll_loss= 1.8658866882324219\n",
      "avg_std= 0.29446858167648315\n",
      "dist std min max: 0.022056668996810913 0.29446858167648315 2.8751285076141357\n",
      "hidden_states min max: -9.505807876586914 10.820507049560547\n",
      "hidden_state minus mean squared max: 92.6267318725586\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.6984634399414 2.8887739181518555\n",
      "loss  1069: 1.8246   grad norm: 1.6005          model param norm: 86.7673        \n",
      "\n",
      "quiet_star_policy_loss= 0.0002561390574555844\n",
      "nll_loss= 1.8493568897247314\n",
      "avg_std= 0.2904662787914276\n",
      "dist std min max: 0.021432986482977867 0.2904662787914276 2.890016555786133\n",
      "hidden_states min max: -10.238687515258789 11.031943321228027\n",
      "hidden_state minus mean squared max: 96.00421142578125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.60889434814453 2.9169321060180664\n",
      "loss  1070: 1.8496   grad norm: 1.6109          model param norm: 86.7702        \n",
      "\n",
      "quiet_star_policy_loss= -0.011918465606868267\n",
      "nll_loss= 1.8612940311431885\n",
      "avg_std= 0.294382244348526\n",
      "dist std min max: 0.022122813388705254 0.294382244348526 2.789304256439209\n",
      "hidden_states min max: -10.202617645263672 10.943488121032715\n",
      "hidden_state minus mean squared max: 102.71017456054688\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.84854125976562 2.882535934448242\n",
      "loss  1071: 1.8494   grad norm: 1.6527          model param norm: 86.7735        \n",
      "\n",
      "quiet_star_policy_loss= -0.024692906066775322\n",
      "nll_loss= 1.861348032951355\n",
      "avg_std= 0.2922535538673401\n",
      "dist std min max: 0.01944485306739807 0.2922535538673401 2.790205955505371\n",
      "hidden_states min max: -9.461524963378906 11.1491060256958\n",
      "hidden_state minus mean squared max: 98.06890106201172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.91116333007812 3.006956100463867\n",
      "loss  1072: 1.8367   grad norm: 1.6385          model param norm: 86.7765        \n",
      "\n",
      "quiet_star_policy_loss= -0.0396999791264534\n",
      "nll_loss= 1.883029580116272\n",
      "avg_std= 0.29196441173553467\n",
      "dist std min max: 0.02259102463722229 0.29196441173553467 2.76318621635437\n",
      "hidden_states min max: -9.459976196289062 11.673641204833984\n",
      "hidden_state minus mean squared max: 88.7020492553711\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.63320922851562 2.856231689453125\n",
      "loss  1073: 1.8433   grad norm: 1.5608          model param norm: 86.7796        \n",
      "\n",
      "quiet_star_policy_loss= -0.03320961818099022\n",
      "nll_loss= 1.8723655939102173\n",
      "avg_std= 0.29324081540107727\n",
      "dist std min max: 0.02178959921002388 0.29324081540107727 2.78611159324646\n",
      "hidden_states min max: -10.412144660949707 11.224933624267578\n",
      "hidden_state minus mean squared max: 101.13773345947266\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.81387329101562 2.8815603256225586\n",
      "loss  1074: 1.8392   grad norm: 1.4610          model param norm: 86.7829        \n",
      "\n",
      "quiet_star_policy_loss= -0.008435464464128017\n",
      "nll_loss= 1.8677921295166016\n",
      "avg_std= 0.2915625274181366\n",
      "dist std min max: 0.020981648936867714 0.2915625274181366 2.829178810119629\n",
      "hidden_states min max: -9.562039375305176 10.135025024414062\n",
      "hidden_state minus mean squared max: 95.97405242919922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.51847076416016 2.925769329071045\n",
      "loss  1075: 1.8594   grad norm: 1.6038          model param norm: 86.7862        \n",
      "\n",
      "quiet_star_policy_loss= -0.0013292462099343538\n",
      "nll_loss= 1.852624535560608\n",
      "avg_std= 0.2903607189655304\n",
      "dist std min max: 0.021439768373966217 0.2903607189655304 2.816500663757324\n",
      "hidden_states min max: -15.444314002990723 10.419260025024414\n",
      "hidden_state minus mean squared max: 203.8377227783203\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.19124603271484 2.919541358947754\n",
      "loss  1076: 1.8513   grad norm: 1.6606          model param norm: 86.7895        \n",
      "\n",
      "quiet_star_policy_loss= -0.00980445183813572\n",
      "nll_loss= 1.8595770597457886\n",
      "avg_std= 0.29019978642463684\n",
      "dist std min max: 0.02115020342171192 0.29019978642463684 2.756066083908081\n",
      "hidden_states min max: -9.307862281799316 10.645591735839844\n",
      "hidden_state minus mean squared max: 80.61714935302734\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.50821685791016 2.935241222381592\n",
      "loss  1077: 1.8498   grad norm: 1.5509          model param norm: 86.7929        \n",
      "\n",
      "quiet_star_policy_loss= -0.04550383239984512\n",
      "nll_loss= 1.8807241916656494\n",
      "avg_std= 0.29087579250335693\n",
      "dist std min max: 0.02085672691464424 0.29087579250335693 2.8572146892547607\n",
      "hidden_states min max: -9.682792663574219 11.03647232055664\n",
      "hidden_state minus mean squared max: 90.8258285522461\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.59565734863281 2.9479432106018066\n",
      "loss  1078: 1.8352   grad norm: 1.5465          model param norm: 86.7961        \n",
      "\n",
      "quiet_star_policy_loss= -0.014504957012832165\n",
      "nll_loss= 1.8517154455184937\n",
      "avg_std= 0.29279249906539917\n",
      "dist std min max: 0.022510146722197533 0.29279249906539917 2.7800750732421875\n",
      "hidden_states min max: -18.997278213500977 10.831592559814453\n",
      "hidden_state minus mean squared max: 282.1841735839844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.3538589477539 2.8638343811035156\n",
      "loss  1079: 1.8372   grad norm: 1.4259          model param norm: 86.7989        \n",
      "\n",
      "quiet_star_policy_loss= -0.02356535755097866\n",
      "nll_loss= 1.8576639890670776\n",
      "avg_std= 0.28952130675315857\n",
      "dist std min max: 0.0226177666336298 0.28952130675315857 2.9873390197753906\n",
      "hidden_states min max: -14.193209648132324 10.700252532958984\n",
      "hidden_state minus mean squared max: 165.86158752441406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0881576538086 2.847982406616211\n",
      "loss  1080: 1.8341   grad norm: 1.5434          model param norm: 86.8017        \n",
      "\n",
      "quiet_star_policy_loss= -0.03984522819519043\n",
      "nll_loss= 1.8488823175430298\n",
      "avg_std= 0.28949761390686035\n",
      "dist std min max: 0.02232438698410988 0.28949761390686035 2.812321186065674\n",
      "hidden_states min max: -16.855806350708008 13.064311027526855\n",
      "hidden_state minus mean squared max: 253.99842834472656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.30123901367188 2.868044376373291\n",
      "loss  1081: 1.8090   grad norm: 1.5548          model param norm: 86.8047        \n",
      "\n",
      "quiet_star_policy_loss= -0.013612836599349976\n",
      "nll_loss= 1.8485301733016968\n",
      "avg_std= 0.28762516379356384\n",
      "dist std min max: 0.021625744178891182 0.28762516379356384 2.8601555824279785\n",
      "hidden_states min max: -9.452521324157715 11.47702693939209\n",
      "hidden_state minus mean squared max: 109.39044952392578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.50248718261719 2.9060611724853516\n",
      "loss  1082: 1.8349   grad norm: 1.7625          model param norm: 86.8079        \n",
      "\n",
      "quiet_star_policy_loss= 0.0032762170303612947\n",
      "nll_loss= 1.859360694885254\n",
      "avg_std= 0.28461894392967224\n",
      "dist std min max: 0.02183712273836136 0.28461894392967224 2.7651352882385254\n",
      "hidden_states min max: -9.420683860778809 10.478249549865723\n",
      "hidden_state minus mean squared max: 85.19188690185547\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.6683120727539 2.897134780883789\n",
      "loss  1083: 1.8626   grad norm: 1.5595          model param norm: 86.8108        \n",
      "\n",
      "quiet_star_policy_loss= -0.02931004762649536\n",
      "nll_loss= 1.8735994100570679\n",
      "avg_std= 0.2847810685634613\n",
      "dist std min max: 0.02027152106165886 0.2847810685634613 2.823885679244995\n",
      "hidden_states min max: -12.610310554504395 10.482099533081055\n",
      "hidden_state minus mean squared max: 84.42591857910156\n",
      "hidden_state minus mean divided by std max: 5.035407543182373\n",
      "log_prob min max: -103.75053405761719 2.9717283248901367\n",
      "loss  1084: 1.8443   grad norm: 1.5044          model param norm: 86.8137        \n",
      "\n",
      "quiet_star_policy_loss= -0.006032073404639959\n",
      "nll_loss= 1.8483781814575195\n",
      "avg_std= 0.288016676902771\n",
      "dist std min max: 0.022604137659072876 0.288016676902771 2.9859347343444824\n",
      "hidden_states min max: -15.064148902893066 10.201189041137695\n",
      "hidden_state minus mean squared max: 196.17959594726562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.17210388183594 2.8670549392700195\n",
      "loss  1085: 1.8423   grad norm: 1.5494          model param norm: 86.8165        \n",
      "\n",
      "quiet_star_policy_loss= -0.0014296770095825195\n",
      "nll_loss= 1.863075613975525\n",
      "avg_std= 0.28553682565689087\n",
      "dist std min max: 0.022166205570101738 0.28553682565689087 2.804598093032837\n",
      "hidden_states min max: -9.891456604003906 11.741491317749023\n",
      "hidden_state minus mean squared max: 138.3413543701172\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.99744415283203 2.843306064605713\n",
      "loss  1086: 1.8616   grad norm: 1.5347          model param norm: 86.8193        \n",
      "\n",
      "quiet_star_policy_loss= 0.005180818028748035\n",
      "nll_loss= 1.891356110572815\n",
      "avg_std= 0.2832006812095642\n",
      "dist std min max: 0.02134741097688675 0.2832006812095642 2.7771432399749756\n",
      "hidden_states min max: -13.969091415405273 10.429043769836426\n",
      "hidden_state minus mean squared max: 139.23219299316406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.0006332397461 2.9033212661743164\n",
      "loss  1087: 1.8965   grad norm: 1.7336          model param norm: 86.8221        \n",
      "\n",
      "quiet_star_policy_loss= -0.022079968824982643\n",
      "nll_loss= 1.8614290952682495\n",
      "avg_std= 0.28553733229637146\n",
      "dist std min max: 0.023029237985610962 0.28553733229637146 2.8506157398223877\n",
      "hidden_states min max: -9.438629150390625 11.52054500579834\n",
      "hidden_state minus mean squared max: 109.90994262695312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.11607360839844 2.8373208045959473\n",
      "loss  1088: 1.8393   grad norm: 1.5220          model param norm: 86.8249        \n",
      "\n",
      "quiet_star_policy_loss= 0.007049012463539839\n",
      "nll_loss= 1.8691171407699585\n",
      "avg_std= 0.28713902831077576\n",
      "dist std min max: 0.022026747465133667 0.28713902831077576 2.995460033416748\n",
      "hidden_states min max: -10.125520706176758 10.809894561767578\n",
      "hidden_state minus mean squared max: 91.68391418457031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.66094207763672 2.892357349395752\n",
      "loss  1089: 1.8762   grad norm: 1.6136          model param norm: 86.8276        \n",
      "\n",
      "quiet_star_policy_loss= -0.03405657038092613\n",
      "nll_loss= 1.86028254032135\n",
      "avg_std= 0.28630760312080383\n",
      "dist std min max: 0.022237628698349 0.28630760312080383 2.7498044967651367\n",
      "hidden_states min max: -10.10027027130127 11.351154327392578\n",
      "hidden_state minus mean squared max: 102.52955627441406\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.65180206298828 2.881415843963623\n",
      "loss  1090: 1.8262   grad norm: 1.5579          model param norm: 86.8305        \n",
      "\n",
      "quiet_star_policy_loss= 0.0040570576675236225\n",
      "nll_loss= 1.8665039539337158\n",
      "avg_std= 0.2836538851261139\n",
      "dist std min max: 0.02211398258805275 0.2836538851261139 2.652635097503662\n",
      "hidden_states min max: -17.87969398498535 9.903409957885742\n",
      "hidden_state minus mean squared max: 142.21571350097656\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.01126098632812 2.8777918815612793\n",
      "loss  1091: 1.8706   grad norm: 3.4745          model param norm: 86.8334        \n",
      "\n",
      "quiet_star_policy_loss= 0.02591913938522339\n",
      "nll_loss= 1.8493015766143799\n",
      "avg_std= 0.2877749502658844\n",
      "dist std min max: 0.022493597120046616 0.2877749502658844 2.72817063331604\n",
      "hidden_states min max: -9.659248352050781 10.744683265686035\n",
      "hidden_state minus mean squared max: 82.10880279541016\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.73658752441406 2.861501693725586\n",
      "loss  1092: 1.8752   grad norm: 1.6850          model param norm: 86.8364        \n",
      "\n",
      "quiet_star_policy_loss= -0.012400236912071705\n",
      "nll_loss= 1.8641834259033203\n",
      "avg_std= 0.28528621792793274\n",
      "dist std min max: 0.02042464166879654 0.28528621792793274 2.886188268661499\n",
      "hidden_states min max: -10.432811737060547 11.483030319213867\n",
      "hidden_state minus mean squared max: 104.61640930175781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.96379852294922 2.951878070831299\n",
      "loss  1093: 1.8518   grad norm: 1.7605          model param norm: 86.8394        \n",
      "\n",
      "quiet_star_policy_loss= -0.028238629922270775\n",
      "nll_loss= 1.8524585962295532\n",
      "avg_std= 0.28546789288520813\n",
      "dist std min max: 0.023388953879475594 0.28546789288520813 2.7506792545318604\n",
      "hidden_states min max: -9.816129684448242 11.45262336730957\n",
      "hidden_state minus mean squared max: 88.986572265625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.28783416748047 2.83058500289917\n",
      "loss  1094: 1.8242   grad norm: 1.6046          model param norm: 86.8427        \n",
      "\n",
      "quiet_star_policy_loss= -0.011003876104950905\n",
      "nll_loss= 1.8597992658615112\n",
      "avg_std= 0.2861720025539398\n",
      "dist std min max: 0.02296215482056141 0.2861720025539398 3.095360279083252\n",
      "hidden_states min max: -10.055110931396484 11.657355308532715\n",
      "hidden_state minus mean squared max: 74.37290954589844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.59410858154297 2.8541150093078613\n",
      "loss  1095: 1.8488   grad norm: 1.5699          model param norm: 86.8461        \n",
      "\n",
      "quiet_star_policy_loss= -0.029443908482789993\n",
      "nll_loss= 1.8343349695205688\n",
      "avg_std= 0.2854214608669281\n",
      "dist std min max: 0.020566590130329132 0.2854214608669281 2.80338978767395\n",
      "hidden_states min max: -9.534820556640625 10.779007911682129\n",
      "hidden_state minus mean squared max: 77.02162170410156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.6617202758789 2.948486328125\n",
      "loss  1096: 1.8049   grad norm: 1.6288          model param norm: 86.8493        \n",
      "\n",
      "quiet_star_policy_loss= -0.006209183018654585\n",
      "nll_loss= 1.8551467657089233\n",
      "avg_std= 0.28362083435058594\n",
      "dist std min max: 0.02388916350901127 0.28362083435058594 2.8135430812835693\n",
      "hidden_states min max: -9.717235565185547 10.164376258850098\n",
      "hidden_state minus mean squared max: 102.53115844726562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.84767150878906 2.803955078125\n",
      "loss  1097: 1.8489   grad norm: 1.5485          model param norm: 86.8528        \n",
      "\n",
      "quiet_star_policy_loss= -0.012356162071228027\n",
      "nll_loss= 1.8625644445419312\n",
      "avg_std= 0.28245463967323303\n",
      "dist std min max: 0.023993687704205513 0.28245463967323303 2.889073610305786\n",
      "hidden_states min max: -9.977337837219238 10.461442947387695\n",
      "hidden_state minus mean squared max: 73.88339233398438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.16644287109375 2.8073782920837402\n",
      "loss  1098: 1.8502   grad norm: 1.5860          model param norm: 86.8564        \n",
      "\n",
      "quiet_star_policy_loss= -0.0074356915429234505\n",
      "nll_loss= 1.8605297803878784\n",
      "avg_std= 0.28425708413124084\n",
      "dist std min max: 0.023199211806058884 0.28425708413124084 2.7785582542419434\n",
      "hidden_states min max: -12.797582626342773 11.137749671936035\n",
      "hidden_state minus mean squared max: 93.17716979980469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.79096984863281 2.8380794525146484\n",
      "loss  1099: 1.8531   grad norm: 1.6051          model param norm: 86.8599        \n",
      "eval loss 1.8673691749572754\n",
      "\n",
      "quiet_star_policy_loss= -0.006741762161254883\n",
      "nll_loss= 1.853980302810669\n",
      "avg_std= 0.2822716236114502\n",
      "dist std min max: 0.023164886981248856 0.2822716236114502 2.8677234649658203\n",
      "hidden_states min max: -10.081384658813477 11.661111831665039\n",
      "hidden_state minus mean squared max: 88.19852447509766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.45355987548828 2.8298988342285156\n",
      "loss  1100: 1.8472   grad norm: 1.5586          model param norm: 86.8633        \n",
      "\n",
      "quiet_star_policy_loss= -0.032081056386232376\n",
      "nll_loss= 1.8708600997924805\n",
      "avg_std= 0.28495484590530396\n",
      "dist std min max: 0.022729216143488884 0.28495484590530396 2.8255200386047363\n",
      "hidden_states min max: -11.923604965209961 12.927547454833984\n",
      "hidden_state minus mean squared max: 104.55338287353516\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.71144104003906 2.831669330596924\n",
      "loss  1101: 1.8388   grad norm: 1.5689          model param norm: 86.8668        \n",
      "\n",
      "quiet_star_policy_loss= -0.008688348345458508\n",
      "nll_loss= 1.8465168476104736\n",
      "avg_std= 0.2829992175102234\n",
      "dist std min max: 0.023685624822974205 0.2829992175102234 2.804379940032959\n",
      "hidden_states min max: -9.929065704345703 11.112003326416016\n",
      "hidden_state minus mean squared max: 93.28741455078125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.80042266845703 2.8129525184631348\n",
      "loss  1102: 1.8378   grad norm: 1.5880          model param norm: 86.8706        \n",
      "\n",
      "quiet_star_policy_loss= -0.018108868971467018\n",
      "nll_loss= 1.8886200189590454\n",
      "avg_std= 0.2813500463962555\n",
      "dist std min max: 0.021582115441560745 0.2813500463962555 2.8409106731414795\n",
      "hidden_states min max: -9.539483070373535 12.193206787109375\n",
      "hidden_state minus mean squared max: 91.92411804199219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.84207916259766 2.899719715118408\n",
      "loss  1103: 1.8705   grad norm: 1.5729          model param norm: 86.8741        \n",
      "\n",
      "quiet_star_policy_loss= -0.02847881428897381\n",
      "nll_loss= 1.8458870649337769\n",
      "avg_std= 0.2827714681625366\n",
      "dist std min max: 0.022832408547401428 0.2827714681625366 2.8324508666992188\n",
      "hidden_states min max: -10.662711143493652 12.567431449890137\n",
      "hidden_state minus mean squared max: 121.2300796508789\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.68367767333984 2.8300747871398926\n",
      "loss  1104: 1.8174   grad norm: 1.5494          model param norm: 86.8774        \n",
      "\n",
      "quiet_star_policy_loss= -0.024145472794771194\n",
      "nll_loss= 1.8643921613693237\n",
      "avg_std= 0.28315699100494385\n",
      "dist std min max: 0.022755390033125877 0.28315699100494385 2.8636744022369385\n",
      "hidden_states min max: -9.66476821899414 11.701309204101562\n",
      "hidden_state minus mean squared max: 114.53155517578125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.16150665283203 2.8529558181762695\n",
      "loss  1105: 1.8402   grad norm: 1.6530          model param norm: 86.8807        \n",
      "\n",
      "quiet_star_policy_loss= -0.0197205301374197\n",
      "nll_loss= 1.859836220741272\n",
      "avg_std= 0.28177475929260254\n",
      "dist std min max: 0.022249629721045494 0.28177475929260254 2.847806215286255\n",
      "hidden_states min max: -9.654739379882812 12.988560676574707\n",
      "hidden_state minus mean squared max: 132.17877197265625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.90594482421875 2.860352039337158\n",
      "loss  1106: 1.8401   grad norm: 1.7052          model param norm: 86.8836        \n",
      "\n",
      "quiet_star_policy_loss= -0.00833970308303833\n",
      "nll_loss= 1.8513246774673462\n",
      "avg_std= 0.28295469284057617\n",
      "dist std min max: 0.022647330537438393 0.28295469284057617 2.8432259559631348\n",
      "hidden_states min max: -9.95655632019043 12.187175750732422\n",
      "hidden_state minus mean squared max: 99.27701568603516\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.8288803100586 2.8588509559631348\n",
      "loss  1107: 1.8430   grad norm: 1.5944          model param norm: 86.8866        \n",
      "\n",
      "quiet_star_policy_loss= -0.021243084222078323\n",
      "nll_loss= 1.8666514158248901\n",
      "avg_std= 0.2813510000705719\n",
      "dist std min max: 0.023408833891153336 0.2813510000705719 3.0034120082855225\n",
      "hidden_states min max: -9.951800346374512 12.7875394821167\n",
      "hidden_state minus mean squared max: 119.56410217285156\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -102.40267181396484 2.8270654678344727\n",
      "loss  1108: 1.8454   grad norm: 1.5253          model param norm: 86.8895        \n",
      "\n",
      "quiet_star_policy_loss= 0.01208521705120802\n",
      "nll_loss= 1.8447521924972534\n",
      "avg_std= 0.28058990836143494\n",
      "dist std min max: 0.02205289900302887 0.28058990836143494 3.108567237854004\n",
      "hidden_states min max: -9.742276191711426 11.53587532043457\n",
      "hidden_state minus mean squared max: 93.8384780883789\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.2777328491211 2.881504535675049\n",
      "loss  1109: 1.8568   grad norm: 1.6063          model param norm: 86.8923        \n",
      "\n",
      "quiet_star_policy_loss= -0.009866404347121716\n",
      "nll_loss= 1.8667240142822266\n",
      "avg_std= 0.2798963785171509\n",
      "dist std min max: 0.022412553429603577 0.2798963785171509 2.9394123554229736\n",
      "hidden_states min max: -9.799263000488281 12.071189880371094\n",
      "hidden_state minus mean squared max: 113.03512573242188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.53716278076172 2.8534860610961914\n",
      "loss  1110: 1.8569   grad norm: 1.5765          model param norm: 86.8948        \n",
      "\n",
      "quiet_star_policy_loss= 0.0003665924014057964\n",
      "nll_loss= 1.874550461769104\n",
      "avg_std= 0.27940571308135986\n",
      "dist std min max: 0.022743865847587585 0.27940571308135986 2.9005093574523926\n",
      "hidden_states min max: -10.162287712097168 12.480708122253418\n",
      "hidden_state minus mean squared max: 117.80654907226562\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.64376831054688 2.8499770164489746\n",
      "loss  1111: 1.8749   grad norm: 1.5882          model param norm: 86.8972        \n",
      "\n",
      "quiet_star_policy_loss= 0.0004947871202602983\n",
      "nll_loss= 1.8392609357833862\n",
      "avg_std= 0.28091996908187866\n",
      "dist std min max: 0.022745171561837196 0.28091996908187866 2.9450628757476807\n",
      "hidden_states min max: -18.259906768798828 12.277364730834961\n",
      "hidden_state minus mean squared max: 278.7488708496094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.34772491455078 2.8573355674743652\n",
      "loss  1112: 1.8398   grad norm: 1.6966          model param norm: 86.8993        \n",
      "\n",
      "quiet_star_policy_loss= 0.008883297443389893\n",
      "nll_loss= 1.8683849573135376\n",
      "avg_std= 0.282624751329422\n",
      "dist std min max: 0.02233305387198925 0.282624751329422 2.8639073371887207\n",
      "hidden_states min max: -10.008770942687988 12.761678695678711\n",
      "hidden_state minus mean squared max: 98.24166870117188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.49494934082031 2.862499713897705\n",
      "loss  1113: 1.8773   grad norm: 1.8674          model param norm: 86.9012        \n",
      "\n",
      "quiet_star_policy_loss= -0.0009003520244732499\n",
      "nll_loss= 1.8700474500656128\n",
      "avg_std= 0.28018417954444885\n",
      "dist std min max: 0.022062579169869423 0.28018417954444885 2.8927810192108154\n",
      "hidden_states min max: -23.926551818847656 12.44127368927002\n",
      "hidden_state minus mean squared max: 643.6893310546875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.76617431640625 2.872148036956787\n",
      "loss  1114: 1.8691   grad norm: 1.5712          model param norm: 86.9034        \n",
      "\n",
      "quiet_star_policy_loss= -0.014819792471826077\n",
      "nll_loss= 1.8573044538497925\n",
      "avg_std= 0.2826220393180847\n",
      "dist std min max: 0.023557618260383606 0.2826220393180847 2.9134531021118164\n",
      "hidden_states min max: -9.642146110534668 12.29581069946289\n",
      "hidden_state minus mean squared max: 96.87488555908203\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.81929779052734 2.8268980979919434\n",
      "loss  1115: 1.8425   grad norm: 1.5998          model param norm: 86.9054        \n",
      "\n",
      "quiet_star_policy_loss= -0.005628657527267933\n",
      "nll_loss= 1.860897421836853\n",
      "avg_std= 0.28205031156539917\n",
      "dist std min max: 0.023019714280962944 0.28205031156539917 2.882347583770752\n",
      "hidden_states min max: -10.276849746704102 12.49191665649414\n",
      "hidden_state minus mean squared max: 121.13990020751953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85871887207031 2.8352842330932617\n",
      "loss  1116: 1.8553   grad norm: 1.6256          model param norm: 86.9076        \n",
      "\n",
      "quiet_star_policy_loss= -0.019578492268919945\n",
      "nll_loss= 1.8548411130905151\n",
      "avg_std= 0.2788248658180237\n",
      "dist std min max: 0.02181856520473957 0.2788248658180237 2.940408229827881\n",
      "hidden_states min max: -12.813770294189453 11.883002281188965\n",
      "hidden_state minus mean squared max: 210.33221435546875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.20692443847656 2.9016504287719727\n",
      "loss  1117: 1.8353   grad norm: 1.6137          model param norm: 86.9099        \n",
      "\n",
      "quiet_star_policy_loss= -0.005144071765244007\n",
      "nll_loss= 1.8499788045883179\n",
      "avg_std= 0.2830343544483185\n",
      "dist std min max: 0.022131316363811493 0.2830343544483185 2.905686378479004\n",
      "hidden_states min max: -9.684934616088867 12.633356094360352\n",
      "hidden_state minus mean squared max: 110.29302978515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.62254333496094 2.871273994445801\n",
      "loss  1118: 1.8448   grad norm: 1.6694          model param norm: 86.9122        \n",
      "\n",
      "quiet_star_policy_loss= -0.011212361976504326\n",
      "nll_loss= 1.8314809799194336\n",
      "avg_std= 0.27806708216667175\n",
      "dist std min max: 0.02359980344772339 0.27806708216667175 2.8835465908050537\n",
      "hidden_states min max: -18.26744270324707 11.27978515625\n",
      "hidden_state minus mean squared max: 266.97637939453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.32615661621094 2.8229918479919434\n",
      "loss  1119: 1.8203   grad norm: 3.6072          model param norm: 86.9145        \n",
      "\n",
      "quiet_star_policy_loss= -0.009223349392414093\n",
      "nll_loss= 1.861246943473816\n",
      "avg_std= 0.2788900136947632\n",
      "dist std min max: 0.021629661321640015 0.2788900136947632 2.8834283351898193\n",
      "hidden_states min max: -9.793396949768066 12.748889923095703\n",
      "hidden_state minus mean squared max: 129.33631896972656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.54364013671875 2.9117398262023926\n",
      "loss  1120: 1.8520   grad norm: 1.6362          model param norm: 86.9166        \n",
      "\n",
      "quiet_star_policy_loss= 0.011412906460464\n",
      "nll_loss= 1.8478363752365112\n",
      "avg_std= 0.2777964472770691\n",
      "dist std min max: 0.021973632276058197 0.2777964472770691 2.8692634105682373\n",
      "hidden_states min max: -9.868965148925781 12.575186729431152\n",
      "hidden_state minus mean squared max: 111.75076293945312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.44458770751953 2.8934741020202637\n",
      "loss  1121: 1.8592   grad norm: 1.6093          model param norm: 86.9189        \n",
      "\n",
      "quiet_star_policy_loss= -0.03624289110302925\n",
      "nll_loss= 1.858123779296875\n",
      "avg_std= 0.2821085453033447\n",
      "dist std min max: 0.022356532514095306 0.2821085453033447 2.8712785243988037\n",
      "hidden_states min max: -9.943105697631836 11.56523323059082\n",
      "hidden_state minus mean squared max: 97.98625946044922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.07209014892578 2.8588314056396484\n",
      "loss  1122: 1.8219   grad norm: 1.6064          model param norm: 86.9209        \n",
      "\n",
      "quiet_star_policy_loss= -0.026374876499176025\n",
      "nll_loss= 1.8517102003097534\n",
      "avg_std= 0.27785876393318176\n",
      "dist std min max: 0.022503245621919632 0.27785876393318176 2.964803695678711\n",
      "hidden_states min max: -10.542014122009277 12.992995262145996\n",
      "hidden_state minus mean squared max: 106.89469909667969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.65634155273438 2.863124370574951\n",
      "loss  1123: 1.8253   grad norm: 1.5397          model param norm: 86.9228        \n",
      "\n",
      "quiet_star_policy_loss= -0.026699675247073174\n",
      "nll_loss= 1.8639973402023315\n",
      "avg_std= 0.2776126265525818\n",
      "dist std min max: 0.02260747365653515 0.2776126265525818 2.836029529571533\n",
      "hidden_states min max: -13.875520706176758 12.390257835388184\n",
      "hidden_state minus mean squared max: 141.05606079101562\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.00715637207031 2.858304500579834\n",
      "loss  1124: 1.8373   grad norm: 1.5511          model param norm: 86.9244        \n",
      "\n",
      "quiet_star_policy_loss= -0.019116127863526344\n",
      "nll_loss= 1.8519432544708252\n",
      "avg_std= 0.27775701880455017\n",
      "dist std min max: 0.0220454391092062 0.27775701880455017 2.784365653991699\n",
      "hidden_states min max: -10.30518913269043 13.337289810180664\n",
      "hidden_state minus mean squared max: 110.7278060913086\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.4225082397461 2.87608003616333\n",
      "loss  1125: 1.8328   grad norm: 1.6873          model param norm: 86.9261        \n",
      "\n",
      "quiet_star_policy_loss= -0.00904551800340414\n",
      "nll_loss= 1.8477009534835815\n",
      "avg_std= 0.27540260553359985\n",
      "dist std min max: 0.02276364341378212 0.27540260553359985 2.8191046714782715\n",
      "hidden_states min max: -25.929500579833984 12.466838836669922\n",
      "hidden_state minus mean squared max: 773.6476440429688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.85814666748047 2.8483519554138184\n",
      "loss  1126: 1.8387   grad norm: 1.6256          model param norm: 86.9280        \n",
      "\n",
      "quiet_star_policy_loss= -0.022150827571749687\n",
      "nll_loss= 1.8699613809585571\n",
      "avg_std= 0.27711302042007446\n",
      "dist std min max: 0.022364523261785507 0.27711302042007446 3.0886385440826416\n",
      "hidden_states min max: -10.24117660522461 12.146655082702637\n",
      "hidden_state minus mean squared max: 96.38396453857422\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.20529174804688 2.8707327842712402\n",
      "loss  1127: 1.8478   grad norm: 1.6498          model param norm: 86.9300        \n",
      "\n",
      "quiet_star_policy_loss= -0.011404124088585377\n",
      "nll_loss= 1.8623332977294922\n",
      "avg_std= 0.275738388299942\n",
      "dist std min max: 0.02195620909333229 0.275738388299942 2.7517294883728027\n",
      "hidden_states min max: -16.100765228271484 12.361703872680664\n",
      "hidden_state minus mean squared max: 130.69329833984375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.96903228759766 2.8875341415405273\n",
      "loss  1128: 1.8509   grad norm: 1.6784          model param norm: 86.9320        \n",
      "\n",
      "quiet_star_policy_loss= -0.018356038257479668\n",
      "nll_loss= 1.84066903591156\n",
      "avg_std= 0.27578774094581604\n",
      "dist std min max: 0.021422674879431725 0.27578774094581604 2.746401309967041\n",
      "hidden_states min max: -10.290112495422363 11.720751762390137\n",
      "hidden_state minus mean squared max: 127.22850036621094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.6777114868164 2.912616729736328\n",
      "loss  1129: 1.8223   grad norm: 1.5947          model param norm: 86.9338        \n",
      "\n",
      "quiet_star_policy_loss= 0.0011115194065496325\n",
      "nll_loss= 1.8437137603759766\n",
      "avg_std= 0.27681782841682434\n",
      "dist std min max: 0.021911388263106346 0.27681782841682434 2.7845299243927\n",
      "hidden_states min max: -9.787940979003906 11.74673843383789\n",
      "hidden_state minus mean squared max: 96.17293548583984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.49657440185547 2.882828712463379\n",
      "loss  1130: 1.8448   grad norm: 1.7426          model param norm: 86.9358        \n",
      "\n",
      "quiet_star_policy_loss= -0.020199045538902283\n",
      "nll_loss= 1.846281886100769\n",
      "avg_std= 0.2747645676136017\n",
      "dist std min max: 0.021268241107463837 0.2747645676136017 2.704782724380493\n",
      "hidden_states min max: -12.238973617553711 11.128288269042969\n",
      "hidden_state minus mean squared max: 100.4158935546875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.83724212646484 2.898106575012207\n",
      "loss  1131: 1.8261   grad norm: 1.7019          model param norm: 86.9375        \n",
      "\n",
      "quiet_star_policy_loss= -0.0200347900390625\n",
      "nll_loss= 1.8454664945602417\n",
      "avg_std= 0.2743770182132721\n",
      "dist std min max: 0.02092558704316616 0.2743770182132721 2.6942830085754395\n",
      "hidden_states min max: -9.89968204498291 11.64956283569336\n",
      "hidden_state minus mean squared max: 81.23828887939453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1318359375 2.9424519538879395\n",
      "loss  1132: 1.8254   grad norm: 1.7090          model param norm: 86.9392        \n",
      "\n",
      "quiet_star_policy_loss= -0.02702122926712036\n",
      "nll_loss= 1.8478691577911377\n",
      "avg_std= 0.2742357850074768\n",
      "dist std min max: 0.022574683651328087 0.2742357850074768 2.7218055725097656\n",
      "hidden_states min max: -18.639705657958984 12.497093200683594\n",
      "hidden_state minus mean squared max: 279.4177551269531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.34891510009766 2.8613133430480957\n",
      "loss  1133: 1.8208   grad norm: 1.5418          model param norm: 86.9409        \n",
      "\n",
      "quiet_star_policy_loss= -0.003874069545418024\n",
      "nll_loss= 1.828181266784668\n",
      "avg_std= 0.2747410237789154\n",
      "dist std min max: 0.020409129559993744 0.2747410237789154 2.727108955383301\n",
      "hidden_states min max: -10.023547172546387 11.287741661071777\n",
      "hidden_state minus mean squared max: 101.2660140991211\n",
      "hidden_state minus mean divided by std max: 5.166576862335205\n",
      "log_prob min max: -103.0548324584961 2.9529623985290527\n",
      "loss  1134: 1.8243   grad norm: 1.5373          model param norm: 86.9424        \n",
      "\n",
      "quiet_star_policy_loss= -0.025080328807234764\n",
      "nll_loss= 1.8537657260894775\n",
      "avg_std= 0.27319276332855225\n",
      "dist std min max: 0.0207989402115345 0.27319276332855225 2.8312032222747803\n",
      "hidden_states min max: -10.063329696655273 11.526952743530273\n",
      "hidden_state minus mean squared max: 92.33873748779297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7869873046875 2.9423370361328125\n",
      "loss  1135: 1.8287   grad norm: 1.6663          model param norm: 86.9440        \n",
      "\n",
      "quiet_star_policy_loss= -0.037861257791519165\n",
      "nll_loss= 1.8473573923110962\n",
      "avg_std= 0.27252376079559326\n",
      "dist std min max: 0.020768608897924423 0.27252376079559326 2.9470419883728027\n",
      "hidden_states min max: -10.480297088623047 11.407520294189453\n",
      "hidden_state minus mean squared max: 87.54581451416016\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.71924591064453 2.914851665496826\n",
      "loss  1136: 1.8095   grad norm: 1.5669          model param norm: 86.9455        \n",
      "\n",
      "quiet_star_policy_loss= -0.012683677487075329\n",
      "nll_loss= 1.8565117120742798\n",
      "avg_std= 0.2725439667701721\n",
      "dist std min max: 0.020678039640188217 0.2725439667701721 2.6742148399353027\n",
      "hidden_states min max: -10.293693542480469 11.39644718170166\n",
      "hidden_state minus mean squared max: 129.51559448242188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.51254272460938 2.9351234436035156\n",
      "loss  1137: 1.8438   grad norm: 1.6700          model param norm: 86.9468        \n",
      "\n",
      "quiet_star_policy_loss= 0.0014613628154620528\n",
      "nll_loss= 1.8434807062149048\n",
      "avg_std= 0.27281802892684937\n",
      "dist std min max: 0.021605344489216805 0.27281802892684937 2.6484363079071045\n",
      "hidden_states min max: -9.807149887084961 11.487115859985352\n",
      "hidden_state minus mean squared max: 82.0069351196289\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85946655273438 2.89573335647583\n",
      "loss  1138: 1.8449   grad norm: 1.5641          model param norm: 86.9484        \n",
      "\n",
      "quiet_star_policy_loss= -0.04837484285235405\n",
      "nll_loss= 1.8392879962921143\n",
      "avg_std= 0.2748280167579651\n",
      "dist std min max: 0.021405959501862526 0.2748280167579651 2.728614330291748\n",
      "hidden_states min max: -9.828423500061035 10.810345649719238\n",
      "hidden_state minus mean squared max: 84.09235382080078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.36849975585938 2.909451961517334\n",
      "loss  1139: 1.7909   grad norm: 1.5555          model param norm: 86.9499        \n",
      "\n",
      "quiet_star_policy_loss= -0.029341256245970726\n",
      "nll_loss= 1.854365587234497\n",
      "avg_std= 0.2714833617210388\n",
      "dist std min max: 0.021159013733267784 0.2714833617210388 2.951085329055786\n",
      "hidden_states min max: -9.858224868774414 11.674524307250977\n",
      "hidden_state minus mean squared max: 82.48162841796875\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.5066146850586 2.9325408935546875\n",
      "loss  1140: 1.8250   grad norm: 1.6935          model param norm: 86.9515        \n",
      "\n",
      "quiet_star_policy_loss= 0.003931927960366011\n",
      "nll_loss= 1.8546556234359741\n",
      "avg_std= 0.27214691042900085\n",
      "dist std min max: 0.021357601508498192 0.27214691042900085 2.796902656555176\n",
      "hidden_states min max: -10.793800354003906 12.737506866455078\n",
      "hidden_state minus mean squared max: 100.069091796875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.79876708984375 2.9133663177490234\n",
      "loss  1141: 1.8586   grad norm: 1.6629          model param norm: 86.9534        \n",
      "\n",
      "quiet_star_policy_loss= -0.001981019973754883\n",
      "nll_loss= 1.8634296655654907\n",
      "avg_std= 0.26993685960769653\n",
      "dist std min max: 0.021545855328440666 0.26993685960769653 2.5761547088623047\n",
      "hidden_states min max: -10.173023223876953 11.147536277770996\n",
      "hidden_state minus mean squared max: 90.58895874023438\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.22378540039062 2.907787799835205\n",
      "loss  1142: 1.8614   grad norm: 1.6356          model param norm: 86.9554        \n",
      "\n",
      "quiet_star_policy_loss= -0.003518527839332819\n",
      "nll_loss= 1.8429559469223022\n",
      "avg_std= 0.2710694968700409\n",
      "dist std min max: 0.02201901562511921 0.2710694968700409 2.607631206512451\n",
      "hidden_states min max: -9.807750701904297 11.78162670135498\n",
      "hidden_state minus mean squared max: 92.10618591308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0190658569336 2.8831095695495605\n",
      "loss  1143: 1.8394   grad norm: 1.6793          model param norm: 86.9577        \n",
      "\n",
      "quiet_star_policy_loss= -0.007933089509606361\n",
      "nll_loss= 1.8577600717544556\n",
      "avg_std= 0.2710396647453308\n",
      "dist std min max: 0.021182838827371597 0.2710396647453308 2.670732259750366\n",
      "hidden_states min max: -9.985626220703125 12.542824745178223\n",
      "hidden_state minus mean squared max: 95.98248291015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.32791137695312 2.91176700592041\n",
      "loss  1144: 1.8498   grad norm: 1.6474          model param norm: 86.9598        \n",
      "\n",
      "quiet_star_policy_loss= -0.026637209579348564\n",
      "nll_loss= 1.846294641494751\n",
      "avg_std= 0.27061566710472107\n",
      "dist std min max: 0.02149013616144657 0.27061566710472107 2.696441173553467\n",
      "hidden_states min max: -9.883796691894531 12.190987586975098\n",
      "hidden_state minus mean squared max: 95.87232208251953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.70571899414062 2.8954548835754395\n",
      "loss  1145: 1.8197   grad norm: 1.6116          model param norm: 86.9619        \n",
      "\n",
      "quiet_star_policy_loss= -0.006843626499176025\n",
      "nll_loss= 1.8398834466934204\n",
      "avg_std= 0.2710053324699402\n",
      "dist std min max: 0.021480955183506012 0.2710053324699402 2.7160353660583496\n",
      "hidden_states min max: -10.30282211303711 14.017431259155273\n",
      "hidden_state minus mean squared max: 128.59725952148438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.81185150146484 2.9053096771240234\n",
      "loss  1146: 1.8330   grad norm: 1.6897          model param norm: 86.9638        \n",
      "\n",
      "quiet_star_policy_loss= -0.0008125559543259442\n",
      "nll_loss= 1.8673956394195557\n",
      "avg_std= 0.2707558870315552\n",
      "dist std min max: 0.02250748500227928 0.2707558870315552 2.6738381385803223\n",
      "hidden_states min max: -10.31452465057373 10.464351654052734\n",
      "hidden_state minus mean squared max: 74.94322204589844\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.61239624023438 2.8604917526245117\n",
      "loss  1147: 1.8666   grad norm: 3.4580          model param norm: 86.9659        \n",
      "\n",
      "quiet_star_policy_loss= -0.0372486375272274\n",
      "nll_loss= 1.8484249114990234\n",
      "avg_std= 0.2704700827598572\n",
      "dist std min max: 0.020923051983118057 0.2704700827598572 2.695158004760742\n",
      "hidden_states min max: -10.110090255737305 13.43420696258545\n",
      "hidden_state minus mean squared max: 110.4728775024414\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.975875854492188 2.9445152282714844\n",
      "loss  1148: 1.8112   grad norm: 1.7319          model param norm: 86.9682        \n",
      "\n",
      "quiet_star_policy_loss= -0.010568994097411633\n",
      "nll_loss= 1.8343137502670288\n",
      "avg_std= 0.2710258662700653\n",
      "dist std min max: 0.0207656379789114 0.2710258662700653 2.7406182289123535\n",
      "hidden_states min max: -10.6782865524292 10.93685245513916\n",
      "hidden_state minus mean squared max: 84.49159240722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5640869140625 2.927597999572754\n",
      "loss  1149: 1.8237   grad norm: 1.6119          model param norm: 86.9701        \n",
      "\n",
      "quiet_star_policy_loss= 0.010573625564575195\n",
      "nll_loss= 1.8497581481933594\n",
      "avg_std= 0.2694014012813568\n",
      "dist std min max: 0.020829588174819946 0.2694014012813568 2.7473132610321045\n",
      "hidden_states min max: -10.317571640014648 13.34885025024414\n",
      "hidden_state minus mean squared max: 115.24302673339844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.10292053222656 2.9405689239501953\n",
      "loss  1150: 1.8603   grad norm: 1.6238          model param norm: 86.9720        \n",
      "\n",
      "quiet_star_policy_loss= -0.012287181802093983\n",
      "nll_loss= 1.8486387729644775\n",
      "avg_std= 0.2706269919872284\n",
      "dist std min max: 0.02125060185790062 0.2706269919872284 3.119891405105591\n",
      "hidden_states min max: -9.920797348022461 11.021885871887207\n",
      "hidden_state minus mean squared max: 118.1196060180664\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.15406799316406 2.9185543060302734\n",
      "loss  1151: 1.8364   grad norm: 1.7990          model param norm: 86.9738        \n",
      "\n",
      "quiet_star_policy_loss= -0.004209053702652454\n",
      "nll_loss= 1.8492730855941772\n",
      "avg_std= 0.27090540528297424\n",
      "dist std min max: 0.02070481702685356 0.27090540528297424 2.7317423820495605\n",
      "hidden_states min max: -10.076095581054688 12.339189529418945\n",
      "hidden_state minus mean squared max: 91.35570526123047\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.38310241699219 2.9419846534729004\n",
      "loss  1152: 1.8451   grad norm: 1.6188          model param norm: 86.9760        \n",
      "\n",
      "quiet_star_policy_loss= -0.026897573843598366\n",
      "nll_loss= 1.8312816619873047\n",
      "avg_std= 0.27424055337905884\n",
      "dist std min max: 0.020211465656757355 0.27424055337905884 2.742586851119995\n",
      "hidden_states min max: -9.787455558776855 12.455597877502441\n",
      "hidden_state minus mean squared max: 100.1811294555664\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.55335235595703 2.9462080001831055\n",
      "loss  1153: 1.8044   grad norm: 1.7354          model param norm: 86.9781        \n",
      "\n",
      "quiet_star_policy_loss= 0.017900411039590836\n",
      "nll_loss= 1.8467432260513306\n",
      "avg_std= 0.27266570925712585\n",
      "dist std min max: 0.019949423149228096 0.27266570925712585 2.8262529373168945\n",
      "hidden_states min max: -9.903131484985352 12.289912223815918\n",
      "hidden_state minus mean squared max: 122.91925811767578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.50940704345703 2.9672741889953613\n",
      "loss  1154: 1.8646   grad norm: 1.5937          model param norm: 86.9805        \n",
      "\n",
      "quiet_star_policy_loss= -0.0257315281778574\n",
      "nll_loss= 1.8383091688156128\n",
      "avg_std= 0.27259448170661926\n",
      "dist std min max: 0.019982311874628067 0.27259448170661926 2.82942271232605\n",
      "hidden_states min max: -20.398544311523438 11.403782844543457\n",
      "hidden_state minus mean squared max: 563.32080078125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.69950103759766 2.9826431274414062\n",
      "loss  1155: 1.8126   grad norm: 1.7684          model param norm: 86.9828        \n",
      "\n",
      "quiet_star_policy_loss= -0.012968945316970348\n",
      "nll_loss= 1.8551063537597656\n",
      "avg_std= 0.27147945761680603\n",
      "dist std min max: 0.019549621269106865 0.27147945761680603 2.7744545936584473\n",
      "hidden_states min max: -18.54303741455078 12.561924934387207\n",
      "hidden_state minus mean squared max: 345.145751953125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.4545669555664 2.994503974914551\n",
      "loss  1156: 1.8421   grad norm: 1.5992          model param norm: 86.9854        \n",
      "\n",
      "quiet_star_policy_loss= -0.02056485414505005\n",
      "nll_loss= 1.8580131530761719\n",
      "avg_std= 0.27204638719558716\n",
      "dist std min max: 0.019948311150074005 0.27204638719558716 2.7881853580474854\n",
      "hidden_states min max: -9.791366577148438 12.600316047668457\n",
      "hidden_state minus mean squared max: 97.65038299560547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.22791290283203 2.988201141357422\n",
      "loss  1157: 1.8374   grad norm: 1.6898          model param norm: 86.9882        \n",
      "\n",
      "quiet_star_policy_loss= -0.02120201662182808\n",
      "nll_loss= 1.845227837562561\n",
      "avg_std= 0.2736014723777771\n",
      "dist std min max: 0.020244097337126732 0.2736014723777771 3.3438217639923096\n",
      "hidden_states min max: -10.769014358520508 13.293571472167969\n",
      "hidden_state minus mean squared max: 114.3587417602539\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.66222381591797 2.9722352027893066\n",
      "loss  1158: 1.8240   grad norm: 1.5838          model param norm: 86.9909        \n",
      "\n",
      "quiet_star_policy_loss= 0.007685613818466663\n",
      "nll_loss= 1.8338507413864136\n",
      "avg_std= 0.2718776762485504\n",
      "dist std min max: 0.01895609125494957 0.2718776762485504 2.744291305541992\n",
      "hidden_states min max: -10.08620834350586 12.271052360534668\n",
      "hidden_state minus mean squared max: 89.5307846069336\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -13.428986549377441 3.0322461128234863\n",
      "loss  1159: 1.8415   grad norm: 1.5678          model param norm: 86.9938        \n",
      "\n",
      "quiet_star_policy_loss= -0.04351387172937393\n",
      "nll_loss= 1.839521050453186\n",
      "avg_std= 0.2720509171485901\n",
      "dist std min max: 0.018933726474642754 0.2720509171485901 2.7700932025909424\n",
      "hidden_states min max: -10.147369384765625 11.323923110961914\n",
      "hidden_state minus mean squared max: 88.65431213378906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.04187774658203 3.041781425476074\n",
      "loss  1160: 1.7960   grad norm: 1.5505          model param norm: 86.9969        \n",
      "\n",
      "quiet_star_policy_loss= -0.004250484984368086\n",
      "nll_loss= 1.8414363861083984\n",
      "avg_std= 0.2711653411388397\n",
      "dist std min max: 0.019071834161877632 0.2711653411388397 2.8223955631256104\n",
      "hidden_states min max: -10.365384101867676 12.147476196289062\n",
      "hidden_state minus mean squared max: 149.7246856689453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.59095001220703 2.9971017837524414\n",
      "loss  1161: 1.8372   grad norm: 1.7286          model param norm: 87.0000        \n",
      "\n",
      "quiet_star_policy_loss= 0.0034756602253764868\n",
      "nll_loss= 1.8514244556427002\n",
      "avg_std= 0.27123957872390747\n",
      "dist std min max: 0.018886040896177292 0.27123957872390747 3.1380186080932617\n",
      "hidden_states min max: -9.925771713256836 11.893781661987305\n",
      "hidden_state minus mean squared max: 87.07318878173828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4344711303711 3.04738187789917\n",
      "loss  1162: 1.8549   grad norm: 1.5986          model param norm: 87.0031        \n",
      "\n",
      "quiet_star_policy_loss= 0.00644110469147563\n",
      "nll_loss= 1.8622610569000244\n",
      "avg_std= 0.27117013931274414\n",
      "dist std min max: 0.01851961761713028 0.27117013931274414 2.7806835174560547\n",
      "hidden_states min max: -9.764758110046387 13.457454681396484\n",
      "hidden_state minus mean squared max: 107.12993621826172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.64222717285156 3.0445990562438965\n",
      "loss  1163: 1.8687   grad norm: 1.7013          model param norm: 87.0061        \n",
      "\n",
      "quiet_star_policy_loss= 0.00023989677720237523\n",
      "nll_loss= 1.8474997282028198\n",
      "avg_std= 0.274358332157135\n",
      "dist std min max: 0.01874549500644207 0.274358332157135 2.7790682315826416\n",
      "hidden_states min max: -9.761425018310547 11.766977310180664\n",
      "hidden_state minus mean squared max: 86.33213806152344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.36052703857422 3.0237321853637695\n",
      "loss  1164: 1.8477   grad norm: 1.8191          model param norm: 87.0088        \n",
      "\n",
      "quiet_star_policy_loss= -0.023773331195116043\n",
      "nll_loss= 1.854861855506897\n",
      "avg_std= 0.27178868651390076\n",
      "dist std min max: 0.018416307866573334 0.27178868651390076 2.7785332202911377\n",
      "hidden_states min max: -13.719043731689453 11.971776962280273\n",
      "hidden_state minus mean squared max: 188.9918670654297\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -104.15343475341797 3.054532051086426\n",
      "loss  1165: 1.8311   grad norm: 1.5990          model param norm: 87.0115        \n",
      "\n",
      "quiet_star_policy_loss= -0.03257658705115318\n",
      "nll_loss= 1.8439902067184448\n",
      "avg_std= 0.27534255385398865\n",
      "dist std min max: 0.018480785191059113 0.27534255385398865 2.791158676147461\n",
      "hidden_states min max: -9.810737609863281 12.781244277954102\n",
      "hidden_state minus mean squared max: 92.88079833984375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77739715576172 3.0322790145874023\n",
      "loss  1166: 1.8114   grad norm: 1.6654          model param norm: 87.0143        \n",
      "\n",
      "quiet_star_policy_loss= -0.005236291792243719\n",
      "nll_loss= 1.8452786207199097\n",
      "avg_std= 0.276041716337204\n",
      "dist std min max: 0.018641548231244087 0.276041716337204 2.7549328804016113\n",
      "hidden_states min max: -9.901200294494629 11.90350341796875\n",
      "hidden_state minus mean squared max: 95.52947998046875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.21231079101562 3.049161434173584\n",
      "loss  1167: 1.8400   grad norm: 1.5526          model param norm: 87.0169        \n",
      "\n",
      "quiet_star_policy_loss= 0.015435958281159401\n",
      "nll_loss= 1.8360790014266968\n",
      "avg_std= 0.2780023217201233\n",
      "dist std min max: 0.019314570352435112 0.2780023217201233 3.095336675643921\n",
      "hidden_states min max: -9.949909210205078 11.528892517089844\n",
      "hidden_state minus mean squared max: 82.77137756347656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.777902603149414 2.988760471343994\n",
      "loss  1168: 1.8515   grad norm: 1.6724          model param norm: 87.0193        \n",
      "\n",
      "quiet_star_policy_loss= -0.018504519015550613\n",
      "nll_loss= 1.8578262329101562\n",
      "avg_std= 0.2769530713558197\n",
      "dist std min max: 0.018906693905591965 0.2769530713558197 3.2629716396331787\n",
      "hidden_states min max: -10.125812530517578 13.182820320129395\n",
      "hidden_state minus mean squared max: 116.0621109008789\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7651138305664 3.037093162536621\n",
      "loss  1169: 1.8393   grad norm: 1.8614          model param norm: 87.0214        \n",
      "\n",
      "quiet_star_policy_loss= -0.005592608358711004\n",
      "nll_loss= 1.8432430028915405\n",
      "avg_std= 0.2757658064365387\n",
      "dist std min max: 0.018257496878504753 0.2757658064365387 2.7243003845214844\n",
      "hidden_states min max: -9.702670097351074 12.108244895935059\n",
      "hidden_state minus mean squared max: 89.20994567871094\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.38294219970703 3.0696659088134766\n",
      "loss  1170: 1.8377   grad norm: 1.5912          model param norm: 87.0236        \n",
      "\n",
      "quiet_star_policy_loss= -0.0092434948310256\n",
      "nll_loss= 1.859408974647522\n",
      "avg_std= 0.2756187617778778\n",
      "dist std min max: 0.017812056466937065 0.2756187617778778 2.6857798099517822\n",
      "hidden_states min max: -9.734055519104004 13.163063049316406\n",
      "hidden_state minus mean squared max: 104.0885238647461\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.50482177734375 3.1088509559631348\n",
      "loss  1171: 1.8502   grad norm: 1.6916          model param norm: 87.0258        \n",
      "\n",
      "quiet_star_policy_loss= 0.0004001975175924599\n",
      "nll_loss= 1.8431342840194702\n",
      "avg_std= 0.27555161714553833\n",
      "dist std min max: 0.018115224316716194 0.27555161714553833 2.667390823364258\n",
      "hidden_states min max: -10.075004577636719 11.114133834838867\n",
      "hidden_state minus mean squared max: 78.46493530273438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.32088470458984 3.0769500732421875\n",
      "loss  1172: 1.8435   grad norm: 1.6245          model param norm: 87.0280        \n",
      "\n",
      "quiet_star_policy_loss= -0.007465946953743696\n",
      "nll_loss= 1.8618751764297485\n",
      "avg_std= 0.2759414315223694\n",
      "dist std min max: 0.017307855188846588 0.2759414315223694 2.6936042308807373\n",
      "hidden_states min max: -10.715852737426758 10.530253410339355\n",
      "hidden_state minus mean squared max: 81.30192565917969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77472686767578 3.071444034576416\n",
      "loss  1173: 1.8544   grad norm: 1.8015          model param norm: 87.0302        \n",
      "\n",
      "quiet_star_policy_loss= -0.005715108010917902\n",
      "nll_loss= 1.8438678979873657\n",
      "avg_std= 0.2757287919521332\n",
      "dist std min max: 0.016928909346461296 0.2757287919521332 2.753303050994873\n",
      "hidden_states min max: -9.69841480255127 12.380084991455078\n",
      "hidden_state minus mean squared max: 102.52287292480469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.69227600097656 3.1452198028564453\n",
      "loss  1174: 1.8382   grad norm: 1.6237          model param norm: 87.0320        \n",
      "\n",
      "quiet_star_policy_loss= -0.050984855741262436\n",
      "nll_loss= 1.8601378202438354\n",
      "avg_std= 0.27670612931251526\n",
      "dist std min max: 0.020615700632333755 0.27670612931251526 2.62439227104187\n",
      "hidden_states min max: -9.610758781433105 10.244619369506836\n",
      "hidden_state minus mean squared max: 75.96931457519531\n",
      "hidden_state minus mean divided by std max: 4.957176208496094\n",
      "log_prob min max: -103.54354858398438 2.940497875213623\n",
      "loss  1175: 1.8092   grad norm: 3.3322          model param norm: 87.0339        \n",
      "\n",
      "quiet_star_policy_loss= 0.0033622384071350098\n",
      "nll_loss= 1.8218754529953003\n",
      "avg_std= 0.2754758298397064\n",
      "dist std min max: 0.016279226168990135 0.2754758298397064 2.643684148788452\n",
      "hidden_states min max: -13.430252075195312 10.540746688842773\n",
      "hidden_state minus mean squared max: 164.3210906982422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08348846435547 3.1949357986450195\n",
      "loss  1176: 1.8252   grad norm: 1.6306          model param norm: 87.0363        \n",
      "\n",
      "quiet_star_policy_loss= -0.02510862983763218\n",
      "nll_loss= 1.8487614393234253\n",
      "avg_std= 0.27400344610214233\n",
      "dist std min max: 0.01612747274339199 0.27400344610214233 2.6664745807647705\n",
      "hidden_states min max: -9.632691383361816 11.01205825805664\n",
      "hidden_state minus mean squared max: 85.10737609863281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.08601379394531 3.1832542419433594\n",
      "loss  1177: 1.8237   grad norm: 1.6494          model param norm: 87.0385        \n",
      "\n",
      "quiet_star_policy_loss= -0.0078209163621068\n",
      "nll_loss= 1.8402035236358643\n",
      "avg_std= 0.27475109696388245\n",
      "dist std min max: 0.01644282229244709 0.27475109696388245 2.890558958053589\n",
      "hidden_states min max: -12.127166748046875 11.464187622070312\n",
      "hidden_state minus mean squared max: 286.9698181152344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.36226654052734 3.0894479751586914\n",
      "loss  1178: 1.8324   grad norm: 1.5802          model param norm: 87.0409        \n",
      "\n",
      "quiet_star_policy_loss= -0.0047058346681296825\n",
      "nll_loss= 1.837624192237854\n",
      "avg_std= 0.2742950916290283\n",
      "dist std min max: 0.018017826601862907 0.2742950916290283 2.693089246749878\n",
      "hidden_states min max: -9.597206115722656 11.371143341064453\n",
      "hidden_state minus mean squared max: 99.96849822998047\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -14.117140769958496 3.0930538177490234\n",
      "loss  1179: 1.8329   grad norm: 1.5963          model param norm: 87.0435        \n",
      "\n",
      "quiet_star_policy_loss= 0.0037664533592760563\n",
      "nll_loss= 1.8510067462921143\n",
      "avg_std= 0.27508091926574707\n",
      "dist std min max: 0.015865057706832886 0.27508091926574707 2.680680274963379\n",
      "hidden_states min max: -10.015308380126953 11.041471481323242\n",
      "hidden_state minus mean squared max: 126.59844970703125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.58248901367188 3.1917667388916016\n",
      "loss  1180: 1.8548   grad norm: 1.6713          model param norm: 87.0462        \n",
      "\n",
      "quiet_star_policy_loss= -0.03353632614016533\n",
      "nll_loss= 1.8415883779525757\n",
      "avg_std= 0.27580001950263977\n",
      "dist std min max: 0.015727752819657326 0.27580001950263977 2.715228319168091\n",
      "hidden_states min max: -9.563786506652832 11.199309349060059\n",
      "hidden_state minus mean squared max: 101.31979370117188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.27609252929688 3.1588501930236816\n",
      "loss  1181: 1.8081   grad norm: 1.6576          model param norm: 87.0486        \n",
      "\n",
      "quiet_star_policy_loss= -0.0011874616611748934\n",
      "nll_loss= 1.8399604558944702\n",
      "avg_std= 0.2744445502758026\n",
      "dist std min max: 0.01570526883006096 0.2744445502758026 2.681318521499634\n",
      "hidden_states min max: -9.54712200164795 10.38442611694336\n",
      "hidden_state minus mean squared max: 71.25939178466797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.44818115234375 3.2183609008789062\n",
      "loss  1182: 1.8388   grad norm: 1.7145          model param norm: 87.0507        \n",
      "\n",
      "quiet_star_policy_loss= -0.014760506339371204\n",
      "nll_loss= 1.845162272453308\n",
      "avg_std= 0.2754096984863281\n",
      "dist std min max: 0.015432797372341156 0.2754096984863281 2.6628332138061523\n",
      "hidden_states min max: -9.543835639953613 12.070511817932129\n",
      "hidden_state minus mean squared max: 78.61858367919922\n",
      "hidden_state minus mean divided by std max: 5.16658353805542\n",
      "log_prob min max: -103.44540405273438 3.2043838500976562\n",
      "loss  1183: 1.8304   grad norm: 1.7308          model param norm: 87.0523        \n",
      "\n",
      "quiet_star_policy_loss= -0.024897074326872826\n",
      "nll_loss= 1.846659541130066\n",
      "avg_std= 0.2739509344100952\n",
      "dist std min max: 0.015245197340846062 0.2739509344100952 2.6811349391937256\n",
      "hidden_states min max: -9.549428939819336 11.81029224395752\n",
      "hidden_state minus mean squared max: 85.43768310546875\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.43452453613281 3.2332310676574707\n",
      "loss  1184: 1.8218   grad norm: 1.6588          model param norm: 87.0536        \n",
      "\n",
      "quiet_star_policy_loss= -0.009930253028869629\n",
      "nll_loss= 1.8438447713851929\n",
      "avg_std= 0.2735975682735443\n",
      "dist std min max: 0.014855927787721157 0.2735975682735443 2.7234108448028564\n",
      "hidden_states min max: -9.66295051574707 11.85986614227295\n",
      "hidden_state minus mean squared max: 80.03791809082031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.05248260498047 3.2598328590393066\n",
      "loss  1185: 1.8339   grad norm: 1.6502          model param norm: 87.0548        \n",
      "\n",
      "quiet_star_policy_loss= -0.030331647023558617\n",
      "nll_loss= 1.829535722732544\n",
      "avg_std= 0.2760290205478668\n",
      "dist std min max: 0.015445437282323837 0.2760290205478668 2.7765004634857178\n",
      "hidden_states min max: -9.4909029006958 11.814932823181152\n",
      "hidden_state minus mean squared max: 93.02662658691406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.63682556152344 3.2187676429748535\n",
      "loss  1186: 1.7992   grad norm: 1.5311          model param norm: 87.0561        \n",
      "\n",
      "quiet_star_policy_loss= -0.026999695226550102\n",
      "nll_loss= 1.8275409936904907\n",
      "avg_std= 0.27634772658348083\n",
      "dist std min max: 0.014905001036822796 0.27634772658348083 2.7906672954559326\n",
      "hidden_states min max: -9.846708297729492 12.763314247131348\n",
      "hidden_state minus mean squared max: 94.76905059814453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.197265625 3.264174461364746\n",
      "loss  1187: 1.8005   grad norm: 1.7865          model param norm: 87.0576        \n",
      "\n",
      "quiet_star_policy_loss= -0.019520027562975883\n",
      "nll_loss= 1.840256929397583\n",
      "avg_std= 0.2731354236602783\n",
      "dist std min max: 0.015467607416212559 0.2731354236602783 3.421985149383545\n",
      "hidden_states min max: -9.505838394165039 11.328076362609863\n",
      "hidden_state minus mean squared max: 66.23954772949219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.95579528808594 3.236205577850342\n",
      "loss  1188: 1.8207   grad norm: 1.7854          model param norm: 87.0593        \n",
      "\n",
      "quiet_star_policy_loss= -0.020014405250549316\n",
      "nll_loss= 1.8570146560668945\n",
      "avg_std= 0.2731729745864868\n",
      "dist std min max: 0.014593611471354961 0.2731729745864868 2.6746928691864014\n",
      "hidden_states min max: -10.26143741607666 11.051862716674805\n",
      "hidden_state minus mean squared max: 65.87693786621094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.03794860839844 3.278076171875\n",
      "loss  1189: 1.8370   grad norm: 1.6781          model param norm: 87.0610        \n",
      "\n",
      "quiet_star_policy_loss= -0.00528873223811388\n",
      "nll_loss= 1.846138596534729\n",
      "avg_std= 0.2736012041568756\n",
      "dist std min max: 0.015392251312732697 0.2736012041568756 3.1219146251678467\n",
      "hidden_states min max: -9.561108589172363 10.662857055664062\n",
      "hidden_state minus mean squared max: 76.44052124023438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24050903320312 3.252195358276367\n",
      "loss  1190: 1.8408   grad norm: 1.7094          model param norm: 87.0626        \n",
      "\n",
      "quiet_star_policy_loss= 0.026617025956511497\n",
      "nll_loss= 1.8610280752182007\n",
      "avg_std= 0.2737427353858948\n",
      "dist std min max: 0.01661122404038906 0.2737427353858948 3.153895616531372\n",
      "hidden_states min max: -9.497986793518066 11.775473594665527\n",
      "hidden_state minus mean squared max: 97.81275177001953\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.13855743408203 3.15266752243042\n",
      "loss  1191: 1.8876   grad norm: 1.6395          model param norm: 87.0641        \n",
      "\n",
      "quiet_star_policy_loss= 0.005799317266792059\n",
      "nll_loss= 1.8371559381484985\n",
      "avg_std= 0.2744416296482086\n",
      "dist std min max: 0.014890572987496853 0.2744416296482086 2.8790392875671387\n",
      "hidden_states min max: -9.699667930603027 10.342061996459961\n",
      "hidden_state minus mean squared max: 78.45068359375\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.03578186035156 3.2386302947998047\n",
      "loss  1192: 1.8430   grad norm: 1.8111          model param norm: 87.0659        \n",
      "\n",
      "quiet_star_policy_loss= -0.020078063011169434\n",
      "nll_loss= 1.8512626886367798\n",
      "avg_std= 0.2751007378101349\n",
      "dist std min max: 0.014974686317145824 0.2751007378101349 2.712028980255127\n",
      "hidden_states min max: -9.733878135681152 11.251473426818848\n",
      "hidden_state minus mean squared max: 80.29237365722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.69074249267578 3.2621641159057617\n",
      "loss  1193: 1.8312   grad norm: 1.7456          model param norm: 87.0674        \n",
      "\n",
      "quiet_star_policy_loss= -0.01598779670894146\n",
      "nll_loss= 1.8382562398910522\n",
      "avg_std= 0.27593594789505005\n",
      "dist std min max: 0.017976729199290276 0.27593594789505005 2.679515838623047\n",
      "hidden_states min max: -10.481575012207031 10.956748008728027\n",
      "hidden_state minus mean squared max: 81.50838470458984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.73294830322266 3.085850715637207\n",
      "loss  1194: 1.8223   grad norm: 1.5877          model param norm: 87.0690        \n",
      "\n",
      "quiet_star_policy_loss= -0.01239101868122816\n",
      "nll_loss= 1.8303406238555908\n",
      "avg_std= 0.2748527228832245\n",
      "dist std min max: 0.01747184805572033 0.2748527228832245 2.6634538173675537\n",
      "hidden_states min max: -9.643793106079102 11.24810791015625\n",
      "hidden_state minus mean squared max: 83.20054626464844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.3427505493164 3.092226028442383\n",
      "loss  1195: 1.8179   grad norm: 1.5792          model param norm: 87.0707        \n",
      "\n",
      "quiet_star_policy_loss= -0.011458826251327991\n",
      "nll_loss= 1.8316357135772705\n",
      "avg_std= 0.27639928460121155\n",
      "dist std min max: 0.017457008361816406 0.27639928460121155 2.6765692234039307\n",
      "hidden_states min max: -12.289610862731934 9.973755836486816\n",
      "hidden_state minus mean squared max: 154.99844360351562\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.05429077148438 3.0990405082702637\n",
      "loss  1196: 1.8202   grad norm: 1.6913          model param norm: 87.0727        \n",
      "\n",
      "quiet_star_policy_loss= -0.0143275773152709\n",
      "nll_loss= 1.8378117084503174\n",
      "avg_std= 0.2745434641838074\n",
      "dist std min max: 0.0174853652715683 0.2745434641838074 2.7113864421844482\n",
      "hidden_states min max: -9.529744148254395 11.705893516540527\n",
      "hidden_state minus mean squared max: 75.67790222167969\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.60176849365234 3.0796804428100586\n",
      "loss  1197: 1.8235   grad norm: 1.6462          model param norm: 87.0741        \n",
      "\n",
      "quiet_star_policy_loss= -0.0016764402389526367\n",
      "nll_loss= 1.8684834241867065\n",
      "avg_std= 0.273942232131958\n",
      "dist std min max: 0.016279231756925583 0.273942232131958 2.7537121772766113\n",
      "hidden_states min max: -9.609113693237305 10.176996231079102\n",
      "hidden_state minus mean squared max: 64.84104919433594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1191635131836 3.163296699523926\n",
      "loss  1198: 1.8668   grad norm: 1.5952          model param norm: 87.0754        \n",
      "\n",
      "quiet_star_policy_loss= 0.02030777931213379\n",
      "nll_loss= 1.8390203714370728\n",
      "avg_std= 0.2735179662704468\n",
      "dist std min max: 0.016507010906934738 0.2735179662704468 3.1624131202697754\n",
      "hidden_states min max: -9.727069854736328 10.79704475402832\n",
      "hidden_state minus mean squared max: 69.11432647705078\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.8109359741211 3.1577610969543457\n",
      "loss  1199: 1.8593   grad norm: 1.6781          model param norm: 87.0767        \n",
      "eval loss 1.8468306064605713\n",
      "\n",
      "quiet_star_policy_loss= 0.0077408673241734505\n",
      "nll_loss= 1.857706904411316\n",
      "avg_std= 0.27650851011276245\n",
      "dist std min max: 0.016812313348054886 0.27650851011276245 2.5375866889953613\n",
      "hidden_states min max: -9.534492492675781 10.20151138305664\n",
      "hidden_state minus mean squared max: 75.46139526367188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.60096740722656 3.1510114669799805\n",
      "loss  1200: 1.8654   grad norm: 1.5680          model param norm: 87.0777        \n",
      "\n",
      "quiet_star_policy_loss= -0.02209567092359066\n",
      "nll_loss= 1.839922308921814\n",
      "avg_std= 0.2765587866306305\n",
      "dist std min max: 0.017058202996850014 0.2765587866306305 2.5326602458953857\n",
      "hidden_states min max: -14.672369003295898 10.105402946472168\n",
      "hidden_state minus mean squared max: 165.4466552734375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.08689880371094 3.1489768028259277\n",
      "loss  1201: 1.8178   grad norm: 1.5185          model param norm: 87.0790        \n",
      "\n",
      "quiet_star_policy_loss= -0.05530310794711113\n",
      "nll_loss= 1.822100043296814\n",
      "avg_std= 0.2780500054359436\n",
      "dist std min max: 0.01798134110867977 0.2780500054359436 2.5206637382507324\n",
      "hidden_states min max: -17.300874710083008 10.952421188354492\n",
      "hidden_state minus mean squared max: 244.7390594482422\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.28265380859375 3.076125144958496\n",
      "loss  1202: 1.7668   grad norm: 1.6171          model param norm: 87.0806        \n",
      "\n",
      "quiet_star_policy_loss= -0.06034257262945175\n",
      "nll_loss= 1.793806552886963\n",
      "avg_std= 0.2816266715526581\n",
      "dist std min max: 0.01591060496866703 0.2816266715526581 2.45831561088562\n",
      "hidden_states min max: -9.406439781188965 10.465946197509766\n",
      "hidden_state minus mean squared max: 78.66529083251953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.71516418457031 3.205833911895752\n",
      "loss  1203: 1.7335   grad norm: 3.4374          model param norm: 87.0823        \n",
      "\n",
      "quiet_star_policy_loss= 0.005385947413742542\n",
      "nll_loss= 1.8428844213485718\n",
      "avg_std= 0.2774291932582855\n",
      "dist std min max: 0.015912486240267754 0.2774291932582855 2.5551233291625977\n",
      "hidden_states min max: -18.10576629638672 10.444636344909668\n",
      "hidden_state minus mean squared max: 256.9682312011719\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.30706787109375 3.20534086227417\n",
      "loss  1204: 1.8483   grad norm: 1.5900          model param norm: 87.0845        \n",
      "\n",
      "quiet_star_policy_loss= -0.002112495945766568\n",
      "nll_loss= 1.8457955121994019\n",
      "avg_std= 0.27758321166038513\n",
      "dist std min max: 0.01759009063243866 0.27758321166038513 2.516395092010498\n",
      "hidden_states min max: -9.500509262084961 10.360004425048828\n",
      "hidden_state minus mean squared max: 63.6295166015625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.52969360351562 3.0744576454162598\n",
      "loss  1205: 1.8437   grad norm: 1.6890          model param norm: 87.0865        \n",
      "\n",
      "quiet_star_policy_loss= -0.003985899966210127\n",
      "nll_loss= 1.8509396314620972\n",
      "avg_std= 0.2796648442745209\n",
      "dist std min max: 0.015897568315267563 0.2796648442745209 2.6016416549682617\n",
      "hidden_states min max: -9.529998779296875 9.943866729736328\n",
      "hidden_state minus mean squared max: 69.09741973876953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06851196289062 3.211608409881592\n",
      "loss  1206: 1.8470   grad norm: 1.6543          model param norm: 87.0884        \n",
      "\n",
      "quiet_star_policy_loss= 0.001511499285697937\n",
      "nll_loss= 1.8438860177993774\n",
      "avg_std= 0.2792086899280548\n",
      "dist std min max: 0.017971988767385483 0.2792086899280548 2.5950567722320557\n",
      "hidden_states min max: -10.045592308044434 11.04561996459961\n",
      "hidden_state minus mean squared max: 65.04592895507812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.43212890625 3.087165355682373\n",
      "loss  1207: 1.8454   grad norm: 1.6955          model param norm: 87.0902        \n",
      "\n",
      "quiet_star_policy_loss= 7.200241270766128e-06\n",
      "nll_loss= 1.8355308771133423\n",
      "avg_std= 0.2812419831752777\n",
      "dist std min max: 0.01725955307483673 0.2812419831752777 3.037788152694702\n",
      "hidden_states min max: -16.666845321655273 11.569189071655273\n",
      "hidden_state minus mean squared max: 165.35552978515625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.08661651611328 3.132681369781494\n",
      "loss  1208: 1.8355   grad norm: 1.6553          model param norm: 87.0923        \n",
      "\n",
      "quiet_star_policy_loss= -0.029644960537552834\n",
      "nll_loss= 1.8441027402877808\n",
      "avg_std= 0.2820190191268921\n",
      "dist std min max: 0.017992371693253517 0.2820190191268921 2.850402593612671\n",
      "hidden_states min max: -9.849543571472168 9.914125442504883\n",
      "hidden_state minus mean squared max: 79.8125228881836\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -103.37788391113281 3.0732369422912598\n",
      "loss  1209: 1.8145   grad norm: 1.6063          model param norm: 87.0944        \n",
      "\n",
      "quiet_star_policy_loss= -0.016612732782959938\n",
      "nll_loss= 1.8530982732772827\n",
      "avg_std= 0.28303810954093933\n",
      "dist std min max: 0.01629539579153061 0.28303810954093933 3.0961923599243164\n",
      "hidden_states min max: -9.685064315795898 10.564994812011719\n",
      "hidden_state minus mean squared max: 81.1413803100586\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.92156219482422 3.189566135406494\n",
      "loss  1210: 1.8365   grad norm: 1.6376          model param norm: 87.0965        \n",
      "\n",
      "quiet_star_policy_loss= -0.01507644634693861\n",
      "nll_loss= 1.8460625410079956\n",
      "avg_std= 0.28144606947898865\n",
      "dist std min max: 0.015434696339070797 0.28144606947898865 2.610553026199341\n",
      "hidden_states min max: -9.725728034973145 10.262895584106445\n",
      "hidden_state minus mean squared max: 81.0334243774414\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.7300033569336 3.1688356399536133\n",
      "loss  1211: 1.8310   grad norm: 1.6751          model param norm: 87.0991        \n",
      "\n",
      "quiet_star_policy_loss= -0.033414434641599655\n",
      "nll_loss= 1.8534278869628906\n",
      "avg_std= 0.28251296281814575\n",
      "dist std min max: 0.014738589525222778 0.28251296281814575 2.6288678646087646\n",
      "hidden_states min max: -9.532712936401367 10.310215950012207\n",
      "hidden_state minus mean squared max: 73.06808471679688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.38166809082031 3.2675466537475586\n",
      "loss  1212: 1.8200   grad norm: 1.9146          model param norm: 87.1014        \n",
      "\n",
      "quiet_star_policy_loss= -0.042368900030851364\n",
      "nll_loss= 1.8270381689071655\n",
      "avg_std= 0.2844829559326172\n",
      "dist std min max: 0.0174239668995142 0.2844829559326172 2.805692672729492\n",
      "hidden_states min max: -9.374360084533691 10.93202018737793\n",
      "hidden_state minus mean squared max: 66.39361572265625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4957504272461 3.0987329483032227\n",
      "loss  1213: 1.7847   grad norm: 1.6977          model param norm: 87.1036        \n",
      "\n",
      "quiet_star_policy_loss= -0.023579472675919533\n",
      "nll_loss= 1.858247995376587\n",
      "avg_std= 0.2836763560771942\n",
      "dist std min max: 0.014532440342009068 0.2836763560771942 2.6281843185424805\n",
      "hidden_states min max: -9.482083320617676 10.270442962646484\n",
      "hidden_state minus mean squared max: 63.60659408569336\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.05925750732422 3.2740635871887207\n",
      "loss  1214: 1.8347   grad norm: 1.6457          model param norm: 87.1057        \n",
      "\n",
      "quiet_star_policy_loss= -0.051729362457990646\n",
      "nll_loss= 1.8306198120117188\n",
      "avg_std= 0.2827783226966858\n",
      "dist std min max: 0.01405793521553278 0.2827783226966858 2.639761209487915\n",
      "hidden_states min max: -9.35554313659668 11.993856430053711\n",
      "hidden_state minus mean squared max: 87.26010131835938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.50007629394531 3.2776126861572266\n",
      "loss  1215: 1.7789   grad norm: 1.8459          model param norm: 87.1078        \n",
      "\n",
      "quiet_star_policy_loss= -0.016612863168120384\n",
      "nll_loss= 1.865909218788147\n",
      "avg_std= 0.28156614303588867\n",
      "dist std min max: 0.016363510861992836 0.28156614303588867 2.618706464767456\n",
      "hidden_states min max: -9.685176849365234 10.868819236755371\n",
      "hidden_state minus mean squared max: 107.96771240234375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.87349700927734 3.191087245941162\n",
      "loss  1216: 1.8493   grad norm: 1.7008          model param norm: 87.1100        \n",
      "\n",
      "quiet_star_policy_loss= -0.0358588807284832\n",
      "nll_loss= 1.8284519910812378\n",
      "avg_std= 0.2817458212375641\n",
      "dist std min max: 0.014501360245049 0.2817458212375641 2.8480300903320312\n",
      "hidden_states min max: -9.253324508666992 10.318260192871094\n",
      "hidden_state minus mean squared max: 64.68026733398438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.55290222167969 3.27470064163208\n",
      "loss  1217: 1.7926   grad norm: 1.8172          model param norm: 87.1121        \n",
      "\n",
      "quiet_star_policy_loss= -0.008495188318192959\n",
      "nll_loss= 1.8306068181991577\n",
      "avg_std= 0.27916261553764343\n",
      "dist std min max: 0.014847890473902225 0.27916261553764343 2.700575351715088\n",
      "hidden_states min max: -13.871230125427246 10.632319450378418\n",
      "hidden_state minus mean squared max: 105.11150360107422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.86009216308594 3.287421703338623\n",
      "loss  1218: 1.8221   grad norm: 1.7852          model param norm: 87.1141        \n",
      "\n",
      "quiet_star_policy_loss= -0.036106597632169724\n",
      "nll_loss= 1.8482179641723633\n",
      "avg_std= 0.27772772312164307\n",
      "dist std min max: 0.014223427511751652 0.27772772312164307 2.604747772216797\n",
      "hidden_states min max: -9.368887901306152 11.874327659606934\n",
      "hidden_state minus mean squared max: 109.24154663085938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.43507385253906 3.2385315895080566\n",
      "loss  1219: 1.8121   grad norm: 1.7019          model param norm: 87.1163        \n",
      "\n",
      "quiet_star_policy_loss= -0.019278347492218018\n",
      "nll_loss= 1.8493045568466187\n",
      "avg_std= 0.27738675475120544\n",
      "dist std min max: 0.013940781354904175 0.27738675475120544 3.0551583766937256\n",
      "hidden_states min max: -14.571577072143555 10.108277320861816\n",
      "hidden_state minus mean squared max: 79.7531509399414\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.72205352783203 3.3193516731262207\n",
      "loss  1220: 1.8300   grad norm: 1.6328          model param norm: 87.1184        \n",
      "\n",
      "quiet_star_policy_loss= -0.004194080829620361\n",
      "nll_loss= 1.8477157354354858\n",
      "avg_std= 0.2781105935573578\n",
      "dist std min max: 0.014988952316343784 0.2781105935573578 2.5913336277008057\n",
      "hidden_states min max: -9.36522388458252 9.577607154846191\n",
      "hidden_state minus mean squared max: 113.46023559570312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.89830017089844 3.26138973236084\n",
      "loss  1221: 1.8435   grad norm: 1.7293          model param norm: 87.1205        \n",
      "\n",
      "quiet_star_policy_loss= -0.0018600166076794267\n",
      "nll_loss= 1.834420084953308\n",
      "avg_std= 0.27602922916412354\n",
      "dist std min max: 0.015973929315805435 0.27602922916412354 2.4996590614318848\n",
      "hidden_states min max: -9.44617748260498 10.642741203308105\n",
      "hidden_state minus mean squared max: 69.44526672363281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.65137481689453 3.1774682998657227\n",
      "loss  1222: 1.8326   grad norm: 1.7590          model param norm: 87.1227        \n",
      "\n",
      "quiet_star_policy_loss= 0.0067050340585410595\n",
      "nll_loss= 1.8316224813461304\n",
      "avg_std= 0.2753181457519531\n",
      "dist std min max: 0.014594669453799725 0.2753181457519531 2.499957799911499\n",
      "hidden_states min max: -9.38310718536377 11.29499626159668\n",
      "hidden_state minus mean squared max: 67.00922393798828\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.28822326660156 3.2666478157043457\n",
      "loss  1223: 1.8383   grad norm: 1.7811          model param norm: 87.1247        \n",
      "\n",
      "quiet_star_policy_loss= -0.03488502651453018\n",
      "nll_loss= 1.8557201623916626\n",
      "avg_std= 0.27203017473220825\n",
      "dist std min max: 0.014832519926130772 0.27203017473220825 2.5145413875579834\n",
      "hidden_states min max: -12.130016326904297 10.662957191467285\n",
      "hidden_state minus mean squared max: 80.5177001953125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.72681427001953 3.2709240913391113\n",
      "loss  1224: 1.8208   grad norm: 1.5908          model param norm: 87.1269        \n",
      "\n",
      "quiet_star_policy_loss= 0.0021197914611548185\n",
      "nll_loss= 1.8242807388305664\n",
      "avg_std= 0.27394893765449524\n",
      "dist std min max: 0.014653999358415604 0.27394893765449524 2.4857094287872314\n",
      "hidden_states min max: -9.848616600036621 9.96410846710205\n",
      "hidden_state minus mean squared max: 73.26655578613281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.25963592529297 3.258368492126465\n",
      "loss  1225: 1.8264   grad norm: 1.6332          model param norm: 87.1292        \n",
      "\n",
      "quiet_star_policy_loss= -0.002121532103046775\n",
      "nll_loss= 1.8466724157333374\n",
      "avg_std= 0.2717854976654053\n",
      "dist std min max: 0.01486138440668583 0.2717854976654053 2.741903781890869\n",
      "hidden_states min max: -9.641700744628906 10.053720474243164\n",
      "hidden_state minus mean squared max: 73.6908187866211\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.63975524902344 3.279205799102783\n",
      "loss  1226: 1.8446   grad norm: 1.6048          model param norm: 87.1314        \n",
      "\n",
      "quiet_star_policy_loss= -0.01781599596142769\n",
      "nll_loss= 1.8281883001327515\n",
      "avg_std= 0.27148669958114624\n",
      "dist std min max: 0.01447097398340702 0.27148669958114624 2.444700241088867\n",
      "hidden_states min max: -9.46188735961914 10.786674499511719\n",
      "hidden_state minus mean squared max: 103.6294937133789\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.85298919677734 3.296631336212158\n",
      "loss  1227: 1.8104   grad norm: 1.6345          model param norm: 87.1334        \n",
      "\n",
      "quiet_star_policy_loss= 0.008107343688607216\n",
      "nll_loss= 1.8242844343185425\n",
      "avg_std= 0.27111607789993286\n",
      "dist std min max: 0.01551893725991249 0.27111607789993286 2.4298977851867676\n",
      "hidden_states min max: -9.415962219238281 9.743947982788086\n",
      "hidden_state minus mean squared max: 61.63250732421875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.86894226074219 3.229887008666992\n",
      "loss  1228: 1.8324   grad norm: 1.6158          model param norm: 87.1358        \n",
      "\n",
      "quiet_star_policy_loss= -0.0017839790089055896\n",
      "nll_loss= 1.831126093864441\n",
      "avg_std= 0.2718536853790283\n",
      "dist std min max: 0.014461261220276356 0.2718536853790283 2.664759635925293\n",
      "hidden_states min max: -9.447059631347656 10.015993118286133\n",
      "hidden_state minus mean squared max: 75.7884521484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79605102539062 3.2709197998046875\n",
      "loss  1229: 1.8293   grad norm: 1.7237          model param norm: 87.1382        \n",
      "\n",
      "quiet_star_policy_loss= -0.03791571408510208\n",
      "nll_loss= 1.8398773670196533\n",
      "avg_std= 0.27056482434272766\n",
      "dist std min max: 0.014334778301417828 0.27056482434272766 3.1073944568634033\n",
      "hidden_states min max: -9.652496337890625 10.295248031616211\n",
      "hidden_state minus mean squared max: 53.31689453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29149627685547 3.3001508712768555\n",
      "loss  1230: 1.8020   grad norm: 1.6514          model param norm: 87.1405        \n",
      "\n",
      "quiet_star_policy_loss= 0.0018412781646475196\n",
      "nll_loss= 1.8389453887939453\n",
      "avg_std= 0.26641789078712463\n",
      "dist std min max: 0.014765419065952301 0.26641789078712463 2.401247978210449\n",
      "hidden_states min max: -9.439230918884277 9.860219955444336\n",
      "hidden_state minus mean squared max: 46.02130889892578\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -13.157574653625488 3.2785215377807617\n",
      "loss  1231: 1.8408   grad norm: 3.5568          model param norm: 87.1428        \n",
      "\n",
      "quiet_star_policy_loss= -0.014401257038116455\n",
      "nll_loss= 1.8485090732574463\n",
      "avg_std= 0.27031436562538147\n",
      "dist std min max: 0.013989086262881756 0.27031436562538147 2.4664225578308105\n",
      "hidden_states min max: -9.331197738647461 10.29524040222168\n",
      "hidden_state minus mean squared max: 74.84484100341797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.54646301269531 3.3195648193359375\n",
      "loss  1232: 1.8341   grad norm: 1.7203          model param norm: 87.1452        \n",
      "\n",
      "quiet_star_policy_loss= -0.025867177173495293\n",
      "nll_loss= 1.8517580032348633\n",
      "avg_std= 0.2705822288990021\n",
      "dist std min max: 0.013677330687642097 0.2705822288990021 2.841033697128296\n",
      "hidden_states min max: -9.613493919372559 9.888710975646973\n",
      "hidden_state minus mean squared max: 68.3084487915039\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.95570373535156 3.359421730041504\n",
      "loss  1233: 1.8259   grad norm: 1.7535          model param norm: 87.1478        \n",
      "\n",
      "quiet_star_policy_loss= -0.02311565913259983\n",
      "nll_loss= 1.8503170013427734\n",
      "avg_std= 0.26931342482566833\n",
      "dist std min max: 0.015251959674060345 0.26931342482566833 2.8687074184417725\n",
      "hidden_states min max: -9.67035961151123 9.932173728942871\n",
      "hidden_state minus mean squared max: 52.768272399902344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.05311584472656 3.2641077041625977\n",
      "loss  1234: 1.8272   grad norm: 1.7863          model param norm: 87.1504        \n",
      "\n",
      "quiet_star_policy_loss= -0.025549566373229027\n",
      "nll_loss= 1.835365653038025\n",
      "avg_std= 0.26961949467658997\n",
      "dist std min max: 0.013801025226712227 0.26961949467658997 2.3934738636016846\n",
      "hidden_states min max: -9.50634765625 10.640045166015625\n",
      "hidden_state minus mean squared max: 75.95172882080078\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.32107543945312 3.3573660850524902\n",
      "loss  1235: 1.8098   grad norm: 1.7089          model param norm: 87.1532        \n",
      "\n",
      "quiet_star_policy_loss= -0.010473394766449928\n",
      "nll_loss= 1.8458881378173828\n",
      "avg_std= 0.2682565450668335\n",
      "dist std min max: 0.013381018303334713 0.2682565450668335 2.338794708251953\n",
      "hidden_states min max: -10.818770408630371 9.837980270385742\n",
      "hidden_state minus mean squared max: 71.043212890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.51509857177734 3.365748882293701\n",
      "loss  1236: 1.8354   grad norm: 1.6808          model param norm: 87.1562        \n",
      "\n",
      "quiet_star_policy_loss= -0.003412815975025296\n",
      "nll_loss= 1.8510007858276367\n",
      "avg_std= 0.267556756734848\n",
      "dist std min max: 0.013103529810905457 0.267556756734848 2.3680028915405273\n",
      "hidden_states min max: -9.600082397460938 11.167695999145508\n",
      "hidden_state minus mean squared max: 59.72322082519531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.354999542236328 3.389061450958252\n",
      "loss  1237: 1.8476   grad norm: 1.6095          model param norm: 87.1590        \n",
      "\n",
      "quiet_star_policy_loss= -0.017681265249848366\n",
      "nll_loss= 1.83873450756073\n",
      "avg_std= 0.26705658435821533\n",
      "dist std min max: 0.013043024577200413 0.26705658435821533 2.7768194675445557\n",
      "hidden_states min max: -9.46848201751709 9.975309371948242\n",
      "hidden_state minus mean squared max: 66.3936767578125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.63037109375 3.4139890670776367\n",
      "loss  1238: 1.8211   grad norm: 1.6522          model param norm: 87.1621        \n",
      "\n",
      "quiet_star_policy_loss= -0.030108124017715454\n",
      "nll_loss= 1.8526493310928345\n",
      "avg_std= 0.2643539309501648\n",
      "dist std min max: 0.012454058043658733 0.2643539309501648 2.2688655853271484\n",
      "hidden_states min max: -9.592802047729492 9.89733600616455\n",
      "hidden_state minus mean squared max: 98.21907806396484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.82617950439453 3.453803539276123\n",
      "loss  1239: 1.8225   grad norm: 1.7265          model param norm: 87.1649        \n",
      "\n",
      "quiet_star_policy_loss= -0.019443035125732422\n",
      "nll_loss= 1.8433935642242432\n",
      "avg_std= 0.26496970653533936\n",
      "dist std min max: 0.012403137981891632 0.26496970653533936 2.3287830352783203\n",
      "hidden_states min max: -9.3568754196167 9.943742752075195\n",
      "hidden_state minus mean squared max: 60.849159240722656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.88504028320312 3.4487357139587402\n",
      "loss  1240: 1.8240   grad norm: 1.5930          model param norm: 87.1677        \n",
      "\n",
      "quiet_star_policy_loss= -0.005414783954620361\n",
      "nll_loss= 1.835457682609558\n",
      "avg_std= 0.26441365480422974\n",
      "dist std min max: 0.014690058305859566 0.26441365480422974 2.255951166152954\n",
      "hidden_states min max: -9.503045082092285 10.147886276245117\n",
      "hidden_state minus mean squared max: 52.308712005615234\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.9130630493164 3.2820615768432617\n",
      "loss  1241: 1.8300   grad norm: 1.6296          model param norm: 87.1705        \n",
      "\n",
      "quiet_star_policy_loss= -0.01752912439405918\n",
      "nll_loss= 1.8364975452423096\n",
      "avg_std= 0.2623175382614136\n",
      "dist std min max: 0.012147694826126099 0.2623175382614136 2.2722296714782715\n",
      "hidden_states min max: -9.511429786682129 10.002496719360352\n",
      "hidden_state minus mean squared max: 50.86860656738281\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -101.21273803710938 3.4781007766723633\n",
      "loss  1242: 1.8190   grad norm: 1.6318          model param norm: 87.1732        \n",
      "\n",
      "quiet_star_policy_loss= -0.01195718627423048\n",
      "nll_loss= 1.8414286375045776\n",
      "avg_std= 0.26299333572387695\n",
      "dist std min max: 0.012773153372108936 0.26299333572387695 2.2333335876464844\n",
      "hidden_states min max: -9.476797103881836 10.296546936035156\n",
      "hidden_state minus mean squared max: 76.39855194091797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7306137084961 3.363361358642578\n",
      "loss  1243: 1.8295   grad norm: 1.8838          model param norm: 87.1760        \n",
      "\n",
      "quiet_star_policy_loss= -0.018247557803988457\n",
      "nll_loss= 1.8310798406600952\n",
      "avg_std= 0.26087912917137146\n",
      "dist std min max: 0.012287330813705921 0.26087912917137146 2.228666305541992\n",
      "hidden_states min max: -9.552351951599121 10.044323921203613\n",
      "hidden_state minus mean squared max: 72.6600112915039\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.95036315917969 3.4527711868286133\n",
      "loss  1244: 1.8128   grad norm: 1.7091          model param norm: 87.1788        \n",
      "\n",
      "quiet_star_policy_loss= -0.0064303576946258545\n",
      "nll_loss= 1.8365522623062134\n",
      "avg_std= 0.2610923647880554\n",
      "dist std min max: 0.012844540178775787 0.2610923647880554 2.2820885181427\n",
      "hidden_states min max: -9.464247703552246 10.886932373046875\n",
      "hidden_state minus mean squared max: 61.40838623046875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77098846435547 3.4217276573181152\n",
      "loss  1245: 1.8301   grad norm: 1.6572          model param norm: 87.1819        \n",
      "\n",
      "quiet_star_policy_loss= 8.506775338901207e-05\n",
      "nll_loss= 1.8348888158798218\n",
      "avg_std= 0.2587798535823822\n",
      "dist std min max: 0.012567127123475075 0.2587798535823822 2.5253913402557373\n",
      "hidden_states min max: -9.547365188598633 10.077025413513184\n",
      "hidden_state minus mean squared max: 51.50944519042969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.37374210357666 3.4326629638671875\n",
      "loss  1246: 1.8350   grad norm: 1.7121          model param norm: 87.1848        \n",
      "\n",
      "quiet_star_policy_loss= 0.007597494404762983\n",
      "nll_loss= 1.8520479202270508\n",
      "avg_std= 0.25848692655563354\n",
      "dist std min max: 0.013305679894983768 0.25848692655563354 2.1788651943206787\n",
      "hidden_states min max: -9.793732643127441 10.227108001708984\n",
      "hidden_state minus mean squared max: 70.25659942626953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65865325927734 3.3595542907714844\n",
      "loss  1247: 1.8596   grad norm: 1.8414          model param norm: 87.1880        \n",
      "\n",
      "quiet_star_policy_loss= -0.012301445007324219\n",
      "nll_loss= 1.8415393829345703\n",
      "avg_std= 0.25845107436180115\n",
      "dist std min max: 0.01405034214258194 0.25845107436180115 2.427039861679077\n",
      "hidden_states min max: -12.773771286010742 10.0667142868042\n",
      "hidden_state minus mean squared max: 61.974212646484375\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.59593963623047 3.316254138946533\n",
      "loss  1248: 1.8292   grad norm: 1.6748          model param norm: 87.1912        \n",
      "\n",
      "quiet_star_policy_loss= -0.018721461296081543\n",
      "nll_loss= 1.8280292749404907\n",
      "avg_std= 0.2576250433921814\n",
      "dist std min max: 0.013439727015793324 0.2576250433921814 2.3540799617767334\n",
      "hidden_states min max: -9.34054946899414 10.104280471801758\n",
      "hidden_state minus mean squared max: 50.81365203857422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.25263214111328 3.3710079193115234\n",
      "loss  1249: 1.8093   grad norm: 1.8060          model param norm: 87.1945        \n",
      "\n",
      "quiet_star_policy_loss= -0.0015273451572284102\n",
      "nll_loss= 1.8292022943496704\n",
      "avg_std= 0.25680869817733765\n",
      "dist std min max: 0.01335236243903637 0.25680869817733765 2.365222215652466\n",
      "hidden_states min max: -10.575757026672363 10.357690811157227\n",
      "hidden_state minus mean squared max: 55.172576904296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.53781127929688 3.3883395195007324\n",
      "loss  1250: 1.8277   grad norm: 1.6631          model param norm: 87.1977        \n",
      "\n",
      "quiet_star_policy_loss= -0.01546547468751669\n",
      "nll_loss= 1.8252476453781128\n",
      "avg_std= 0.2571966350078583\n",
      "dist std min max: 0.013295246288180351 0.2571966350078583 2.475334644317627\n",
      "hidden_states min max: -19.27059555053711 10.0770845413208\n",
      "hidden_state minus mean squared max: 297.0494384765625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.37950897216797 3.364694595336914\n",
      "loss  1251: 1.8098   grad norm: 1.7875          model param norm: 87.2009        \n",
      "\n",
      "quiet_star_policy_loss= -0.0014856279594823718\n",
      "nll_loss= 1.845373511314392\n",
      "avg_std= 0.25634440779685974\n",
      "dist std min max: 0.014848804101347923 0.25634440779685974 2.3194456100463867\n",
      "hidden_states min max: -13.47734546661377 10.179757118225098\n",
      "hidden_state minus mean squared max: 250.82884216308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.29496765136719 3.273153781890869\n",
      "loss  1252: 1.8439   grad norm: 1.7341          model param norm: 87.2041        \n",
      "\n",
      "quiet_star_policy_loss= -0.026421070098876953\n",
      "nll_loss= 1.8517683744430542\n",
      "avg_std= 0.2565033435821533\n",
      "dist std min max: 0.013521420769393444 0.2565033435821533 2.524350166320801\n",
      "hidden_states min max: -9.661686897277832 10.102307319641113\n",
      "hidden_state minus mean squared max: 50.38880920410156\n",
      "hidden_state minus mean divided by std max: 5.035404682159424\n",
      "log_prob min max: -102.14958190917969 3.362638473510742\n",
      "loss  1253: 1.8253   grad norm: 1.6720          model param norm: 87.2074        \n",
      "\n",
      "quiet_star_policy_loss= 0.00531051168218255\n",
      "nll_loss= 1.8328121900558472\n",
      "avg_std= 0.25576266646385193\n",
      "dist std min max: 0.014475272968411446 0.25576266646385193 2.207550525665283\n",
      "hidden_states min max: -9.64775562286377 10.201000213623047\n",
      "hidden_state minus mean squared max: 55.45839309692383\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.48773956298828 3.3027548789978027\n",
      "loss  1254: 1.8381   grad norm: 1.8519          model param norm: 87.2104        \n",
      "\n",
      "quiet_star_policy_loss= -0.017479682341217995\n",
      "nll_loss= 1.8362385034561157\n",
      "avg_std= 0.25691598653793335\n",
      "dist std min max: 0.013343509286642075 0.25691598653793335 2.200680732727051\n",
      "hidden_states min max: -9.649849891662598 10.074088096618652\n",
      "hidden_state minus mean squared max: 50.485652923583984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -12.990455627441406 3.3458094596862793\n",
      "loss  1255: 1.8188   grad norm: 1.7489          model param norm: 87.2131        \n",
      "\n",
      "quiet_star_policy_loss= -0.01226460374891758\n",
      "nll_loss= 1.8406227827072144\n",
      "avg_std= 0.2558979094028473\n",
      "dist std min max: 0.013725330121815205 0.2558979094028473 2.1858396530151367\n",
      "hidden_states min max: -9.663185119628906 10.071924209594727\n",
      "hidden_state minus mean squared max: 53.615665435791016\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.02583312988281 3.339292526245117\n",
      "loss  1256: 1.8284   grad norm: 1.7739          model param norm: 87.2158        \n",
      "\n",
      "quiet_star_policy_loss= -0.03530208021402359\n",
      "nll_loss= 1.8473304510116577\n",
      "avg_std= 0.25531721115112305\n",
      "dist std min max: 0.015695305541157722 0.25531721115112305 2.5626425743103027\n",
      "hidden_states min max: -18.398242950439453 10.467748641967773\n",
      "hidden_state minus mean squared max: 298.5732727050781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.38211059570312 3.2293238639831543\n",
      "loss  1257: 1.8120   grad norm: 1.7653          model param norm: 87.2184        \n",
      "\n",
      "quiet_star_policy_loss= 0.0002753615553956479\n",
      "nll_loss= 1.824172854423523\n",
      "avg_std= 0.25496020913124084\n",
      "dist std min max: 0.013494117185473442 0.25496020913124084 2.242410659790039\n",
      "hidden_states min max: -9.786423683166504 10.561365127563477\n",
      "hidden_state minus mean squared max: 48.42734146118164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.62545013427734 3.374093532562256\n",
      "loss  1258: 1.8244   grad norm: 1.7115          model param norm: 87.2208        \n",
      "\n",
      "quiet_star_policy_loss= -0.02232508361339569\n",
      "nll_loss= 1.8368339538574219\n",
      "avg_std= 0.25290337204933167\n",
      "dist std min max: 0.013819312676787376 0.25290337204933167 2.144195795059204\n",
      "hidden_states min max: -9.492551803588867 10.047736167907715\n",
      "hidden_state minus mean squared max: 75.42264556884766\n",
      "hidden_state minus mean divided by std max: 4.900964736938477\n",
      "log_prob min max: -103.47074890136719 3.2517600059509277\n",
      "loss  1259: 1.8145   grad norm: 3.3152          model param norm: 87.2231        \n",
      "\n",
      "quiet_star_policy_loss= 0.021341174840927124\n",
      "nll_loss= 1.8415606021881104\n",
      "avg_std= 0.25329744815826416\n",
      "dist std min max: 0.013251789845526218 0.25329744815826416 2.6758248805999756\n",
      "hidden_states min max: -9.6829252243042 10.616645812988281\n",
      "hidden_state minus mean squared max: 52.90700149536133\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.30567932128906 3.395883560180664\n",
      "loss  1260: 1.8629   grad norm: 1.6524          model param norm: 87.2251        \n",
      "\n",
      "quiet_star_policy_loss= -0.009308472275733948\n",
      "nll_loss= 1.8241150379180908\n",
      "avg_std= 0.253703236579895\n",
      "dist std min max: 0.013377861119806767 0.253703236579895 2.443125009536743\n",
      "hidden_states min max: -9.746563911437988 10.706847190856934\n",
      "hidden_state minus mean squared max: 58.20206832885742\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7421875 3.3693599700927734\n",
      "loss  1261: 1.8148   grad norm: 1.7619          model param norm: 87.2268        \n",
      "\n",
      "quiet_star_policy_loss= -0.007747650146484375\n",
      "nll_loss= 1.8407105207443237\n",
      "avg_std= 0.2521766126155853\n",
      "dist std min max: 0.01352880522608757 0.2521766126155853 2.3073136806488037\n",
      "hidden_states min max: -9.782647132873535 10.444350242614746\n",
      "hidden_state minus mean squared max: 41.9425048828125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.76168060302734 3.372067928314209\n",
      "loss  1262: 1.8330   grad norm: 1.8357          model param norm: 87.2284        \n",
      "\n",
      "quiet_star_policy_loss= -0.04255428910255432\n",
      "nll_loss= 1.8453972339630127\n",
      "avg_std= 0.25333061814308167\n",
      "dist std min max: 0.0129837142303586 0.25333061814308167 2.2421674728393555\n",
      "hidden_states min max: -11.331801414489746 10.103219032287598\n",
      "hidden_state minus mean squared max: 249.8096466064453\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29293823242188 3.3974952697753906\n",
      "loss  1263: 1.8028   grad norm: 1.6372          model param norm: 87.2296        \n",
      "\n",
      "quiet_star_policy_loss= -0.0164016243070364\n",
      "nll_loss= 1.837340235710144\n",
      "avg_std= 0.2539726495742798\n",
      "dist std min max: 0.014643577858805656 0.2539726495742798 2.141629219055176\n",
      "hidden_states min max: -9.987998008728027 10.332117080688477\n",
      "hidden_state minus mean squared max: 50.3146858215332\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.02078247070312 3.2761545181274414\n",
      "loss  1264: 1.8209   grad norm: 1.8520          model param norm: 87.2309        \n",
      "\n",
      "quiet_star_policy_loss= -0.0034827471245080233\n",
      "nll_loss= 1.8274449110031128\n",
      "avg_std= 0.2518726885318756\n",
      "dist std min max: 0.012743057683110237 0.2518726885318756 2.3377909660339355\n",
      "hidden_states min max: -13.510461807250977 9.952295303344727\n",
      "hidden_state minus mean squared max: 166.24267578125\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.08930206298828 3.433025360107422\n",
      "loss  1265: 1.8240   grad norm: 1.7398          model param norm: 87.2324        \n",
      "\n",
      "quiet_star_policy_loss= -0.00986905675381422\n",
      "nll_loss= 1.8367359638214111\n",
      "avg_std= 0.2536737620830536\n",
      "dist std min max: 0.012975291348993778 0.2536737620830536 2.309086322784424\n",
      "hidden_states min max: -9.80544662475586 10.067310333251953\n",
      "hidden_state minus mean squared max: 56.924888610839844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.5676040649414 3.407985210418701\n",
      "loss  1266: 1.8269   grad norm: 1.7865          model param norm: 87.2336        \n",
      "\n",
      "quiet_star_policy_loss= -0.01104350108653307\n",
      "nll_loss= 1.8255910873413086\n",
      "avg_std= 0.2519371509552002\n",
      "dist std min max: 0.012588262557983398 0.2519371509552002 2.5286576747894287\n",
      "hidden_states min max: -13.990194320678711 11.652387619018555\n",
      "hidden_state minus mean squared max: 258.0724182128906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.30919647216797 3.437389373779297\n",
      "loss  1267: 1.8145   grad norm: 1.7469          model param norm: 87.2353        \n",
      "\n",
      "quiet_star_policy_loss= 0.03059716336429119\n",
      "nll_loss= 1.8229163885116577\n",
      "avg_std= 0.25147536396980286\n",
      "dist std min max: 0.012876827269792557 0.25147536396980286 2.1549625396728516\n",
      "hidden_states min max: -9.722307205200195 9.977736473083496\n",
      "hidden_state minus mean squared max: 57.31418991088867\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0406494140625 3.3685593605041504\n",
      "loss  1268: 1.8535   grad norm: 1.8291          model param norm: 87.2370        \n",
      "\n",
      "quiet_star_policy_loss= -0.00019047260866500437\n",
      "nll_loss= 1.8357715606689453\n",
      "avg_std= 0.25093069672584534\n",
      "dist std min max: 0.012317589484155178 0.25093069672584534 2.2656426429748535\n",
      "hidden_states min max: -17.14498519897461 10.070205688476562\n",
      "hidden_state minus mean squared max: 252.9709014892578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.29923248291016 3.443728446960449\n",
      "loss  1269: 1.8356   grad norm: 1.7964          model param norm: 87.2390        \n",
      "\n",
      "quiet_star_policy_loss= -0.010863876901566982\n",
      "nll_loss= 1.8431885242462158\n",
      "avg_std= 0.24915501475334167\n",
      "dist std min max: 0.01316864788532257 0.24915501475334167 2.2164905071258545\n",
      "hidden_states min max: -9.794493675231934 10.645613670349121\n",
      "hidden_state minus mean squared max: 48.46921920776367\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.89411163330078 3.4109745025634766\n",
      "loss  1270: 1.8323   grad norm: 1.8007          model param norm: 87.2409        \n",
      "\n",
      "quiet_star_policy_loss= -0.01522140484303236\n",
      "nll_loss= 1.8152267932891846\n",
      "avg_std= 0.25078800320625305\n",
      "dist std min max: 0.012502563185989857 0.25078800320625305 2.308678388595581\n",
      "hidden_states min max: -12.247625350952148 10.870260238647461\n",
      "hidden_state minus mean squared max: 104.22562408447266\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.8558578491211 3.427553176879883\n",
      "loss  1271: 1.8000   grad norm: 1.8548          model param norm: 87.2428        \n",
      "\n",
      "quiet_star_policy_loss= 0.017076600342988968\n",
      "nll_loss= 1.8368488550186157\n",
      "avg_std= 0.2509578764438629\n",
      "dist std min max: 0.012469175271689892 0.2509578764438629 2.2795300483703613\n",
      "hidden_states min max: -9.81000804901123 10.072948455810547\n",
      "hidden_state minus mean squared max: 53.206390380859375\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.04710388183594 3.4534382820129395\n",
      "loss  1272: 1.8539   grad norm: 1.6516          model param norm: 87.2447        \n",
      "\n",
      "quiet_star_policy_loss= -0.0038792432751506567\n",
      "nll_loss= 1.8474911451339722\n",
      "avg_std= 0.25104567408561707\n",
      "dist std min max: 0.013102029450237751 0.25104567408561707 2.373912811279297\n",
      "hidden_states min max: -9.645397186279297 10.121438026428223\n",
      "hidden_state minus mean squared max: 58.6806755065918\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.49944305419922 3.4107489585876465\n",
      "loss  1273: 1.8436   grad norm: 1.7677          model param norm: 87.2463        \n",
      "\n",
      "quiet_star_policy_loss= -0.004315292928367853\n",
      "nll_loss= 1.8431488275527954\n",
      "avg_std= 0.25078722834587097\n",
      "dist std min max: 0.011917617172002792 0.25078722834587097 2.332841157913208\n",
      "hidden_states min max: -11.874795913696289 10.509507179260254\n",
      "hidden_state minus mean squared max: 187.0227813720703\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.14820098876953 3.5102405548095703\n",
      "loss  1274: 1.8388   grad norm: 1.6895          model param norm: 87.2483        \n",
      "\n",
      "quiet_star_policy_loss= -0.04963059350848198\n",
      "nll_loss= 1.8271658420562744\n",
      "avg_std= 0.25092822313308716\n",
      "dist std min max: 0.011954355984926224 0.25092822313308716 2.6320502758026123\n",
      "hidden_states min max: -10.789268493652344 10.701330184936523\n",
      "hidden_state minus mean squared max: 114.1982192993164\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.90155029296875 3.5068392753601074\n",
      "loss  1275: 1.7775   grad norm: 1.7269          model param norm: 87.2505        \n",
      "\n",
      "quiet_star_policy_loss= -0.026018185541033745\n",
      "nll_loss= 1.8182239532470703\n",
      "avg_std= 0.2493826299905777\n",
      "dist std min max: 0.012024922296404839 0.2493826299905777 2.2318592071533203\n",
      "hidden_states min max: -12.905692100524902 10.226861000061035\n",
      "hidden_state minus mean squared max: 95.9805908203125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.81464385986328 3.477475643157959\n",
      "loss  1276: 1.7922   grad norm: 1.7988          model param norm: 87.2525        \n",
      "\n",
      "quiet_star_policy_loss= -0.02068709209561348\n",
      "nll_loss= 1.8309071063995361\n",
      "avg_std= 0.24951332807540894\n",
      "dist std min max: 0.011913815513253212 0.24951332807540894 2.2036044597625732\n",
      "hidden_states min max: -9.630475997924805 10.868175506591797\n",
      "hidden_state minus mean squared max: 69.0708236694336\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65015411376953 3.451838970184326\n",
      "loss  1277: 1.8102   grad norm: 1.6356          model param norm: 87.2548        \n",
      "\n",
      "quiet_star_policy_loss= -0.006040775682777166\n",
      "nll_loss= 1.8449954986572266\n",
      "avg_std= 0.250517874956131\n",
      "dist std min max: 0.012086368165910244 0.250517874956131 2.2612431049346924\n",
      "hidden_states min max: -9.354521751403809 11.756570816040039\n",
      "hidden_state minus mean squared max: 89.46205139160156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.81956481933594 3.4950242042541504\n",
      "loss  1278: 1.8390   grad norm: 1.8302          model param norm: 87.2569        \n",
      "\n",
      "quiet_star_policy_loss= -0.013202500529587269\n",
      "nll_loss= 1.8353550434112549\n",
      "avg_std= 0.24865606427192688\n",
      "dist std min max: 0.012025753036141396 0.24865606427192688 2.4452946186065674\n",
      "hidden_states min max: -10.726099014282227 10.81257438659668\n",
      "hidden_state minus mean squared max: 47.48210525512695\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.28657531738281 3.4629387855529785\n",
      "loss  1279: 1.8222   grad norm: 1.7581          model param norm: 87.2591        \n",
      "\n",
      "quiet_star_policy_loss= -0.017324233427643776\n",
      "nll_loss= 1.8212636709213257\n",
      "avg_std= 0.24800223112106323\n",
      "dist std min max: 0.011896450072526932 0.24800223112106323 2.390998601913452\n",
      "hidden_states min max: -9.431814193725586 10.995674133300781\n",
      "hidden_state minus mean squared max: 54.79696273803711\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.19996643066406 3.4777474403381348\n",
      "loss  1280: 1.8039   grad norm: 1.7420          model param norm: 87.2611        \n",
      "\n",
      "quiet_star_policy_loss= -0.0125046968460083\n",
      "nll_loss= 1.8278827667236328\n",
      "avg_std= 0.2501929700374603\n",
      "dist std min max: 0.012149895541369915 0.2501929700374603 2.5907180309295654\n",
      "hidden_states min max: -9.407537460327148 11.871143341064453\n",
      "hidden_state minus mean squared max: 68.61724090576172\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.26115417480469 3.4473166465759277\n",
      "loss  1281: 1.8154   grad norm: 1.6656          model param norm: 87.2629        \n",
      "\n",
      "quiet_star_policy_loss= 0.008335578255355358\n",
      "nll_loss= 1.829048752784729\n",
      "avg_std= 0.2477104514837265\n",
      "dist std min max: 0.0121004618704319 0.2477104514837265 2.439647674560547\n",
      "hidden_states min max: -10.118751525878906 10.288358688354492\n",
      "hidden_state minus mean squared max: 146.65589904785156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.026611328125 3.4937877655029297\n",
      "loss  1282: 1.8374   grad norm: 1.7257          model param norm: 87.2648        \n",
      "\n",
      "quiet_star_policy_loss= 0.012560153380036354\n",
      "nll_loss= 1.8335583209991455\n",
      "avg_std= 0.2487400621175766\n",
      "dist std min max: 0.01179767120629549 0.2487400621175766 2.6227760314941406\n",
      "hidden_states min max: -9.376364707946777 10.547170639038086\n",
      "hidden_state minus mean squared max: 53.807220458984375\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.54579162597656 3.5184388160705566\n",
      "loss  1283: 1.8461   grad norm: 1.6600          model param norm: 87.2669        \n",
      "\n",
      "quiet_star_policy_loss= 0.009817242622375488\n",
      "nll_loss= 1.8334228992462158\n",
      "avg_std= 0.24799975752830505\n",
      "dist std min max: 0.012061801739037037 0.24799975752830505 2.5515048503875732\n",
      "hidden_states min max: -9.416045188903809 10.430213928222656\n",
      "hidden_state minus mean squared max: 45.32980728149414\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.58378601074219 3.4764442443847656\n",
      "loss  1284: 1.8432   grad norm: 1.7839          model param norm: 87.2690        \n",
      "\n",
      "quiet_star_policy_loss= -0.010302424430847168\n",
      "nll_loss= 1.8096929788589478\n",
      "avg_std= 0.24818558990955353\n",
      "dist std min max: 0.011967422440648079 0.24818558990955353 2.443235158920288\n",
      "hidden_states min max: -9.526410102844238 10.437652587890625\n",
      "hidden_state minus mean squared max: 58.360145568847656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.61310577392578 3.4867076873779297\n",
      "loss  1285: 1.7994   grad norm: 1.7817          model param norm: 87.2709        \n",
      "\n",
      "quiet_star_policy_loss= 0.014160096645355225\n",
      "nll_loss= 1.809031367301941\n",
      "avg_std= 0.2506513297557831\n",
      "dist std min max: 0.012676894664764404 0.2506513297557831 2.521915912628174\n",
      "hidden_states min max: -9.564471244812012 11.02538013458252\n",
      "hidden_state minus mean squared max: 47.22683334350586\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.08203125 3.4297385215759277\n",
      "loss  1286: 1.8232   grad norm: 1.7124          model param norm: 87.2728        \n",
      "\n",
      "quiet_star_policy_loss= 0.05642944574356079\n",
      "nll_loss= 1.837307333946228\n",
      "avg_std= 0.24908724427223206\n",
      "dist std min max: 0.013336359523236752 0.24908724427223206 2.253788709640503\n",
      "hidden_states min max: -9.497370719909668 10.022276878356934\n",
      "hidden_state minus mean squared max: 47.75193405151367\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -102.00869750976562 3.3647618293762207\n",
      "loss  1287: 1.8937   grad norm: 3.8302          model param norm: 87.2749        \n",
      "\n",
      "quiet_star_policy_loss= 0.009256863966584206\n",
      "nll_loss= 1.8250713348388672\n",
      "avg_std= 0.25164929032325745\n",
      "dist std min max: 0.01177009753882885 0.25164929032325745 2.5545709133148193\n",
      "hidden_states min max: -9.392497062683105 10.522188186645508\n",
      "hidden_state minus mean squared max: 48.63502883911133\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.29026794433594 3.522336006164551\n",
      "loss  1288: 1.8343   grad norm: 1.8156          model param norm: 87.2768        \n",
      "\n",
      "quiet_star_policy_loss= -0.005041675176471472\n",
      "nll_loss= 1.825835108757019\n",
      "avg_std= 0.24953624606132507\n",
      "dist std min max: 0.011893387883901596 0.24953624606132507 2.310831069946289\n",
      "hidden_states min max: -13.457995414733887 9.932821273803711\n",
      "hidden_state minus mean squared max: 251.85665893554688\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.2969970703125 3.5068869590759277\n",
      "loss  1289: 1.8208   grad norm: 1.9247          model param norm: 87.2784        \n",
      "\n",
      "quiet_star_policy_loss= -0.00433356175199151\n",
      "nll_loss= 1.8089474439620972\n",
      "avg_std= 0.24946658313274384\n",
      "dist std min max: 0.012035190127789974 0.24946658313274384 2.4187674522399902\n",
      "hidden_states min max: -9.452600479125977 10.493535041809082\n",
      "hidden_state minus mean squared max: 54.196231842041016\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.76082611083984 3.4644908905029297\n",
      "loss  1290: 1.8046   grad norm: 1.9452          model param norm: 87.2800        \n",
      "\n",
      "quiet_star_policy_loss= -0.01165084820240736\n",
      "nll_loss= 1.8276211023330688\n",
      "avg_std= 0.24948950111865997\n",
      "dist std min max: 0.011816328391432762 0.24948950111865997 2.2314088344573975\n",
      "hidden_states min max: -9.34130859375 10.309354782104492\n",
      "hidden_state minus mean squared max: 70.44213104248047\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.65997314453125 3.5148229598999023\n",
      "loss  1291: 1.8160   grad norm: 1.7252          model param norm: 87.2816        \n",
      "\n",
      "quiet_star_policy_loss= -0.0033715367317199707\n",
      "nll_loss= 1.824243187904358\n",
      "avg_std= 0.24906879663467407\n",
      "dist std min max: 0.011923745274543762 0.24906879663467407 2.496361017227173\n",
      "hidden_states min max: -9.875299453735352 10.885831832885742\n",
      "hidden_state minus mean squared max: 56.406707763671875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.89997863769531 3.4930267333984375\n",
      "loss  1292: 1.8209   grad norm: 1.7497          model param norm: 87.2833        \n",
      "\n",
      "quiet_star_policy_loss= -0.018622374162077904\n",
      "nll_loss= 1.8293184041976929\n",
      "avg_std= 0.2500944435596466\n",
      "dist std min max: 0.01251684408634901 0.2500944435596466 2.3517651557922363\n",
      "hidden_states min max: -9.324488639831543 10.337263107299805\n",
      "hidden_state minus mean squared max: 83.02359008789062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.74214172363281 3.408839225769043\n",
      "loss  1293: 1.8107   grad norm: 1.9462          model param norm: 87.2851        \n",
      "\n",
      "quiet_star_policy_loss= 0.02193130925297737\n",
      "nll_loss= 1.8365089893341064\n",
      "avg_std= 0.24798208475112915\n",
      "dist std min max: 0.0119129354134202 0.24798208475112915 2.310861110687256\n",
      "hidden_states min max: -12.404507637023926 9.96672248840332\n",
      "hidden_state minus mean squared max: 50.420536041259766\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.4927749633789 3.502577304840088\n",
      "loss  1294: 1.8584   grad norm: 1.7328          model param norm: 87.2872        \n",
      "\n",
      "quiet_star_policy_loss= -0.017270123586058617\n",
      "nll_loss= 1.833197832107544\n",
      "avg_std= 0.250973105430603\n",
      "dist std min max: 0.011844811961054802 0.250973105430603 2.294419765472412\n",
      "hidden_states min max: -9.457087516784668 10.901671409606934\n",
      "hidden_state minus mean squared max: 50.35551452636719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.24156951904297 3.506296157836914\n",
      "loss  1295: 1.8159   grad norm: 2.0148          model param norm: 87.2890        \n",
      "\n",
      "quiet_star_policy_loss= -0.03627319261431694\n",
      "nll_loss= 1.833713173866272\n",
      "avg_std= 0.25163012742996216\n",
      "dist std min max: 0.011698388494551182 0.25163012742996216 2.7936794757843018\n",
      "hidden_states min max: -9.555303573608398 11.08716106414795\n",
      "hidden_state minus mean squared max: 63.90021896362305\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.91014099121094 3.518148422241211\n",
      "loss  1296: 1.7974   grad norm: 1.8801          model param norm: 87.2908        \n",
      "\n",
      "quiet_star_policy_loss= 0.01798884943127632\n",
      "nll_loss= 1.8250764608383179\n",
      "avg_std= 0.2529297471046448\n",
      "dist std min max: 0.011994696222245693 0.2529297471046448 2.298093557357788\n",
      "hidden_states min max: -9.46231746673584 10.415603637695312\n",
      "hidden_state minus mean squared max: 54.80926513671875\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.53450012207031 3.487114906311035\n",
      "loss  1297: 1.8431   grad norm: 1.7277          model param norm: 87.2927        \n",
      "\n",
      "quiet_star_policy_loss= -0.000980722950771451\n",
      "nll_loss= 1.81816565990448\n",
      "avg_std= 0.2528308629989624\n",
      "dist std min max: 0.011993306688964367 0.2528308629989624 2.6504220962524414\n",
      "hidden_states min max: -9.500100135803223 10.521241188049316\n",
      "hidden_state minus mean squared max: 53.96553421020508\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.53076934814453 3.501281261444092\n",
      "loss  1298: 1.8172   grad norm: 1.8078          model param norm: 87.2948        \n",
      "\n",
      "quiet_star_policy_loss= 0.012704926542937756\n",
      "nll_loss= 1.8254226446151733\n",
      "avg_std= 0.25033730268478394\n",
      "dist std min max: 0.012028368189930916 0.25033730268478394 2.674802303314209\n",
      "hidden_states min max: -9.716940879821777 9.98723030090332\n",
      "hidden_state minus mean squared max: 53.320167541503906\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.48196411132812 3.490828514099121\n",
      "loss  1299: 1.8381   grad norm: 1.9195          model param norm: 87.2968        \n",
      "eval loss 1.8352890014648438\n",
      "\n",
      "quiet_star_policy_loss= 0.012323522940278053\n",
      "nll_loss= 1.8137859106063843\n",
      "avg_std= 0.2528762221336365\n",
      "dist std min max: 0.012285889126360416 0.2528762221336365 2.4143593311309814\n",
      "hidden_states min max: -14.93588924407959 10.971062660217285\n",
      "hidden_state minus mean squared max: 281.2375793457031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.35218048095703 3.4497413635253906\n",
      "loss  1300: 1.8261   grad norm: 1.7547          model param norm: 87.2990        \n",
      "\n",
      "quiet_star_policy_loss= -0.03035186603665352\n",
      "nll_loss= 1.8361263275146484\n",
      "avg_std= 0.2521577477455139\n",
      "dist std min max: 0.012353381142020226 0.2521577477455139 2.350005865097046\n",
      "hidden_states min max: -9.452226638793945 11.282530784606934\n",
      "hidden_state minus mean squared max: 54.3450813293457\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.1343994140625 3.458873748779297\n",
      "loss  1301: 1.8058   grad norm: 1.8816          model param norm: 87.3006        \n",
      "\n",
      "quiet_star_policy_loss= -0.008555578999221325\n",
      "nll_loss= 1.8375908136367798\n",
      "avg_std= 0.25150710344314575\n",
      "dist std min max: 0.012177370488643646 0.25150710344314575 2.516242504119873\n",
      "hidden_states min max: -9.396760940551758 10.33119010925293\n",
      "hidden_state minus mean squared max: 62.18898010253906\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.17111206054688 3.4697518348693848\n",
      "loss  1302: 1.8290   grad norm: 1.7167          model param norm: 87.3026        \n",
      "\n",
      "quiet_star_policy_loss= -0.0051811458542943\n",
      "nll_loss= 1.8495782613754272\n",
      "avg_std= 0.25037264823913574\n",
      "dist std min max: 0.011757325381040573 0.25037264823913574 2.648698568344116\n",
      "hidden_states min max: -12.634819030761719 10.27916145324707\n",
      "hidden_state minus mean squared max: 210.72756958007812\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -104.2078628540039 3.4941186904907227\n",
      "loss  1303: 1.8444   grad norm: 1.7002          model param norm: 87.3045        \n",
      "\n",
      "quiet_star_policy_loss= 0.012278533540666103\n",
      "nll_loss= 1.8268747329711914\n",
      "avg_std= 0.2496890425682068\n",
      "dist std min max: 0.011846257373690605 0.2496890425682068 2.3281095027923584\n",
      "hidden_states min max: -9.403207778930664 10.681617736816406\n",
      "hidden_state minus mean squared max: 54.87967300415039\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.13213348388672 3.498077392578125\n",
      "loss  1304: 1.8392   grad norm: 1.7334          model param norm: 87.3066        \n",
      "\n",
      "quiet_star_policy_loss= -0.03643224760890007\n",
      "nll_loss= 1.8011385202407837\n",
      "avg_std= 0.25291725993156433\n",
      "dist std min max: 0.012102740816771984 0.25291725993156433 2.3581740856170654\n",
      "hidden_states min max: -9.892472267150879 10.363627433776855\n",
      "hidden_state minus mean squared max: 59.10295104980469\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.30962371826172 3.490809917449951\n",
      "loss  1305: 1.7647   grad norm: 1.7039          model param norm: 87.3088        \n",
      "\n",
      "quiet_star_policy_loss= -0.010003424249589443\n",
      "nll_loss= 1.830210566520691\n",
      "avg_std= 0.25203782320022583\n",
      "dist std min max: 0.012293553911149502 0.25203782320022583 3.0570077896118164\n",
      "hidden_states min max: -13.671006202697754 10.558963775634766\n",
      "hidden_state minus mean squared max: 97.62584686279297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.82316589355469 3.457428455352783\n",
      "loss  1306: 1.8202   grad norm: 1.7885          model param norm: 87.3111        \n",
      "\n",
      "quiet_star_policy_loss= -0.0078277587890625\n",
      "nll_loss= 1.8274307250976562\n",
      "avg_std= 0.2514990568161011\n",
      "dist std min max: 0.011995679698884487 0.2514990568161011 2.4811666011810303\n",
      "hidden_states min max: -9.599967002868652 11.278095245361328\n",
      "hidden_state minus mean squared max: 53.89468765258789\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.90180969238281 3.4293131828308105\n",
      "loss  1307: 1.8196   grad norm: 1.8789          model param norm: 87.3132        \n",
      "\n",
      "quiet_star_policy_loss= -0.007310402579605579\n",
      "nll_loss= 1.8402178287506104\n",
      "avg_std= 0.25122490525245667\n",
      "dist std min max: 0.011637932620942593 0.25122490525245667 2.5255448818206787\n",
      "hidden_states min max: -9.81923770904541 10.547403335571289\n",
      "hidden_state minus mean squared max: 47.21759796142578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.12983703613281 3.5292186737060547\n",
      "loss  1308: 1.8329   grad norm: 1.7644          model param norm: 87.3151        \n",
      "\n",
      "quiet_star_policy_loss= -0.03216903284192085\n",
      "nll_loss= 1.8216530084609985\n",
      "avg_std= 0.2522341012954712\n",
      "dist std min max: 0.012402327731251717 0.2522341012954712 2.372180700302124\n",
      "hidden_states min max: -9.857385635375977 10.663671493530273\n",
      "hidden_state minus mean squared max: 46.999794006347656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.10621643066406 3.4470887184143066\n",
      "loss  1309: 1.7895   grad norm: 1.9463          model param norm: 87.3171        \n",
      "\n",
      "quiet_star_policy_loss= -0.030331779271364212\n",
      "nll_loss= 1.8255313634872437\n",
      "avg_std= 0.2513510286808014\n",
      "dist std min max: 0.011491606943309307 0.2513510286808014 2.5286288261413574\n",
      "hidden_states min max: -9.759488105773926 10.821735382080078\n",
      "hidden_state minus mean squared max: 48.06294631958008\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.29732513427734 3.513967990875244\n",
      "loss  1310: 1.7952   grad norm: 1.8761          model param norm: 87.3190        \n",
      "\n",
      "quiet_star_policy_loss= 0.01736152172088623\n",
      "nll_loss= 1.8330354690551758\n",
      "avg_std= 0.24982623755931854\n",
      "dist std min max: 0.01188697014003992 0.24982623755931854 2.41729474067688\n",
      "hidden_states min max: -11.982077598571777 10.514455795288086\n",
      "hidden_state minus mean squared max: 83.02812194824219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.74215698242188 3.5028090476989746\n",
      "loss  1311: 1.8504   grad norm: 1.6780          model param norm: 87.3209        \n",
      "\n",
      "quiet_star_policy_loss= 0.014570570550858974\n",
      "nll_loss= 1.8213436603546143\n",
      "avg_std= 0.25191813707351685\n",
      "dist std min max: 0.011791910976171494 0.25191813707351685 2.666886568069458\n",
      "hidden_states min max: -9.39204216003418 11.862126350402832\n",
      "hidden_state minus mean squared max: 62.811805725097656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.02725219726562 3.5065689086914062\n",
      "loss  1312: 1.8359   grad norm: 1.5912          model param norm: 87.3229        \n",
      "\n",
      "quiet_star_policy_loss= -0.019225163385272026\n",
      "nll_loss= 1.8365989923477173\n",
      "avg_std= 0.2504764497280121\n",
      "dist std min max: 0.012997857294976711 0.2504764497280121 2.4451940059661865\n",
      "hidden_states min max: -10.074091911315918 10.566703796386719\n",
      "hidden_state minus mean squared max: 48.73988723754883\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.8069076538086 3.408392906188965\n",
      "loss  1313: 1.8174   grad norm: 1.7550          model param norm: 87.3248        \n",
      "\n",
      "quiet_star_policy_loss= -0.025117134675383568\n",
      "nll_loss= 1.817179560661316\n",
      "avg_std= 0.2502388656139374\n",
      "dist std min max: 0.011498880572617054 0.2502388656139374 2.4467039108276367\n",
      "hidden_states min max: -9.447139739990234 10.57056713104248\n",
      "hidden_state minus mean squared max: 46.284706115722656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.31452941894531 3.5159406661987305\n",
      "loss  1314: 1.7921   grad norm: 1.8659          model param norm: 87.3268        \n",
      "\n",
      "quiet_star_policy_loss= -0.0082555515691638\n",
      "nll_loss= 1.8436617851257324\n",
      "avg_std= 0.25084078311920166\n",
      "dist std min max: 0.011673307977616787 0.25084078311920166 2.213885545730591\n",
      "hidden_states min max: -11.985669136047363 10.574231147766113\n",
      "hidden_state minus mean squared max: 219.0208282470703\n",
      "hidden_state minus mean divided by std max: 5.035405158996582\n",
      "log_prob min max: -104.22716522216797 3.487304210662842\n",
      "loss  1315: 1.8354   grad norm: 3.5838          model param norm: 87.3284        \n",
      "\n",
      "quiet_star_policy_loss= -0.005225497763603926\n",
      "nll_loss= 1.8184212446212769\n",
      "avg_std= 0.2534581124782562\n",
      "dist std min max: 0.01230552513152361 0.2534581124782562 2.4766807556152344\n",
      "hidden_states min max: -9.732693672180176 10.57239055633545\n",
      "hidden_state minus mean squared max: 57.977928161621094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.31004333496094 3.467219829559326\n",
      "loss  1316: 1.8132   grad norm: 1.7560          model param norm: 87.3304        \n",
      "\n",
      "quiet_star_policy_loss= -0.03792274743318558\n",
      "nll_loss= 1.8327926397323608\n",
      "avg_std= 0.25164124369621277\n",
      "dist std min max: 0.011973907239735126 0.25164124369621277 2.3866608142852783\n",
      "hidden_states min max: -9.690472602844238 10.591715812683105\n",
      "hidden_state minus mean squared max: 52.768287658691406\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.73180389404297 3.500427722930908\n",
      "loss  1317: 1.7949   grad norm: 1.9784          model param norm: 87.3324        \n",
      "\n",
      "quiet_star_policy_loss= -0.02975686825811863\n",
      "nll_loss= 1.8375011682510376\n",
      "avg_std= 0.2507700026035309\n",
      "dist std min max: 0.01108461432158947 0.2507700026035309 2.6238856315612793\n",
      "hidden_states min max: -9.536785125732422 10.739704132080078\n",
      "hidden_state minus mean squared max: 54.50014877319336\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.83496856689453 3.5700554847717285\n",
      "loss  1318: 1.8077   grad norm: 1.7622          model param norm: 87.3346        \n",
      "\n",
      "quiet_star_policy_loss= 0.011596107855439186\n",
      "nll_loss= 1.8249130249023438\n",
      "avg_std= 0.25156688690185547\n",
      "dist std min max: 0.012512702494859695 0.25156688690185547 2.3926680088043213\n",
      "hidden_states min max: -12.073502540588379 10.648836135864258\n",
      "hidden_state minus mean squared max: 118.13544464111328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.91849517822266 3.442896842956543\n",
      "loss  1319: 1.8365   grad norm: 1.8230          model param norm: 87.3369        \n",
      "\n",
      "quiet_star_policy_loss= -0.011959529481828213\n",
      "nll_loss= 1.8097842931747437\n",
      "avg_std= 0.2528208792209625\n",
      "dist std min max: 0.011430496349930763 0.2528208792209625 2.414255380630493\n",
      "hidden_states min max: -9.435989379882812 10.713539123535156\n",
      "hidden_state minus mean squared max: 72.32453918457031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.545273780822754 3.5439720153808594\n",
      "loss  1320: 1.7978   grad norm: 1.8726          model param norm: 87.3390        \n",
      "\n",
      "quiet_star_policy_loss= -0.0594860203564167\n",
      "nll_loss= 1.8216615915298462\n",
      "avg_std= 0.2528153359889984\n",
      "dist std min max: 0.011330030858516693 0.2528153359889984 2.6685843467712402\n",
      "hidden_states min max: -9.445074081420898 10.62710952758789\n",
      "hidden_state minus mean squared max: 46.786075592041016\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.5884017944336 3.554598808288574\n",
      "loss  1321: 1.7622   grad norm: 1.9448          model param norm: 87.3406        \n",
      "\n",
      "quiet_star_policy_loss= -0.009439492598176003\n",
      "nll_loss= 1.8241608142852783\n",
      "avg_std= 0.25020524859428406\n",
      "dist std min max: 0.011209866032004356 0.25020524859428406 2.4765024185180664\n",
      "hidden_states min max: -9.405509948730469 11.119508743286133\n",
      "hidden_state minus mean squared max: 55.44157028198242\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.18238830566406 3.564873218536377\n",
      "loss  1322: 1.8147   grad norm: 1.7809          model param norm: 87.3426        \n",
      "\n",
      "quiet_star_policy_loss= -0.009504342451691628\n",
      "nll_loss= 1.8274868726730347\n",
      "avg_std= 0.2501947283744812\n",
      "dist std min max: 0.011390147730708122 0.2501947283744812 2.3486788272857666\n",
      "hidden_states min max: -9.384268760681152 10.941478729248047\n",
      "hidden_state minus mean squared max: 77.16543579101562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.47848510742188 3.5468263626098633\n",
      "loss  1323: 1.8180   grad norm: 2.0787          model param norm: 87.3445        \n",
      "\n",
      "quiet_star_policy_loss= -0.03371081501245499\n",
      "nll_loss= 1.8372585773468018\n",
      "avg_std= 0.2496664971113205\n",
      "dist std min max: 0.012232799082994461 0.2496664971113205 2.430799722671509\n",
      "hidden_states min max: -21.565364837646484 10.926145553588867\n",
      "hidden_state minus mean squared max: 397.0346984863281\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.52458953857422 3.4590182304382324\n",
      "loss  1324: 1.8035   grad norm: 1.7187          model param norm: 87.3465        \n",
      "\n",
      "quiet_star_policy_loss= -0.01616992987692356\n",
      "nll_loss= 1.8214561939239502\n",
      "avg_std= 0.2510359585285187\n",
      "dist std min max: 0.01146041601896286 0.2510359585285187 2.287961483001709\n",
      "hidden_states min max: -12.343546867370605 11.263067245483398\n",
      "hidden_state minus mean squared max: 59.65776062011719\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.5768814086914 3.5136499404907227\n",
      "loss  1325: 1.8053   grad norm: 1.6767          model param norm: 87.3483        \n",
      "\n",
      "quiet_star_policy_loss= 0.0023422003723680973\n",
      "nll_loss= 1.8338731527328491\n",
      "avg_std= 0.251228392124176\n",
      "dist std min max: 0.011147839948534966 0.251228392124176 2.6622607707977295\n",
      "hidden_states min max: -10.271915435791016 10.571600914001465\n",
      "hidden_state minus mean squared max: 60.71943664550781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.5131607055664 3.544940948486328\n",
      "loss  1326: 1.8362   grad norm: 1.7384          model param norm: 87.3499        \n",
      "\n",
      "quiet_star_policy_loss= -0.01637669838964939\n",
      "nll_loss= 1.8455950021743774\n",
      "avg_std= 0.24842123687267303\n",
      "dist std min max: 0.01126054022461176 0.24842123687267303 2.42388916015625\n",
      "hidden_states min max: -9.4663724899292 10.673131942749023\n",
      "hidden_state minus mean squared max: 67.26266479492188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.48262786865234 3.563384532928467\n",
      "loss  1327: 1.8292   grad norm: 1.9419          model param norm: 87.3516        \n",
      "\n",
      "quiet_star_policy_loss= -0.006322157569229603\n",
      "nll_loss= 1.8266397714614868\n",
      "avg_std= 0.24945254623889923\n",
      "dist std min max: 0.010760664939880371 0.24945254623889923 2.5439698696136475\n",
      "hidden_states min max: -9.370338439941406 11.113418579101562\n",
      "hidden_state minus mean squared max: 53.6501350402832\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.67863464355469 3.608692169189453\n",
      "loss  1328: 1.8203   grad norm: 1.7932          model param norm: 87.3535        \n",
      "\n",
      "quiet_star_policy_loss= 0.004340624902397394\n",
      "nll_loss= 1.8278896808624268\n",
      "avg_std= 0.2500615417957306\n",
      "dist std min max: 0.010850735940039158 0.2500615417957306 2.6206328868865967\n",
      "hidden_states min max: -10.074453353881836 10.888659477233887\n",
      "hidden_state minus mean squared max: 65.7013168334961\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.42554473876953 3.586617946624756\n",
      "loss  1329: 1.8322   grad norm: 1.7520          model param norm: 87.3555        \n",
      "\n",
      "quiet_star_policy_loss= 0.00043854714022018015\n",
      "nll_loss= 1.8319618701934814\n",
      "avg_std= 0.2478126436471939\n",
      "dist std min max: 0.010958002880215645 0.2478126436471939 2.460811138153076\n",
      "hidden_states min max: -9.436439514160156 11.356171607971191\n",
      "hidden_state minus mean squared max: 64.34846496582031\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -101.92012786865234 3.576012134552002\n",
      "loss  1330: 1.8324   grad norm: 1.8757          model param norm: 87.3574        \n",
      "\n",
      "quiet_star_policy_loss= 0.0003596395254135132\n",
      "nll_loss= 1.8257858753204346\n",
      "avg_std= 0.2494143694639206\n",
      "dist std min max: 0.011077560484409332 0.2494143694639206 2.4913723468780518\n",
      "hidden_states min max: -10.970632553100586 10.33407211303711\n",
      "hidden_state minus mean squared max: 59.289527893066406\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4730453491211 3.5673131942749023\n",
      "loss  1331: 1.8261   grad norm: 1.8354          model param norm: 87.3591        \n",
      "\n",
      "quiet_star_policy_loss= -0.039733219891786575\n",
      "nll_loss= 1.8269973993301392\n",
      "avg_std= 0.24735011160373688\n",
      "dist std min max: 0.010838883928954601 0.24735011160373688 2.562208890914917\n",
      "hidden_states min max: -14.675047874450684 10.513998031616211\n",
      "hidden_state minus mean squared max: 246.80540466308594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.28688049316406 3.599813461303711\n",
      "loss  1332: 1.7873   grad norm: 1.7986          model param norm: 87.3610        \n",
      "\n",
      "quiet_star_policy_loss= -0.044295262545347214\n",
      "nll_loss= 1.8206157684326172\n",
      "avg_std= 0.24885034561157227\n",
      "dist std min max: 0.010572106577455997 0.24885034561157227 2.4270639419555664\n",
      "hidden_states min max: -9.431625366210938 11.549722671508789\n",
      "hidden_state minus mean squared max: 53.083126068115234\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.00403594970703 3.614323616027832\n",
      "loss  1333: 1.7763   grad norm: 1.8479          model param norm: 87.3629        \n",
      "\n",
      "quiet_star_policy_loss= 0.0068850042298436165\n",
      "nll_loss= 1.8191769123077393\n",
      "avg_std= 0.2482401430606842\n",
      "dist std min max: 0.010819272138178349 0.2482401430606842 2.4490625858306885\n",
      "hidden_states min max: -9.658985137939453 11.242341995239258\n",
      "hidden_state minus mean squared max: 77.95172119140625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.89320373535156 3.584280490875244\n",
      "loss  1334: 1.8261   grad norm: 1.6312          model param norm: 87.3649        \n",
      "\n",
      "quiet_star_policy_loss= -0.021388603374361992\n",
      "nll_loss= 1.8137178421020508\n",
      "avg_std= 0.24672077596187592\n",
      "dist std min max: 0.0106678307056427 0.24672077596187592 2.488069534301758\n",
      "hidden_states min max: -9.265189170837402 10.770391464233398\n",
      "hidden_state minus mean squared max: 54.39915084838867\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.8056640625 3.597050666809082\n",
      "loss  1335: 1.7923   grad norm: 1.7115          model param norm: 87.3671        \n",
      "\n",
      "quiet_star_policy_loss= 0.0007746338960714638\n",
      "nll_loss= 1.8265259265899658\n",
      "avg_std= 0.24711057543754578\n",
      "dist std min max: 0.010685226880013943 0.24711057543754578 2.4571452140808105\n",
      "hidden_states min max: -9.835641860961914 11.590837478637695\n",
      "hidden_state minus mean squared max: 72.98851013183594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.677734375 3.619687080383301\n",
      "loss  1336: 1.8273   grad norm: 1.7255          model param norm: 87.3692        \n",
      "\n",
      "quiet_star_policy_loss= -0.026778722181916237\n",
      "nll_loss= 1.8069469928741455\n",
      "avg_std= 0.24716638028621674\n",
      "dist std min max: 0.011107767932116985 0.24716638028621674 2.401671886444092\n",
      "hidden_states min max: -9.684813499450684 10.826532363891602\n",
      "hidden_state minus mean squared max: 66.36578369140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.702486991882324 3.569985866546631\n",
      "loss  1337: 1.7802   grad norm: 2.0223          model param norm: 87.3710        \n",
      "\n",
      "quiet_star_policy_loss= -0.01499877031892538\n",
      "nll_loss= 1.8256844282150269\n",
      "avg_std= 0.24638713896274567\n",
      "dist std min max: 0.010527456179261208 0.24638713896274567 2.4073214530944824\n",
      "hidden_states min max: -11.32492446899414 10.864255905151367\n",
      "hidden_state minus mean squared max: 164.25540161132812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0832748413086 3.6117329597473145\n",
      "loss  1338: 1.8107   grad norm: 1.7270          model param norm: 87.3727        \n",
      "\n",
      "quiet_star_policy_loss= 0.022271830588579178\n",
      "nll_loss= 1.8375869989395142\n",
      "avg_std= 0.2469145804643631\n",
      "dist std min max: 0.010508698411285877 0.2469145804643631 2.569868326187134\n",
      "hidden_states min max: -18.33837127685547 11.135129928588867\n",
      "hidden_state minus mean squared max: 385.1497802734375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.50938415527344 3.6138434410095215\n",
      "loss  1339: 1.8599   grad norm: 1.7466          model param norm: 87.3744        \n",
      "\n",
      "quiet_star_policy_loss= 0.00484498729929328\n",
      "nll_loss= 1.8229881525039673\n",
      "avg_std= 0.24694493412971497\n",
      "dist std min max: 0.010331345722079277 0.24694493412971497 2.5170462131500244\n",
      "hidden_states min max: -13.735690116882324 10.948110580444336\n",
      "hidden_state minus mean squared max: 145.99984741210938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.0243911743164 3.6530895233154297\n",
      "loss  1340: 1.8278   grad norm: 1.8256          model param norm: 87.3762        \n",
      "\n",
      "quiet_star_policy_loss= 0.008658910170197487\n",
      "nll_loss= 1.8325637578964233\n",
      "avg_std= 0.24678830802440643\n",
      "dist std min max: 0.010290700010955334 0.24678830802440643 2.5994958877563477\n",
      "hidden_states min max: -9.525882720947266 10.86428165435791\n",
      "hidden_state minus mean squared max: 55.639408111572266\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.30027770996094 3.6407828330993652\n",
      "loss  1341: 1.8412   grad norm: 1.8277          model param norm: 87.3781        \n",
      "\n",
      "quiet_star_policy_loss= -0.0060961246490478516\n",
      "nll_loss= 1.8305448293685913\n",
      "avg_std= 0.24603530764579773\n",
      "dist std min max: 0.01086886040866375 0.24603530764579773 2.5760931968688965\n",
      "hidden_states min max: -9.087801933288574 11.0305814743042\n",
      "hidden_state minus mean squared max: 55.1556282043457\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.43286895751953 3.588040828704834\n",
      "loss  1342: 1.8244   grad norm: 1.7207          model param norm: 87.3798        \n",
      "\n",
      "quiet_star_policy_loss= -0.04870758205652237\n",
      "nll_loss= 1.8407189846038818\n",
      "avg_std= 0.2484527826309204\n",
      "dist std min max: 0.010537238791584969 0.2484527826309204 2.2460572719573975\n",
      "hidden_states min max: -9.37757396697998 10.474428176879883\n",
      "hidden_state minus mean squared max: 60.3857536315918\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -12.411149024963379 3.6161646842956543\n",
      "loss  1343: 1.7920   grad norm: 3.9798          model param norm: 87.3813        \n",
      "\n",
      "quiet_star_policy_loss= -0.008105874061584473\n",
      "nll_loss= 1.8056554794311523\n",
      "avg_std= 0.24723583459854126\n",
      "dist std min max: 0.010506954975426197 0.24723583459854126 2.395130157470703\n",
      "hidden_states min max: -9.378114700317383 10.719160079956055\n",
      "hidden_state minus mean squared max: 54.02315139770508\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.23820495605469 3.6304259300231934\n",
      "loss  1344: 1.7975   grad norm: 1.8124          model param norm: 87.3830        \n",
      "\n",
      "quiet_star_policy_loss= -0.011096668429672718\n",
      "nll_loss= 1.823569893836975\n",
      "avg_std= 0.24583442509174347\n",
      "dist std min max: 0.010281523689627647 0.24583442509174347 2.635519504547119\n",
      "hidden_states min max: -9.253228187561035 10.899651527404785\n",
      "hidden_state minus mean squared max: 60.1731071472168\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.14542388916016 3.6403818130493164\n",
      "loss  1345: 1.8125   grad norm: 1.8037          model param norm: 87.3847        \n",
      "\n",
      "quiet_star_policy_loss= 0.006268334574997425\n",
      "nll_loss= 1.8282616138458252\n",
      "avg_std= 0.24726907908916473\n",
      "dist std min max: 0.010511785745620728 0.24726907908916473 2.50474214553833\n",
      "hidden_states min max: -9.30794906616211 10.21175479888916\n",
      "hidden_state minus mean squared max: 62.9950065612793\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.24818420410156 3.60835599899292\n",
      "loss  1346: 1.8345   grad norm: 1.8323          model param norm: 87.3862        \n",
      "\n",
      "quiet_star_policy_loss= -0.00023756027803756297\n",
      "nll_loss= 1.8269814252853394\n",
      "avg_std= 0.24577642977237701\n",
      "dist std min max: 0.010369334369897842 0.24577642977237701 2.3884034156799316\n",
      "hidden_states min max: -9.19107437133789 11.09140396118164\n",
      "hidden_state minus mean squared max: 64.72250366210938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.26426696777344 3.634157657623291\n",
      "loss  1347: 1.8267   grad norm: 1.7600          model param norm: 87.3876        \n",
      "\n",
      "quiet_star_policy_loss= -0.003980791661888361\n",
      "nll_loss= 1.8252105712890625\n",
      "avg_std= 0.2500583231449127\n",
      "dist std min max: 0.01005881279706955 0.2500583231449127 2.578470230102539\n",
      "hidden_states min max: -9.326004028320312 10.344945907592773\n",
      "hidden_state minus mean squared max: 58.06578063964844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29895782470703 3.6629700660705566\n",
      "loss  1348: 1.8212   grad norm: 1.9297          model param norm: 87.3894        \n",
      "\n",
      "quiet_star_policy_loss= -0.003925824072211981\n",
      "nll_loss= 1.8119763135910034\n",
      "avg_std= 0.24813497066497803\n",
      "dist std min max: 0.009978569112718105 0.24813497066497803 2.4478681087493896\n",
      "hidden_states min max: -9.267191886901855 10.410518646240234\n",
      "hidden_state minus mean squared max: 58.97294235229492\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.2647476196289 3.663252353668213\n",
      "loss  1349: 1.8081   grad norm: 1.6959          model param norm: 87.3909        \n",
      "\n",
      "quiet_star_policy_loss= 0.0031446933280676603\n",
      "nll_loss= 1.8422149419784546\n",
      "avg_std= 0.24791140854358673\n",
      "dist std min max: 0.010334184393286705 0.24791140854358673 2.65388560295105\n",
      "hidden_states min max: -9.466252326965332 10.378870010375977\n",
      "hidden_state minus mean squared max: 56.38905334472656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.2355728149414 3.6329941749572754\n",
      "loss  1350: 1.8454   grad norm: 1.7258          model param norm: 87.3922        \n",
      "\n",
      "quiet_star_policy_loss= -0.011479616165161133\n",
      "nll_loss= 1.8332151174545288\n",
      "avg_std= 0.2487739473581314\n",
      "dist std min max: 0.009932412765920162 0.2487739473581314 2.593820810317993\n",
      "hidden_states min max: -9.102954864501953 10.856782913208008\n",
      "hidden_state minus mean squared max: 59.8565788269043\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.57854461669922 3.692796230316162\n",
      "loss  1351: 1.8217   grad norm: 1.7662          model param norm: 87.3936        \n",
      "\n",
      "quiet_star_policy_loss= 0.023754602298140526\n",
      "nll_loss= 1.8153167963027954\n",
      "avg_std= 0.24677398800849915\n",
      "dist std min max: 0.010054320096969604 0.24677398800849915 2.4948697090148926\n",
      "hidden_states min max: -9.600111961364746 10.471735000610352\n",
      "hidden_state minus mean squared max: 52.58358383178711\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.98194885253906 3.659501552581787\n",
      "loss  1352: 1.8391   grad norm: 1.7699          model param norm: 87.3951        \n",
      "\n",
      "quiet_star_policy_loss= -0.033425699919462204\n",
      "nll_loss= 1.8164348602294922\n",
      "avg_std= 0.24783623218536377\n",
      "dist std min max: 0.010236288420855999 0.24783623218536377 2.5737862586975098\n",
      "hidden_states min max: -10.761643409729004 11.110514640808105\n",
      "hidden_state minus mean squared max: 66.94132232666016\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.40460205078125 3.653149127960205\n",
      "loss  1353: 1.7830   grad norm: 1.8213          model param norm: 87.3965        \n",
      "\n",
      "quiet_star_policy_loss= 0.001191854476928711\n",
      "nll_loss= 1.8133009672164917\n",
      "avg_std= 0.24617750942707062\n",
      "dist std min max: 0.010501444339752197 0.24617750942707062 2.5611844062805176\n",
      "hidden_states min max: -9.307395935058594 11.25017261505127\n",
      "hidden_state minus mean squared max: 57.659053802490234\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.92988586425781 3.6232824325561523\n",
      "loss  1354: 1.8145   grad norm: 1.6988          model param norm: 87.3984        \n",
      "\n",
      "quiet_star_policy_loss= -0.02273111417889595\n",
      "nll_loss= 1.830870270729065\n",
      "avg_std= 0.24813446402549744\n",
      "dist std min max: 0.0100016500800848 0.24813446402549744 2.6155707836151123\n",
      "hidden_states min max: -9.49225902557373 10.572211265563965\n",
      "hidden_state minus mean squared max: 64.41961669921875\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.3473892211914 3.6714344024658203\n",
      "loss  1355: 1.8081   grad norm: 1.8921          model param norm: 87.4000        \n",
      "\n",
      "quiet_star_policy_loss= -0.0367155447602272\n",
      "nll_loss= 1.8223040103912354\n",
      "avg_std= 0.24620305001735687\n",
      "dist std min max: 0.009827090427279472 0.24620305001735687 2.4799084663391113\n",
      "hidden_states min max: -9.544048309326172 10.876684188842773\n",
      "hidden_state minus mean squared max: 82.63888549804688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.44740295410156 3.6939826011657715\n",
      "loss  1356: 1.7856   grad norm: 1.8380          model param norm: 87.4017        \n",
      "\n",
      "quiet_star_policy_loss= 0.020685434341430664\n",
      "nll_loss= 1.840164065361023\n",
      "avg_std= 0.24450354278087616\n",
      "dist std min max: 0.01032773032784462 0.24450354278087616 2.4724717140197754\n",
      "hidden_states min max: -9.239442825317383 10.875208854675293\n",
      "hidden_state minus mean squared max: 65.58269500732422\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -103.1473617553711 3.642118453979492\n",
      "loss  1357: 1.8608   grad norm: 1.9654          model param norm: 87.4034        \n",
      "\n",
      "quiet_star_policy_loss= -0.025085831061005592\n",
      "nll_loss= 1.820339560508728\n",
      "avg_std= 0.24553121626377106\n",
      "dist std min max: 0.00997787993401289 0.24553121626377106 2.397965669631958\n",
      "hidden_states min max: -10.272433280944824 11.524883270263672\n",
      "hidden_state minus mean squared max: 51.03664016723633\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.02717590332031 3.685995101928711\n",
      "loss  1358: 1.7953   grad norm: 1.8208          model param norm: 87.4051        \n",
      "\n",
      "quiet_star_policy_loss= -0.008624863810837269\n",
      "nll_loss= 1.830135703086853\n",
      "avg_std= 0.24325330555438995\n",
      "dist std min max: 0.010064532049000263 0.24325330555438995 2.3991806507110596\n",
      "hidden_states min max: -9.27596378326416 10.69293212890625\n",
      "hidden_state minus mean squared max: 57.33092498779297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.52015686035156 3.6623477935791016\n",
      "loss  1359: 1.8215   grad norm: 1.7395          model param norm: 87.4070        \n",
      "\n",
      "quiet_star_policy_loss= -0.018095647916197777\n",
      "nll_loss= 1.8391464948654175\n",
      "avg_std= 0.242280974984169\n",
      "dist std min max: 0.009552459232509136 0.242280974984169 2.5384719371795654\n",
      "hidden_states min max: -9.198161125183105 11.676973342895508\n",
      "hidden_state minus mean squared max: 64.71761322021484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.940185546875 3.7267465591430664\n",
      "loss  1360: 1.8211   grad norm: 1.9007          model param norm: 87.4087        \n",
      "\n",
      "quiet_star_policy_loss= -0.04479622840881348\n",
      "nll_loss= 1.8229659795761108\n",
      "avg_std= 0.24312163889408112\n",
      "dist std min max: 0.009617594070732594 0.24312163889408112 2.401484727859497\n",
      "hidden_states min max: -15.590542793273926 10.815812110900879\n",
      "hidden_state minus mean squared max: 300.5420837402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.38536834716797 3.6591782569885254\n",
      "loss  1361: 1.7782   grad norm: 1.7454          model param norm: 87.4105        \n",
      "\n",
      "quiet_star_policy_loss= -0.00932323932647705\n",
      "nll_loss= 1.8236258029937744\n",
      "avg_std= 0.24454636871814728\n",
      "dist std min max: 0.010584226809442043 0.24454636871814728 2.4739489555358887\n",
      "hidden_states min max: -9.63319206237793 10.987398147583008\n",
      "hidden_state minus mean squared max: 148.95297241210938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.03440856933594 3.612250804901123\n",
      "loss  1362: 1.8143   grad norm: 1.7339          model param norm: 87.4120        \n",
      "\n",
      "quiet_star_policy_loss= -0.00030553340911865234\n",
      "nll_loss= 1.8144944906234741\n",
      "avg_std= 0.24270164966583252\n",
      "dist std min max: 0.00999476108700037 0.24270164966583252 2.5101423263549805\n",
      "hidden_states min max: -9.221314430236816 10.82831859588623\n",
      "hidden_state minus mean squared max: 48.14841079711914\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.28363800048828 3.686603546142578\n",
      "loss  1363: 1.8142   grad norm: 1.7592          model param norm: 87.4138        \n",
      "\n",
      "quiet_star_policy_loss= -0.025216519832611084\n",
      "nll_loss= 1.8144222497940063\n",
      "avg_std= 0.24171848595142365\n",
      "dist std min max: 0.009459681808948517 0.24171848595142365 2.411952018737793\n",
      "hidden_states min max: -9.551000595092773 10.840044975280762\n",
      "hidden_state minus mean squared max: 65.00718688964844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.26962280273438 3.7323756217956543\n",
      "loss  1364: 1.7892   grad norm: 1.6430          model param norm: 87.4158        \n",
      "\n",
      "quiet_star_policy_loss= -0.022197140380740166\n",
      "nll_loss= 1.8020375967025757\n",
      "avg_std= 0.24419096112251282\n",
      "dist std min max: 0.009282246232032776 0.24419096112251282 2.4946234226226807\n",
      "hidden_states min max: -9.675909042358398 10.890732765197754\n",
      "hidden_state minus mean squared max: 70.11376190185547\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.65765380859375 3.752199172973633\n",
      "loss  1365: 1.7798   grad norm: 1.8561          model param norm: 87.4178        \n",
      "\n",
      "quiet_star_policy_loss= -0.02075488679111004\n",
      "nll_loss= 1.799904704093933\n",
      "avg_std= 0.2421722561120987\n",
      "dist std min max: 0.009066575206816196 0.2421722561120987 2.3631432056427\n",
      "hidden_states min max: -9.31871509552002 11.154020309448242\n",
      "hidden_state minus mean squared max: 62.46991729736328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.9025650024414 3.762681007385254\n",
      "loss  1366: 1.7791   grad norm: 1.7710          model param norm: 87.4195        \n",
      "\n",
      "quiet_star_policy_loss= 0.004978537559509277\n",
      "nll_loss= 1.8243988752365112\n",
      "avg_std= 0.2412596046924591\n",
      "dist std min max: 0.00951456930488348 0.2412596046924591 2.355351686477661\n",
      "hidden_states min max: -9.381454467773438 10.87565803527832\n",
      "hidden_state minus mean squared max: 61.20415496826172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.42304992675781 3.730287551879883\n",
      "loss  1367: 1.8294   grad norm: 1.9214          model param norm: 87.4212        \n",
      "\n",
      "quiet_star_policy_loss= -0.009217143058776855\n",
      "nll_loss= 1.8255754709243774\n",
      "avg_std= 0.24088160693645477\n",
      "dist std min max: 0.009495072066783905 0.24088160693645477 2.4513986110687256\n",
      "hidden_states min max: -9.298020362854004 10.858325004577637\n",
      "hidden_state minus mean squared max: 63.3262939453125\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.25485229492188 3.707703113555908\n",
      "loss  1368: 1.8164   grad norm: 1.8113          model param norm: 87.4226        \n",
      "\n",
      "quiet_star_policy_loss= 0.005531966686248779\n",
      "nll_loss= 1.8218425512313843\n",
      "avg_std= 0.23913215100765228\n",
      "dist std min max: 0.009663664735853672 0.23913215100765228 2.483344793319702\n",
      "hidden_states min max: -9.283411026000977 10.861978530883789\n",
      "hidden_state minus mean squared max: 49.107357025146484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.56448364257812 3.712207317352295\n",
      "loss  1369: 1.8274   grad norm: 1.9149          model param norm: 87.4238        \n",
      "\n",
      "quiet_star_policy_loss= -0.004734325688332319\n",
      "nll_loss= 1.8187083005905151\n",
      "avg_std= 0.2394728660583496\n",
      "dist std min max: 0.008978713303804398 0.2394728660583496 2.3666810989379883\n",
      "hidden_states min max: -11.91612434387207 10.813679695129395\n",
      "hidden_state minus mean squared max: 78.78976440429688\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.71595001220703 3.7681236267089844\n",
      "loss  1370: 1.8140   grad norm: 1.7646          model param norm: 87.4250        \n",
      "\n",
      "quiet_star_policy_loss= -0.016984254121780396\n",
      "nll_loss= 1.8099080324172974\n",
      "avg_std= 0.2379201501607895\n",
      "dist std min max: 0.009972091764211655 0.2379201501607895 2.4813477993011475\n",
      "hidden_states min max: -9.088802337646484 10.786992073059082\n",
      "hidden_state minus mean squared max: 48.495452880859375\n",
      "hidden_state minus mean divided by std max: 4.957176208496094\n",
      "log_prob min max: -12.699694633483887 3.6274490356445312\n",
      "loss  1371: 1.7929   grad norm: 3.7524          model param norm: 87.4261        \n",
      "\n",
      "quiet_star_policy_loss= -0.005485672038048506\n",
      "nll_loss= 1.822998046875\n",
      "avg_std= 0.23907753825187683\n",
      "dist std min max: 0.008802039548754692 0.23907753825187683 2.4733293056488037\n",
      "hidden_states min max: -9.230081558227539 10.83626937866211\n",
      "hidden_state minus mean squared max: 54.84550857543945\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.98680877685547 3.7983179092407227\n",
      "loss  1372: 1.8175   grad norm: 1.9128          model param norm: 87.4277        \n",
      "\n",
      "quiet_star_policy_loss= 0.00808937568217516\n",
      "nll_loss= 1.8493369817733765\n",
      "avg_std= 0.2384662628173828\n",
      "dist std min max: 0.008761998265981674 0.2384662628173828 2.403395175933838\n",
      "hidden_states min max: -9.90896987915039 10.796032905578613\n",
      "hidden_state minus mean squared max: 61.571041107177734\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -103.21375274658203 3.7930350303649902\n",
      "loss  1373: 1.8574   grad norm: 2.0772          model param norm: 87.4294        \n",
      "\n",
      "quiet_star_policy_loss= 0.005942344665527344\n",
      "nll_loss= 1.8217884302139282\n",
      "avg_std= 0.23835930228233337\n",
      "dist std min max: 0.009027468971908092 0.23835930228233337 2.4613707065582275\n",
      "hidden_states min max: -9.878471374511719 10.788666725158691\n",
      "hidden_state minus mean squared max: 80.73165893554688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7281494140625 3.7775816917419434\n",
      "loss  1374: 1.8277   grad norm: 1.8907          model param norm: 87.4308        \n",
      "\n",
      "quiet_star_policy_loss= -0.0006233215681277215\n",
      "nll_loss= 1.8128808736801147\n",
      "avg_std= 0.23920731246471405\n",
      "dist std min max: 0.009588630869984627 0.23920731246471405 2.5231776237487793\n",
      "hidden_states min max: -9.190303802490234 12.198928833007812\n",
      "hidden_state minus mean squared max: 58.361053466796875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.396240234375 3.717069149017334\n",
      "loss  1375: 1.8123   grad norm: 2.0387          model param norm: 87.4322        \n",
      "\n",
      "quiet_star_policy_loss= -0.05190684273838997\n",
      "nll_loss= 1.831309199333191\n",
      "avg_std= 0.2376241832971573\n",
      "dist std min max: 0.008861701935529709 0.2376241832971573 2.3570330142974854\n",
      "hidden_states min max: -18.22736930847168 11.089539527893066\n",
      "hidden_state minus mean squared max: 379.07568359375\n",
      "hidden_state minus mean divided by std max: 5.1665825843811035\n",
      "log_prob min max: -104.50145721435547 3.798483371734619\n",
      "loss  1376: 1.7794   grad norm: 1.6486          model param norm: 87.4334        \n",
      "\n",
      "quiet_star_policy_loss= -0.018614334985613823\n",
      "nll_loss= 1.8056856393814087\n",
      "avg_std= 0.23987479507923126\n",
      "dist std min max: 0.009507172740995884 0.23987479507923126 2.589449405670166\n",
      "hidden_states min max: -13.526702880859375 11.047941207885742\n",
      "hidden_state minus mean squared max: 137.97897338867188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9961166381836 3.7119131088256836\n",
      "loss  1377: 1.7871   grad norm: 1.8837          model param norm: 87.4348        \n",
      "\n",
      "quiet_star_policy_loss= 0.011476672254502773\n",
      "nll_loss= 1.8130314350128174\n",
      "avg_std= 0.23835106194019318\n",
      "dist std min max: 0.009244096465408802 0.23835106194019318 2.6232292652130127\n",
      "hidden_states min max: -9.260072708129883 10.891329765319824\n",
      "hidden_state minus mean squared max: 65.27517700195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06932067871094 3.729459285736084\n",
      "loss  1378: 1.8245   grad norm: 1.8318          model param norm: 87.4365        \n",
      "\n",
      "quiet_star_policy_loss= -0.001434582518413663\n",
      "nll_loss= 1.8283376693725586\n",
      "avg_std= 0.2391560822725296\n",
      "dist std min max: 0.01001131720840931 0.2391560822725296 2.40199613571167\n",
      "hidden_states min max: -13.504188537597656 11.500151634216309\n",
      "hidden_state minus mean squared max: 139.33840942382812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.00102996826172 3.6717166900634766\n",
      "loss  1379: 1.8269   grad norm: 2.0163          model param norm: 87.4382        \n",
      "\n",
      "quiet_star_policy_loss= -0.00022313595400191844\n",
      "nll_loss= 1.8307472467422485\n",
      "avg_std= 0.23792293667793274\n",
      "dist std min max: 0.008833995088934898 0.23792293667793274 2.3703815937042236\n",
      "hidden_states min max: -9.17814826965332 10.987046241760254\n",
      "hidden_state minus mean squared max: 57.68092346191406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.55891418457031 3.7855100631713867\n",
      "loss  1380: 1.8305   grad norm: 1.6870          model param norm: 87.4400        \n",
      "\n",
      "quiet_star_policy_loss= -0.0005336403846740723\n",
      "nll_loss= 1.8159881830215454\n",
      "avg_std= 0.2401392012834549\n",
      "dist std min max: 0.00910150445997715 0.2401392012834549 2.6348154544830322\n",
      "hidden_states min max: -9.258857727050781 10.993620872497559\n",
      "hidden_state minus mean squared max: 67.72909545898438\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.0010986328125 3.7156567573547363\n",
      "loss  1381: 1.8155   grad norm: 1.9925          model param norm: 87.4419        \n",
      "\n",
      "quiet_star_policy_loss= -0.020163774490356445\n",
      "nll_loss= 1.8221409320831299\n",
      "avg_std= 0.23966391384601593\n",
      "dist std min max: 0.009414857253432274 0.23966391384601593 2.450359344482422\n",
      "hidden_states min max: -9.023475646972656 10.993057250976562\n",
      "hidden_state minus mean squared max: 65.75250244140625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.886332511901855 3.6776790618896484\n",
      "loss  1382: 1.8020   grad norm: 1.7126          model param norm: 87.4435        \n",
      "\n",
      "quiet_star_policy_loss= 0.004864215850830078\n",
      "nll_loss= 1.8077633380889893\n",
      "avg_std= 0.23915567994117737\n",
      "dist std min max: 0.00916183739900589 0.23915567994117737 2.4163386821746826\n",
      "hidden_states min max: -9.028951644897461 11.011569023132324\n",
      "hidden_state minus mean squared max: 51.966365814208984\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.1737060546875 3.762816905975342\n",
      "loss  1383: 1.8126   grad norm: 1.9617          model param norm: 87.4453        \n",
      "\n",
      "quiet_star_policy_loss= -0.004641521256417036\n",
      "nll_loss= 1.816817283630371\n",
      "avg_std= 0.24026985466480255\n",
      "dist std min max: 0.009311567060649395 0.24026985466480255 2.428344964981079\n",
      "hidden_states min max: -9.12816047668457 11.700748443603516\n",
      "hidden_state minus mean squared max: 79.40745544433594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.54486846923828 3.7242684364318848\n",
      "loss  1384: 1.8122   grad norm: 1.8386          model param norm: 87.4471        \n",
      "\n",
      "quiet_star_policy_loss= -0.03389155864715576\n",
      "nll_loss= 1.8239192962646484\n",
      "avg_std= 0.23997153341770172\n",
      "dist std min max: 0.008880728855729103 0.23997153341770172 2.6471128463745117\n",
      "hidden_states min max: -9.061193466186523 10.987016677856445\n",
      "hidden_state minus mean squared max: 65.60324096679688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.30249786376953 3.789231777191162\n",
      "loss  1385: 1.7900   grad norm: 1.8446          model param norm: 87.4492        \n",
      "\n",
      "quiet_star_policy_loss= 0.012901472859084606\n",
      "nll_loss= 1.8032842874526978\n",
      "avg_std= 0.2409684658050537\n",
      "dist std min max: 0.009071946144104004 0.2409684658050537 2.467690944671631\n",
      "hidden_states min max: -10.187482833862305 11.015909194946289\n",
      "hidden_state minus mean squared max: 107.304931640625\n",
      "hidden_state minus mean divided by std max: 5.166576385498047\n",
      "log_prob min max: -103.87040710449219 3.782684803009033\n",
      "loss  1386: 1.8162   grad norm: 1.8897          model param norm: 87.4516        \n",
      "\n",
      "quiet_star_policy_loss= 0.0028014183044433594\n",
      "nll_loss= 1.830451250076294\n",
      "avg_std= 0.238705113530159\n",
      "dist std min max: 0.008936278522014618 0.238705113530159 2.549440860748291\n",
      "hidden_states min max: -10.835521697998047 11.026717185974121\n",
      "hidden_state minus mean squared max: 163.46788024902344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08088684082031 3.778263568878174\n",
      "loss  1387: 1.8333   grad norm: 2.0396          model param norm: 87.4540        \n",
      "\n",
      "quiet_star_policy_loss= -0.024060696363449097\n",
      "nll_loss= 1.840733528137207\n",
      "avg_std= 0.24119159579277039\n",
      "dist std min max: 0.008903995156288147 0.24119159579277039 2.4407858848571777\n",
      "hidden_states min max: -9.018868446350098 11.034170150756836\n",
      "hidden_state minus mean squared max: 59.01022720336914\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.02486419677734 3.782118320465088\n",
      "loss  1388: 1.8167   grad norm: 1.7325          model param norm: 87.4559        \n",
      "\n",
      "quiet_star_policy_loss= -0.01903507672250271\n",
      "nll_loss= 1.814254641532898\n",
      "avg_std= 0.2406938374042511\n",
      "dist std min max: 0.009322629310190678 0.2406938374042511 2.4654974937438965\n",
      "hidden_states min max: -9.03404426574707 11.545732498168945\n",
      "hidden_state minus mean squared max: 64.2266845703125\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.20663452148438 3.7403249740600586\n",
      "loss  1389: 1.7952   grad norm: 2.1471          model param norm: 87.4577        \n",
      "\n",
      "quiet_star_policy_loss= -0.023278523236513138\n",
      "nll_loss= 1.8061422109603882\n",
      "avg_std= 0.24167783558368683\n",
      "dist std min max: 0.009205431677401066 0.24167783558368683 2.460388660430908\n",
      "hidden_states min max: -11.448527336120605 11.805301666259766\n",
      "hidden_state minus mean squared max: 63.834022521972656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.56011962890625 3.7453598976135254\n",
      "loss  1390: 1.7829   grad norm: 1.8001          model param norm: 87.4592        \n",
      "\n",
      "quiet_star_policy_loss= -0.033515382558107376\n",
      "nll_loss= 1.816179633140564\n",
      "avg_std= 0.24016405642032623\n",
      "dist std min max: 0.008700898848474026 0.24016405642032623 2.506354570388794\n",
      "hidden_states min max: -9.949947357177734 10.999075889587402\n",
      "hidden_state minus mean squared max: 69.42943572998047\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.28397369384766 3.795393466949463\n",
      "loss  1391: 1.7827   grad norm: 1.8804          model param norm: 87.4607        \n",
      "\n",
      "quiet_star_policy_loss= -0.01602168194949627\n",
      "nll_loss= 1.8280349969863892\n",
      "avg_std= 0.24074320495128632\n",
      "dist std min max: 0.00866404827684164 0.24074320495128632 2.5243024826049805\n",
      "hidden_states min max: -9.419920921325684 11.011865615844727\n",
      "hidden_state minus mean squared max: 56.69573211669922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.74871826171875 3.809152603149414\n",
      "loss  1392: 1.8120   grad norm: 1.8888          model param norm: 87.4623        \n",
      "\n",
      "quiet_star_policy_loss= -0.0113772451877594\n",
      "nll_loss= 1.815362572669983\n",
      "avg_std= 0.24144741892814636\n",
      "dist std min max: 0.00921377632766962 0.24144741892814636 2.508580207824707\n",
      "hidden_states min max: -9.15017318725586 11.112348556518555\n",
      "hidden_state minus mean squared max: 78.23053741455078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.71240234375 3.763892650604248\n",
      "loss  1393: 1.8040   grad norm: 1.8947          model param norm: 87.4638        \n",
      "\n",
      "quiet_star_policy_loss= -0.01062843855470419\n",
      "nll_loss= 1.8037261962890625\n",
      "avg_std= 0.2413317859172821\n",
      "dist std min max: 0.008716093376278877 0.2413317859172821 2.486020803451538\n",
      "hidden_states min max: -9.108901977539062 11.025613784790039\n",
      "hidden_state minus mean squared max: 62.21616744995117\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.31954956054688 3.7622694969177246\n",
      "loss  1394: 1.7931   grad norm: 1.7560          model param norm: 87.4656        \n",
      "\n",
      "quiet_star_policy_loss= -0.004701948259025812\n",
      "nll_loss= 1.8172988891601562\n",
      "avg_std= 0.239850714802742\n",
      "dist std min max: 0.009067206643521786 0.239850714802742 2.496769905090332\n",
      "hidden_states min max: -15.636677742004395 11.363199234008789\n",
      "hidden_state minus mean squared max: 277.6040954589844\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -104.34566497802734 3.770291805267334\n",
      "loss  1395: 1.8126   grad norm: 1.8121          model param norm: 87.4678        \n",
      "\n",
      "quiet_star_policy_loss= -0.011498761363327503\n",
      "nll_loss= 1.8303139209747314\n",
      "avg_std= 0.23888753354549408\n",
      "dist std min max: 0.00844548549503088 0.23888753354549408 2.5491487979888916\n",
      "hidden_states min max: -9.784372329711914 11.0410737991333\n",
      "hidden_state minus mean squared max: 163.8861083984375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08216857910156 3.8319501876831055\n",
      "loss  1396: 1.8188   grad norm: 1.9072          model param norm: 87.4697        \n",
      "\n",
      "quiet_star_policy_loss= -0.03136632591485977\n",
      "nll_loss= 1.8108761310577393\n",
      "avg_std= 0.2402356117963791\n",
      "dist std min max: 0.008522308431565762 0.2402356117963791 2.505418062210083\n",
      "hidden_states min max: -9.032574653625488 11.191473960876465\n",
      "hidden_state minus mean squared max: 50.735713958740234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.19031524658203 3.825181007385254\n",
      "loss  1397: 1.7795   grad norm: 1.8967          model param norm: 87.4719        \n",
      "\n",
      "quiet_star_policy_loss= -0.011946308426558971\n",
      "nll_loss= 1.801416277885437\n",
      "avg_std= 0.2395394891500473\n",
      "dist std min max: 0.00839431956410408 0.2395394891500473 2.5098628997802734\n",
      "hidden_states min max: -9.640653610229492 11.114240646362305\n",
      "hidden_state minus mean squared max: 54.46281051635742\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.4570541381836 3.8431572914123535\n",
      "loss  1398: 1.7895   grad norm: 1.7724          model param norm: 87.4742        \n",
      "\n",
      "quiet_star_policy_loss= -0.018561402335762978\n",
      "nll_loss= 1.866873860359192\n",
      "avg_std= 0.23589487373828888\n",
      "dist std min max: 0.009781223721802235 0.23589487373828888 2.42387318611145\n",
      "hidden_states min max: -9.036121368408203 11.081315994262695\n",
      "hidden_state minus mean squared max: 71.50297546386719\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -13.787225723266602 3.633795738220215\n",
      "loss  1399: 1.8483   grad norm: 3.8582          model param norm: 87.4767        \n",
      "eval loss 1.8235265016555786\n",
      "\n",
      "quiet_star_policy_loss= -0.010031539015471935\n",
      "nll_loss= 1.8110779523849487\n",
      "avg_std= 0.23834143579006195\n",
      "dist std min max: 0.008461185730993748 0.23834143579006195 2.4911751747131348\n",
      "hidden_states min max: -22.33990478515625 11.372442245483398\n",
      "hidden_state minus mean squared max: 442.7693786621094\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.57909393310547 3.8384904861450195\n",
      "loss  1400: 1.8010   grad norm: 1.7248          model param norm: 87.4792        \n",
      "\n",
      "quiet_star_policy_loss= -0.015838170424103737\n",
      "nll_loss= 1.8234981298446655\n",
      "avg_std= 0.2395944744348526\n",
      "dist std min max: 0.008190234191715717 0.2395944744348526 2.5155534744262695\n",
      "hidden_states min max: -12.715627670288086 11.535346984863281\n",
      "hidden_state minus mean squared max: 141.79591369628906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.00979614257812 3.851689338684082\n",
      "loss  1401: 1.8077   grad norm: 1.8828          model param norm: 87.4818        \n",
      "\n",
      "quiet_star_policy_loss= -0.009967184625566006\n",
      "nll_loss= 1.8011562824249268\n",
      "avg_std= 0.2389848232269287\n",
      "dist std min max: 0.008358373306691647 0.2389848232269287 2.5270442962646484\n",
      "hidden_states min max: -9.035011291503906 11.798532485961914\n",
      "hidden_state minus mean squared max: 64.6255111694336\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.58604431152344 3.8643789291381836\n",
      "loss  1402: 1.7912   grad norm: 1.8954          model param norm: 87.4842        \n",
      "\n",
      "quiet_star_policy_loss= -0.023718882352113724\n",
      "nll_loss= 1.8156582117080688\n",
      "avg_std= 0.23992405831813812\n",
      "dist std min max: 0.007937727496027946 0.23992405831813812 2.521324396133423\n",
      "hidden_states min max: -9.95134449005127 11.205886840820312\n",
      "hidden_state minus mean squared max: 66.46367645263672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46105194091797 3.8822855949401855\n",
      "loss  1403: 1.7919   grad norm: 1.7750          model param norm: 87.4867        \n",
      "\n",
      "quiet_star_policy_loss= -0.00832150038331747\n",
      "nll_loss= 1.8231582641601562\n",
      "avg_std= 0.2391427755355835\n",
      "dist std min max: 0.007921840064227581 0.2391427755355835 2.4964025020599365\n",
      "hidden_states min max: -9.227583885192871 11.735207557678223\n",
      "hidden_state minus mean squared max: 71.18408203125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.88565063476562 3.9127979278564453\n",
      "loss  1404: 1.8148   grad norm: 1.8854          model param norm: 87.4892        \n",
      "\n",
      "quiet_star_policy_loss= -0.007548952009528875\n",
      "nll_loss= 1.819435477256775\n",
      "avg_std= 0.23833365738391876\n",
      "dist std min max: 0.008603730238974094 0.23833365738391876 2.502978563308716\n",
      "hidden_states min max: -9.31893539428711 12.946779251098633\n",
      "hidden_state minus mean squared max: 71.67618560791016\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.66866302490234 3.8242197036743164\n",
      "loss  1405: 1.8119   grad norm: 1.7693          model param norm: 87.4919        \n",
      "\n",
      "quiet_star_policy_loss= 0.016466820612549782\n",
      "nll_loss= 1.8309494256973267\n",
      "avg_std= 0.23837338387966156\n",
      "dist std min max: 0.00875745341181755 0.23837338387966156 2.5126898288726807\n",
      "hidden_states min max: -13.871564865112305 11.549690246582031\n",
      "hidden_state minus mean squared max: 148.5149688720703\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.03291320800781 3.8119282722473145\n",
      "loss  1406: 1.8474   grad norm: 1.8924          model param norm: 87.4949        \n",
      "\n",
      "quiet_star_policy_loss= -0.01836942508816719\n",
      "nll_loss= 1.7986338138580322\n",
      "avg_std= 0.236518993973732\n",
      "dist std min max: 0.00790814682841301 0.236518993973732 2.518561363220215\n",
      "hidden_states min max: -9.272234916687012 11.240983963012695\n",
      "hidden_state minus mean squared max: 90.78972625732422\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.0330810546875 3.920919418334961\n",
      "loss  1407: 1.7803   grad norm: 1.7408          model param norm: 87.4980        \n",
      "\n",
      "quiet_star_policy_loss= -0.015265083871781826\n",
      "nll_loss= 1.8172672986984253\n",
      "avg_std= 0.23687584698200226\n",
      "dist std min max: 0.007983727380633354 0.23687584698200226 2.521648645401001\n",
      "hidden_states min max: -9.294219970703125 11.295685768127441\n",
      "hidden_state minus mean squared max: 61.407867431640625\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.68537902832031 3.9094109535217285\n",
      "loss  1408: 1.8020   grad norm: 1.7873          model param norm: 87.5010        \n",
      "\n",
      "quiet_star_policy_loss= -0.019190311431884766\n",
      "nll_loss= 1.8194817304611206\n",
      "avg_std= 0.2386515587568283\n",
      "dist std min max: 0.0077942004427313805 0.2386515587568283 2.6099660396575928\n",
      "hidden_states min max: -9.864941596984863 11.55160903930664\n",
      "hidden_state minus mean squared max: 67.11295318603516\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.19292449951172 3.920124053955078\n",
      "loss  1409: 1.8003   grad norm: 1.7868          model param norm: 87.5036        \n",
      "\n",
      "quiet_star_policy_loss= -0.0021691799629479647\n",
      "nll_loss= 1.8245805501937866\n",
      "avg_std= 0.23867562413215637\n",
      "dist std min max: 0.008474906906485558 0.23867562413215637 2.5234594345092773\n",
      "hidden_states min max: -9.375670433044434 11.326949119567871\n",
      "hidden_state minus mean squared max: 63.36948776245117\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4229736328125 3.846169948577881\n",
      "loss  1410: 1.8224   grad norm: 1.9964          model param norm: 87.5063        \n",
      "\n",
      "quiet_star_policy_loss= -0.017691541463136673\n",
      "nll_loss= 1.8051592111587524\n",
      "avg_std= 0.23933203518390656\n",
      "dist std min max: 0.007649415638297796 0.23933203518390656 2.5668725967407227\n",
      "hidden_states min max: -9.656210899353027 11.348400115966797\n",
      "hidden_state minus mean squared max: 67.78780364990234\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.65570068359375 3.9398422241210938\n",
      "loss  1411: 1.7875   grad norm: 1.9369          model param norm: 87.5087        \n",
      "\n",
      "quiet_star_policy_loss= 0.000738057482521981\n",
      "nll_loss= 1.823297381401062\n",
      "avg_std= 0.2384040355682373\n",
      "dist std min max: 0.007668523117899895 0.2384040355682373 2.6873319149017334\n",
      "hidden_states min max: -9.679826736450195 12.231532096862793\n",
      "hidden_state minus mean squared max: 74.62569427490234\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.6888198852539 3.924691677093506\n",
      "loss  1412: 1.8240   grad norm: 2.0788          model param norm: 87.5112        \n",
      "\n",
      "quiet_star_policy_loss= -0.009770489297807217\n",
      "nll_loss= 1.79524827003479\n",
      "avg_std= 0.23958973586559296\n",
      "dist std min max: 0.007639486342668533 0.23958973586559296 2.6395938396453857\n",
      "hidden_states min max: -9.360249519348145 11.763121604919434\n",
      "hidden_state minus mean squared max: 70.71024322509766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.89827728271484 3.944751739501953\n",
      "loss  1413: 1.7855   grad norm: 1.8124          model param norm: 87.5136        \n",
      "\n",
      "quiet_star_policy_loss= 0.012780904769897461\n",
      "nll_loss= 1.8095654249191284\n",
      "avg_std= 0.23829148709774017\n",
      "dist std min max: 0.007498722989112139 0.23829148709774017 2.528257369995117\n",
      "hidden_states min max: -9.798827171325684 11.331753730773926\n",
      "hidden_state minus mean squared max: 61.41366195678711\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.94541931152344 3.961733341217041\n",
      "loss  1414: 1.8223   grad norm: 1.8635          model param norm: 87.5159        \n",
      "\n",
      "quiet_star_policy_loss= 0.0038622647989541292\n",
      "nll_loss= 1.8089615106582642\n",
      "avg_std= 0.23851722478866577\n",
      "dist std min max: 0.007404020521789789 0.23851722478866577 2.5774149894714355\n",
      "hidden_states min max: -9.330403327941895 11.357004165649414\n",
      "hidden_state minus mean squared max: 76.24458312988281\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.69955444335938 3.95853853225708\n",
      "loss  1415: 1.8128   grad norm: 1.7959          model param norm: 87.5178        \n",
      "\n",
      "quiet_star_policy_loss= -0.038973547518253326\n",
      "nll_loss= 1.7844260931015015\n",
      "avg_std= 0.24006517231464386\n",
      "dist std min max: 0.007411063648760319 0.24006517231464386 2.585644483566284\n",
      "hidden_states min max: -24.527267456054688 11.760656356811523\n",
      "hidden_state minus mean squared max: 614.0985717773438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.74264526367188 3.9751362800598145\n",
      "loss  1416: 1.7455   grad norm: 1.9897          model param norm: 87.5200        \n",
      "\n",
      "quiet_star_policy_loss= -0.01984472945332527\n",
      "nll_loss= 1.8196642398834229\n",
      "avg_std= 0.23917469382286072\n",
      "dist std min max: 0.007421509828418493 0.23917469382286072 2.5592539310455322\n",
      "hidden_states min max: -9.41152572631836 12.363483428955078\n",
      "hidden_state minus mean squared max: 71.91448974609375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.67778015136719 3.963150978088379\n",
      "loss  1417: 1.7998   grad norm: 1.9891          model param norm: 87.5223        \n",
      "\n",
      "quiet_star_policy_loss= 0.01069785375148058\n",
      "nll_loss= 1.8081684112548828\n",
      "avg_std= 0.23763586580753326\n",
      "dist std min max: 0.00732473935931921 0.23763586580753326 2.611886501312256\n",
      "hidden_states min max: -11.95441722869873 11.907052993774414\n",
      "hidden_state minus mean squared max: 210.36477661132812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.20700073242188 3.989696979522705\n",
      "loss  1418: 1.8189   grad norm: 1.7726          model param norm: 87.5248        \n",
      "\n",
      "quiet_star_policy_loss= -0.0034720301628112793\n",
      "nll_loss= 1.8183910846710205\n",
      "avg_std= 0.2373490333557129\n",
      "dist std min max: 0.009152445942163467 0.2373490333557129 2.5403566360473633\n",
      "hidden_states min max: -9.768348693847656 12.609325408935547\n",
      "hidden_state minus mean squared max: 83.62848663330078\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.1124496459961 3.7631773948669434\n",
      "loss  1419: 1.8149   grad norm: 1.9169          model param norm: 87.5271        \n",
      "\n",
      "quiet_star_policy_loss= -0.01907062530517578\n",
      "nll_loss= 1.8115837574005127\n",
      "avg_std= 0.23696407675743103\n",
      "dist std min max: 0.007551970891654491 0.23696407675743103 2.6433937549591064\n",
      "hidden_states min max: -10.2992582321167 11.576983451843262\n",
      "hidden_state minus mean squared max: 72.75926971435547\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.03555297851562 3.952251434326172\n",
      "loss  1420: 1.7925   grad norm: 1.8334          model param norm: 87.5293        \n",
      "\n",
      "quiet_star_policy_loss= -0.013524091802537441\n",
      "nll_loss= 1.811522364616394\n",
      "avg_std= 0.2372964471578598\n",
      "dist std min max: 0.0072431256994605064 0.2372964471578598 2.620790958404541\n",
      "hidden_states min max: -9.33797836303711 11.55226993560791\n",
      "hidden_state minus mean squared max: 73.24577331542969\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.05239868164062 3.9823546409606934\n",
      "loss  1421: 1.7980   grad norm: 1.8994          model param norm: 87.5313        \n",
      "\n",
      "quiet_star_policy_loss= 0.004689693450927734\n",
      "nll_loss= 1.8393980264663696\n",
      "avg_std= 0.23631252348423004\n",
      "dist std min max: 0.007215819787234068 0.23631252348423004 2.6128182411193848\n",
      "hidden_states min max: -9.755672454833984 11.774113655090332\n",
      "hidden_state minus mean squared max: 70.52812957763672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24606323242188 3.997284412384033\n",
      "loss  1422: 1.8441   grad norm: 1.9322          model param norm: 87.5336        \n",
      "\n",
      "quiet_star_policy_loss= -0.016001541167497635\n",
      "nll_loss= 1.823534607887268\n",
      "avg_std= 0.2374432384967804\n",
      "dist std min max: 0.007221193052828312 0.2374432384967804 2.619079113006592\n",
      "hidden_states min max: -9.548505783081055 11.3381929397583\n",
      "hidden_state minus mean squared max: 57.92347717285156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.57135772705078 3.9932961463928223\n",
      "loss  1423: 1.8075   grad norm: 1.9894          model param norm: 87.5359        \n",
      "\n",
      "quiet_star_policy_loss= -0.021703505888581276\n",
      "nll_loss= 1.8172937631607056\n",
      "avg_std= 0.23686467111110687\n",
      "dist std min max: 0.007168537937104702 0.23686467111110687 2.647944927215576\n",
      "hidden_states min max: -9.451157569885254 12.163021087646484\n",
      "hidden_state minus mean squared max: 71.69671630859375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.64810943603516 4.009803771972656\n",
      "loss  1424: 1.7956   grad norm: 1.8785          model param norm: 87.5380        \n",
      "\n",
      "quiet_star_policy_loss= -0.010408878326416016\n",
      "nll_loss= 1.8068817853927612\n",
      "avg_std= 0.23815125226974487\n",
      "dist std min max: 0.007177803665399551 0.23815125226974487 2.6613175868988037\n",
      "hidden_states min max: -9.423379898071289 12.828397750854492\n",
      "hidden_state minus mean squared max: 97.1383285522461\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.80276489257812 3.9887313842773438\n",
      "loss  1425: 1.7965   grad norm: 1.9712          model param norm: 87.5399        \n",
      "\n",
      "quiet_star_policy_loss= -0.033258117735385895\n",
      "nll_loss= 1.817088007926941\n",
      "avg_std= 0.23737630248069763\n",
      "dist std min max: 0.007481775712221861 0.23737630248069763 2.622631072998047\n",
      "hidden_states min max: -9.358479499816895 11.854890823364258\n",
      "hidden_state minus mean squared max: 71.83854675292969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.95231628417969 3.9590988159179688\n",
      "loss  1426: 1.7838   grad norm: 2.0358          model param norm: 87.5415        \n",
      "\n",
      "quiet_star_policy_loss= 0.014558461494743824\n",
      "nll_loss= 1.838613510131836\n",
      "avg_std= 0.2324778139591217\n",
      "dist std min max: 0.007369375322014093 0.2324778139591217 2.6290674209594727\n",
      "hidden_states min max: -9.397708892822266 11.342432022094727\n",
      "hidden_state minus mean squared max: 56.207122802734375\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.48958587646484 3.9803991317749023\n",
      "loss  1427: 1.8532   grad norm: 3.7644          model param norm: 87.5433        \n",
      "\n",
      "quiet_star_policy_loss= -0.008574641309678555\n",
      "nll_loss= 1.8062626123428345\n",
      "avg_std= 0.23672577738761902\n",
      "dist std min max: 0.008596443571150303 0.23672577738761902 2.7867977619171143\n",
      "hidden_states min max: -9.620258331298828 11.364103317260742\n",
      "hidden_state minus mean squared max: 87.97427368164062\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.68601989746094 3.806917667388916\n",
      "loss  1428: 1.7977   grad norm: 1.9227          model param norm: 87.5445        \n",
      "\n",
      "quiet_star_policy_loss= -0.019818395376205444\n",
      "nll_loss= 1.8260672092437744\n",
      "avg_std= 0.23480992019176483\n",
      "dist std min max: 0.007743767462670803 0.23480992019176483 2.675708293914795\n",
      "hidden_states min max: -9.350387573242188 11.976102828979492\n",
      "hidden_state minus mean squared max: 80.014404296875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.09994506835938 3.9390835762023926\n",
      "loss  1429: 1.8062   grad norm: 2.0377          model param norm: 87.5460        \n",
      "\n",
      "quiet_star_policy_loss= 0.004895019810646772\n",
      "nll_loss= 1.8137695789337158\n",
      "avg_std= 0.23593340814113617\n",
      "dist std min max: 0.008947974070906639 0.23593340814113617 2.7764227390289307\n",
      "hidden_states min max: -10.581277847290039 12.053300857543945\n",
      "hidden_state minus mean squared max: 159.3709716796875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.06819152832031 3.7899632453918457\n",
      "loss  1430: 1.8187   grad norm: 1.8326          model param norm: 87.5477        \n",
      "\n",
      "quiet_star_policy_loss= -0.029946541413664818\n",
      "nll_loss= 1.809956431388855\n",
      "avg_std= 0.23444810509681702\n",
      "dist std min max: 0.0072859651409089565 0.23444810509681702 2.7150766849517822\n",
      "hidden_states min max: -9.34471321105957 12.069610595703125\n",
      "hidden_state minus mean squared max: 74.13462829589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.36431884765625 3.9806461334228516\n",
      "loss  1431: 1.7800   grad norm: 1.9990          model param norm: 87.5495        \n",
      "\n",
      "quiet_star_policy_loss= -0.01943683624267578\n",
      "nll_loss= 1.8048858642578125\n",
      "avg_std= 0.23577937483787537\n",
      "dist std min max: 0.007360458839684725 0.23577937483787537 2.6692769527435303\n",
      "hidden_states min max: -9.475433349609375 11.618169784545898\n",
      "hidden_state minus mean squared max: 70.69502258300781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.01528930664062 3.9776978492736816\n",
      "loss  1432: 1.7854   grad norm: 1.9107          model param norm: 87.5510        \n",
      "\n",
      "quiet_star_policy_loss= -0.004343563225120306\n",
      "nll_loss= 1.8310972452163696\n",
      "avg_std= 0.2325851172208786\n",
      "dist std min max: 0.007496499456465244 0.2325851172208786 2.6361241340637207\n",
      "hidden_states min max: -9.536802291870117 11.540153503417969\n",
      "hidden_state minus mean squared max: 60.841766357421875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.92752075195312 3.9463539123535156\n",
      "loss  1433: 1.8268   grad norm: 2.0171          model param norm: 87.5525        \n",
      "\n",
      "quiet_star_policy_loss= -0.00807266216725111\n",
      "nll_loss= 1.825151801109314\n",
      "avg_std= 0.23367929458618164\n",
      "dist std min max: 0.007472203113138676 0.23367929458618164 2.687068462371826\n",
      "hidden_states min max: -9.593708038330078 11.255770683288574\n",
      "hidden_state minus mean squared max: 66.56304931640625\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.85724639892578 3.9643712043762207\n",
      "loss  1434: 1.8171   grad norm: 1.9264          model param norm: 87.5540        \n",
      "\n",
      "quiet_star_policy_loss= 0.013556599617004395\n",
      "nll_loss= 1.8246456384658813\n",
      "avg_std= 0.23513217270374298\n",
      "dist std min max: 0.008031317964196205 0.23513217270374298 2.7371127605438232\n",
      "hidden_states min max: -9.349868774414062 11.122880935668945\n",
      "hidden_state minus mean squared max: 70.951904296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.2624740600586 3.901911735534668\n",
      "loss  1435: 1.8382   grad norm: 1.9126          model param norm: 87.5553        \n",
      "\n",
      "quiet_star_policy_loss= -0.03180570527911186\n",
      "nll_loss= 1.8280994892120361\n",
      "avg_std= 0.23401324450969696\n",
      "dist std min max: 0.008232485502958298 0.23401324450969696 2.6328980922698975\n",
      "hidden_states min max: -9.34843921661377 12.148289680480957\n",
      "hidden_state minus mean squared max: 75.19127655029297\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.6568374633789 3.860851287841797\n",
      "loss  1436: 1.7963   grad norm: 1.9980          model param norm: 87.5565        \n",
      "\n",
      "quiet_star_policy_loss= -0.011063647456467152\n",
      "nll_loss= 1.8352596759796143\n",
      "avg_std= 0.23233993351459503\n",
      "dist std min max: 0.007818179205060005 0.23233993351459503 2.760190486907959\n",
      "hidden_states min max: -9.396544456481934 11.245680809020996\n",
      "hidden_state minus mean squared max: 74.65318298339844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.16136932373047 3.912212371826172\n",
      "loss  1437: 1.8242   grad norm: 2.0331          model param norm: 87.5579        \n",
      "\n",
      "quiet_star_policy_loss= -0.024871034547686577\n",
      "nll_loss= 1.809335708618164\n",
      "avg_std= 0.23374304175376892\n",
      "dist std min max: 0.007678596768528223 0.23374304175376892 2.8228368759155273\n",
      "hidden_states min max: -9.736568450927734 11.224431037902832\n",
      "hidden_state minus mean squared max: 81.11775207519531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.67491149902344 3.9436163902282715\n",
      "loss  1438: 1.7845   grad norm: 1.9699          model param norm: 87.5593        \n",
      "\n",
      "quiet_star_policy_loss= -0.01525260228663683\n",
      "nll_loss= 1.8100054264068604\n",
      "avg_std= 0.23346668481826782\n",
      "dist std min max: 0.008147161453962326 0.23346668481826782 2.8392465114593506\n",
      "hidden_states min max: -10.455174446105957 11.412748336791992\n",
      "hidden_state minus mean squared max: 64.8561782836914\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.50703430175781 3.8862085342407227\n",
      "loss  1439: 1.7948   grad norm: 2.1497          model param norm: 87.5607        \n",
      "\n",
      "quiet_star_policy_loss= -0.01516261138021946\n",
      "nll_loss= 1.8144609928131104\n",
      "avg_std= 0.23359856009483337\n",
      "dist std min max: 0.007995293475687504 0.23359856009483337 2.7415220737457275\n",
      "hidden_states min max: -9.675509452819824 11.992149353027344\n",
      "hidden_state minus mean squared max: 73.0808334350586\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.68965148925781 3.894441604614258\n",
      "loss  1440: 1.7993   grad norm: 1.9446          model param norm: 87.5620        \n",
      "\n",
      "quiet_star_policy_loss= -0.037091780453920364\n",
      "nll_loss= 1.8083893060684204\n",
      "avg_std= 0.23240138590335846\n",
      "dist std min max: 0.008449776098132133 0.23240138590335846 2.8040573596954346\n",
      "hidden_states min max: -9.537317276000977 11.622481346130371\n",
      "hidden_state minus mean squared max: 76.10613250732422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85890197753906 3.8036341667175293\n",
      "loss  1441: 1.7713   grad norm: 1.9640          model param norm: 87.5633        \n",
      "\n",
      "quiet_star_policy_loss= -0.010244387201964855\n",
      "nll_loss= 1.7983925342559814\n",
      "avg_std= 0.23091450333595276\n",
      "dist std min max: 0.008241530507802963 0.23091450333595276 2.714259147644043\n",
      "hidden_states min max: -9.4653959274292 11.550912857055664\n",
      "hidden_state minus mean squared max: 75.04408264160156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.32788848876953 3.8640832901000977\n",
      "loss  1442: 1.7881   grad norm: 2.0090          model param norm: 87.5645        \n",
      "\n",
      "quiet_star_policy_loss= -0.006619310472160578\n",
      "nll_loss= 1.8022464513778687\n",
      "avg_std= 0.23124432563781738\n",
      "dist std min max: 0.008269949816167355 0.23124432563781738 2.812178611755371\n",
      "hidden_states min max: -20.305095672607422 10.757404327392578\n",
      "hidden_state minus mean squared max: 457.1947326660156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.59512329101562 3.8612937927246094\n",
      "loss  1443: 1.7956   grad norm: 2.0400          model param norm: 87.5660        \n",
      "\n",
      "quiet_star_policy_loss= -0.01456234510987997\n",
      "nll_loss= 1.8129128217697144\n",
      "avg_std= 0.23173467814922333\n",
      "dist std min max: 0.00827556662261486 0.23173467814922333 2.9034903049468994\n",
      "hidden_states min max: -9.40003776550293 11.465325355529785\n",
      "hidden_state minus mean squared max: 67.0365219116211\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.26004028320312 3.8709359169006348\n",
      "loss  1444: 1.7984   grad norm: 2.1654          model param norm: 87.5676        \n",
      "\n",
      "quiet_star_policy_loss= -0.014346159063279629\n",
      "nll_loss= 1.814696192741394\n",
      "avg_std= 0.22959157824516296\n",
      "dist std min max: 0.00816272385418415 0.22959157824516296 2.6366324424743652\n",
      "hidden_states min max: -9.48660659790039 10.80876636505127\n",
      "hidden_state minus mean squared max: 61.06982421875\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.4433822631836 3.882084846496582\n",
      "loss  1445: 1.8004   grad norm: 1.8907          model param norm: 87.5694        \n",
      "\n",
      "quiet_star_policy_loss= -0.03593377023935318\n",
      "nll_loss= 1.8030685186386108\n",
      "avg_std= 0.23301680386066437\n",
      "dist std min max: 0.0080688102170825 0.23301680386066437 2.6915805339813232\n",
      "hidden_states min max: -10.871576309204102 10.996231079101562\n",
      "hidden_state minus mean squared max: 66.49092864990234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.6913833618164 3.9004902839660645\n",
      "loss  1446: 1.7671   grad norm: 2.0022          model param norm: 87.5712        \n",
      "\n",
      "quiet_star_policy_loss= -0.015798235312104225\n",
      "nll_loss= 1.818266749382019\n",
      "avg_std= 0.23111003637313843\n",
      "dist std min max: 0.008150249719619751 0.23111003637313843 2.708510398864746\n",
      "hidden_states min max: -25.639019012451172 12.38929271697998\n",
      "hidden_state minus mean squared max: 703.3992309570312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.8105239868164 3.869129180908203\n",
      "loss  1447: 1.8025   grad norm: 1.8573          model param norm: 87.5731        \n",
      "\n",
      "quiet_star_policy_loss= -0.004462689161300659\n",
      "nll_loss= 1.8070968389511108\n",
      "avg_std= 0.23510971665382385\n",
      "dist std min max: 0.008024726063013077 0.23510971665382385 2.8248133659362793\n",
      "hidden_states min max: -9.44629192352295 12.16203784942627\n",
      "hidden_state minus mean squared max: 86.45402526855469\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.22367095947266 3.8983583450317383\n",
      "loss  1448: 1.8026   grad norm: 1.8702          model param norm: 87.5749        \n",
      "\n",
      "quiet_star_policy_loss= 0.0010932922596111894\n",
      "nll_loss= 1.8098175525665283\n",
      "avg_std= 0.2309282422065735\n",
      "dist std min max: 0.007910341024398804 0.2309282422065735 2.7005016803741455\n",
      "hidden_states min max: -9.580118179321289 12.159067153930664\n",
      "hidden_state minus mean squared max: 82.51142883300781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.68199920654297 3.898775100708008\n",
      "loss  1449: 1.8109   grad norm: 1.8732          model param norm: 87.5766        \n",
      "\n",
      "quiet_star_policy_loss= -0.02140185795724392\n",
      "nll_loss= 1.8088371753692627\n",
      "avg_std= 0.23280160129070282\n",
      "dist std min max: 0.007839484140276909 0.23280160129070282 2.7404942512512207\n",
      "hidden_states min max: -9.532663345336914 11.735113143920898\n",
      "hidden_state minus mean squared max: 85.64241027832031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.96764373779297 3.91664457321167\n",
      "loss  1450: 1.7874   grad norm: 1.9809          model param norm: 87.5784        \n",
      "\n",
      "quiet_star_policy_loss= -0.0140933096408844\n",
      "nll_loss= 1.805948257446289\n",
      "avg_std= 0.23216725885868073\n",
      "dist std min max: 0.007970821112394333 0.23216725885868073 2.795316457748413\n",
      "hidden_states min max: -9.437116622924805 12.52163314819336\n",
      "hidden_state minus mean squared max: 72.46537017822266\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.91939544677734 3.8988285064697266\n",
      "loss  1451: 1.7919   grad norm: 2.1136          model param norm: 87.5806        \n",
      "\n",
      "quiet_star_policy_loss= -0.006812334060668945\n",
      "nll_loss= 1.8307656049728394\n",
      "avg_std= 0.23184834420681\n",
      "dist std min max: 0.007837297394871712 0.23184834420681 2.7567789554595947\n",
      "hidden_states min max: -9.465839385986328 10.614463806152344\n",
      "hidden_state minus mean squared max: 73.36616516113281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.30904388427734 3.929335594177246\n",
      "loss  1452: 1.8240   grad norm: 1.8211          model param norm: 87.5828        \n",
      "\n",
      "quiet_star_policy_loss= -0.013793778605759144\n",
      "nll_loss= 1.8236087560653687\n",
      "avg_std= 0.2319110482931137\n",
      "dist std min max: 0.007387396413832903 0.2319110482931137 2.8714687824249268\n",
      "hidden_states min max: -9.294814109802246 11.496646881103516\n",
      "hidden_state minus mean squared max: 66.49982452392578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.97525787353516 3.9648871421813965\n",
      "loss  1453: 1.8098   grad norm: 1.9379          model param norm: 87.5854        \n",
      "\n",
      "quiet_star_policy_loss= 0.004226863384246826\n",
      "nll_loss= 1.8144149780273438\n",
      "avg_std= 0.23148423433303833\n",
      "dist std min max: 0.007873211055994034 0.23148423433303833 2.748274087905884\n",
      "hidden_states min max: -9.331989288330078 11.3893404006958\n",
      "hidden_state minus mean squared max: 72.3622055053711\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.3210220336914 3.9145278930664062\n",
      "loss  1454: 1.8186   grad norm: 1.9240          model param norm: 87.5879        \n",
      "\n",
      "quiet_star_policy_loss= -0.0324094258248806\n",
      "nll_loss= 1.8187358379364014\n",
      "avg_std= 0.23368152976036072\n",
      "dist std min max: 0.007537731435149908 0.23368152976036072 2.737527370452881\n",
      "hidden_states min max: -9.368953704833984 10.949750900268555\n",
      "hidden_state minus mean squared max: 70.1304702758789\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.22350311279297 3.9674601554870605\n",
      "loss  1455: 1.7863   grad norm: 4.1757          model param norm: 87.5901        \n",
      "\n",
      "quiet_star_policy_loss= -0.022616123780608177\n",
      "nll_loss= 1.8161720037460327\n",
      "avg_std= 0.23029018938541412\n",
      "dist std min max: 0.007365735713392496 0.23029018938541412 2.7223172187805176\n",
      "hidden_states min max: -9.303264617919922 11.478583335876465\n",
      "hidden_state minus mean squared max: 88.24581909179688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.3196029663086 3.9859485626220703\n",
      "loss  1456: 1.7936   grad norm: 1.9784          model param norm: 87.5922        \n",
      "\n",
      "quiet_star_policy_loss= -0.013017356395721436\n",
      "nll_loss= 1.8061399459838867\n",
      "avg_std= 0.2311541587114334\n",
      "dist std min max: 0.0072921449318528175 0.2311541587114334 2.8864686489105225\n",
      "hidden_states min max: -12.389869689941406 11.04697036743164\n",
      "hidden_state minus mean squared max: 97.80224609375\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.82405853271484 3.9975075721740723\n",
      "loss  1457: 1.7931   grad norm: 2.0980          model param norm: 87.5943        \n",
      "\n",
      "quiet_star_policy_loss= -0.011834573931992054\n",
      "nll_loss= 1.8154577016830444\n",
      "avg_std= 0.23356658220291138\n",
      "dist std min max: 0.007414068095386028 0.23356658220291138 2.9642465114593506\n",
      "hidden_states min max: -9.430176734924316 10.818819999694824\n",
      "hidden_state minus mean squared max: 70.89794158935547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.44245147705078 3.979829788208008\n",
      "loss  1458: 1.8036   grad norm: 2.1800          model param norm: 87.5963        \n",
      "\n",
      "quiet_star_policy_loss= -0.02310762368142605\n",
      "nll_loss= 1.8155529499053955\n",
      "avg_std= 0.23264244198799133\n",
      "dist std min max: 0.007314289920032024 0.23264244198799133 2.7912909984588623\n",
      "hidden_states min max: -9.747159004211426 11.457904815673828\n",
      "hidden_state minus mean squared max: 127.12650299072266\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9551773071289 3.997581958770752\n",
      "loss  1459: 1.7924   grad norm: 1.9016          model param norm: 87.5981        \n",
      "\n",
      "quiet_star_policy_loss= 0.005268692970275879\n",
      "nll_loss= 1.8213142156600952\n",
      "avg_std= 0.2334040105342865\n",
      "dist std min max: 0.007201894652098417 0.2334040105342865 2.8407928943634033\n",
      "hidden_states min max: -18.568267822265625 10.950543403625488\n",
      "hidden_state minus mean squared max: 336.8172912597656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.44233703613281 4.0088677406311035\n",
      "loss  1460: 1.8266   grad norm: 2.0226          model param norm: 87.5995        \n",
      "\n",
      "quiet_star_policy_loss= -0.011864880099892616\n",
      "nll_loss= 1.8357467651367188\n",
      "avg_std= 0.2325035184621811\n",
      "dist std min max: 0.006996140815317631 0.2325035184621811 2.916497230529785\n",
      "hidden_states min max: -15.816006660461426 10.76135540008545\n",
      "hidden_state minus mean squared max: 261.3863220214844\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -104.31555938720703 4.032644271850586\n",
      "loss  1461: 1.8239   grad norm: 2.1029          model param norm: 87.6011        \n",
      "\n",
      "quiet_star_policy_loss= -0.006952929776161909\n",
      "nll_loss= 1.8206291198730469\n",
      "avg_std= 0.23411892354488373\n",
      "dist std min max: 0.007019313983619213 0.23411892354488373 2.7350282669067383\n",
      "hidden_states min max: -9.609798431396484 11.41748332977295\n",
      "hidden_state minus mean squared max: 115.7822036743164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.9084243774414 3.9942359924316406\n",
      "loss  1462: 1.8137   grad norm: 2.1053          model param norm: 87.6026        \n",
      "\n",
      "quiet_star_policy_loss= 0.0014060974353924394\n",
      "nll_loss= 1.8065052032470703\n",
      "avg_std= 0.23486417531967163\n",
      "dist std min max: 0.007435143459588289 0.23486417531967163 2.901484966278076\n",
      "hidden_states min max: -9.99374008178711 11.83814811706543\n",
      "hidden_state minus mean squared max: 80.46115112304688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.97364044189453 3.969364643096924\n",
      "loss  1463: 1.8079   grad norm: 2.0247          model param norm: 87.6043        \n",
      "\n",
      "quiet_star_policy_loss= -0.023259950801730156\n",
      "nll_loss= 1.8075802326202393\n",
      "avg_std= 0.23262974619865417\n",
      "dist std min max: 0.0074617029167711735 0.23262974619865417 2.9279510974884033\n",
      "hidden_states min max: -9.611272811889648 11.538398742675781\n",
      "hidden_state minus mean squared max: 82.86151885986328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.79715728759766 3.971334457397461\n",
      "loss  1464: 1.7843   grad norm: 1.9077          model param norm: 87.6060        \n",
      "\n",
      "quiet_star_policy_loss= -0.0022008181549608707\n",
      "nll_loss= 1.8058710098266602\n",
      "avg_std= 0.23644764721393585\n",
      "dist std min max: 0.007425279822200537 0.23644764721393585 2.9157142639160156\n",
      "hidden_states min max: -10.417238235473633 10.960967063903809\n",
      "hidden_state minus mean squared max: 77.27833557128906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.04103088378906 3.960880756378174\n",
      "loss  1465: 1.8037   grad norm: 1.9796          model param norm: 87.6077        \n",
      "\n",
      "quiet_star_policy_loss= -0.0018744468688964844\n",
      "nll_loss= 1.8277559280395508\n",
      "avg_std= 0.23650188744068146\n",
      "dist std min max: 0.007284603547304869 0.23650188744068146 2.7950289249420166\n",
      "hidden_states min max: -15.258027076721191 10.725956916809082\n",
      "hidden_state minus mean squared max: 298.94036865234375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.38269805908203 3.9866414070129395\n",
      "loss  1466: 1.8259   grad norm: 2.0597          model param norm: 87.6092        \n",
      "\n",
      "quiet_star_policy_loss= -0.023476218804717064\n",
      "nll_loss= 1.802659273147583\n",
      "avg_std= 0.23618079721927643\n",
      "dist std min max: 0.007438785396516323 0.23618079721927643 2.807086229324341\n",
      "hidden_states min max: -12.254711151123047 11.458053588867188\n",
      "hidden_state minus mean squared max: 78.23163604736328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.38005828857422 3.973275661468506\n",
      "loss  1467: 1.7792   grad norm: 2.1148          model param norm: 87.6107        \n",
      "\n",
      "quiet_star_policy_loss= -0.027824735268950462\n",
      "nll_loss= 1.808916449546814\n",
      "avg_std= 0.2363678514957428\n",
      "dist std min max: 0.007561620324850082 0.2363678514957428 2.917532444000244\n",
      "hidden_states min max: -10.341194152832031 11.356741905212402\n",
      "hidden_state minus mean squared max: 90.25145721435547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.75648498535156 3.940011978149414\n",
      "loss  1468: 1.7811   grad norm: 2.0063          model param norm: 87.6120        \n",
      "\n",
      "quiet_star_policy_loss= 0.00074729323387146\n",
      "nll_loss= 1.8221323490142822\n",
      "avg_std= 0.23782767355442047\n",
      "dist std min max: 0.007581336423754692 0.23782767355442047 2.794010877609253\n",
      "hidden_states min max: -10.277949333190918 10.593589782714844\n",
      "hidden_state minus mean squared max: 68.13805389404297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.963623046875 3.9629812240600586\n",
      "loss  1469: 1.8229   grad norm: 2.0519          model param norm: 87.6133        \n",
      "\n",
      "quiet_star_policy_loss= -0.009493994526565075\n",
      "nll_loss= 1.809813141822815\n",
      "avg_std= 0.2366955280303955\n",
      "dist std min max: 0.007388012949377298 0.2366955280303955 2.8331711292266846\n",
      "hidden_states min max: -9.441459655761719 11.36269760131836\n",
      "hidden_state minus mean squared max: 105.74958801269531\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.80156707763672 3.9632201194763184\n",
      "loss  1470: 1.8003   grad norm: 1.9220          model param norm: 87.6147        \n",
      "\n",
      "quiet_star_policy_loss= -0.006300401873886585\n",
      "nll_loss= 1.8016360998153687\n",
      "avg_std= 0.23806986212730408\n",
      "dist std min max: 0.00728244986385107 0.23806986212730408 3.0297067165374756\n",
      "hidden_states min max: -9.210480690002441 10.169403076171875\n",
      "hidden_state minus mean squared max: 66.87541961669922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.37887573242188 3.9790196418762207\n",
      "loss  1471: 1.7953   grad norm: 2.0338          model param norm: 87.6161        \n",
      "\n",
      "quiet_star_policy_loss= -0.02696402184665203\n",
      "nll_loss= 1.8073269128799438\n",
      "avg_std= 0.23726460337638855\n",
      "dist std min max: 0.007531055714935064 0.23726460337638855 2.7026050090789795\n",
      "hidden_states min max: -10.327903747558594 10.588008880615234\n",
      "hidden_state minus mean squared max: 78.50819396972656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.92410278320312 3.9544763565063477\n",
      "loss  1472: 1.7804   grad norm: 1.9414          model param norm: 87.6174        \n",
      "\n",
      "quiet_star_policy_loss= -0.022098397836089134\n",
      "nll_loss= 1.8257362842559814\n",
      "avg_std= 0.23732717335224152\n",
      "dist std min max: 0.0071654110215604305 0.23732717335224152 2.7852745056152344\n",
      "hidden_states min max: -10.715253829956055 11.132143020629883\n",
      "hidden_state minus mean squared max: 192.756103515625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.16328430175781 4.013402462005615\n",
      "loss  1473: 1.8036   grad norm: 2.0211          model param norm: 87.6184        \n",
      "\n",
      "quiet_star_policy_loss= -0.0007674694061279297\n",
      "nll_loss= 1.8368107080459595\n",
      "avg_std= 0.23646269738674164\n",
      "dist std min max: 0.007050908170640469 0.23646269738674164 3.051975727081299\n",
      "hidden_states min max: -9.148938179016113 11.936843872070312\n",
      "hidden_state minus mean squared max: 81.34182739257812\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.37268829345703 4.0336384773254395\n",
      "loss  1474: 1.8360   grad norm: 2.0365          model param norm: 87.6196        \n",
      "\n",
      "quiet_star_policy_loss= 0.030100181698799133\n",
      "nll_loss= 1.8226016759872437\n",
      "avg_std= 0.23722568154335022\n",
      "dist std min max: 0.007580356672406197 0.23722568154335022 2.708312749862671\n",
      "hidden_states min max: -9.501742362976074 11.391977310180664\n",
      "hidden_state minus mean squared max: 86.71727752685547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.4859848022461 3.9481449127197266\n",
      "loss  1475: 1.8527   grad norm: 2.0595          model param norm: 87.6207        \n",
      "\n",
      "quiet_star_policy_loss= -0.02688775025308132\n",
      "nll_loss= 1.822104811668396\n",
      "avg_std= 0.23641987144947052\n",
      "dist std min max: 0.007805320899933577 0.23641987144947052 2.752592086791992\n",
      "hidden_states min max: -12.683895111083984 10.699585914611816\n",
      "hidden_state minus mean squared max: 203.2874755859375\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -104.18988800048828 3.9246315956115723\n",
      "loss  1476: 1.7952   grad norm: 1.9119          model param norm: 87.6221        \n",
      "\n",
      "quiet_star_policy_loss= -0.018938779830932617\n",
      "nll_loss= 1.8037360906600952\n",
      "avg_std= 0.23813435435295105\n",
      "dist std min max: 0.007102296222001314 0.23813435435295105 2.9992055892944336\n",
      "hidden_states min max: -9.179819107055664 10.139508247375488\n",
      "hidden_state minus mean squared max: 79.55607604980469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.83016204833984 4.017827033996582\n",
      "loss  1477: 1.7848   grad norm: 2.0041          model param norm: 87.6235        \n",
      "\n",
      "quiet_star_policy_loss= -0.01706216298043728\n",
      "nll_loss= 1.825931191444397\n",
      "avg_std= 0.2354596108198166\n",
      "dist std min max: 0.007033165078610182 0.2354596108198166 2.6715636253356934\n",
      "hidden_states min max: -11.932509422302246 10.673477172851562\n",
      "hidden_state minus mean squared max: 95.87754821777344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.36219024658203 4.036149024963379\n",
      "loss  1478: 1.8089   grad norm: 2.0275          model param norm: 87.6248        \n",
      "\n",
      "quiet_star_policy_loss= -0.009315776638686657\n",
      "nll_loss= 1.7981125116348267\n",
      "avg_std= 0.23750841617584229\n",
      "dist std min max: 0.007233296055346727 0.23750841617584229 3.0393505096435547\n",
      "hidden_states min max: -9.69143009185791 11.725893020629883\n",
      "hidden_state minus mean squared max: 95.50077819824219\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.8121566772461 3.994980812072754\n",
      "loss  1479: 1.7888   grad norm: 2.0999          model param norm: 87.6261        \n",
      "\n",
      "quiet_star_policy_loss= -0.014369201846420765\n",
      "nll_loss= 1.8078339099884033\n",
      "avg_std= 0.23721331357955933\n",
      "dist std min max: 0.007045076694339514 0.23721331357955933 2.76385498046875\n",
      "hidden_states min max: -9.224883079528809 10.766222953796387\n",
      "hidden_state minus mean squared max: 71.31470489501953\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -13.781195640563965 4.034135341644287\n",
      "loss  1480: 1.7935   grad norm: 1.9212          model param norm: 87.6273        \n",
      "\n",
      "quiet_star_policy_loss= -0.026757383719086647\n",
      "nll_loss= 1.8100322484970093\n",
      "avg_std= 0.23716464638710022\n",
      "dist std min max: 0.007418449502438307 0.23716464638710022 2.705960512161255\n",
      "hidden_states min max: -10.024639129638672 10.872330665588379\n",
      "hidden_state minus mean squared max: 87.06689453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.2071533203125 3.9783191680908203\n",
      "loss  1481: 1.7833   grad norm: 2.0174          model param norm: 87.6287        \n",
      "\n",
      "quiet_star_policy_loss= -0.016508912667632103\n",
      "nll_loss= 1.8165909051895142\n",
      "avg_std= 0.23629134893417358\n",
      "dist std min max: 0.007280117366462946 0.23629134893417358 3.075547218322754\n",
      "hidden_states min max: -10.530778884887695 9.742883682250977\n",
      "hidden_state minus mean squared max: 108.6863784790039\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.1155014038086 3.978114128112793\n",
      "loss  1482: 1.8001   grad norm: 1.9492          model param norm: 87.6302        \n",
      "\n",
      "quiet_star_policy_loss= 0.05457387492060661\n",
      "nll_loss= 1.7812758684158325\n",
      "avg_std= 0.23750975728034973\n",
      "dist std min max: 0.007716645486652851 0.23750975728034973 2.659517765045166\n",
      "hidden_states min max: -13.657187461853027 10.044624328613281\n",
      "hidden_state minus mean squared max: 170.14234924316406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.10090637207031 3.9447426795959473\n",
      "loss  1483: 1.8358   grad norm: 4.1378          model param norm: 87.6317        \n",
      "\n",
      "quiet_star_policy_loss= -0.01158361416310072\n",
      "nll_loss= 1.794723391532898\n",
      "avg_std= 0.23784208297729492\n",
      "dist std min max: 0.007266979198902845 0.23784208297729492 2.8097543716430664\n",
      "hidden_states min max: -9.275432586669922 10.421110153198242\n",
      "hidden_state minus mean squared max: 81.84063720703125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.60601806640625 3.9918389320373535\n",
      "loss  1484: 1.7831   grad norm: 1.9736          model param norm: 87.6330        \n",
      "\n",
      "quiet_star_policy_loss= -0.009500718675553799\n",
      "nll_loss= 1.809702754020691\n",
      "avg_std= 0.2358938455581665\n",
      "dist std min max: 0.00743095763027668 0.2358938455581665 2.724149227142334\n",
      "hidden_states min max: -9.961204528808594 10.413580894470215\n",
      "hidden_state minus mean squared max: 84.32783508300781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1076431274414 3.964310646057129\n",
      "loss  1485: 1.8002   grad norm: 2.0707          model param norm: 87.6343        \n",
      "\n",
      "quiet_star_policy_loss= -0.02220434881746769\n",
      "nll_loss= 1.8302291631698608\n",
      "avg_std= 0.2360522598028183\n",
      "dist std min max: 0.007275973446667194 0.2360522598028183 2.8080358505249023\n",
      "hidden_states min max: -9.86286449432373 10.410303115844727\n",
      "hidden_state minus mean squared max: 88.6720962524414\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.30488586425781 3.9907455444335938\n",
      "loss  1486: 1.8080   grad norm: 2.0067          model param norm: 87.6357        \n",
      "\n",
      "quiet_star_policy_loss= -0.004720479249954224\n",
      "nll_loss= 1.8080586194992065\n",
      "avg_std= 0.237734854221344\n",
      "dist std min max: 0.007176243234425783 0.237734854221344 2.753725290298462\n",
      "hidden_states min max: -9.816662788391113 11.48271369934082\n",
      "hidden_state minus mean squared max: 126.10965728759766\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.27241516113281 4.002871036529541\n",
      "loss  1487: 1.8033   grad norm: 1.9427          model param norm: 87.6371        \n",
      "\n",
      "quiet_star_policy_loss= -0.028386402875185013\n",
      "nll_loss= 1.8214247226715088\n",
      "avg_std= 0.23651590943336487\n",
      "dist std min max: 0.007422996684908867 0.23651590943336487 2.83939528465271\n",
      "hidden_states min max: -9.325369834899902 10.735383033752441\n",
      "hidden_state minus mean squared max: 90.63560485839844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.39118957519531 3.9657206535339355\n",
      "loss  1488: 1.7930   grad norm: 2.0522          model param norm: 87.6386        \n",
      "\n",
      "quiet_star_policy_loss= -0.003288817359134555\n",
      "nll_loss= 1.814038872718811\n",
      "avg_std= 0.23703300952911377\n",
      "dist std min max: 0.007112917955964804 0.23703300952911377 3.0214128494262695\n",
      "hidden_states min max: -10.115204811096191 10.25493335723877\n",
      "hidden_state minus mean squared max: 90.05345153808594\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.4451904296875 4.014658451080322\n",
      "loss  1489: 1.8108   grad norm: 1.9288          model param norm: 87.6400        \n",
      "\n",
      "quiet_star_policy_loss= -0.034864187240600586\n",
      "nll_loss= 1.8033539056777954\n",
      "avg_std= 0.236333966255188\n",
      "dist std min max: 0.007925095967948437 0.236333966255188 2.8342626094818115\n",
      "hidden_states min max: -9.84523868560791 10.211739540100098\n",
      "hidden_state minus mean squared max: 83.37884521484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.41360473632812 3.8730430603027344\n",
      "loss  1490: 1.7685   grad norm: 1.8557          model param norm: 87.6414        \n",
      "\n",
      "quiet_star_policy_loss= 0.009686184115707874\n",
      "nll_loss= 1.8101532459259033\n",
      "avg_std= 0.2334173321723938\n",
      "dist std min max: 0.007387107238173485 0.2334173321723938 2.8345110416412354\n",
      "hidden_states min max: -9.707329750061035 10.486894607543945\n",
      "hidden_state minus mean squared max: 72.98916625976562\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.62442016601562 3.9688801765441895\n",
      "loss  1491: 1.8198   grad norm: 2.0111          model param norm: 87.6428        \n",
      "\n",
      "quiet_star_policy_loss= -0.016415119171142578\n",
      "nll_loss= 1.8237905502319336\n",
      "avg_std= 0.23483188450336456\n",
      "dist std min max: 0.007256580516695976 0.23483188450336456 2.8849661350250244\n",
      "hidden_states min max: -9.335453033447266 10.538805961608887\n",
      "hidden_state minus mean squared max: 74.39868927001953\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.45970916748047 3.9780917167663574\n",
      "loss  1492: 1.8074   grad norm: 2.0003          model param norm: 87.6442        \n",
      "\n",
      "quiet_star_policy_loss= 0.0036907971370965242\n",
      "nll_loss= 1.798958420753479\n",
      "avg_std= 0.23389071226119995\n",
      "dist std min max: 0.007728381082415581 0.23389071226119995 2.8117434978485107\n",
      "hidden_states min max: -9.755269050598145 10.106096267700195\n",
      "hidden_state minus mean squared max: 100.47555541992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.80885314941406 3.923518657684326\n",
      "loss  1493: 1.8026   grad norm: 1.9056          model param norm: 87.6457        \n",
      "\n",
      "quiet_star_policy_loss= 0.0032986165024340153\n",
      "nll_loss= 1.8159879446029663\n",
      "avg_std= 0.23291951417922974\n",
      "dist std min max: 0.0075907777063548565 0.23291951417922974 2.87616229057312\n",
      "hidden_states min max: -10.211544036865234 10.521928787231445\n",
      "hidden_state minus mean squared max: 72.56149291992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.94731903076172 3.959871768951416\n",
      "loss  1494: 1.8193   grad norm: 2.0510          model param norm: 87.6471        \n",
      "\n",
      "quiet_star_policy_loss= 0.014571810141205788\n",
      "nll_loss= 1.815232276916504\n",
      "avg_std= 0.23137037456035614\n",
      "dist std min max: 0.0073395599611103535 0.23137037456035614 2.992553949356079\n",
      "hidden_states min max: -9.563458442687988 10.043264389038086\n",
      "hidden_state minus mean squared max: 76.23530578613281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.50475311279297 3.965799331665039\n",
      "loss  1495: 1.8298   grad norm: 1.8453          model param norm: 87.6484        \n",
      "\n",
      "quiet_star_policy_loss= -0.00772865442559123\n",
      "nll_loss= 1.8042734861373901\n",
      "avg_std= 0.23185914754867554\n",
      "dist std min max: 0.007448223419487476 0.23185914754867554 2.720151901245117\n",
      "hidden_states min max: -13.063262939453125 10.251829147338867\n",
      "hidden_state minus mean squared max: 105.61431121826172\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.86247253417969 3.9533615112304688\n",
      "loss  1496: 1.7965   grad norm: 2.0315          model param norm: 87.6499        \n",
      "\n",
      "quiet_star_policy_loss= -0.014285064302384853\n",
      "nll_loss= 1.8022569417953491\n",
      "avg_std= 0.23288381099700928\n",
      "dist std min max: 0.0071595944464206696 0.23288381099700928 2.918022632598877\n",
      "hidden_states min max: -9.73639965057373 10.440112113952637\n",
      "hidden_state minus mean squared max: 105.52808380126953\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.66832733154297 4.012906074523926\n",
      "loss  1497: 1.7880   grad norm: 2.0475          model param norm: 87.6515        \n",
      "\n",
      "quiet_star_policy_loss= -0.007610917091369629\n",
      "nll_loss= 1.820970892906189\n",
      "avg_std= 0.23125097155570984\n",
      "dist std min max: 0.007485322654247284 0.23125097155570984 2.902039051055908\n",
      "hidden_states min max: -13.422843933105469 10.374584197998047\n",
      "hidden_state minus mean squared max: 93.74068450927734\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.51167297363281 3.9498343467712402\n",
      "loss  1498: 1.8134   grad norm: 2.0121          model param norm: 87.6534        \n",
      "\n",
      "quiet_star_policy_loss= 0.006024408619850874\n",
      "nll_loss= 1.8205757141113281\n",
      "avg_std= 0.23181501030921936\n",
      "dist std min max: 0.007678926456719637 0.23181501030921936 2.732137441635132\n",
      "hidden_states min max: -10.45621109008789 11.189502716064453\n",
      "hidden_state minus mean squared max: 110.7013168334961\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.29235076904297 3.9022088050842285\n",
      "loss  1499: 1.8266   grad norm: 1.9915          model param norm: 87.6553        \n",
      "eval loss 1.8229379653930664\n",
      "\n",
      "quiet_star_policy_loss= 0.01094441395252943\n",
      "nll_loss= 1.8259994983673096\n",
      "avg_std= 0.2316569834947586\n",
      "dist std min max: 0.008146706037223339 0.2316569834947586 2.8130977153778076\n",
      "hidden_states min max: -9.815168380737305 10.637805938720703\n",
      "hidden_state minus mean squared max: 85.50776672363281\n",
      "hidden_state minus mean divided by std max: 5.035405158996582\n",
      "log_prob min max: -103.04554748535156 3.8772988319396973\n",
      "loss  1500: 1.8369   grad norm: 1.9981          model param norm: 87.6568        \n",
      "\n",
      "quiet_star_policy_loss= -0.012453210540115833\n",
      "nll_loss= 1.8012560606002808\n",
      "avg_std= 0.23111285269260406\n",
      "dist std min max: 0.0074602942913770676 0.23111285269260406 2.804396390914917\n",
      "hidden_states min max: -10.164292335510254 10.037891387939453\n",
      "hidden_state minus mean squared max: 111.63481903076172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.90508270263672 3.9449729919433594\n",
      "loss  1501: 1.7888   grad norm: 2.0542          model param norm: 87.6581        \n",
      "\n",
      "quiet_star_policy_loss= 2.593994213384576e-05\n",
      "nll_loss= 1.7889641523361206\n",
      "avg_std= 0.23196452856063843\n",
      "dist std min max: 0.007542505860328674 0.23196452856063843 2.7904958724975586\n",
      "hidden_states min max: -10.521162033081055 9.99099349975586\n",
      "hidden_state minus mean squared max: 71.38009643554688\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.01667022705078 3.968261241912842\n",
      "loss  1502: 1.7890   grad norm: 1.9517          model param norm: 87.6594        \n",
      "\n",
      "quiet_star_policy_loss= -0.0018231034046038985\n",
      "nll_loss= 1.8180550336837769\n",
      "avg_std= 0.2293170839548111\n",
      "dist std min max: 0.007622457109391689 0.2293170839548111 2.866215705871582\n",
      "hidden_states min max: -9.620476722717285 10.308770179748535\n",
      "hidden_state minus mean squared max: 78.82449340820312\n",
      "hidden_state minus mean divided by std max: 5.166576385498047\n",
      "log_prob min max: -103.24877166748047 3.9499640464782715\n",
      "loss  1503: 1.8162   grad norm: 2.0774          model param norm: 87.6604        \n",
      "\n",
      "quiet_star_policy_loss= -0.021152997389435768\n",
      "nll_loss= 1.8183010816574097\n",
      "avg_std= 0.2289482057094574\n",
      "dist std min max: 0.007489096373319626 0.2289482057094574 2.794546604156494\n",
      "hidden_states min max: -10.892280578613281 10.164009094238281\n",
      "hidden_state minus mean squared max: 100.95709991455078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.83992767333984 3.952324867248535\n",
      "loss  1504: 1.7971   grad norm: 1.8452          model param norm: 87.6612        \n",
      "\n",
      "quiet_star_policy_loss= 0.003303319215774536\n",
      "nll_loss= 1.8268743753433228\n",
      "avg_std= 0.22968260943889618\n",
      "dist std min max: 0.007496835198253393 0.22968260943889618 2.78078293800354\n",
      "hidden_states min max: -9.648576736450195 10.490337371826172\n",
      "hidden_state minus mean squared max: 105.98860168457031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.61980438232422 3.962338447570801\n",
      "loss  1505: 1.8302   grad norm: 2.0813          model param norm: 87.6619        \n",
      "\n",
      "quiet_star_policy_loss= -0.015781927853822708\n",
      "nll_loss= 1.834215521812439\n",
      "avg_std= 0.2301737517118454\n",
      "dist std min max: 0.007896224036812782 0.2301737517118454 2.7638306617736816\n",
      "hidden_states min max: -9.785414695739746 9.725707054138184\n",
      "hidden_state minus mean squared max: 70.94647216796875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.92122650146484 3.8775649070739746\n",
      "loss  1506: 1.8184   grad norm: 2.0385          model param norm: 87.6628        \n",
      "\n",
      "quiet_star_policy_loss= -0.004504537675529718\n",
      "nll_loss= 1.81798255443573\n",
      "avg_std= 0.2300095111131668\n",
      "dist std min max: 0.007702172268182039 0.2300095111131668 2.7667174339294434\n",
      "hidden_states min max: -9.914280891418457 10.285860061645508\n",
      "hidden_state minus mean squared max: 86.95748901367188\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.38507080078125 3.931551456451416\n",
      "loss  1507: 1.8135   grad norm: 2.2215          model param norm: 87.6637        \n",
      "\n",
      "quiet_star_policy_loss= -0.015286862850189209\n",
      "nll_loss= 1.8178478479385376\n",
      "avg_std= 0.23013141751289368\n",
      "dist std min max: 0.0075983572751283646 0.23013141751289368 2.8239858150482178\n",
      "hidden_states min max: -9.393571853637695 10.185293197631836\n",
      "hidden_state minus mean squared max: 88.5385513305664\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.61077117919922 3.9240360260009766\n",
      "loss  1508: 1.8026   grad norm: 2.1545          model param norm: 87.6647        \n",
      "\n",
      "quiet_star_policy_loss= -0.01202859915792942\n",
      "nll_loss= 1.811275839805603\n",
      "avg_std= 0.22946807742118835\n",
      "dist std min max: 0.007564501836895943 0.22946807742118835 2.807811975479126\n",
      "hidden_states min max: -13.345254898071289 9.962347030639648\n",
      "hidden_state minus mean squared max: 151.80661010742188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.04389953613281 3.9486641883850098\n",
      "loss  1509: 1.7992   grad norm: 1.9312          model param norm: 87.6657        \n",
      "\n",
      "quiet_star_policy_loss= -0.0017685414059087634\n",
      "nll_loss= 1.8091182708740234\n",
      "avg_std= 0.22670966386795044\n",
      "dist std min max: 0.007630468346178532 0.22670966386795044 2.797558546066284\n",
      "hidden_states min max: -10.649927139282227 10.194421768188477\n",
      "hidden_state minus mean squared max: 74.41780853271484\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.29811096191406 3.9450597763061523\n",
      "loss  1510: 1.8073   grad norm: 2.1372          model param norm: 87.6667        \n",
      "\n",
      "quiet_star_policy_loss= -0.0048300172202289104\n",
      "nll_loss= 1.7961008548736572\n",
      "avg_std= 0.23124389350414276\n",
      "dist std min max: 0.00844433531165123 0.23124389350414276 2.7855424880981445\n",
      "hidden_states min max: -9.329980850219727 9.796720504760742\n",
      "hidden_state minus mean squared max: 63.86333084106445\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -12.690385818481445 3.8353519439697266\n",
      "loss  1511: 1.7913   grad norm: 4.1110          model param norm: 87.6679        \n",
      "\n",
      "quiet_star_policy_loss= 0.017679667100310326\n",
      "nll_loss= 1.8323506116867065\n",
      "avg_std= 0.2255443036556244\n",
      "dist std min max: 0.007528776302933693 0.2255443036556244 2.7476277351379395\n",
      "hidden_states min max: -9.476194381713867 11.779016494750977\n",
      "hidden_state minus mean squared max: 87.07186126708984\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.05027770996094 3.937509536743164\n",
      "loss  1512: 1.8500   grad norm: 1.9564          model param norm: 87.6685        \n",
      "\n",
      "quiet_star_policy_loss= 0.0020248175133019686\n",
      "nll_loss= 1.8260735273361206\n",
      "avg_std= 0.22586923837661743\n",
      "dist std min max: 0.007516644429415464 0.22586923837661743 2.811875820159912\n",
      "hidden_states min max: -9.806922912597656 10.178714752197266\n",
      "hidden_state minus mean squared max: 88.95228576660156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.58749389648438 3.9715161323547363\n",
      "loss  1513: 1.8281   grad norm: 2.0063          model param norm: 87.6693        \n",
      "\n",
      "quiet_star_policy_loss= -0.007776832673698664\n",
      "nll_loss= 1.8021610975265503\n",
      "avg_std= 0.22763793170452118\n",
      "dist std min max: 0.00801039393991232 0.22763793170452118 2.823237419128418\n",
      "hidden_states min max: -9.91464614868164 11.364643096923828\n",
      "hidden_state minus mean squared max: 95.38922882080078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24467468261719 3.887442111968994\n",
      "loss  1514: 1.7944   grad norm: 2.1497          model param norm: 87.6701        \n",
      "\n",
      "quiet_star_policy_loss= -0.013421202078461647\n",
      "nll_loss= 1.817743182182312\n",
      "avg_std= 0.22695399820804596\n",
      "dist std min max: 0.007633641827851534 0.22695399820804596 2.8581814765930176\n",
      "hidden_states min max: -12.11800479888916 10.290634155273438\n",
      "hidden_state minus mean squared max: 155.21878051757812\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.05498504638672 3.9436793327331543\n",
      "loss  1515: 1.8043   grad norm: 1.9933          model param norm: 87.6712        \n",
      "\n",
      "quiet_star_policy_loss= -0.009960412979125977\n",
      "nll_loss= 1.8076953887939453\n",
      "avg_std= 0.22845788300037384\n",
      "dist std min max: 0.007530233357101679 0.22845788300037384 2.8496060371398926\n",
      "hidden_states min max: -9.663336753845215 10.074813842773438\n",
      "hidden_state minus mean squared max: 96.03058624267578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.33778381347656 3.9620509147644043\n",
      "loss  1516: 1.7977   grad norm: 1.8952          model param norm: 87.6728        \n",
      "\n",
      "quiet_star_policy_loss= -0.030964994803071022\n",
      "nll_loss= 1.7947624921798706\n",
      "avg_std= 0.22703230381011963\n",
      "dist std min max: 0.0075813839212059975 0.22703230381011963 2.6934993267059326\n",
      "hidden_states min max: -9.588615417480469 10.664159774780273\n",
      "hidden_state minus mean squared max: 80.9552230834961\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.14305877685547 3.941525936126709\n",
      "loss  1517: 1.7638   grad norm: 2.2546          model param norm: 87.6748        \n",
      "\n",
      "quiet_star_policy_loss= -0.030120600014925003\n",
      "nll_loss= 1.8067487478256226\n",
      "avg_std= 0.22756633162498474\n",
      "dist std min max: 0.007224416825920343 0.22756633162498474 2.8330166339874268\n",
      "hidden_states min max: -9.7838134765625 10.160560607910156\n",
      "hidden_state minus mean squared max: 83.25984954833984\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.823974609375 4.008632659912109\n",
      "loss  1518: 1.7766   grad norm: 2.0323          model param norm: 87.6771        \n",
      "\n",
      "quiet_star_policy_loss= -0.02628650702536106\n",
      "nll_loss= 1.8166000843048096\n",
      "avg_std= 0.22690165042877197\n",
      "dist std min max: 0.007445982191711664 0.22690165042877197 2.8060812950134277\n",
      "hidden_states min max: -9.503656387329102 10.710813522338867\n",
      "hidden_state minus mean squared max: 86.8774185180664\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.02728271484375 3.96212100982666\n",
      "loss  1519: 1.7903   grad norm: 1.9312          model param norm: 87.6797        \n",
      "\n",
      "quiet_star_policy_loss= -0.011078285984694958\n",
      "nll_loss= 1.8160346746444702\n",
      "avg_std= 0.22629444301128387\n",
      "dist std min max: 0.008372562006115913 0.22629444301128387 2.8835394382476807\n",
      "hidden_states min max: -11.346272468566895 10.776847839355469\n",
      "hidden_state minus mean squared max: 93.88648223876953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.65910339355469 3.826632499694824\n",
      "loss  1520: 1.8050   grad norm: 2.0292          model param norm: 87.6822        \n",
      "\n",
      "quiet_star_policy_loss= -0.024629419669508934\n",
      "nll_loss= 1.8001928329467773\n",
      "avg_std= 0.2260364592075348\n",
      "dist std min max: 0.007310119923204184 0.2260364592075348 2.8156113624572754\n",
      "hidden_states min max: -9.818384170532227 10.854293823242188\n",
      "hidden_state minus mean squared max: 97.11030578613281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.10785675048828 3.9684653282165527\n",
      "loss  1521: 1.7756   grad norm: 2.2993          model param norm: 87.6845        \n",
      "\n",
      "quiet_star_policy_loss= 0.0015470654470846057\n",
      "nll_loss= 1.7940292358398438\n",
      "avg_std= 0.2267162948846817\n",
      "dist std min max: 0.0073915328830480576 0.2267162948846817 2.8037946224212646\n",
      "hidden_states min max: -9.584355354309082 10.604174613952637\n",
      "hidden_state minus mean squared max: 68.70545959472656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.88932800292969 3.9720020294189453\n",
      "loss  1522: 1.7956   grad norm: 2.1550          model param norm: 87.6870        \n",
      "\n",
      "quiet_star_policy_loss= -0.013424110598862171\n",
      "nll_loss= 1.8115015029907227\n",
      "avg_std= 0.22446629405021667\n",
      "dist std min max: 0.008265159092843533 0.22446629405021667 2.8291614055633545\n",
      "hidden_states min max: -10.846744537353516 11.296977996826172\n",
      "hidden_state minus mean squared max: 119.2529067993164\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.94712829589844 3.8717737197875977\n",
      "loss  1523: 1.7981   grad norm: 2.1077          model param norm: 87.6895        \n",
      "\n",
      "quiet_star_policy_loss= -0.05206289514899254\n",
      "nll_loss= 1.7956421375274658\n",
      "avg_std= 0.22527603805065155\n",
      "dist std min max: 0.008047042414546013 0.22527603805065155 2.759441614151001\n",
      "hidden_states min max: -16.220476150512695 10.466991424560547\n",
      "hidden_state minus mean squared max: 239.50315856933594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.27185821533203 3.889103412628174\n",
      "loss  1524: 1.7436   grad norm: 2.0334          model param norm: 87.6921        \n",
      "\n",
      "quiet_star_policy_loss= 0.007791114039719105\n",
      "nll_loss= 1.7983814477920532\n",
      "avg_std= 0.2257617563009262\n",
      "dist std min max: 0.007742611691355705 0.2257617563009262 2.7020301818847656\n",
      "hidden_states min max: -10.161703109741211 10.392789840698242\n",
      "hidden_state minus mean squared max: 106.17266845703125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.22132873535156 3.9326014518737793\n",
      "loss  1525: 1.8062   grad norm: 2.0105          model param norm: 87.6945        \n",
      "\n",
      "quiet_star_policy_loss= 0.007298946380615234\n",
      "nll_loss= 1.812044382095337\n",
      "avg_std= 0.22357428073883057\n",
      "dist std min max: 0.007910161279141903 0.22357428073883057 2.744128942489624\n",
      "hidden_states min max: -23.487369537353516 10.585156440734863\n",
      "hidden_state minus mean squared max: 425.0393371582031\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.55867767333984 3.8969197273254395\n",
      "loss  1526: 1.8193   grad norm: 2.1631          model param norm: 87.6969        \n",
      "\n",
      "quiet_star_policy_loss= -0.0178359504789114\n",
      "nll_loss= 1.819417953491211\n",
      "avg_std= 0.22407647967338562\n",
      "dist std min max: 0.007399104069918394 0.22407647967338562 2.7077527046203613\n",
      "hidden_states min max: -9.628941535949707 10.42271900177002\n",
      "hidden_state minus mean squared max: 76.90802764892578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.53357696533203 3.9783263206481934\n",
      "loss  1527: 1.8016   grad norm: 1.9132          model param norm: 87.6991        \n",
      "\n",
      "quiet_star_policy_loss= -0.006461405660957098\n",
      "nll_loss= 1.8169821500778198\n",
      "avg_std= 0.2251192182302475\n",
      "dist std min max: 0.007720958907157183 0.2251192182302475 2.6887733936309814\n",
      "hidden_states min max: -9.81654167175293 10.56907844543457\n",
      "hidden_state minus mean squared max: 92.94304656982422\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.86844635009766 3.937763214111328\n",
      "loss  1528: 1.8105   grad norm: 1.7353          model param norm: 87.7015        \n",
      "\n",
      "quiet_star_policy_loss= -0.006610250566154718\n",
      "nll_loss= 1.8124557733535767\n",
      "avg_std= 0.22379730641841888\n",
      "dist std min max: 0.007565634790807962 0.22379730641841888 2.6585140228271484\n",
      "hidden_states min max: -9.695608139038086 10.545278549194336\n",
      "hidden_state minus mean squared max: 90.43226623535156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.06436157226562 3.960287570953369\n",
      "loss  1529: 1.8058   grad norm: 2.2671          model param norm: 87.7037        \n",
      "\n",
      "quiet_star_policy_loss= -0.02457284927368164\n",
      "nll_loss= 1.8104950189590454\n",
      "avg_std= 0.2221525013446808\n",
      "dist std min max: 0.007508337032049894 0.2221525013446808 2.730804920196533\n",
      "hidden_states min max: -10.435379028320312 11.740345001220703\n",
      "hidden_state minus mean squared max: 129.01077270507812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.9625244140625 3.925363063812256\n",
      "loss  1530: 1.7859   grad norm: 1.9441          model param norm: 87.7061        \n",
      "\n",
      "quiet_star_policy_loss= -0.02246279828250408\n",
      "nll_loss= 1.8177448511123657\n",
      "avg_std= 0.2237004041671753\n",
      "dist std min max: 0.007239393424242735 0.2237004041671753 2.6911795139312744\n",
      "hidden_states min max: -9.655549049377441 10.64249038696289\n",
      "hidden_state minus mean squared max: 78.86710357666016\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.68031311035156 3.995999336242676\n",
      "loss  1531: 1.7953   grad norm: 2.0446          model param norm: 87.7082        \n",
      "\n",
      "quiet_star_policy_loss= -0.022168636322021484\n",
      "nll_loss= 1.8293918371200562\n",
      "avg_std= 0.22363369166851044\n",
      "dist std min max: 0.007857706397771835 0.22363369166851044 2.7312638759613037\n",
      "hidden_states min max: -9.711413383483887 10.513420104980469\n",
      "hidden_state minus mean squared max: 68.18353271484375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85816192626953 3.91817045211792\n",
      "loss  1532: 1.8072   grad norm: 2.1162          model param norm: 87.7101        \n",
      "\n",
      "quiet_star_policy_loss= -0.019776035100221634\n",
      "nll_loss= 1.805496096611023\n",
      "avg_std= 0.22397837042808533\n",
      "dist std min max: 0.007560639642179012 0.22397837042808533 2.762878656387329\n",
      "hidden_states min max: -9.715530395507812 10.613361358642578\n",
      "hidden_state minus mean squared max: 89.2632064819336\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.12169647216797 3.961759090423584\n",
      "loss  1533: 1.7857   grad norm: 2.0076          model param norm: 87.7121        \n",
      "\n",
      "quiet_star_policy_loss= -0.017525697126984596\n",
      "nll_loss= 1.7928056716918945\n",
      "avg_std= 0.22362391650676727\n",
      "dist std min max: 0.007435097359120846 0.22362391650676727 2.7441980838775635\n",
      "hidden_states min max: -9.86082649230957 10.792755126953125\n",
      "hidden_state minus mean squared max: 78.77241516113281\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.24568176269531 3.960799217224121\n",
      "loss  1534: 1.7753   grad norm: 2.0567          model param norm: 87.7140        \n",
      "\n",
      "quiet_star_policy_loss= 0.018211936578154564\n",
      "nll_loss= 1.8186969757080078\n",
      "avg_std= 0.22258584201335907\n",
      "dist std min max: 0.007754419930279255 0.22258584201335907 2.7998902797698975\n",
      "hidden_states min max: -9.711997985839844 11.257787704467773\n",
      "hidden_state minus mean squared max: 71.08432006835938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.26199340820312 3.9385385513305664\n",
      "loss  1535: 1.8369   grad norm: 2.2618          model param norm: 87.7160        \n",
      "\n",
      "quiet_star_policy_loss= 0.0026406049728393555\n",
      "nll_loss= 1.8074191808700562\n",
      "avg_std= 0.22218281030654907\n",
      "dist std min max: 0.007469214498996735 0.22218281030654907 2.6934566497802734\n",
      "hidden_states min max: -9.754353523254395 10.811094284057617\n",
      "hidden_state minus mean squared max: 89.37024688720703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.785881996154785 3.9583516120910645\n",
      "loss  1536: 1.8101   grad norm: 1.9781          model param norm: 87.7178        \n",
      "\n",
      "quiet_star_policy_loss= -0.018660331144928932\n",
      "nll_loss= 1.8143532276153564\n",
      "avg_std= 0.223441019654274\n",
      "dist std min max: 0.007851121947169304 0.223441019654274 2.8240718841552734\n",
      "hidden_states min max: -9.682815551757812 10.691251754760742\n",
      "hidden_state minus mean squared max: 78.08392333984375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.71147155761719 3.908059597015381\n",
      "loss  1537: 1.7957   grad norm: 1.9826          model param norm: 87.7198        \n",
      "\n",
      "quiet_star_policy_loss= -0.0026141167618334293\n",
      "nll_loss= 1.8185871839523315\n",
      "avg_std= 0.22405613958835602\n",
      "dist std min max: 0.00797174870967865 0.22405613958835602 2.829403877258301\n",
      "hidden_states min max: -13.928335189819336 10.472644805908203\n",
      "hidden_state minus mean squared max: 172.98458862304688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.10917663574219 3.909115791320801\n",
      "loss  1538: 1.8160   grad norm: 2.0348          model param norm: 87.7220        \n",
      "\n",
      "quiet_star_policy_loss= 0.055951233953237534\n",
      "nll_loss= 1.7863900661468506\n",
      "avg_std= 0.2285085916519165\n",
      "dist std min max: 0.008514928631484509 0.2285085916519165 2.7973248958587646\n",
      "hidden_states min max: -9.698577880859375 10.397512435913086\n",
      "hidden_state minus mean squared max: 59.0020637512207\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.89969635009766 3.8402652740478516\n",
      "loss  1539: 1.8423   grad norm: 4.1575          model param norm: 87.7240        \n",
      "\n",
      "quiet_star_policy_loss= 0.01017505582422018\n",
      "nll_loss= 1.813083291053772\n",
      "avg_std= 0.22543413937091827\n",
      "dist std min max: 0.008108563721179962 0.22543413937091827 2.9051947593688965\n",
      "hidden_states min max: -12.692316055297852 10.711313247680664\n",
      "hidden_state minus mean squared max: 140.085205078125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.00370025634766 3.8635377883911133\n",
      "loss  1540: 1.8233   grad norm: 2.1196          model param norm: 87.7268        \n",
      "\n",
      "quiet_star_policy_loss= -0.00454669026657939\n",
      "nll_loss= 1.8051834106445312\n",
      "avg_std= 0.22477397322654724\n",
      "dist std min max: 0.007592924404889345 0.22477397322654724 2.8938543796539307\n",
      "hidden_states min max: -9.732815742492676 10.411585807800293\n",
      "hidden_state minus mean squared max: 105.69306182861328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.7905044555664 3.955148696899414\n",
      "loss  1541: 1.8006   grad norm: 2.1070          model param norm: 87.7297        \n",
      "\n",
      "quiet_star_policy_loss= -0.014367341995239258\n",
      "nll_loss= 1.8131500482559204\n",
      "avg_std= 0.22650820016860962\n",
      "dist std min max: 0.00799531675875187 0.22650820016860962 2.98215913772583\n",
      "hidden_states min max: -9.890605926513672 10.774650573730469\n",
      "hidden_state minus mean squared max: 86.89083099365234\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.25748443603516 3.9077048301696777\n",
      "loss  1542: 1.7988   grad norm: 2.3328          model param norm: 87.7327        \n",
      "\n",
      "quiet_star_policy_loss= 0.000606834888458252\n",
      "nll_loss= 1.8086105585098267\n",
      "avg_std= 0.22800926864147186\n",
      "dist std min max: 0.007529059890657663 0.22800926864147186 3.0548689365386963\n",
      "hidden_states min max: -9.850678443908691 11.381061553955078\n",
      "hidden_state minus mean squared max: 94.63655853271484\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.33558654785156 3.945727825164795\n",
      "loss  1543: 1.8092   grad norm: 2.1346          model param norm: 87.7354        \n",
      "\n",
      "quiet_star_policy_loss= -0.012465298175811768\n",
      "nll_loss= 1.817381739616394\n",
      "avg_std= 0.22611607611179352\n",
      "dist std min max: 0.007526780012995005 0.22611607611179352 3.0536789894104004\n",
      "hidden_states min max: -9.766653060913086 11.319255828857422\n",
      "hidden_state minus mean squared max: 181.0178985595703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.13188171386719 3.9608168601989746\n",
      "loss  1544: 1.8049   grad norm: 2.1197          model param norm: 87.7380        \n",
      "\n",
      "quiet_star_policy_loss= 0.003939914982765913\n",
      "nll_loss= 1.8160361051559448\n",
      "avg_std= 0.22764746844768524\n",
      "dist std min max: 0.007622852921485901 0.22764746844768524 3.1085586547851562\n",
      "hidden_states min max: -9.857320785522461 11.113975524902344\n",
      "hidden_state minus mean squared max: 93.26280212402344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.727783203125 3.9569506645202637\n",
      "loss  1545: 1.8200   grad norm: 2.0812          model param norm: 87.7403        \n",
      "\n",
      "quiet_star_policy_loss= -0.0023239850997924805\n",
      "nll_loss= 1.7979087829589844\n",
      "avg_std= 0.22901864349842072\n",
      "dist std min max: 0.007589837070554495 0.22901864349842072 3.093447208404541\n",
      "hidden_states min max: -9.761898040771484 10.92391300201416\n",
      "hidden_state minus mean squared max: 106.68791961669922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.74951171875 3.954021453857422\n",
      "loss  1546: 1.7956   grad norm: 2.2843          model param norm: 87.7425        \n",
      "\n",
      "quiet_star_policy_loss= -0.023922253400087357\n",
      "nll_loss= 1.828530192375183\n",
      "avg_std= 0.2269960343837738\n",
      "dist std min max: 0.007833960466086864 0.2269960343837738 3.1190249919891357\n",
      "hidden_states min max: -10.662606239318848 11.065591812133789\n",
      "hidden_state minus mean squared max: 102.61981964111328\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.41344451904297 3.9062271118164062\n",
      "loss  1547: 1.8046   grad norm: 2.1342          model param norm: 87.7443        \n",
      "\n",
      "quiet_star_policy_loss= 0.011881589889526367\n",
      "nll_loss= 1.8078514337539673\n",
      "avg_std= 0.22954511642456055\n",
      "dist std min max: 0.007932252250611782 0.22954511642456055 3.0651185512542725\n",
      "hidden_states min max: -14.09226131439209 10.556565284729004\n",
      "hidden_state minus mean squared max: 225.7081756591797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.24219512939453 3.901582717895508\n",
      "loss  1548: 1.8197   grad norm: 1.9667          model param norm: 87.7462        \n",
      "\n",
      "quiet_star_policy_loss= -0.00679474463686347\n",
      "nll_loss= 1.8103466033935547\n",
      "avg_std= 0.22892789542675018\n",
      "dist std min max: 0.00782059971243143 0.22892789542675018 3.1234850883483887\n",
      "hidden_states min max: -10.490289688110352 10.707324981689453\n",
      "hidden_state minus mean squared max: 103.3193588256836\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.66255950927734 3.8760666847229004\n",
      "loss  1549: 1.8036   grad norm: 2.0865          model param norm: 87.7480        \n",
      "\n",
      "quiet_star_policy_loss= -0.005144661758095026\n",
      "nll_loss= 1.8110417127609253\n",
      "avg_std= 0.23043422400951385\n",
      "dist std min max: 0.007935808971524239 0.23043422400951385 3.1139166355133057\n",
      "hidden_states min max: -9.969983100891113 10.921175003051758\n",
      "hidden_state minus mean squared max: 101.91329193115234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.42794036865234 3.855421543121338\n",
      "loss  1550: 1.8059   grad norm: 2.2684          model param norm: 87.7499        \n",
      "\n",
      "quiet_star_policy_loss= 0.0022954822052270174\n",
      "nll_loss= 1.7979835271835327\n",
      "avg_std= 0.23108340799808502\n",
      "dist std min max: 0.008029759861528873 0.23108340799808502 3.145845651626587\n",
      "hidden_states min max: -9.890164375305176 11.504719734191895\n",
      "hidden_state minus mean squared max: 96.15613555908203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.118036270141602 3.8986339569091797\n",
      "loss  1551: 1.8003   grad norm: 2.3323          model param norm: 87.7516        \n",
      "\n",
      "quiet_star_policy_loss= -0.021376943215727806\n",
      "nll_loss= 1.822031855583191\n",
      "avg_std= 0.22986237704753876\n",
      "dist std min max: 0.007966550067067146 0.22986237704753876 3.115095615386963\n",
      "hidden_states min max: -11.136821746826172 10.542531967163086\n",
      "hidden_state minus mean squared max: 115.37303924560547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.22493743896484 3.9050188064575195\n",
      "loss  1552: 1.8007   grad norm: 2.2221          model param norm: 87.7533        \n",
      "\n",
      "quiet_star_policy_loss= -0.006694507785141468\n",
      "nll_loss= 1.8154067993164062\n",
      "avg_std= 0.23001259565353394\n",
      "dist std min max: 0.008253687061369419 0.23001259565353394 3.0951576232910156\n",
      "hidden_states min max: -13.356197357177734 10.863296508789062\n",
      "hidden_state minus mean squared max: 178.1110076904297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.12378692626953 3.856739044189453\n",
      "loss  1553: 1.8087   grad norm: 2.1553          model param norm: 87.7547        \n",
      "\n",
      "quiet_star_policy_loss= -0.015641426667571068\n",
      "nll_loss= 1.8184130191802979\n",
      "avg_std= 0.23024876415729523\n",
      "dist std min max: 0.008292659185826778 0.23024876415729523 3.0656137466430664\n",
      "hidden_states min max: -10.012004852294922 10.813511848449707\n",
      "hidden_state minus mean squared max: 82.71528625488281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.67159271240234 3.8409762382507324\n",
      "loss  1554: 1.8028   grad norm: 2.1557          model param norm: 87.7560        \n",
      "\n",
      "quiet_star_policy_loss= -0.017960740253329277\n",
      "nll_loss= 1.7984644174575806\n",
      "avg_std= 0.23005032539367676\n",
      "dist std min max: 0.008729569613933563 0.23005032539367676 3.0236101150512695\n",
      "hidden_states min max: -10.030411720275879 11.23481559753418\n",
      "hidden_state minus mean squared max: 100.58039093017578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.3651351928711 3.809107780456543\n",
      "loss  1555: 1.7805   grad norm: 2.2560          model param norm: 87.7573        \n",
      "\n",
      "quiet_star_policy_loss= -0.000735425972379744\n",
      "nll_loss= 1.7992409467697144\n",
      "avg_std= 0.23182909190654755\n",
      "dist std min max: 0.008947547525167465 0.23182909190654755 3.0476737022399902\n",
      "hidden_states min max: -10.400212287902832 11.032462120056152\n",
      "hidden_state minus mean squared max: 108.38948059082031\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.64908599853516 3.7631468772888184\n",
      "loss  1556: 1.7985   grad norm: 2.3324          model param norm: 87.7587        \n",
      "\n",
      "quiet_star_policy_loss= -0.02845693938434124\n",
      "nll_loss= 1.8142722845077515\n",
      "avg_std= 0.22848525643348694\n",
      "dist std min max: 0.008790173567831516 0.22848525643348694 3.0563392639160156\n",
      "hidden_states min max: -10.079879760742188 10.669315338134766\n",
      "hidden_state minus mean squared max: 98.7284927368164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4796142578125 3.780857563018799\n",
      "loss  1557: 1.7858   grad norm: 2.0171          model param norm: 87.7604        \n",
      "\n",
      "quiet_star_policy_loss= 0.001528835273347795\n",
      "nll_loss= 1.8114356994628906\n",
      "avg_std= 0.22860361635684967\n",
      "dist std min max: 0.009011908434331417 0.22860361635684967 2.996325731277466\n",
      "hidden_states min max: -10.58151912689209 10.561835289001465\n",
      "hidden_state minus mean squared max: 117.12112426757812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.62516784667969 3.773397445678711\n",
      "loss  1558: 1.8130   grad norm: 2.1897          model param norm: 87.7622        \n",
      "\n",
      "quiet_star_policy_loss= 0.00592810520902276\n",
      "nll_loss= 1.8062820434570312\n",
      "avg_std= 0.22911152243614197\n",
      "dist std min max: 0.009406098164618015 0.22911152243614197 3.060530424118042\n",
      "hidden_states min max: -15.665508270263672 11.018900871276855\n",
      "hidden_state minus mean squared max: 219.92503356933594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.2292251586914 3.721961498260498\n",
      "loss  1559: 1.8122   grad norm: 2.1017          model param norm: 87.7637        \n",
      "\n",
      "quiet_star_policy_loss= -0.017871379852294922\n",
      "nll_loss= 1.8148186206817627\n",
      "avg_std= 0.22877289354801178\n",
      "dist std min max: 0.008999677374958992 0.22877289354801178 3.0156431198120117\n",
      "hidden_states min max: -11.172033309936523 10.435725212097168\n",
      "hidden_state minus mean squared max: 120.50173950195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.0120620727539 3.780158519744873\n",
      "loss  1560: 1.7969   grad norm: 2.2393          model param norm: 87.7657        \n",
      "\n",
      "quiet_star_policy_loss= -0.01743905618786812\n",
      "nll_loss= 1.7905486822128296\n",
      "avg_std= 0.2283821851015091\n",
      "dist std min max: 0.009684469550848007 0.2283821851015091 3.0062308311462402\n",
      "hidden_states min max: -10.98950481414795 12.485573768615723\n",
      "hidden_state minus mean squared max: 116.90203094482422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.9132308959961 3.6872034072875977\n",
      "loss  1561: 1.7731   grad norm: 2.1920          model param norm: 87.7676        \n",
      "\n",
      "quiet_star_policy_loss= -0.014607572928071022\n",
      "nll_loss= 1.8071073293685913\n",
      "avg_std= 0.22741706669330597\n",
      "dist std min max: 0.009597601369023323 0.22741706669330597 3.011054277420044\n",
      "hidden_states min max: -10.075961112976074 10.618675231933594\n",
      "hidden_state minus mean squared max: 96.59125518798828\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.31189727783203 3.724487781524658\n",
      "loss  1562: 1.7925   grad norm: 2.0793          model param norm: 87.7696        \n",
      "\n",
      "quiet_star_policy_loss= -0.03311796113848686\n",
      "nll_loss= 1.8091646432876587\n",
      "avg_std= 0.2268865555524826\n",
      "dist std min max: 0.009609383530914783 0.2268865555524826 2.9297139644622803\n",
      "hidden_states min max: -13.131986618041992 12.129613876342773\n",
      "hidden_state minus mean squared max: 125.44662475585938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.94851684570312 3.703249454498291\n",
      "loss  1563: 1.7760   grad norm: 2.0123          model param norm: 87.7715        \n",
      "\n",
      "quiet_star_policy_loss= -0.008465254679322243\n",
      "nll_loss= 1.7888740301132202\n",
      "avg_std= 0.22773464024066925\n",
      "dist std min max: 0.009134495630860329 0.22773464024066925 2.935553550720215\n",
      "hidden_states min max: -10.031556129455566 11.41211223602295\n",
      "hidden_state minus mean squared max: 102.77764129638672\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.65442657470703 3.7563905715942383\n",
      "loss  1564: 1.7804   grad norm: 2.1056          model param norm: 87.7734        \n",
      "\n",
      "quiet_star_policy_loss= -0.0008785605314187706\n",
      "nll_loss= 1.7824747562408447\n",
      "avg_std= 0.22587062418460846\n",
      "dist std min max: 0.008648094721138477 0.22587062418460846 2.969775915145874\n",
      "hidden_states min max: -12.034542083740234 11.433500289916992\n",
      "hidden_state minus mean squared max: 160.628662109375\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.07213592529297 3.8297324180603027\n",
      "loss  1565: 1.7816   grad norm: 1.9701          model param norm: 87.7754        \n",
      "\n",
      "quiet_star_policy_loss= -0.0064900401048362255\n",
      "nll_loss= 1.8025776147842407\n",
      "avg_std= 0.2256942242383957\n",
      "dist std min max: 0.009077307768166065 0.2256942242383957 2.930337429046631\n",
      "hidden_states min max: -10.078513145446777 10.674598693847656\n",
      "hidden_state minus mean squared max: 95.0212631225586\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.32384490966797 3.762301445007324\n",
      "loss  1566: 1.7961   grad norm: 2.0804          model param norm: 87.7774        \n",
      "\n",
      "quiet_star_policy_loss= -0.012839559465646744\n",
      "nll_loss= 1.7983696460723877\n",
      "avg_std= 0.22695839405059814\n",
      "dist std min max: 0.009987147524952888 0.22695839405059814 2.7666237354278564\n",
      "hidden_states min max: -9.976089477539062 10.590241432189941\n",
      "hidden_state minus mean squared max: 63.22462463378906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79033660888672 3.684863567352295\n",
      "loss  1567: 1.7855   grad norm: 4.3238          model param norm: 87.7794        \n",
      "\n",
      "quiet_star_policy_loss= -0.01131129264831543\n",
      "nll_loss= 1.8123725652694702\n",
      "avg_std= 0.22633600234985352\n",
      "dist std min max: 0.009375236928462982 0.22633600234985352 2.8106462955474854\n",
      "hidden_states min max: -10.06734848022461 11.450956344604492\n",
      "hidden_state minus mean squared max: 77.63825988769531\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.10189819335938 3.7066969871520996\n",
      "loss  1568: 1.8011   grad norm: 2.0208          model param norm: 87.7806        \n",
      "\n",
      "quiet_star_policy_loss= -0.011046028696000576\n",
      "nll_loss= 1.815834641456604\n",
      "avg_std= 0.22564885020256042\n",
      "dist std min max: 0.009868125431239605 0.22564885020256042 2.9014525413513184\n",
      "hidden_states min max: -10.263614654541016 10.59227180480957\n",
      "hidden_state minus mean squared max: 101.70858001708984\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -102.79364013671875 3.6977477073669434\n",
      "loss  1569: 1.8048   grad norm: 2.0705          model param norm: 87.7816        \n",
      "\n",
      "quiet_star_policy_loss= -0.010894328355789185\n",
      "nll_loss= 1.796180009841919\n",
      "avg_std= 0.22645124793052673\n",
      "dist std min max: 0.009573056362569332 0.22645124793052673 2.8748867511749268\n",
      "hidden_states min max: -9.971839904785156 11.320454597473145\n",
      "hidden_state minus mean squared max: 95.83936309814453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.01942443847656 3.7119951248168945\n",
      "loss  1570: 1.7853   grad norm: 2.0360          model param norm: 87.7829        \n",
      "\n",
      "quiet_star_policy_loss= -0.03070859983563423\n",
      "nll_loss= 1.8017539978027344\n",
      "avg_std= 0.22714804112911224\n",
      "dist std min max: 0.00967676006257534 0.22714804112911224 2.9367406368255615\n",
      "hidden_states min max: -9.993440628051758 10.475930213928223\n",
      "hidden_state minus mean squared max: 77.02946472167969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.96186065673828 3.6887474060058594\n",
      "loss  1571: 1.7710   grad norm: 2.1679          model param norm: 87.7841        \n",
      "\n",
      "quiet_star_policy_loss= -0.01159761007875204\n",
      "nll_loss= 1.8144725561141968\n",
      "avg_std= 0.2265537530183792\n",
      "dist std min max: 0.009675842709839344 0.2265537530183792 2.9207680225372314\n",
      "hidden_states min max: -10.092612266540527 10.77784252166748\n",
      "hidden_state minus mean squared max: 82.29847717285156\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.61421966552734 3.714653968811035\n",
      "loss  1572: 1.8029   grad norm: 1.8683          model param norm: 87.7854        \n",
      "\n",
      "quiet_star_policy_loss= 0.018584681674838066\n",
      "nll_loss= 1.80354905128479\n",
      "avg_std= 0.2239484041929245\n",
      "dist std min max: 0.009749679826200008 0.2239484041929245 2.8103694915771484\n",
      "hidden_states min max: -10.17462158203125 10.993395805358887\n",
      "hidden_state minus mean squared max: 203.4497833251953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19029235839844 3.7052197456359863\n",
      "loss  1573: 1.8221   grad norm: 2.0971          model param norm: 87.7869        \n",
      "\n",
      "quiet_star_policy_loss= -0.023900557309389114\n",
      "nll_loss= 1.7952078580856323\n",
      "avg_std= 0.2243029922246933\n",
      "dist std min max: 0.009786378592252731 0.2243029922246933 2.845677614212036\n",
      "hidden_states min max: -10.212528228759766 12.270626068115234\n",
      "hidden_state minus mean squared max: 149.5032196044922\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.90689086914062 3.700169086456299\n",
      "loss  1574: 1.7713   grad norm: 2.1797          model param norm: 87.7881        \n",
      "\n",
      "quiet_star_policy_loss= -0.03275487571954727\n",
      "nll_loss= 1.8167835474014282\n",
      "avg_std= 0.22428303956985474\n",
      "dist std min max: 0.009775525890290737 0.22428303956985474 2.836315631866455\n",
      "hidden_states min max: -10.268670082092285 11.053571701049805\n",
      "hidden_state minus mean squared max: 107.23261260986328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.16671752929688 3.6965723037719727\n",
      "loss  1575: 1.7840   grad norm: 2.0495          model param norm: 87.7890        \n",
      "\n",
      "quiet_star_policy_loss= -0.026058828458189964\n",
      "nll_loss= 1.799021601676941\n",
      "avg_std= 0.22589942812919617\n",
      "dist std min max: 0.009667439386248589 0.22589942812919617 2.9415078163146973\n",
      "hidden_states min max: -13.85722541809082 12.311250686645508\n",
      "hidden_state minus mean squared max: 114.9364242553711\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90476989746094 3.7129383087158203\n",
      "loss  1576: 1.7730   grad norm: 2.1555          model param norm: 87.7900        \n",
      "\n",
      "quiet_star_policy_loss= -0.009641326032578945\n",
      "nll_loss= 1.80870521068573\n",
      "avg_std= 0.22521042823791504\n",
      "dist std min max: 0.009765658527612686 0.22521042823791504 2.894596815109253\n",
      "hidden_states min max: -9.91708755493164 10.521282196044922\n",
      "hidden_state minus mean squared max: 68.12454986572266\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.10088348388672 3.7002315521240234\n",
      "loss  1577: 1.7991   grad norm: 2.3543          model param norm: 87.7909        \n",
      "\n",
      "quiet_star_policy_loss= -0.006111502647399902\n",
      "nll_loss= 1.8209863901138306\n",
      "avg_std= 0.2248552292585373\n",
      "dist std min max: 0.0095378952100873 0.2248552292585373 2.8827669620513916\n",
      "hidden_states min max: -9.988543510437012 10.519609451293945\n",
      "hidden_state minus mean squared max: 93.2439193725586\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.9477310180664 3.7147774696350098\n",
      "loss  1578: 1.8149   grad norm: 2.0075          model param norm: 87.7921        \n",
      "\n",
      "quiet_star_policy_loss= -0.010039472952485085\n",
      "nll_loss= 1.811296820640564\n",
      "avg_std= 0.2231455147266388\n",
      "dist std min max: 0.009559772908687592 0.2231455147266388 2.8943288326263428\n",
      "hidden_states min max: -9.831930160522461 11.067497253417969\n",
      "hidden_state minus mean squared max: 97.99372100830078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.0655746459961 3.7223854064941406\n",
      "loss  1579: 1.8013   grad norm: 1.8668          model param norm: 87.7935        \n",
      "\n",
      "quiet_star_policy_loss= 0.00762939453125\n",
      "nll_loss= 1.8019462823867798\n",
      "avg_std= 0.22483623027801514\n",
      "dist std min max: 0.009549608454108238 0.22483623027801514 2.904489517211914\n",
      "hidden_states min max: -9.977699279785156 11.366785049438477\n",
      "hidden_state minus mean squared max: 77.4175033569336\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.29916381835938 3.7196574211120605\n",
      "loss  1580: 1.8096   grad norm: 2.0495          model param norm: 87.7948        \n",
      "\n",
      "quiet_star_policy_loss= -0.02103372849524021\n",
      "nll_loss= 1.8142178058624268\n",
      "avg_std= 0.2259538769721985\n",
      "dist std min max: 0.009398077614605427 0.2259538769721985 3.0626494884490967\n",
      "hidden_states min max: -10.092718124389648 10.639962196350098\n",
      "hidden_state minus mean squared max: 67.26139831542969\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.58964538574219 3.7397069931030273\n",
      "loss  1581: 1.7932   grad norm: 1.9757          model param norm: 87.7961        \n",
      "\n",
      "quiet_star_policy_loss= -0.030421782284975052\n",
      "nll_loss= 1.8040417432785034\n",
      "avg_std= 0.2247972935438156\n",
      "dist std min max: 0.009241464547812939 0.2247972935438156 2.885531187057495\n",
      "hidden_states min max: -12.406563758850098 11.245007514953613\n",
      "hidden_state minus mean squared max: 92.31655883789062\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.79515838623047 3.7362380027770996\n",
      "loss  1582: 1.7736   grad norm: 1.9702          model param norm: 87.7975        \n",
      "\n",
      "quiet_star_policy_loss= 0.010890522971749306\n",
      "nll_loss= 1.7984297275543213\n",
      "avg_std= 0.2242717444896698\n",
      "dist std min max: 0.009183349087834358 0.2242717444896698 2.879965305328369\n",
      "hidden_states min max: -9.756237030029297 11.594911575317383\n",
      "hidden_state minus mean squared max: 78.65290069580078\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -102.89825439453125 3.761167526245117\n",
      "loss  1583: 1.8093   grad norm: 2.0510          model param norm: 87.7989        \n",
      "\n",
      "quiet_star_policy_loss= -0.043464161455631256\n",
      "nll_loss= 1.8135534524917603\n",
      "avg_std= 0.22252722084522247\n",
      "dist std min max: 0.009284468367695808 0.22252722084522247 2.8221147060394287\n",
      "hidden_states min max: -9.99689769744873 11.490861892700195\n",
      "hidden_state minus mean squared max: 97.06427764892578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.57351684570312 3.748389720916748\n",
      "loss  1584: 1.7701   grad norm: 2.2561          model param norm: 87.8003        \n",
      "\n",
      "quiet_star_policy_loss= -0.01741924323141575\n",
      "nll_loss= 1.8056739568710327\n",
      "avg_std= 0.22462551295757294\n",
      "dist std min max: 0.00933091901242733 0.22462551295757294 2.798214912414551\n",
      "hidden_states min max: -11.176602363586426 10.360483169555664\n",
      "hidden_state minus mean squared max: 76.7586898803711\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.0217056274414 3.7505812644958496\n",
      "loss  1585: 1.7883   grad norm: 1.9964          model param norm: 87.8016        \n",
      "\n",
      "quiet_star_policy_loss= -0.011050415225327015\n",
      "nll_loss= 1.8064543008804321\n",
      "avg_std= 0.22230248153209686\n",
      "dist std min max: 0.009272190742194653 0.22230248153209686 2.7798008918762207\n",
      "hidden_states min max: -9.958307266235352 12.170074462890625\n",
      "hidden_state minus mean squared max: 79.7853012084961\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.47016143798828 3.7449698448181152\n",
      "loss  1586: 1.7954   grad norm: 2.0428          model param norm: 87.8026        \n",
      "\n",
      "quiet_star_policy_loss= -0.025743484497070312\n",
      "nll_loss= 1.8088674545288086\n",
      "avg_std= 0.22242030501365662\n",
      "dist std min max: 0.009061634540557861 0.22242030501365662 2.8829760551452637\n",
      "hidden_states min max: -10.06923770904541 10.366738319396973\n",
      "hidden_state minus mean squared max: 65.99784088134766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79830169677734 3.77772855758667\n",
      "loss  1587: 1.7831   grad norm: 2.3245          model param norm: 87.8038        \n",
      "\n",
      "quiet_star_policy_loss= -0.012887346558272839\n",
      "nll_loss= 1.781232476234436\n",
      "avg_std= 0.2222684770822525\n",
      "dist std min max: 0.009100415743887424 0.2222684770822525 2.8466439247131348\n",
      "hidden_states min max: -9.905683517456055 10.5105562210083\n",
      "hidden_state minus mean squared max: 89.3025894165039\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.76817321777344 3.765336036682129\n",
      "loss  1588: 1.7683   grad norm: 2.3200          model param norm: 87.8051        \n",
      "\n",
      "quiet_star_policy_loss= 0.0005405425908975303\n",
      "nll_loss= 1.807119369506836\n",
      "avg_std= 0.2209988534450531\n",
      "dist std min max: 0.0090783154591918 0.2209988534450531 2.7915115356445312\n",
      "hidden_states min max: -10.05993938446045 11.079401016235352\n",
      "hidden_state minus mean squared max: 64.24818420410156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.51651000976562 3.7790298461914062\n",
      "loss  1589: 1.8077   grad norm: 2.0513          model param norm: 87.8068        \n",
      "\n",
      "quiet_star_policy_loss= 0.026233196258544922\n",
      "nll_loss= 1.7983369827270508\n",
      "avg_std= 0.22252358496189117\n",
      "dist std min max: 0.009031903930008411 0.22252358496189117 2.923128843307495\n",
      "hidden_states min max: -13.121908187866211 11.299114227294922\n",
      "hidden_state minus mean squared max: 125.7272720336914\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.94963836669922 3.766594886779785\n",
      "loss  1590: 1.8246   grad norm: 2.1895          model param norm: 87.8083        \n",
      "\n",
      "quiet_star_policy_loss= -0.014847541227936745\n",
      "nll_loss= 1.7860459089279175\n",
      "avg_std= 0.22259792685508728\n",
      "dist std min max: 0.009167501702904701 0.22259792685508728 2.935227394104004\n",
      "hidden_states min max: -10.003786087036133 11.06296443939209\n",
      "hidden_state minus mean squared max: 72.78754425048828\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.1751937866211 3.764591693878174\n",
      "loss  1591: 1.7712   grad norm: 2.2709          model param norm: 87.8095        \n",
      "\n",
      "quiet_star_policy_loss= 0.009312999434769154\n",
      "nll_loss= 1.815087914466858\n",
      "avg_std= 0.21940337121486664\n",
      "dist std min max: 0.009171852841973305 0.21940337121486664 2.8893961906433105\n",
      "hidden_states min max: -11.245499610900879 12.012545585632324\n",
      "hidden_state minus mean squared max: 112.10308837890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.89065551757812 3.7664809226989746\n",
      "loss  1592: 1.8244   grad norm: 1.9773          model param norm: 87.8111        \n",
      "\n",
      "quiet_star_policy_loss= -0.004197502043098211\n",
      "nll_loss= 1.8028885126113892\n",
      "avg_std= 0.22163018584251404\n",
      "dist std min max: 0.009179132990539074 0.22163018584251404 3.1722893714904785\n",
      "hidden_states min max: -9.98651123046875 11.056546211242676\n",
      "hidden_state minus mean squared max: 106.03910827636719\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.50138092041016 3.756669521331787\n",
      "loss  1593: 1.7987   grad norm: 2.0443          model param norm: 87.8127        \n",
      "\n",
      "quiet_star_policy_loss= 0.006799447815865278\n",
      "nll_loss= 1.8006877899169922\n",
      "avg_std= 0.22128863632678986\n",
      "dist std min max: 0.009192487224936485 0.22128863632678986 2.8204798698425293\n",
      "hidden_states min max: -10.013126373291016 10.438837051391602\n",
      "hidden_state minus mean squared max: 71.46574401855469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.6232681274414 3.760289192199707\n",
      "loss  1594: 1.8075   grad norm: 2.1876          model param norm: 87.8142        \n",
      "\n",
      "quiet_star_policy_loss= -0.0028139750938862562\n",
      "nll_loss= 1.7914067506790161\n",
      "avg_std= 0.2205418050289154\n",
      "dist std min max: 0.009317345917224884 0.2205418050289154 2.762573003768921\n",
      "hidden_states min max: -10.105419158935547 10.639389038085938\n",
      "hidden_state minus mean squared max: 62.48723602294922\n",
      "hidden_state minus mean divided by std max: 4.90096378326416\n",
      "log_prob min max: -102.54887390136719 3.745126724243164\n",
      "loss  1595: 1.7886   grad norm: 4.7539          model param norm: 87.8157        \n",
      "\n",
      "quiet_star_policy_loss= -0.02743205986917019\n",
      "nll_loss= 1.802988052368164\n",
      "avg_std= 0.2222883403301239\n",
      "dist std min max: 0.009329148568212986 0.2222883403301239 2.9506657123565674\n",
      "hidden_states min max: -10.3358154296875 11.141289710998535\n",
      "hidden_state minus mean squared max: 76.40513610839844\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.63677215576172 3.7468700408935547\n",
      "loss  1596: 1.7756   grad norm: 1.9586          model param norm: 87.8171        \n",
      "\n",
      "quiet_star_policy_loss= -0.003007316729053855\n",
      "nll_loss= 1.8085564374923706\n",
      "avg_std= 0.22114337980747223\n",
      "dist std min max: 0.0097008952870965 0.22114337980747223 2.9048311710357666\n",
      "hidden_states min max: -10.191948890686035 11.201332092285156\n",
      "hidden_state minus mean squared max: 67.97628021240234\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.64215850830078 3.709242820739746\n",
      "loss  1597: 1.8055   grad norm: 2.2046          model param norm: 87.8183        \n",
      "\n",
      "quiet_star_policy_loss= -0.037636399269104004\n",
      "nll_loss= 1.8003746271133423\n",
      "avg_std= 0.22232894599437714\n",
      "dist std min max: 0.009649758227169514 0.22232894599437714 2.9227747917175293\n",
      "hidden_states min max: -10.074078559875488 11.275078773498535\n",
      "hidden_state minus mean squared max: 84.08289337158203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.74848937988281 3.7184996604919434\n",
      "loss  1598: 1.7627   grad norm: 1.9497          model param norm: 87.8191        \n",
      "\n",
      "quiet_star_policy_loss= 0.0009118795278482139\n",
      "nll_loss= 1.8062294721603394\n",
      "avg_std= 0.22304052114486694\n",
      "dist std min max: 0.00982487853616476 0.22304052114486694 2.915363073348999\n",
      "hidden_states min max: -10.113730430603027 11.566018104553223\n",
      "hidden_state minus mean squared max: 97.45025634765625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.58765411376953 3.696629047393799\n",
      "loss  1599: 1.8071   grad norm: 1.9311          model param norm: 87.8202        \n",
      "eval loss 1.8177239894866943\n",
      "\n",
      "quiet_star_policy_loss= -0.0040934085845947266\n",
      "nll_loss= 1.8145573139190674\n",
      "avg_std= 0.2210128754377365\n",
      "dist std min max: 0.010066811926662922 0.2210128754377365 2.8465678691864014\n",
      "hidden_states min max: -10.28925895690918 10.965089797973633\n",
      "hidden_state minus mean squared max: 81.32896423339844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.2781982421875 3.6679625511169434\n",
      "loss  1600: 1.8105   grad norm: 2.0873          model param norm: 87.8211        \n",
      "\n",
      "quiet_star_policy_loss= -0.014343190006911755\n",
      "nll_loss= 1.7907793521881104\n",
      "avg_std= 0.22111937403678894\n",
      "dist std min max: 0.009812676347792149 0.22111937403678894 2.7887680530548096\n",
      "hidden_states min max: -10.216565132141113 10.969108581542969\n",
      "hidden_state minus mean squared max: 59.71728515625\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -102.82777404785156 3.666759490966797\n",
      "loss  1601: 1.7764   grad norm: 2.0553          model param norm: 87.8222        \n",
      "\n",
      "quiet_star_policy_loss= -0.01862056367099285\n",
      "nll_loss= 1.8042678833007812\n",
      "avg_std= 0.22001340985298157\n",
      "dist std min max: 0.00943965744227171 0.22001340985298157 2.802386999130249\n",
      "hidden_states min max: -10.281540870666504 12.047476768493652\n",
      "hidden_state minus mean squared max: 69.15684509277344\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.10581970214844 3.73325252532959\n",
      "loss  1602: 1.7856   grad norm: 1.9678          model param norm: 87.8234        \n",
      "\n",
      "quiet_star_policy_loss= 0.007932973094284534\n",
      "nll_loss= 1.7856788635253906\n",
      "avg_std= 0.22128510475158691\n",
      "dist std min max: 0.010260360315442085 0.22128510475158691 2.8185324668884277\n",
      "hidden_states min max: -10.170097351074219 11.055984497070312\n",
      "hidden_state minus mean squared max: 72.22763061523438\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.22791290283203 3.6526880264282227\n",
      "loss  1603: 1.7936   grad norm: 1.9489          model param norm: 87.8248        \n",
      "\n",
      "quiet_star_policy_loss= 0.01390221156179905\n",
      "nll_loss= 1.7970393896102905\n",
      "avg_std= 0.22022540867328644\n",
      "dist std min max: 0.010013612918555737 0.22022540867328644 3.2624764442443848\n",
      "hidden_states min max: -10.198556900024414 11.157491683959961\n",
      "hidden_state minus mean squared max: 69.42494201660156\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.02737426757812 3.6770477294921875\n",
      "loss  1604: 1.8109   grad norm: 2.0391          model param norm: 87.8260        \n",
      "\n",
      "quiet_star_policy_loss= 0.006179166026413441\n",
      "nll_loss= 1.8080705404281616\n",
      "avg_std= 0.22067542374134064\n",
      "dist std min max: 0.009635950438678265 0.22067542374134064 2.850595712661743\n",
      "hidden_states min max: -10.183708190917969 11.012980461120605\n",
      "hidden_state minus mean squared max: 139.66673278808594\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.00221252441406 3.69480037689209\n",
      "loss  1605: 1.8142   grad norm: 2.3514          model param norm: 87.8271        \n",
      "\n",
      "quiet_star_policy_loss= -0.015865135937929153\n",
      "nll_loss= 1.7984832525253296\n",
      "avg_std= 0.21893060207366943\n",
      "dist std min max: 0.009882073849439621 0.21893060207366943 2.9101035594940186\n",
      "hidden_states min max: -10.879205703735352 10.771526336669922\n",
      "hidden_state minus mean squared max: 60.77162551879883\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0348129272461 3.693267822265625\n",
      "loss  1606: 1.7826   grad norm: 2.0197          model param norm: 87.8279        \n",
      "\n",
      "quiet_star_policy_loss= -0.0069203139282763\n",
      "nll_loss= 1.791046380996704\n",
      "avg_std= 0.21907271444797516\n",
      "dist std min max: 0.010044232942163944 0.21907271444797516 2.8526132106781006\n",
      "hidden_states min max: -10.125914573669434 10.473382949829102\n",
      "hidden_state minus mean squared max: 65.53797149658203\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.06195068359375 3.621311664581299\n",
      "loss  1607: 1.7841   grad norm: 2.2010          model param norm: 87.8287        \n",
      "\n",
      "quiet_star_policy_loss= -0.014707493595778942\n",
      "nll_loss= 1.8160619735717773\n",
      "avg_std= 0.21893183887004852\n",
      "dist std min max: 0.010175150819122791 0.21893183887004852 3.066394805908203\n",
      "hidden_states min max: -10.11023235321045 10.529220581054688\n",
      "hidden_state minus mean squared max: 94.68584442138672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.4579086303711 3.649354934692383\n",
      "loss  1608: 1.8014   grad norm: 2.2454          model param norm: 87.8295        \n",
      "\n",
      "quiet_star_policy_loss= -0.026720857247710228\n",
      "nll_loss= 1.8043415546417236\n",
      "avg_std= 0.21960993111133575\n",
      "dist std min max: 0.010799243114888668 0.21960993111133575 2.953705310821533\n",
      "hidden_states min max: -10.34765625 10.689393043518066\n",
      "hidden_state minus mean squared max: 68.84172821044922\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.63089752197266 3.6087470054626465\n",
      "loss  1609: 1.7776   grad norm: 2.0511          model param norm: 87.8301        \n",
      "\n",
      "quiet_star_policy_loss= -0.017450202256441116\n",
      "nll_loss= 1.8062618970870972\n",
      "avg_std= 0.22002923488616943\n",
      "dist std min max: 0.009509398601949215 0.22002923488616943 3.0988218784332275\n",
      "hidden_states min max: -10.199762344360352 10.495447158813477\n",
      "hidden_state minus mean squared max: 77.832275390625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.35277557373047 3.7084450721740723\n",
      "loss  1610: 1.7888   grad norm: 2.0766          model param norm: 87.8310        \n",
      "\n",
      "quiet_star_policy_loss= 0.002472519874572754\n",
      "nll_loss= 1.8199104070663452\n",
      "avg_std= 0.21850238740444183\n",
      "dist std min max: 0.01074120495468378 0.21850238740444183 3.398319721221924\n",
      "hidden_states min max: -10.333142280578613 11.436044692993164\n",
      "hidden_state minus mean squared max: 71.31147003173828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.85523986816406 3.6009039878845215\n",
      "loss  1611: 1.8224   grad norm: 1.8904          model param norm: 87.8319        \n",
      "\n",
      "quiet_star_policy_loss= 0.013797259889543056\n",
      "nll_loss= 1.8131424188613892\n",
      "avg_std= 0.21706488728523254\n",
      "dist std min max: 0.010539515875279903 0.21706488728523254 2.91143536567688\n",
      "hidden_states min max: -10.412394523620605 10.607574462890625\n",
      "hidden_state minus mean squared max: 56.90461349487305\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79010772705078 3.6246542930603027\n",
      "loss  1612: 1.8269   grad norm: 2.0542          model param norm: 87.8329        \n",
      "\n",
      "quiet_star_policy_loss= -0.019880199804902077\n",
      "nll_loss= 1.80768620967865\n",
      "avg_std= 0.21776093542575836\n",
      "dist std min max: 0.010462982580065727 0.21776093542575836 2.971637487411499\n",
      "hidden_states min max: -10.241509437561035 10.770803451538086\n",
      "hidden_state minus mean squared max: 57.83999252319336\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.14390563964844 3.607537269592285\n",
      "loss  1613: 1.7878   grad norm: 2.1136          model param norm: 87.8340        \n",
      "\n",
      "quiet_star_policy_loss= 0.0008522272109985352\n",
      "nll_loss= 1.8203668594360352\n",
      "avg_std= 0.21681930124759674\n",
      "dist std min max: 0.010422291234135628 0.21681930124759674 2.918105363845825\n",
      "hidden_states min max: -10.427632331848145 11.896331787109375\n",
      "hidden_state minus mean squared max: 86.1297836303711\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.553704261779785 3.6097946166992188\n",
      "loss  1614: 1.8212   grad norm: 2.1013          model param norm: 87.8350        \n",
      "\n",
      "quiet_star_policy_loss= -0.02632017247378826\n",
      "nll_loss= 1.7900965213775635\n",
      "avg_std= 0.21957097947597504\n",
      "dist std min max: 0.010255225002765656 0.21957097947597504 3.241281270980835\n",
      "hidden_states min max: -10.220125198364258 10.763666152954102\n",
      "hidden_state minus mean squared max: 77.83692169189453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.17513275146484 3.6341004371643066\n",
      "loss  1615: 1.7638   grad norm: 1.9259          model param norm: 87.8361        \n",
      "\n",
      "quiet_star_policy_loss= 0.0018316268688067794\n",
      "nll_loss= 1.802598237991333\n",
      "avg_std= 0.2162720113992691\n",
      "dist std min max: 0.010668772272765636 0.2162720113992691 3.0838372707366943\n",
      "hidden_states min max: -10.313796997070312 11.338770866394043\n",
      "hidden_state minus mean squared max: 76.08512115478516\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.08341979980469 3.61606502532959\n",
      "loss  1616: 1.8044   grad norm: 2.0459          model param norm: 87.8373        \n",
      "\n",
      "quiet_star_policy_loss= -0.029822183772921562\n",
      "nll_loss= 1.8202155828475952\n",
      "avg_std= 0.21524055302143097\n",
      "dist std min max: 0.009649606421589851 0.21524055302143097 3.099823474884033\n",
      "hidden_states min max: -10.242063522338867 11.524574279785156\n",
      "hidden_state minus mean squared max: 78.85225677490234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.22969055175781 3.641078472137451\n",
      "loss  1617: 1.7904   grad norm: 2.1276          model param norm: 87.8384        \n",
      "\n",
      "quiet_star_policy_loss= -0.0241438876837492\n",
      "nll_loss= 1.781237244606018\n",
      "avg_std= 0.21675539016723633\n",
      "dist std min max: 0.010976013727486134 0.21675539016723633 2.8718204498291016\n",
      "hidden_states min max: -10.289602279663086 10.81458854675293\n",
      "hidden_state minus mean squared max: 71.08003997802734\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.41020965576172 3.584113121032715\n",
      "loss  1618: 1.7571   grad norm: 2.0699          model param norm: 87.8397        \n",
      "\n",
      "quiet_star_policy_loss= 0.004339945502579212\n",
      "nll_loss= 1.8128219842910767\n",
      "avg_std= 0.21598726511001587\n",
      "dist std min max: 0.010585817508399487 0.21598726511001587 2.8593554496765137\n",
      "hidden_states min max: -10.702127456665039 10.830938339233398\n",
      "hidden_state minus mean squared max: 78.60087585449219\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.53099060058594 3.5857796669006348\n",
      "loss  1619: 1.8172   grad norm: 2.1512          model param norm: 87.8406        \n",
      "\n",
      "quiet_star_policy_loss= 0.0066200257278978825\n",
      "nll_loss= 1.8142036199569702\n",
      "avg_std= 0.21551987528800964\n",
      "dist std min max: 0.010736447758972645 0.21551987528800964 3.1712591648101807\n",
      "hidden_states min max: -10.184008598327637 10.960151672363281\n",
      "hidden_state minus mean squared max: 58.567298889160156\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -102.43000793457031 3.611372947692871\n",
      "loss  1620: 1.8208   grad norm: 2.1609          model param norm: 87.8417        \n",
      "\n",
      "quiet_star_policy_loss= -0.01563042402267456\n",
      "nll_loss= 1.803253173828125\n",
      "avg_std= 0.21564650535583496\n",
      "dist std min max: 0.010058961808681488 0.21564650535583496 2.896519422531128\n",
      "hidden_states min max: -10.200212478637695 11.634298324584961\n",
      "hidden_state minus mean squared max: 77.39107513427734\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.8104248046875 3.650117874145508\n",
      "loss  1621: 1.7876   grad norm: 2.0088          model param norm: 87.8430        \n",
      "\n",
      "quiet_star_policy_loss= 0.02798280492424965\n",
      "nll_loss= 1.8151825666427612\n",
      "avg_std= 0.2146017700433731\n",
      "dist std min max: 0.011151482351124287 0.2146017700433731 2.969569683074951\n",
      "hidden_states min max: -10.404853820800781 11.91661262512207\n",
      "hidden_state minus mean squared max: 74.35192108154297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.83999633789062 3.5721964836120605\n",
      "loss  1622: 1.8432   grad norm: 1.9998          model param norm: 87.8443        \n",
      "\n",
      "quiet_star_policy_loss= 0.03704457730054855\n",
      "nll_loss= 1.7898777723312378\n",
      "avg_std= 0.21899771690368652\n",
      "dist std min max: 0.010390319861471653 0.21899771690368652 2.850954532623291\n",
      "hidden_states min max: -10.14238166809082 10.693977355957031\n",
      "hidden_state minus mean squared max: 87.73651885986328\n",
      "hidden_state minus mean divided by std max: 5.035408020019531\n",
      "log_prob min max: -103.08087921142578 3.546661853790283\n",
      "loss  1623: 1.8269   grad norm: 3.6205          model param norm: 87.8453        \n",
      "\n",
      "quiet_star_policy_loss= -0.012228203006088734\n",
      "nll_loss= 1.7899255752563477\n",
      "avg_std= 0.21692195534706116\n",
      "dist std min max: 0.010974139906466007 0.21692195534706116 3.1316375732421875\n",
      "hidden_states min max: -10.858829498291016 11.857015609741211\n",
      "hidden_state minus mean squared max: 70.99742889404297\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -102.07173919677734 3.548919200897217\n",
      "loss  1624: 1.7777   grad norm: 2.2952          model param norm: 87.8466        \n",
      "\n",
      "quiet_star_policy_loss= -0.014301324263215065\n",
      "nll_loss= 1.784605860710144\n",
      "avg_std= 0.21576468646526337\n",
      "dist std min max: 0.010656970553100109 0.21576468646526337 2.8406121730804443\n",
      "hidden_states min max: -11.657903671264648 11.877866744995117\n",
      "hidden_state minus mean squared max: 114.03153228759766\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.90081787109375 3.611361503601074\n",
      "loss  1625: 1.7703   grad norm: 2.0680          model param norm: 87.8478        \n",
      "\n",
      "quiet_star_policy_loss= 0.0007486343383789062\n",
      "nll_loss= 1.7953979969024658\n",
      "avg_std= 0.2174156904220581\n",
      "dist std min max: 0.01147065032273531 0.2174156904220581 2.716604709625244\n",
      "hidden_states min max: -11.51006031036377 11.07026481628418\n",
      "hidden_state minus mean squared max: 100.26014709472656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.83647155761719 3.527695655822754\n",
      "loss  1626: 1.7961   grad norm: 2.0794          model param norm: 87.8493        \n",
      "\n",
      "quiet_star_policy_loss= -0.0199049711227417\n",
      "nll_loss= 1.7863880395889282\n",
      "avg_std= 0.21868577599525452\n",
      "dist std min max: 0.010238328948616982 0.21868577599525452 2.804203987121582\n",
      "hidden_states min max: -10.391857147216797 11.751233100891113\n",
      "hidden_state minus mean squared max: 88.30850982666016\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.80706024169922 3.6618871688842773\n",
      "loss  1627: 1.7665   grad norm: 1.9327          model param norm: 87.8506        \n",
      "\n",
      "quiet_star_policy_loss= 0.0005253792041912675\n",
      "nll_loss= 1.8021191358566284\n",
      "avg_std= 0.21923115849494934\n",
      "dist std min max: 0.011568136513233185 0.21923115849494934 2.838343381881714\n",
      "hidden_states min max: -10.260855674743652 11.115556716918945\n",
      "hidden_state minus mean squared max: 80.27954864501953\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.40790557861328 3.5332260131835938\n",
      "loss  1628: 1.8026   grad norm: 1.9836          model param norm: 87.8517        \n",
      "\n",
      "quiet_star_policy_loss= -0.011345112696290016\n",
      "nll_loss= 1.8180255889892578\n",
      "avg_std= 0.2191823571920395\n",
      "dist std min max: 0.010958818718791008 0.2191823571920395 2.805305242538452\n",
      "hidden_states min max: -10.291203498840332 12.442912101745605\n",
      "hidden_state minus mean squared max: 91.71698760986328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.6430435180664 3.5941123962402344\n",
      "loss  1629: 1.8067   grad norm: 2.1175          model param norm: 87.8526        \n",
      "\n",
      "quiet_star_policy_loss= -0.030463457107543945\n",
      "nll_loss= 1.8233795166015625\n",
      "avg_std= 0.21946048736572266\n",
      "dist std min max: 0.01143683772534132 0.21946048736572266 2.7663583755493164\n",
      "hidden_states min max: -10.830307006835938 10.843823432922363\n",
      "hidden_state minus mean squared max: 106.3095474243164\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.8657455444336 3.5398902893066406\n",
      "loss  1630: 1.7929   grad norm: 2.0779          model param norm: 87.8534        \n",
      "\n",
      "quiet_star_policy_loss= -0.032038163393735886\n",
      "nll_loss= 1.811968207359314\n",
      "avg_std= 0.22018331289291382\n",
      "dist std min max: 0.011998133733868599 0.22018331289291382 2.7934277057647705\n",
      "hidden_states min max: -15.551655769348145 12.00439167022705\n",
      "hidden_state minus mean squared max: 210.73886108398438\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.20787811279297 3.4954113960266113\n",
      "loss  1631: 1.7799   grad norm: 2.1337          model param norm: 87.8543        \n",
      "\n",
      "quiet_star_policy_loss= -0.025571132078766823\n",
      "nll_loss= 1.8117538690567017\n",
      "avg_std= 0.22039709985256195\n",
      "dist std min max: 0.011679939925670624 0.22039709985256195 2.7803728580474854\n",
      "hidden_states min max: -11.064655303955078 11.214393615722656\n",
      "hidden_state minus mean squared max: 90.71239471435547\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.6119384765625 3.5081605911254883\n",
      "loss  1632: 1.7862   grad norm: 1.9820          model param norm: 87.8551        \n",
      "\n",
      "quiet_star_policy_loss= -0.015025279484689236\n",
      "nll_loss= 1.7918592691421509\n",
      "avg_std= 0.22042937576770782\n",
      "dist std min max: 0.010757294483482838 0.22042937576770782 2.740591526031494\n",
      "hidden_states min max: -11.078186988830566 11.06966781616211\n",
      "hidden_state minus mean squared max: 87.56170654296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.05091094970703 3.61019229888916\n",
      "loss  1633: 1.7768   grad norm: 2.0866          model param norm: 87.8557        \n",
      "\n",
      "quiet_star_policy_loss= -0.016021329909563065\n",
      "nll_loss= 1.7845004796981812\n",
      "avg_std= 0.22211864590644836\n",
      "dist std min max: 0.010626641102135181 0.22211864590644836 2.725496292114258\n",
      "hidden_states min max: -11.086882591247559 11.292169570922852\n",
      "hidden_state minus mean squared max: 84.9772720336914\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.74839782714844 3.604976177215576\n",
      "loss  1634: 1.7685   grad norm: 2.0983          model param norm: 87.8563        \n",
      "\n",
      "quiet_star_policy_loss= -0.00030750036239624023\n",
      "nll_loss= 1.8028868436813354\n",
      "avg_std= 0.2209712713956833\n",
      "dist std min max: 0.011815940029919147 0.2209712713956833 2.9221785068511963\n",
      "hidden_states min max: -10.818873405456543 10.884305953979492\n",
      "hidden_state minus mean squared max: 79.44556427001953\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.7201156616211 3.5171337127685547\n",
      "loss  1635: 1.8026   grad norm: 1.9921          model param norm: 87.8571        \n",
      "\n",
      "quiet_star_policy_loss= -0.01787724532186985\n",
      "nll_loss= 1.782954454421997\n",
      "avg_std= 0.22007320821285248\n",
      "dist std min max: 0.011161264963448048 0.22007320821285248 2.9316906929016113\n",
      "hidden_states min max: -11.64315414428711 10.977577209472656\n",
      "hidden_state minus mean squared max: 78.17118835449219\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.26097869873047 3.550696849822998\n",
      "loss  1636: 1.7651   grad norm: 1.9434          model param norm: 87.8577        \n",
      "\n",
      "quiet_star_policy_loss= -0.006660175509750843\n",
      "nll_loss= 1.807617425918579\n",
      "avg_std= 0.2198803871870041\n",
      "dist std min max: 0.012020932510495186 0.2198803871870041 2.990401268005371\n",
      "hidden_states min max: -11.203629493713379 11.117513656616211\n",
      "hidden_state minus mean squared max: 100.51911163330078\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.6026611328125 3.487082004547119\n",
      "loss  1637: 1.8010   grad norm: 2.1053          model param norm: 87.8588        \n",
      "\n",
      "quiet_star_policy_loss= -0.019938254728913307\n",
      "nll_loss= 1.7995293140411377\n",
      "avg_std= 0.21948328614234924\n",
      "dist std min max: 0.009984681382775307 0.21948328614234924 2.9898717403411865\n",
      "hidden_states min max: -11.204079627990723 11.912422180175781\n",
      "hidden_state minus mean squared max: 97.51715850830078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.71205139160156 3.6751890182495117\n",
      "loss  1638: 1.7796   grad norm: 1.9923          model param norm: 87.8598        \n",
      "\n",
      "quiet_star_policy_loss= -0.019279098138213158\n",
      "nll_loss= 1.7945358753204346\n",
      "avg_std= 0.21912652254104614\n",
      "dist std min max: 0.010693233460187912 0.21912652254104614 2.782681465148926\n",
      "hidden_states min max: -11.670231819152832 12.077284812927246\n",
      "hidden_state minus mean squared max: 72.2921142578125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.85240173339844 3.590322494506836\n",
      "loss  1639: 1.7753   grad norm: 1.9544          model param norm: 87.8606        \n",
      "\n",
      "quiet_star_policy_loss= 0.006477737333625555\n",
      "nll_loss= 1.80718994140625\n",
      "avg_std= 0.21886342763900757\n",
      "dist std min max: 0.01050333771854639 0.21886342763900757 2.8285861015319824\n",
      "hidden_states min max: -12.059749603271484 11.424629211425781\n",
      "hidden_state minus mean squared max: 87.53782653808594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.46945190429688 3.6308035850524902\n",
      "loss  1640: 1.8137   grad norm: 1.9308          model param norm: 87.8614        \n",
      "\n",
      "quiet_star_policy_loss= 0.005213403608649969\n",
      "nll_loss= 1.8210700750350952\n",
      "avg_std= 0.21822813153266907\n",
      "dist std min max: 0.011074734851717949 0.21822813153266907 2.724097967147827\n",
      "hidden_states min max: -12.914621353149414 11.588106155395508\n",
      "hidden_state minus mean squared max: 92.46326446533203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.33708953857422 3.5450453758239746\n",
      "loss  1641: 1.8263   grad norm: 1.9603          model param norm: 87.8625        \n",
      "\n",
      "quiet_star_policy_loss= -0.027763819321990013\n",
      "nll_loss= 1.7815617322921753\n",
      "avg_std= 0.22095191478729248\n",
      "dist std min max: 0.011106877587735653 0.22095191478729248 2.9045166969299316\n",
      "hidden_states min max: -10.873350143432617 11.624971389770508\n",
      "hidden_state minus mean squared max: 80.94943237304688\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.26850128173828 3.578610897064209\n",
      "loss  1642: 1.7538   grad norm: 2.1857          model param norm: 87.8638        \n",
      "\n",
      "quiet_star_policy_loss= -0.011861324310302734\n",
      "nll_loss= 1.7998046875\n",
      "avg_std= 0.21871380507946014\n",
      "dist std min max: 0.010317886248230934 0.21871380507946014 2.6755340099334717\n",
      "hidden_states min max: -13.810373306274414 11.017622947692871\n",
      "hidden_state minus mean squared max: 183.3574981689453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.13829803466797 3.6487693786621094\n",
      "loss  1643: 1.7879   grad norm: 2.0706          model param norm: 87.8651        \n",
      "\n",
      "quiet_star_policy_loss= -0.01756279543042183\n",
      "nll_loss= 1.82199227809906\n",
      "avg_std= 0.21696598827838898\n",
      "dist std min max: 0.010424239560961723 0.21696598827838898 2.6763737201690674\n",
      "hidden_states min max: -11.864933013916016 10.812167167663574\n",
      "hidden_state minus mean squared max: 82.62242126464844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.83824157714844 3.601822853088379\n",
      "loss  1644: 1.8044   grad norm: 2.0491          model param norm: 87.8667        \n",
      "\n",
      "quiet_star_policy_loss= -0.03208322450518608\n",
      "nll_loss= 1.8001203536987305\n",
      "avg_std= 0.21799957752227783\n",
      "dist std min max: 0.00992222223430872 0.21799957752227783 2.692002534866333\n",
      "hidden_states min max: -12.105754852294922 11.468170166015625\n",
      "hidden_state minus mean squared max: 91.8271713256836\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.19294738769531 3.665886878967285\n",
      "loss  1645: 1.7680   grad norm: 2.1553          model param norm: 87.8683        \n",
      "\n",
      "quiet_star_policy_loss= -0.028199220076203346\n",
      "nll_loss= 1.8239929676055908\n",
      "avg_std= 0.21703766286373138\n",
      "dist std min max: 0.01017413754016161 0.21703766286373138 2.8567495346069336\n",
      "hidden_states min max: -13.094757080078125 10.843605041503906\n",
      "hidden_state minus mean squared max: 114.55978393554688\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.28362274169922 3.6538000106811523\n",
      "loss  1646: 1.7958   grad norm: 2.0292          model param norm: 87.8701        \n",
      "\n",
      "quiet_star_policy_loss= 0.011887860484421253\n",
      "nll_loss= 1.800352931022644\n",
      "avg_std= 0.2181754857301712\n",
      "dist std min max: 0.010572938248515129 0.2181754857301712 3.2577226161956787\n",
      "hidden_states min max: -11.497962951660156 11.992175102233887\n",
      "hidden_state minus mean squared max: 84.48938751220703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.87109375 3.605254650115967\n",
      "loss  1647: 1.8122   grad norm: 1.9534          model param norm: 87.8717        \n",
      "\n",
      "quiet_star_policy_loss= -0.012832343578338623\n",
      "nll_loss= 1.792066216468811\n",
      "avg_std= 0.21910391747951508\n",
      "dist std min max: 0.009669403545558453 0.21910391747951508 2.6623997688293457\n",
      "hidden_states min max: -11.777453422546387 10.825547218322754\n",
      "hidden_state minus mean squared max: 106.34416198730469\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.43036651611328 3.7039794921875\n",
      "loss  1648: 1.7792   grad norm: 1.9842          model param norm: 87.8734        \n",
      "\n",
      "quiet_star_policy_loss= 0.022568583488464355\n",
      "nll_loss= 1.7807601690292358\n",
      "avg_std= 0.2170623242855072\n",
      "dist std min max: 0.010743909515440464 0.2170623242855072 2.7692179679870605\n",
      "hidden_states min max: -11.949686050415039 11.23277759552002\n",
      "hidden_state minus mean squared max: 108.18959045410156\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.75981140136719 3.6010231971740723\n",
      "loss  1649: 1.8033   grad norm: 1.9884          model param norm: 87.8750        \n",
      "\n",
      "quiet_star_policy_loss= -0.012113547883927822\n",
      "nll_loss= 1.7809536457061768\n",
      "avg_std= 0.21783602237701416\n",
      "dist std min max: 0.009823833592236042 0.21783602237701416 3.060811758041382\n",
      "hidden_states min max: -12.47684383392334 10.813508033752441\n",
      "hidden_state minus mean squared max: 89.56608581542969\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -101.97815704345703 3.6611294746398926\n",
      "loss  1650: 1.7688   grad norm: 2.1783          model param norm: 87.8765        \n",
      "\n",
      "quiet_star_policy_loss= 0.006497917231172323\n",
      "nll_loss= 1.7569700479507446\n",
      "avg_std= 0.2176245003938675\n",
      "dist std min max: 0.010207360610365868 0.2176245003938675 2.5792808532714844\n",
      "hidden_states min max: -10.57846736907959 10.787248611450195\n",
      "hidden_state minus mean squared max: 74.80964660644531\n",
      "hidden_state minus mean divided by std max: 5.035406589508057\n",
      "log_prob min max: -103.47562408447266 3.649317741394043\n",
      "loss  1651: 1.7635   grad norm: 3.9851          model param norm: 87.8783        \n",
      "\n",
      "quiet_star_policy_loss= -0.002535319421440363\n",
      "nll_loss= 1.7829818725585938\n",
      "avg_std= 0.2163371443748474\n",
      "dist std min max: 0.009721849113702774 0.2163371443748474 2.8683080673217773\n",
      "hidden_states min max: -13.092684745788574 11.242110252380371\n",
      "hidden_state minus mean squared max: 98.0458755493164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.1895980834961 3.6699886322021484\n",
      "loss  1652: 1.7804   grad norm: 1.9905          model param norm: 87.8797        \n",
      "\n",
      "quiet_star_policy_loss= -0.0006949067465029657\n",
      "nll_loss= 1.7985622882843018\n",
      "avg_std= 0.21497607231140137\n",
      "dist std min max: 0.010350552387535572 0.21497607231140137 2.806898832321167\n",
      "hidden_states min max: -11.865683555603027 10.841402053833008\n",
      "hidden_state minus mean squared max: 82.93798065185547\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.46975708007812 3.6303892135620117\n",
      "loss  1653: 1.7979   grad norm: 2.0335          model param norm: 87.8814        \n",
      "\n",
      "quiet_star_policy_loss= -0.00973743200302124\n",
      "nll_loss= 1.8069336414337158\n",
      "avg_std= 0.21507610380649567\n",
      "dist std min max: 0.00970986858010292 0.21507610380649567 2.725371837615967\n",
      "hidden_states min max: -11.424837112426758 10.850412368774414\n",
      "hidden_state minus mean squared max: 79.50475311279297\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.93374633789062 3.69747257232666\n",
      "loss  1654: 1.7972   grad norm: 2.1345          model param norm: 87.8831        \n",
      "\n",
      "quiet_star_policy_loss= -0.02504410222172737\n",
      "nll_loss= 1.7845085859298706\n",
      "avg_std= 0.21677792072296143\n",
      "dist std min max: 0.009895993396639824 0.21677792072296143 2.7066378593444824\n",
      "hidden_states min max: -12.570377349853516 10.827570915222168\n",
      "hidden_state minus mean squared max: 101.29109191894531\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.71465301513672 3.6872000694274902\n",
      "loss  1655: 1.7595   grad norm: 2.0267          model param norm: 87.8844        \n",
      "\n",
      "quiet_star_policy_loss= -0.03146710619330406\n",
      "nll_loss= 1.79193913936615\n",
      "avg_std= 0.21678364276885986\n",
      "dist std min max: 0.010238753631711006 0.21678364276885986 2.845700263977051\n",
      "hidden_states min max: -12.388373374938965 11.391925811767578\n",
      "hidden_state minus mean squared max: 97.41867065429688\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.8077163696289 3.613900661468506\n",
      "loss  1656: 1.7605   grad norm: 2.0575          model param norm: 87.8856        \n",
      "\n",
      "quiet_star_policy_loss= -0.008656871505081654\n",
      "nll_loss= 1.7789074182510376\n",
      "avg_std= 0.21773728728294373\n",
      "dist std min max: 0.009683642536401749 0.21773728728294373 2.87507963180542\n",
      "hidden_states min max: -10.941747665405273 10.804697036743164\n",
      "hidden_state minus mean squared max: 90.47570037841797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.62445831298828 3.7025809288024902\n",
      "loss  1657: 1.7703   grad norm: 2.1460          model param norm: 87.8870        \n",
      "\n",
      "quiet_star_policy_loss= 0.01175005454570055\n",
      "nll_loss= 1.7765220403671265\n",
      "avg_std= 0.21698148548603058\n",
      "dist std min max: 0.010094369761645794 0.21698148548603058 2.7071163654327393\n",
      "hidden_states min max: -12.606128692626953 11.445589065551758\n",
      "hidden_state minus mean squared max: 97.13304901123047\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.2315673828125 3.6688337326049805\n",
      "loss  1658: 1.7883   grad norm: 2.1058          model param norm: 87.8885        \n",
      "\n",
      "quiet_star_policy_loss= -0.01987638510763645\n",
      "nll_loss= 1.8156635761260986\n",
      "avg_std= 0.21315787732601166\n",
      "dist std min max: 0.009813809767365456 0.21315787732601166 2.802098512649536\n",
      "hidden_states min max: -11.794187545776367 10.707463264465332\n",
      "hidden_state minus mean squared max: 86.10448455810547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.79460144042969 3.6817426681518555\n",
      "loss  1659: 1.7958   grad norm: 1.8856          model param norm: 87.8901        \n",
      "\n",
      "quiet_star_policy_loss= -0.004528713412582874\n",
      "nll_loss= 1.7914146184921265\n",
      "avg_std= 0.21579024195671082\n",
      "dist std min max: 0.010014725849032402 0.21579024195671082 2.9110300540924072\n",
      "hidden_states min max: -13.16343879699707 12.237442016601562\n",
      "hidden_state minus mean squared max: 107.98233032226562\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.25907135009766 3.6406779289245605\n",
      "loss  1660: 1.7869   grad norm: 2.3050          model param norm: 87.8917        \n",
      "\n",
      "quiet_star_policy_loss= -0.01819329336285591\n",
      "nll_loss= 1.7853935956954956\n",
      "avg_std= 0.21546463668346405\n",
      "dist std min max: 0.010575853288173676 0.21546463668346405 3.1386542320251465\n",
      "hidden_states min max: -12.491868019104004 12.552901268005371\n",
      "hidden_state minus mean squared max: 109.72150421142578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.70840454101562 3.6075024604797363\n",
      "loss  1661: 1.7672   grad norm: 2.2037          model param norm: 87.8934        \n",
      "\n",
      "quiet_star_policy_loss= -0.009350920096039772\n",
      "nll_loss= 1.826215386390686\n",
      "avg_std= 0.21314725279808044\n",
      "dist std min max: 0.010531236417591572 0.21314725279808044 2.917722463607788\n",
      "hidden_states min max: -12.242363929748535 12.146077156066895\n",
      "hidden_state minus mean squared max: 85.73489379882812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.43746185302734 3.6305294036865234\n",
      "loss  1662: 1.8169   grad norm: 2.3522          model param norm: 87.8950        \n",
      "\n",
      "quiet_star_policy_loss= -0.007416200824081898\n",
      "nll_loss= 1.797951102256775\n",
      "avg_std= 0.214633047580719\n",
      "dist std min max: 0.010713710449635983 0.214633047580719 3.083322048187256\n",
      "hidden_states min max: -13.378628730773926 11.767814636230469\n",
      "hidden_state minus mean squared max: 119.4952621459961\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.82464599609375 3.615248680114746\n",
      "loss  1663: 1.7905   grad norm: 2.0469          model param norm: 87.8965        \n",
      "\n",
      "quiet_star_policy_loss= -0.008321869187057018\n",
      "nll_loss= 1.7966594696044922\n",
      "avg_std= 0.21209467947483063\n",
      "dist std min max: 0.010303367860615253 0.21209467947483063 3.311732292175293\n",
      "hidden_states min max: -12.314058303833008 11.927940368652344\n",
      "hidden_state minus mean squared max: 89.96344757080078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.0558853149414 3.6483850479125977\n",
      "loss  1664: 1.7883   grad norm: 2.0736          model param norm: 87.8980        \n",
      "\n",
      "quiet_star_policy_loss= -0.010339927859604359\n",
      "nll_loss= 1.813690185546875\n",
      "avg_std= 0.2125302255153656\n",
      "dist std min max: 0.010471138171851635 0.2125302255153656 2.7825751304626465\n",
      "hidden_states min max: -12.496343612670898 11.577144622802734\n",
      "hidden_state minus mean squared max: 93.54387664794922\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -101.77294158935547 3.6231937408447266\n",
      "loss  1665: 1.8034   grad norm: 2.1065          model param norm: 87.8993        \n",
      "\n",
      "quiet_star_policy_loss= -0.011245310306549072\n",
      "nll_loss= 1.7992820739746094\n",
      "avg_std= 0.21362760663032532\n",
      "dist std min max: 0.010677831247448921 0.21362760663032532 2.828566789627075\n",
      "hidden_states min max: -10.855204582214355 12.48613452911377\n",
      "hidden_state minus mean squared max: 101.84800720214844\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.58833312988281 3.616352081298828\n",
      "loss  1666: 1.7880   grad norm: 2.1648          model param norm: 87.9006        \n",
      "\n",
      "quiet_star_policy_loss= 0.016179252415895462\n",
      "nll_loss= 1.8067439794540405\n",
      "avg_std= 0.21302074193954468\n",
      "dist std min max: 0.010707765817642212 0.21302074193954468 3.361875295639038\n",
      "hidden_states min max: -11.394219398498535 11.344270706176758\n",
      "hidden_state minus mean squared max: 108.24786376953125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77034759521484 3.605856418609619\n",
      "loss  1667: 1.8229   grad norm: 2.1631          model param norm: 87.9018        \n",
      "\n",
      "quiet_star_policy_loss= -0.03755021095275879\n",
      "nll_loss= 1.8234935998916626\n",
      "avg_std= 0.21260249614715576\n",
      "dist std min max: 0.010782787576317787 0.21260249614715576 2.7052528858184814\n",
      "hidden_states min max: -11.685131072998047 12.284051895141602\n",
      "hidden_state minus mean squared max: 81.6419448852539\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.75696563720703 3.6055588722229004\n",
      "loss  1668: 1.7859   grad norm: 2.2257          model param norm: 87.9032        \n",
      "\n",
      "quiet_star_policy_loss= -0.0093392850831151\n",
      "nll_loss= 1.7768665552139282\n",
      "avg_std= 0.214416041970253\n",
      "dist std min max: 0.010926133953034878 0.214416041970253 3.0376029014587402\n",
      "hidden_states min max: -11.934846878051758 11.648801803588867\n",
      "hidden_state minus mean squared max: 82.90618133544922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5361557006836 3.597126007080078\n",
      "loss  1669: 1.7675   grad norm: 2.0828          model param norm: 87.9044        \n",
      "\n",
      "quiet_star_policy_loss= -0.012868881225585938\n",
      "nll_loss= 1.7940685749053955\n",
      "avg_std= 0.21318145096302032\n",
      "dist std min max: 0.010855481959879398 0.21318145096302032 2.8491551876068115\n",
      "hidden_states min max: -11.151232719421387 11.811297416687012\n",
      "hidden_state minus mean squared max: 97.28783416748047\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.85708618164062 3.5869879722595215\n",
      "loss  1670: 1.7812   grad norm: 1.9763          model param norm: 87.9056        \n",
      "\n",
      "quiet_star_policy_loss= -0.006846261210739613\n",
      "nll_loss= 1.7891892194747925\n",
      "avg_std= 0.21286910772323608\n",
      "dist std min max: 0.01096983440220356 0.21286910772323608 2.868189811706543\n",
      "hidden_states min max: -12.519746780395508 11.26622200012207\n",
      "hidden_state minus mean squared max: 92.62825775146484\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.83915710449219 3.5630784034729004\n",
      "loss  1671: 1.7823   grad norm: 2.0406          model param norm: 87.9068        \n",
      "\n",
      "quiet_star_policy_loss= -0.012365126982331276\n",
      "nll_loss= 1.795871615409851\n",
      "avg_std= 0.21306923031806946\n",
      "dist std min max: 0.011203693225979805 0.21306923031806946 2.6797218322753906\n",
      "hidden_states min max: -11.207137107849121 10.833876609802246\n",
      "hidden_state minus mean squared max: 79.87853240966797\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.71969604492188 3.5567078590393066\n",
      "loss  1672: 1.7835   grad norm: 1.9547          model param norm: 87.9082        \n",
      "\n",
      "quiet_star_policy_loss= -0.027213597670197487\n",
      "nll_loss= 1.8077768087387085\n",
      "avg_std= 0.21313805878162384\n",
      "dist std min max: 0.011348824948072433 0.21313805878162384 2.7831485271453857\n",
      "hidden_states min max: -11.339753150939941 11.589122772216797\n",
      "hidden_state minus mean squared max: 80.45926666259766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.52954864501953 3.5516624450683594\n",
      "loss  1673: 1.7806   grad norm: 1.9883          model param norm: 87.9097        \n",
      "\n",
      "quiet_star_policy_loss= -0.011725581251084805\n",
      "nll_loss= 1.7867506742477417\n",
      "avg_std= 0.21264620125293732\n",
      "dist std min max: 0.011381889693439007 0.21264620125293732 2.815232038497925\n",
      "hidden_states min max: -12.744182586669922 11.158330917358398\n",
      "hidden_state minus mean squared max: 96.94535064697266\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.25704956054688 3.5510172843933105\n",
      "loss  1674: 1.7750   grad norm: 2.0490          model param norm: 87.9110        \n",
      "\n",
      "quiet_star_policy_loss= -0.03288856893777847\n",
      "nll_loss= 1.7888908386230469\n",
      "avg_std= 0.2126581072807312\n",
      "dist std min max: 0.011135880835354328 0.2126581072807312 2.6704108715057373\n",
      "hidden_states min max: -10.84496021270752 11.747831344604492\n",
      "hidden_state minus mean squared max: 83.37296295166016\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.21826934814453 3.546205520629883\n",
      "loss  1675: 1.7560   grad norm: 2.0121          model param norm: 87.9123        \n",
      "\n",
      "quiet_star_policy_loss= -0.0069248913787305355\n",
      "nll_loss= 1.7934626340866089\n",
      "avg_std= 0.21179717779159546\n",
      "dist std min max: 0.011361024342477322 0.21179717779159546 2.6776883602142334\n",
      "hidden_states min max: -11.246834754943848 13.382813453674316\n",
      "hidden_state minus mean squared max: 109.17082977294922\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.67379760742188 3.5436148643493652\n",
      "loss  1676: 1.7865   grad norm: 1.9687          model param norm: 87.9138        \n",
      "\n",
      "quiet_star_policy_loss= -0.016415072605013847\n",
      "nll_loss= 1.7825554609298706\n",
      "avg_std= 0.21145948767662048\n",
      "dist std min max: 0.011250601150095463 0.21145948767662048 2.9921305179595947\n",
      "hidden_states min max: -12.279781341552734 12.327247619628906\n",
      "hidden_state minus mean squared max: 114.80821990966797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.07453155517578 3.5519251823425293\n",
      "loss  1677: 1.7661   grad norm: 2.0656          model param norm: 87.9154        \n",
      "\n",
      "quiet_star_policy_loss= -0.0055325510911643505\n",
      "nll_loss= 1.7879688739776611\n",
      "avg_std= 0.21125148236751556\n",
      "dist std min max: 0.011070183478295803 0.21125148236751556 2.7427961826324463\n",
      "hidden_states min max: -11.51389217376709 12.043033599853516\n",
      "hidden_state minus mean squared max: 83.10490417480469\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.19547271728516 3.5678696632385254\n",
      "loss  1678: 1.7824   grad norm: 2.1819          model param norm: 87.9171        \n",
      "\n",
      "quiet_star_policy_loss= -0.03309839963912964\n",
      "nll_loss= 1.7957987785339355\n",
      "avg_std= 0.20850646495819092\n",
      "dist std min max: 0.0117263188585639 0.20850646495819092 2.722320556640625\n",
      "hidden_states min max: -9.894783973693848 11.157912254333496\n",
      "hidden_state minus mean squared max: 93.73688507080078\n",
      "hidden_state minus mean divided by std max: 4.9571757316589355\n",
      "log_prob min max: -12.625641822814941 3.508063316345215\n",
      "loss  1679: 1.7627   grad norm: 3.9502          model param norm: 87.9185        \n",
      "\n",
      "quiet_star_policy_loss= -0.038409650325775146\n",
      "nll_loss= 1.7704756259918213\n",
      "avg_std= 0.2115357667207718\n",
      "dist std min max: 0.011404365301132202 0.2115357667207718 2.6799793243408203\n",
      "hidden_states min max: -11.159233093261719 12.156478881835938\n",
      "hidden_state minus mean squared max: 73.91970825195312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.10440063476562 3.5441527366638184\n",
      "loss  1680: 1.7321   grad norm: 2.1231          model param norm: 87.9196        \n",
      "\n",
      "quiet_star_policy_loss= -0.003328990889713168\n",
      "nll_loss= 1.7941311597824097\n",
      "avg_std= 0.21034805476665497\n",
      "dist std min max: 0.011094989255070686 0.21034805476665497 2.616586923599243\n",
      "hidden_states min max: -11.530013084411621 12.1333589553833\n",
      "hidden_state minus mean squared max: 71.43692779541016\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.09891510009766 3.5734128952026367\n",
      "loss  1681: 1.7908   grad norm: 1.9841          model param norm: 87.9206        \n",
      "\n",
      "quiet_star_policy_loss= -0.004887199494987726\n",
      "nll_loss= 1.7855262756347656\n",
      "avg_std= 0.21033890545368195\n",
      "dist std min max: 0.010721770115196705 0.21033890545368195 2.864166259765625\n",
      "hidden_states min max: -19.58975601196289 11.465065002441406\n",
      "hidden_state minus mean squared max: 336.5264892578125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.4419174194336 3.568089485168457\n",
      "loss  1682: 1.7806   grad norm: 1.9712          model param norm: 87.9219        \n",
      "\n",
      "quiet_star_policy_loss= 0.00013573169417213649\n",
      "nll_loss= 1.7643826007843018\n",
      "avg_std= 0.21046018600463867\n",
      "dist std min max: 0.011083763092756271 0.21046018600463867 2.617291212081909\n",
      "hidden_states min max: -11.063596725463867 11.095314979553223\n",
      "hidden_state minus mean squared max: 71.5248794555664\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.9840087890625 3.562551975250244\n",
      "loss  1683: 1.7645   grad norm: 2.2004          model param norm: 87.9232        \n",
      "\n",
      "quiet_star_policy_loss= -0.031591035425662994\n",
      "nll_loss= 1.780848741531372\n",
      "avg_std= 0.20883838832378387\n",
      "dist std min max: 0.011183186434209347 0.20883838832378387 2.702501058578491\n",
      "hidden_states min max: -11.48327922821045 11.966575622558594\n",
      "hidden_state minus mean squared max: 102.2303237915039\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.89696502685547 3.552738666534424\n",
      "loss  1684: 1.7493   grad norm: 1.9095          model param norm: 87.9245        \n",
      "\n",
      "quiet_star_policy_loss= -0.0031970024574548006\n",
      "nll_loss= 1.8127275705337524\n",
      "avg_std= 0.20733769237995148\n",
      "dist std min max: 0.01118476502597332 0.20733769237995148 2.6585872173309326\n",
      "hidden_states min max: -10.418105125427246 11.88743782043457\n",
      "hidden_state minus mean squared max: 94.30740356445312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.06523895263672 3.5660367012023926\n",
      "loss  1685: 1.8095   grad norm: 1.9641          model param norm: 87.9254        \n",
      "\n",
      "quiet_star_policy_loss= -0.012666106224060059\n",
      "nll_loss= 1.8138188123703003\n",
      "avg_std= 0.20704875886440277\n",
      "dist std min max: 0.011023198254406452 0.20704875886440277 2.5574443340301514\n",
      "hidden_states min max: -11.109911918640137 12.199272155761719\n",
      "hidden_state minus mean squared max: 73.05024719238281\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.49687194824219 3.581055164337158\n",
      "loss  1686: 1.8012   grad norm: 2.1497          model param norm: 87.9264        \n",
      "\n",
      "quiet_star_policy_loss= -0.009947902522981167\n",
      "nll_loss= 1.7910804748535156\n",
      "avg_std= 0.2076444923877716\n",
      "dist std min max: 0.010802270844578743 0.2076444923877716 2.923020362854004\n",
      "hidden_states min max: -27.182226181030273 11.023816108703613\n",
      "hidden_state minus mean squared max: 909.8839111328125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.9392318725586 3.5846939086914062\n",
      "loss  1687: 1.7811   grad norm: 2.0217          model param norm: 87.9273        \n",
      "\n",
      "quiet_star_policy_loss= -0.02592930756509304\n",
      "nll_loss= 1.7993004322052002\n",
      "avg_std= 0.20690485835075378\n",
      "dist std min max: 0.011280866339802742 0.20690485835075378 3.153130531311035\n",
      "hidden_states min max: -11.186202049255371 12.033941268920898\n",
      "hidden_state minus mean squared max: 72.40660095214844\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.51677703857422 3.5456018447875977\n",
      "loss  1688: 1.7734   grad norm: 2.1054          model param norm: 87.9285        \n",
      "\n",
      "quiet_star_policy_loss= -0.002062511397525668\n",
      "nll_loss= 1.785552978515625\n",
      "avg_std= 0.2076084315776825\n",
      "dist std min max: 0.010831453837454319 0.2076084315776825 2.735323667526245\n",
      "hidden_states min max: -29.965341567993164 10.967591285705566\n",
      "hidden_state minus mean squared max: 735.3572998046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.832763671875 3.596522808074951\n",
      "loss  1689: 1.7835   grad norm: 2.1955          model param norm: 87.9293        \n",
      "\n",
      "quiet_star_policy_loss= -0.008816778659820557\n",
      "nll_loss= 1.8032639026641846\n",
      "avg_std= 0.20596621930599213\n",
      "dist std min max: 0.010748802684247494 0.20596621930599213 2.5767734050750732\n",
      "hidden_states min max: -10.884733200073242 11.13190746307373\n",
      "hidden_state minus mean squared max: 78.01058959960938\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.48696899414062 3.5959601402282715\n",
      "loss  1690: 1.7944   grad norm: 2.0970          model param norm: 87.9307        \n",
      "\n",
      "quiet_star_policy_loss= -0.010829783044755459\n",
      "nll_loss= 1.7917717695236206\n",
      "avg_std= 0.20678533613681793\n",
      "dist std min max: 0.01080289389938116 0.20678533613681793 2.917617082595825\n",
      "hidden_states min max: -10.721633911132812 12.30917739868164\n",
      "hidden_state minus mean squared max: 86.65531158447266\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.3534927368164 3.593641757965088\n",
      "loss  1691: 1.7809   grad norm: 1.9793          model param norm: 87.9321        \n",
      "\n",
      "quiet_star_policy_loss= -0.0167723186314106\n",
      "nll_loss= 1.7994438409805298\n",
      "avg_std= 0.20580293238162994\n",
      "dist std min max: 0.01100309006869793 0.20580293238162994 2.5634868144989014\n",
      "hidden_states min max: -10.72149658203125 11.372505187988281\n",
      "hidden_state minus mean squared max: 78.3744125366211\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0770492553711 3.5829644203186035\n",
      "loss  1692: 1.7827   grad norm: 2.0090          model param norm: 87.9335        \n",
      "\n",
      "quiet_star_policy_loss= -0.01607685163617134\n",
      "nll_loss= 1.8087631464004517\n",
      "avg_std= 0.2066725641489029\n",
      "dist std min max: 0.010744406841695309 0.2066725641489029 2.5365028381347656\n",
      "hidden_states min max: -11.036293029785156 11.75123405456543\n",
      "hidden_state minus mean squared max: 71.32795715332031\n",
      "hidden_state minus mean divided by std max: 5.035407066345215\n",
      "log_prob min max: -103.24822235107422 3.6056809425354004\n",
      "loss  1693: 1.7927   grad norm: 1.9485          model param norm: 87.9351        \n",
      "\n",
      "quiet_star_policy_loss= -0.0033731579314917326\n",
      "nll_loss= 1.7898186445236206\n",
      "avg_std= 0.20701196789741516\n",
      "dist std min max: 0.010826528072357178 0.20701196789741516 2.63908314704895\n",
      "hidden_states min max: -11.46568489074707 11.592740058898926\n",
      "hidden_state minus mean squared max: 92.75962829589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.59923553466797 3.606675148010254\n",
      "loss  1694: 1.7864   grad norm: 1.8682          model param norm: 87.9364        \n",
      "\n",
      "quiet_star_policy_loss= -0.003500762628391385\n",
      "nll_loss= 1.7681998014450073\n",
      "avg_std= 0.2086924910545349\n",
      "dist std min max: 0.010466220788657665 0.2086924910545349 2.5961365699768066\n",
      "hidden_states min max: -11.454700469970703 11.296674728393555\n",
      "hidden_state minus mean squared max: 80.037109375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.494012832641602 3.635444164276123\n",
      "loss  1695: 1.7647   grad norm: 2.0939          model param norm: 87.9379        \n",
      "\n",
      "quiet_star_policy_loss= 0.00524857034906745\n",
      "nll_loss= 1.7865171432495117\n",
      "avg_std= 0.20635393261909485\n",
      "dist std min max: 0.010789720341563225 0.20635393261909485 2.8160102367401123\n",
      "hidden_states min max: -11.947322845458984 11.482698440551758\n",
      "hidden_state minus mean squared max: 79.26383209228516\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.71763610839844 3.5983457565307617\n",
      "loss  1696: 1.7918   grad norm: 2.1385          model param norm: 87.9395        \n",
      "\n",
      "quiet_star_policy_loss= -0.002573812147602439\n",
      "nll_loss= 1.7998218536376953\n",
      "avg_std= 0.20495030283927917\n",
      "dist std min max: 0.009893784299492836 0.20495030283927917 2.5062241554260254\n",
      "hidden_states min max: -10.99968433380127 11.359701156616211\n",
      "hidden_state minus mean squared max: 63.867469787597656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.63697052001953 3.6283950805664062\n",
      "loss  1697: 1.7972   grad norm: 2.0669          model param norm: 87.9411        \n",
      "\n",
      "quiet_star_policy_loss= 0.0028246999718248844\n",
      "nll_loss= 1.7850826978683472\n",
      "avg_std= 0.20633521676063538\n",
      "dist std min max: 0.009637241251766682 0.20633521676063538 2.6008214950561523\n",
      "hidden_states min max: -10.445568084716797 11.427082061767578\n",
      "hidden_state minus mean squared max: 72.03178405761719\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.44805145263672 3.700899124145508\n",
      "loss  1698: 1.7879   grad norm: 2.0559          model param norm: 87.9427        \n",
      "\n",
      "quiet_star_policy_loss= -0.025145744904875755\n",
      "nll_loss= 1.7844562530517578\n",
      "avg_std= 0.20425006747245789\n",
      "dist std min max: 0.009674148634076118 0.20425006747245789 2.6959729194641113\n",
      "hidden_states min max: -11.926212310791016 11.956392288208008\n",
      "hidden_state minus mean squared max: 113.94353485107422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.90043640136719 3.7001705169677734\n",
      "loss  1699: 1.7593   grad norm: 2.1235          model param norm: 87.9445        \n",
      "eval loss 1.7992523908615112\n",
      "\n",
      "quiet_star_policy_loss= -0.012265252880752087\n",
      "nll_loss= 1.795019507408142\n",
      "avg_std= 0.2052568644285202\n",
      "dist std min max: 0.00942286103963852 0.2052568644285202 3.0456736087799072\n",
      "hidden_states min max: -10.331289291381836 11.535526275634766\n",
      "hidden_state minus mean squared max: 73.11856842041016\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.35763549804688 3.736908435821533\n",
      "loss  1700: 1.7828   grad norm: 2.0967          model param norm: 87.9463        \n",
      "\n",
      "quiet_star_policy_loss= -0.028711188584566116\n",
      "nll_loss= 1.7921806573867798\n",
      "avg_std= 0.20849289000034332\n",
      "dist std min max: 0.009798729792237282 0.20849289000034332 2.682468891143799\n",
      "hidden_states min max: -11.204504013061523 11.011011123657227\n",
      "hidden_state minus mean squared max: 91.96947479248047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.88414764404297 3.687565803527832\n",
      "loss  1701: 1.7635   grad norm: 2.0120          model param norm: 87.9480        \n",
      "\n",
      "quiet_star_policy_loss= -0.014860439114272594\n",
      "nll_loss= 1.7869571447372437\n",
      "avg_std= 0.2037075161933899\n",
      "dist std min max: 0.009580043144524097 0.2037075161933899 2.898719310760498\n",
      "hidden_states min max: -11.118135452270508 11.086151123046875\n",
      "hidden_state minus mean squared max: 74.11509704589844\n",
      "hidden_state minus mean divided by std max: 5.166581630706787\n",
      "log_prob min max: -103.12225341796875 3.71152925491333\n",
      "loss  1702: 1.7721   grad norm: 1.9803          model param norm: 87.9497        \n",
      "\n",
      "quiet_star_policy_loss= -0.04571840912103653\n",
      "nll_loss= 1.7969356775283813\n",
      "avg_std= 0.20336291193962097\n",
      "dist std min max: 0.009136303327977657 0.20336291193962097 2.679521083831787\n",
      "hidden_states min max: -10.340229988098145 11.542716026306152\n",
      "hidden_state minus mean squared max: 75.3481216430664\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.6936264038086 3.769998550415039\n",
      "loss  1703: 1.7512   grad norm: 1.8736          model param norm: 87.9516        \n",
      "\n",
      "quiet_star_policy_loss= -0.002606010530143976\n",
      "nll_loss= 1.7853482961654663\n",
      "avg_std= 0.2042769342660904\n",
      "dist std min max: 0.009261074475944042 0.2042769342660904 2.820779323577881\n",
      "hidden_states min max: -10.308222770690918 11.168888092041016\n",
      "hidden_state minus mean squared max: 70.62884521484375\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -102.69978332519531 3.7579870223999023\n",
      "loss  1704: 1.7827   grad norm: 2.0562          model param norm: 87.9535        \n",
      "\n",
      "quiet_star_policy_loss= -0.010176586918532848\n",
      "nll_loss= 1.8149263858795166\n",
      "avg_std= 0.20309719443321228\n",
      "dist std min max: 0.009401852265000343 0.20309719443321228 2.807560920715332\n",
      "hidden_states min max: -10.265189170837402 11.1901216506958\n",
      "hidden_state minus mean squared max: 97.83242797851562\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.82421112060547 3.7100706100463867\n",
      "loss  1705: 1.8047   grad norm: 1.9364          model param norm: 87.9554        \n",
      "\n",
      "quiet_star_policy_loss= -0.027859974652528763\n",
      "nll_loss= 1.7874354124069214\n",
      "avg_std= 0.20378287136554718\n",
      "dist std min max: 0.009090254083275795 0.20378287136554718 2.705193042755127\n",
      "hidden_states min max: -10.149557113647461 11.702313423156738\n",
      "hidden_state minus mean squared max: 68.4721908569336\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.86115264892578 3.7722859382629395\n",
      "loss  1706: 1.7596   grad norm: 2.1154          model param norm: 87.9572        \n",
      "\n",
      "quiet_star_policy_loss= 0.016555583104491234\n",
      "nll_loss= 1.7899430990219116\n",
      "avg_std= 0.2027171552181244\n",
      "dist std min max: 0.010305353440344334 0.2027171552181244 2.3793599605560303\n",
      "hidden_states min max: -10.27984619140625 11.045469284057617\n",
      "hidden_state minus mean squared max: 53.15389633178711\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -12.68040657043457 3.6498684883117676\n",
      "loss  1707: 1.8065   grad norm: 4.6770          model param norm: 87.9590        \n",
      "\n",
      "quiet_star_policy_loss= -0.006171798799186945\n",
      "nll_loss= 1.7845802307128906\n",
      "avg_std= 0.20417127013206482\n",
      "dist std min max: 0.009409436024725437 0.20417127013206482 2.727705717086792\n",
      "hidden_states min max: -10.637554168701172 11.586660385131836\n",
      "hidden_state minus mean squared max: 76.71490478515625\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.44509887695312 3.716641902923584\n",
      "loss  1708: 1.7784   grad norm: 2.1168          model param norm: 87.9604        \n",
      "\n",
      "quiet_star_policy_loss= -0.015262520872056484\n",
      "nll_loss= 1.7967690229415894\n",
      "avg_std= 0.20259813964366913\n",
      "dist std min max: 0.009226635098457336 0.20259813964366913 2.595977544784546\n",
      "hidden_states min max: -10.543113708496094 11.549783706665039\n",
      "hidden_state minus mean squared max: 81.22721862792969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.30201721191406 3.7549896240234375\n",
      "loss  1709: 1.7815   grad norm: 2.0928          model param norm: 87.9619        \n",
      "\n",
      "quiet_star_policy_loss= -0.012263012118637562\n",
      "nll_loss= 1.7899181842803955\n",
      "avg_std= 0.20207655429840088\n",
      "dist std min max: 0.00938271451741457 0.20207655429840088 2.5886385440826416\n",
      "hidden_states min max: -10.48106861114502 11.046846389770508\n",
      "hidden_state minus mean squared max: 70.3443603515625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.5048599243164 3.709554672241211\n",
      "loss  1710: 1.7777   grad norm: 2.1743          model param norm: 87.9635        \n",
      "\n",
      "quiet_star_policy_loss= -0.03220825269818306\n",
      "nll_loss= 1.794716477394104\n",
      "avg_std= 0.20178672671318054\n",
      "dist std min max: 0.009819421917200089 0.20178672671318054 2.6571476459503174\n",
      "hidden_states min max: -10.620759963989258 11.314666748046875\n",
      "hidden_state minus mean squared max: 112.25113677978516\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.89293670654297 3.676358222961426\n",
      "loss  1711: 1.7625   grad norm: 2.0661          model param norm: 87.9653        \n",
      "\n",
      "quiet_star_policy_loss= -0.009775233455002308\n",
      "nll_loss= 1.7885922193527222\n",
      "avg_std= 0.20100028812885284\n",
      "dist std min max: 0.009312099777162075 0.20100028812885284 2.6153454780578613\n",
      "hidden_states min max: -13.694710731506348 11.077130317687988\n",
      "hidden_state minus mean squared max: 163.20465087890625\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.08008575439453 3.672732353210449\n",
      "loss  1712: 1.7788   grad norm: 2.0836          model param norm: 87.9674        \n",
      "\n",
      "quiet_star_policy_loss= -0.003438758896663785\n",
      "nll_loss= 1.80632483959198\n",
      "avg_std= 0.20243313908576965\n",
      "dist std min max: 0.009391018189489841 0.20243313908576965 2.655369520187378\n",
      "hidden_states min max: -12.303197860717773 12.236659049987793\n",
      "hidden_state minus mean squared max: 83.02212524414062\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.55341339111328 3.745121955871582\n",
      "loss  1713: 1.8029   grad norm: 2.0017          model param norm: 87.9691        \n",
      "\n",
      "quiet_star_policy_loss= -0.008987975306808949\n",
      "nll_loss= 1.810285210609436\n",
      "avg_std= 0.20271185040473938\n",
      "dist std min max: 0.009237430058419704 0.20271185040473938 2.625870704650879\n",
      "hidden_states min max: -11.231200218200684 10.989684104919434\n",
      "hidden_state minus mean squared max: 69.75662994384766\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -13.278749465942383 3.75412654876709\n",
      "loss  1714: 1.8013   grad norm: 2.0009          model param norm: 87.9707        \n",
      "\n",
      "quiet_star_policy_loss= -0.00873334426432848\n",
      "nll_loss= 1.7735893726348877\n",
      "avg_std= 0.20314720273017883\n",
      "dist std min max: 0.00943421758711338 0.20314720273017883 2.606053113937378\n",
      "hidden_states min max: -11.140336036682129 10.958086967468262\n",
      "hidden_state minus mean squared max: 73.92665100097656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.22269439697266 3.732870101928711\n",
      "loss  1715: 1.7649   grad norm: 2.0101          model param norm: 87.9719        \n",
      "\n",
      "quiet_star_policy_loss= -0.014952803030610085\n",
      "nll_loss= 1.7843812704086304\n",
      "avg_std= 0.20281386375427246\n",
      "dist std min max: 0.009335828013718128 0.20281386375427246 2.593877077102661\n",
      "hidden_states min max: -10.553156852722168 11.644835472106934\n",
      "hidden_state minus mean squared max: 78.0323257446289\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.28289031982422 3.7436728477478027\n",
      "loss  1716: 1.7694   grad norm: 2.2443          model param norm: 87.9730        \n",
      "\n",
      "quiet_star_policy_loss= -0.007514381315559149\n",
      "nll_loss= 1.7953615188598633\n",
      "avg_std= 0.20228172838687897\n",
      "dist std min max: 0.009665784426033497 0.20228172838687897 2.735806703567505\n",
      "hidden_states min max: -11.294506072998047 10.899364471435547\n",
      "hidden_state minus mean squared max: 71.03346252441406\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.30921173095703 3.7182960510253906\n",
      "loss  1717: 1.7878   grad norm: 1.9891          model param norm: 87.9740        \n",
      "\n",
      "quiet_star_policy_loss= -0.0027246475219726562\n",
      "nll_loss= 1.7936184406280518\n",
      "avg_std= 0.20150229334831238\n",
      "dist std min max: 0.010013188235461712 0.20150229334831238 2.8459603786468506\n",
      "hidden_states min max: -10.885171890258789 11.194873809814453\n",
      "hidden_state minus mean squared max: 77.57637786865234\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.14183044433594 3.67901611328125\n",
      "loss  1718: 1.7909   grad norm: 2.2462          model param norm: 87.9747        \n",
      "\n",
      "quiet_star_policy_loss= -0.03365898132324219\n",
      "nll_loss= 1.8008140325546265\n",
      "avg_std= 0.20095448195934296\n",
      "dist std min max: 0.00965446699410677 0.20095448195934296 2.7726893424987793\n",
      "hidden_states min max: -13.011306762695312 12.87631893157959\n",
      "hidden_state minus mean squared max: 205.57855224609375\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.19549560546875 3.721048355102539\n",
      "loss  1719: 1.7672   grad norm: 2.1051          model param norm: 87.9754        \n",
      "\n",
      "quiet_star_policy_loss= -0.014875352382659912\n",
      "nll_loss= 1.7691220045089722\n",
      "avg_std= 0.20201678574085236\n",
      "dist std min max: 0.009738050401210785 0.20201678574085236 2.580535888671875\n",
      "hidden_states min max: -11.634969711303711 11.210548400878906\n",
      "hidden_state minus mean squared max: 81.26897430419922\n",
      "hidden_state minus mean divided by std max: 5.166585922241211\n",
      "log_prob min max: -103.55360412597656 3.7122225761413574\n",
      "loss  1720: 1.7542   grad norm: 2.1000          model param norm: 87.9764        \n",
      "\n",
      "quiet_star_policy_loss= -0.02553739584982395\n",
      "nll_loss= 1.8012937307357788\n",
      "avg_std= 0.20197232067584991\n",
      "dist std min max: 0.009695114567875862 0.20197232067584991 2.715557336807251\n",
      "hidden_states min max: -12.240635871887207 13.285642623901367\n",
      "hidden_state minus mean squared max: 106.59275817871094\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.93415832519531 3.7153706550598145\n",
      "loss  1721: 1.7758   grad norm: 1.9970          model param norm: 87.9772        \n",
      "\n",
      "quiet_star_policy_loss= -0.010975265875458717\n",
      "nll_loss= 1.783188819885254\n",
      "avg_std= 0.20139838755130768\n",
      "dist std min max: 0.009518776088953018 0.20139838755130768 2.7393739223480225\n",
      "hidden_states min max: -11.207785606384277 11.756400108337402\n",
      "hidden_state minus mean squared max: 78.2251205444336\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.6431884765625 3.7246756553649902\n",
      "loss  1722: 1.7722   grad norm: 1.8853          model param norm: 87.9783        \n",
      "\n",
      "quiet_star_policy_loss= -0.01806914247572422\n",
      "nll_loss= 1.7817442417144775\n",
      "avg_std= 0.20032073557376862\n",
      "dist std min max: 0.009773577563464642 0.20032073557376862 2.6704728603363037\n",
      "hidden_states min max: -10.9437255859375 12.390846252441406\n",
      "hidden_state minus mean squared max: 82.18544006347656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.12310791015625 3.7039361000061035\n",
      "loss  1723: 1.7637   grad norm: 2.0054          model param norm: 87.9794        \n",
      "\n",
      "quiet_star_policy_loss= 0.009307563304901123\n",
      "nll_loss= 1.7847648859024048\n",
      "avg_std= 0.20140032470226288\n",
      "dist std min max: 0.009676364250481129 0.20140032470226288 2.785264730453491\n",
      "hidden_states min max: -10.65815544128418 10.951902389526367\n",
      "hidden_state minus mean squared max: 61.19548034667969\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.95598602294922 3.709982395172119\n",
      "loss  1724: 1.7941   grad norm: 2.0116          model param norm: 87.9808        \n",
      "\n",
      "quiet_star_policy_loss= 0.014474279247224331\n",
      "nll_loss= 1.7886673212051392\n",
      "avg_std= 0.20083749294281006\n",
      "dist std min max: 0.009705235250294209 0.20083749294281006 3.260385036468506\n",
      "hidden_states min max: -11.043432235717773 11.515522956848145\n",
      "hidden_state minus mean squared max: 82.81660461425781\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -101.53449249267578 3.6984548568725586\n",
      "loss  1725: 1.8031   grad norm: 1.9214          model param norm: 87.9822        \n",
      "\n",
      "quiet_star_policy_loss= -0.021401310339570045\n",
      "nll_loss= 1.7961753606796265\n",
      "avg_std= 0.1994151473045349\n",
      "dist std min max: 0.009653974324464798 0.1994151473045349 2.761955499649048\n",
      "hidden_states min max: -10.810003280639648 12.37298583984375\n",
      "hidden_state minus mean squared max: 78.40752410888672\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -102.97650909423828 3.7140512466430664\n",
      "loss  1726: 1.7748   grad norm: 1.9153          model param norm: 87.9834        \n",
      "\n",
      "quiet_star_policy_loss= -0.03031907044351101\n",
      "nll_loss= 1.7881771326065063\n",
      "avg_std= 0.20068837702274323\n",
      "dist std min max: 0.009636957198381424 0.20068837702274323 3.2186830043792725\n",
      "hidden_states min max: -12.930201530456543 12.020368576049805\n",
      "hidden_state minus mean squared max: 94.98692321777344\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.05608367919922 3.7061662673950195\n",
      "loss  1727: 1.7579   grad norm: 2.1629          model param norm: 87.9847        \n",
      "\n",
      "quiet_star_policy_loss= 0.010181344114243984\n",
      "nll_loss= 1.7736011743545532\n",
      "avg_std= 0.20071010291576385\n",
      "dist std min max: 0.009418795816600323 0.20071010291576385 2.9121129512786865\n",
      "hidden_states min max: -11.48445987701416 12.9170503616333\n",
      "hidden_state minus mean squared max: 100.29314422607422\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.17022705078125 3.732445240020752\n",
      "loss  1728: 1.7838   grad norm: 2.0398          model param norm: 87.9859        \n",
      "\n",
      "quiet_star_policy_loss= -0.0010091543663293123\n",
      "nll_loss= 1.7992550134658813\n",
      "avg_std= 0.19840595126152039\n",
      "dist std min max: 0.00968458503484726 0.19840595126152039 2.7168314456939697\n",
      "hidden_states min max: -12.039312362670898 11.609529495239258\n",
      "hidden_state minus mean squared max: 78.40389251708984\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.98125457763672 3.7162060737609863\n",
      "loss  1729: 1.7982   grad norm: 2.0223          model param norm: 87.9871        \n",
      "\n",
      "quiet_star_policy_loss= -0.02222271077334881\n",
      "nll_loss= 1.7695525884628296\n",
      "avg_std= 0.19993287324905396\n",
      "dist std min max: 0.009386495687067509 0.19993287324905396 2.761197566986084\n",
      "hidden_states min max: -10.468178749084473 11.440841674804688\n",
      "hidden_state minus mean squared max: 83.41320037841797\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.13094329833984 3.7465124130249023\n",
      "loss  1730: 1.7473   grad norm: 2.0716          model param norm: 87.9886        \n",
      "\n",
      "quiet_star_policy_loss= -0.007860803976655006\n",
      "nll_loss= 1.8052219152450562\n",
      "avg_std= 0.19956882297992706\n",
      "dist std min max: 0.0090695321559906 0.19956882297992706 2.9757463932037354\n",
      "hidden_states min max: -31.929040908813477 12.306633949279785\n",
      "hidden_state minus mean squared max: 833.324951171875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.895263671875 3.7741942405700684\n",
      "loss  1731: 1.7974   grad norm: 2.2185          model param norm: 87.9900        \n",
      "\n",
      "quiet_star_policy_loss= -0.006643438246101141\n",
      "nll_loss= 1.7884851694107056\n",
      "avg_std= 0.19978651404380798\n",
      "dist std min max: 0.009534653276205063 0.19978651404380798 2.8090100288391113\n",
      "hidden_states min max: -11.719527244567871 11.811394691467285\n",
      "hidden_state minus mean squared max: 74.49385070800781\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.78900909423828 3.712155818939209\n",
      "loss  1732: 1.7818   grad norm: 2.2157          model param norm: 87.9914        \n",
      "\n",
      "quiet_star_policy_loss= 0.006454735994338989\n",
      "nll_loss= 1.821559190750122\n",
      "avg_std= 0.1993761658668518\n",
      "dist std min max: 0.009524901397526264 0.1993761658668518 3.087479591369629\n",
      "hidden_states min max: -12.140826225280762 11.220877647399902\n",
      "hidden_state minus mean squared max: 99.97997283935547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.4170150756836 3.7166099548339844\n",
      "loss  1733: 1.8280   grad norm: 2.0106          model param norm: 87.9929        \n",
      "\n",
      "quiet_star_policy_loss= -0.008744621649384499\n",
      "nll_loss= 1.792507529258728\n",
      "avg_std= 0.20042048394680023\n",
      "dist std min max: 0.009566489607095718 0.20042048394680023 2.7460412979125977\n",
      "hidden_states min max: -12.349732398986816 11.766515731811523\n",
      "hidden_state minus mean squared max: 112.62554931640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.85958099365234 3.7245073318481445\n",
      "loss  1734: 1.7838   grad norm: 2.0415          model param norm: 87.9944        \n",
      "\n",
      "quiet_star_policy_loss= -0.01671142689883709\n",
      "nll_loss= 1.7493270635604858\n",
      "avg_std= 0.20132958889007568\n",
      "dist std min max: 0.009709901176393032 0.20132958889007568 2.6796176433563232\n",
      "hidden_states min max: -11.263757705688477 11.227422714233398\n",
      "hidden_state minus mean squared max: 81.56916046142578\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.88568878173828 3.702144145965576\n",
      "loss  1735: 1.7326   grad norm: 4.2318          model param norm: 87.9958        \n",
      "\n",
      "quiet_star_policy_loss= -0.02251906506717205\n",
      "nll_loss= 1.7718747854232788\n",
      "avg_std= 0.1997469812631607\n",
      "dist std min max: 0.009658219292759895 0.1997469812631607 2.696099042892456\n",
      "hidden_states min max: -12.477678298950195 11.69839859008789\n",
      "hidden_state minus mean squared max: 87.32926177978516\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.40491485595703 3.7066125869750977\n",
      "loss  1736: 1.7494   grad norm: 2.3306          model param norm: 87.9970        \n",
      "\n",
      "quiet_star_policy_loss= -0.005571603775024414\n",
      "nll_loss= 1.800167441368103\n",
      "avg_std= 0.198930025100708\n",
      "dist std min max: 0.00950139481574297 0.198930025100708 2.891387462615967\n",
      "hidden_states min max: -11.72846794128418 11.881704330444336\n",
      "hidden_state minus mean squared max: 78.3897705078125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.27064514160156 3.722285270690918\n",
      "loss  1737: 1.7946   grad norm: 2.2142          model param norm: 87.9985        \n",
      "\n",
      "quiet_star_policy_loss= -0.0199724193662405\n",
      "nll_loss= 1.7988184690475464\n",
      "avg_std= 0.20026478171348572\n",
      "dist std min max: 0.009509607218205929 0.20026478171348572 2.71722412109375\n",
      "hidden_states min max: -18.235145568847656 12.038296699523926\n",
      "hidden_state minus mean squared max: 231.25387573242188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -104.25433349609375 3.7247161865234375\n",
      "loss  1738: 1.7788   grad norm: 2.0930          model param norm: 87.9999        \n",
      "\n",
      "quiet_star_policy_loss= 0.02227157913148403\n",
      "nll_loss= 1.7915023565292358\n",
      "avg_std= 0.19894954562187195\n",
      "dist std min max: 0.009524837136268616 0.19894954562187195 2.690739154815674\n",
      "hidden_states min max: -11.119400024414062 12.610902786254883\n",
      "hidden_state minus mean squared max: 86.9804916381836\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.33995819091797 3.7319698333740234\n",
      "loss  1739: 1.8138   grad norm: 2.1788          model param norm: 88.0015        \n",
      "\n",
      "quiet_star_policy_loss= -0.02313542366027832\n",
      "nll_loss= 1.7710199356079102\n",
      "avg_std= 0.20108573138713837\n",
      "dist std min max: 0.00954520609229803 0.20108573138713837 2.715273141860962\n",
      "hidden_states min max: -11.58439826965332 12.237686157226562\n",
      "hidden_state minus mean squared max: 85.49935150146484\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.86692810058594 3.7116384506225586\n",
      "loss  1740: 1.7479   grad norm: 1.9994          model param norm: 88.0029        \n",
      "\n",
      "quiet_star_policy_loss= 0.008314657025039196\n",
      "nll_loss= 1.7780697345733643\n",
      "avg_std= 0.19968868792057037\n",
      "dist std min max: 0.009513839147984982 0.19968868792057037 2.6688852310180664\n",
      "hidden_states min max: -11.48642635345459 11.872396469116211\n",
      "hidden_state minus mean squared max: 92.22245788574219\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.49430847167969 3.7249417304992676\n",
      "loss  1741: 1.7864   grad norm: 2.1407          model param norm: 88.0041        \n",
      "\n",
      "quiet_star_policy_loss= -0.004009866621345282\n",
      "nll_loss= 1.7689030170440674\n",
      "avg_std= 0.20150607824325562\n",
      "dist std min max: 0.009121125563979149 0.20150607824325562 2.6975038051605225\n",
      "hidden_states min max: -11.580750465393066 12.383065223693848\n",
      "hidden_state minus mean squared max: 118.02913665771484\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -103.12693786621094 3.771479606628418\n",
      "loss  1742: 1.7649   grad norm: 2.0732          model param norm: 88.0051        \n",
      "\n",
      "quiet_star_policy_loss= -0.0020273923873901367\n",
      "nll_loss= 1.779893159866333\n",
      "avg_std= 0.20075713098049164\n",
      "dist std min max: 0.009628105908632278 0.20075713098049164 2.744443655014038\n",
      "hidden_states min max: -12.626220703125 11.861794471740723\n",
      "hidden_state minus mean squared max: 107.58706665039062\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.87786102294922 3.724118232727051\n",
      "loss  1743: 1.7779   grad norm: 2.4201          model param norm: 88.0062        \n",
      "\n",
      "quiet_star_policy_loss= -0.015159988775849342\n",
      "nll_loss= 1.803954005241394\n",
      "avg_std= 0.19908273220062256\n",
      "dist std min max: 0.009604871273040771 0.19908273220062256 2.704296350479126\n",
      "hidden_states min max: -11.453004837036133 11.664225578308105\n",
      "hidden_state minus mean squared max: 88.78645324707031\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.5362777709961 3.723437786102295\n",
      "loss  1744: 1.7888   grad norm: 1.9913          model param norm: 88.0072        \n",
      "\n",
      "quiet_star_policy_loss= -0.008152961730957031\n",
      "nll_loss= 1.7991619110107422\n",
      "avg_std= 0.19812482595443726\n",
      "dist std min max: 0.00940703134983778 0.19812482595443726 2.7218408584594727\n",
      "hidden_states min max: -12.415794372558594 12.253113746643066\n",
      "hidden_state minus mean squared max: 91.1725082397461\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.7889633178711 3.7340221405029297\n",
      "loss  1745: 1.7910   grad norm: 2.1223          model param norm: 88.0083        \n",
      "\n",
      "quiet_star_policy_loss= -0.0009006500476971269\n",
      "nll_loss= 1.776023507118225\n",
      "avg_std= 0.19982534646987915\n",
      "dist std min max: 0.009417098015546799 0.19982534646987915 3.1242618560791016\n",
      "hidden_states min max: -11.6682767868042 12.884532928466797\n",
      "hidden_state minus mean squared max: 119.87635040283203\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.64295196533203 3.732962131500244\n",
      "loss  1746: 1.7751   grad norm: 2.1485          model param norm: 88.0092        \n",
      "\n",
      "quiet_star_policy_loss= -0.012033653445541859\n",
      "nll_loss= 1.8043829202651978\n",
      "avg_std= 0.19883808493614197\n",
      "dist std min max: 0.009367170743644238 0.19883808493614197 2.6886942386627197\n",
      "hidden_states min max: -14.488287925720215 11.944711685180664\n",
      "hidden_state minus mean squared max: 213.5858154296875\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.21459197998047 3.7350821495056152\n",
      "loss  1747: 1.7923   grad norm: 2.0876          model param norm: 88.0101        \n",
      "\n",
      "quiet_star_policy_loss= -0.030590582638978958\n",
      "nll_loss= 1.8235901594161987\n",
      "avg_std= 0.19779284298419952\n",
      "dist std min max: 0.008946605026721954 0.19779284298419952 2.795452117919922\n",
      "hidden_states min max: -12.149155616760254 12.30994701385498\n",
      "hidden_state minus mean squared max: 89.58116149902344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.62085723876953 3.7813358306884766\n",
      "loss  1748: 1.7930   grad norm: 2.2075          model param norm: 88.0108        \n",
      "\n",
      "quiet_star_policy_loss= -0.008771277032792568\n",
      "nll_loss= 1.7839266061782837\n",
      "avg_std= 0.19749215245246887\n",
      "dist std min max: 0.00942155346274376 0.19749215245246887 2.6516833305358887\n",
      "hidden_states min max: -12.962800979614258 12.097919464111328\n",
      "hidden_state minus mean squared max: 94.97571563720703\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.68286895751953 3.721949577331543\n",
      "loss  1749: 1.7752   grad norm: 2.1453          model param norm: 88.0115        \n",
      "\n",
      "quiet_star_policy_loss= 0.009293866343796253\n",
      "nll_loss= 1.7964147329330444\n",
      "avg_std= 0.19777525961399078\n",
      "dist std min max: 0.009320580400526524 0.19777525961399078 2.943969488143921\n",
      "hidden_states min max: -12.01039981842041 13.209883689880371\n",
      "hidden_state minus mean squared max: 113.47624969482422\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.10747528076172 3.7496113777160645\n",
      "loss  1750: 1.8057   grad norm: 1.9266          model param norm: 88.0125        \n",
      "\n",
      "quiet_star_policy_loss= 0.013139056973159313\n",
      "nll_loss= 1.7768586874008179\n",
      "avg_std= 0.19903385639190674\n",
      "dist std min max: 0.00936421100050211 0.19903385639190674 2.7836968898773193\n",
      "hidden_states min max: -12.766260147094727 12.372354507446289\n",
      "hidden_state minus mean squared max: 93.79500579833984\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.75996398925781 3.724087715148926\n",
      "loss  1751: 1.7900   grad norm: 2.2058          model param norm: 88.0137        \n",
      "\n",
      "quiet_star_policy_loss= -0.004149579908698797\n",
      "nll_loss= 1.7845405340194702\n",
      "avg_std= 0.19799648225307465\n",
      "dist std min max: 0.009525132365524769 0.19799648225307465 3.0387613773345947\n",
      "hidden_states min max: -13.2122220993042 13.491800308227539\n",
      "hidden_state minus mean squared max: 102.55558776855469\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.72128295898438 3.7287344932556152\n",
      "loss  1752: 1.7804   grad norm: 2.0762          model param norm: 88.0150        \n",
      "\n",
      "quiet_star_policy_loss= -0.013995385728776455\n",
      "nll_loss= 1.7819477319717407\n",
      "avg_std= 0.19845421612262726\n",
      "dist std min max: 0.009613481350243092 0.19845421612262726 2.990103244781494\n",
      "hidden_states min max: -14.092721939086914 13.150293350219727\n",
      "hidden_state minus mean squared max: 116.63932800292969\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.36026763916016 3.7051830291748047\n",
      "loss  1753: 1.7680   grad norm: 2.1804          model param norm: 88.0164        \n",
      "\n",
      "quiet_star_policy_loss= 0.0005857229116372764\n",
      "nll_loss= 1.7960529327392578\n",
      "avg_std= 0.19826462864875793\n",
      "dist std min max: 0.009528880938887596 0.19826462864875793 2.7947657108306885\n",
      "hidden_states min max: -11.712679862976074 12.415800094604492\n",
      "hidden_state minus mean squared max: 91.65992736816406\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.66613006591797 3.714390277862549\n",
      "loss  1754: 1.7966   grad norm: 2.0797          model param norm: 88.0176        \n",
      "\n",
      "quiet_star_policy_loss= -0.001511526177637279\n",
      "nll_loss= 1.8066354990005493\n",
      "avg_std= 0.19721953570842743\n",
      "dist std min max: 0.009753307327628136 0.19721953570842743 2.8402442932128906\n",
      "hidden_states min max: -12.246542930603027 13.872635841369629\n",
      "hidden_state minus mean squared max: 100.52412414550781\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.77735137939453 3.7032103538513184\n",
      "loss  1755: 1.8051   grad norm: 2.1287          model param norm: 88.0190        \n",
      "\n",
      "quiet_star_policy_loss= 0.011213160119950771\n",
      "nll_loss= 1.791550874710083\n",
      "avg_std= 0.1969054788351059\n",
      "dist std min max: 0.009884485974907875 0.1969054788351059 3.118701219558716\n",
      "hidden_states min max: -12.059469223022461 12.513225555419922\n",
      "hidden_state minus mean squared max: 115.45172882080078\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.78070068359375 3.695133686065674\n",
      "loss  1756: 1.8028   grad norm: 1.9695          model param norm: 88.0206        \n",
      "\n",
      "quiet_star_policy_loss= -0.0033872604835778475\n",
      "nll_loss= 1.7855802774429321\n",
      "avg_std= 0.19578605890274048\n",
      "dist std min max: 0.009834839031100273 0.19578605890274048 2.866513729095459\n",
      "hidden_states min max: -12.097688674926758 12.819238662719727\n",
      "hidden_state minus mean squared max: 91.3788833618164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -13.379141807556152 3.692457675933838\n",
      "loss  1757: 1.7822   grad norm: 2.0896          model param norm: 88.0222        \n",
      "\n",
      "quiet_star_policy_loss= -0.011726832948625088\n",
      "nll_loss= 1.777462363243103\n",
      "avg_std= 0.1974191814661026\n",
      "dist std min max: 0.009724918752908707 0.1974191814661026 3.0451948642730713\n",
      "hidden_states min max: -12.758496284484863 12.303156852722168\n",
      "hidden_state minus mean squared max: 109.68931579589844\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -103.08177947998047 3.704648017883301\n",
      "loss  1758: 1.7657   grad norm: 2.0776          model param norm: 88.0235        \n",
      "\n",
      "quiet_star_policy_loss= -0.01165847759693861\n",
      "nll_loss= 1.8030744791030884\n",
      "avg_std= 0.19447453320026398\n",
      "dist std min max: 0.009703430347144604 0.19447453320026398 2.7882578372955322\n",
      "hidden_states min max: -12.290833473205566 13.054447174072266\n",
      "hidden_state minus mean squared max: 116.80712890625\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -102.26093292236328 3.7136263847351074\n",
      "loss  1759: 1.7914   grad norm: 2.0976          model param norm: 88.0247        \n",
      "\n",
      "quiet_star_policy_loss= -0.005796933081001043\n",
      "nll_loss= 1.7832454442977905\n",
      "avg_std= 0.19518029689788818\n",
      "dist std min max: 0.009569560177624226 0.19518029689788818 3.108950614929199\n",
      "hidden_states min max: -12.029120445251465 13.051369667053223\n",
      "hidden_state minus mean squared max: 90.31983184814453\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.50507354736328 3.7163853645324707\n",
      "loss  1760: 1.7774   grad norm: 1.9560          model param norm: 88.0259        \n",
      "\n",
      "quiet_star_policy_loss= -0.008563578128814697\n",
      "nll_loss= 1.783556342124939\n",
      "avg_std= 0.19545960426330566\n",
      "dist std min max: 0.009572052396833897 0.19545960426330566 2.7614781856536865\n",
      "hidden_states min max: -13.957170486450195 12.16041374206543\n",
      "hidden_state minus mean squared max: 109.49886322021484\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.88052368164062 3.729865074157715\n",
      "loss  1761: 1.7750   grad norm: 2.1719          model param norm: 88.0273        \n",
      "\n",
      "quiet_star_policy_loss= -0.012672185897827148\n",
      "nll_loss= 1.7920372486114502\n",
      "avg_std= 0.19464533030986786\n",
      "dist std min max: 0.009451296180486679 0.19464533030986786 2.686577320098877\n",
      "hidden_states min max: -35.6859016418457 12.989692687988281\n",
      "hidden_state minus mean squared max: 1052.778564453125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -105.01216888427734 3.7341198921203613\n",
      "loss  1762: 1.7794   grad norm: 2.0728          model param norm: 88.0286        \n",
      "\n",
      "quiet_star_policy_loss= -0.005732574500143528\n",
      "nll_loss= 1.8216073513031006\n",
      "avg_std= 0.19498476386070251\n",
      "dist std min max: 0.009914455935359001 0.19498476386070251 2.6917126178741455\n",
      "hidden_states min max: -12.220352172851562 12.084352493286133\n",
      "hidden_state minus mean squared max: 80.59261322021484\n",
      "hidden_state minus mean divided by std max: 4.957175254821777\n",
      "log_prob min max: -102.6707992553711 3.679018497467041\n",
      "loss  1763: 1.8159   grad norm: 4.4484          model param norm: 88.0296        \n",
      "\n",
      "quiet_star_policy_loss= 0.000683689140714705\n",
      "nll_loss= 1.7988150119781494\n",
      "avg_std= 0.19562272727489471\n",
      "dist std min max: 0.009586765430867672 0.19562272727489471 2.7682712078094482\n",
      "hidden_states min max: -14.187450408935547 13.200634002685547\n",
      "hidden_state minus mean squared max: 125.8825454711914\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.148193359375 3.711958885192871\n",
      "loss  1764: 1.7995   grad norm: 2.0313          model param norm: 88.0305        \n",
      "\n",
      "quiet_star_policy_loss= -0.018116116523742676\n",
      "nll_loss= 1.7762184143066406\n",
      "avg_std= 0.1953638643026352\n",
      "dist std min max: 0.009669680148363113 0.1953638643026352 2.7332701683044434\n",
      "hidden_states min max: -12.422327995300293 14.009450912475586\n",
      "hidden_state minus mean squared max: 98.5341567993164\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.42972564697266 3.7062268257141113\n",
      "loss  1765: 1.7581   grad norm: 2.1048          model param norm: 88.0311        \n",
      "\n",
      "quiet_star_policy_loss= -0.002592325210571289\n",
      "nll_loss= 1.784584879875183\n",
      "avg_std= 0.19490814208984375\n",
      "dist std min max: 0.009547787718474865 0.19490814208984375 2.7650372982025146\n",
      "hidden_states min max: -12.805874824523926 13.844942092895508\n",
      "hidden_state minus mean squared max: 110.38555145263672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -14.256360054016113 3.709512233734131\n",
      "loss  1766: 1.7820   grad norm: 2.0222          model param norm: 88.0318        \n",
      "\n",
      "quiet_star_policy_loss= -0.005595350172370672\n",
      "nll_loss= 1.7984145879745483\n",
      "avg_std= 0.1956501454114914\n",
      "dist std min max: 0.009797168895602226 0.1956501454114914 2.723757266998291\n",
      "hidden_states min max: -12.957554817199707 14.13849925994873\n",
      "hidden_state minus mean squared max: 112.85044860839844\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.11772155761719 3.6960091590881348\n",
      "loss  1767: 1.7928   grad norm: 1.9760          model param norm: 88.0325        \n",
      "\n",
      "quiet_star_policy_loss= 2.7704239982995205e-05\n",
      "nll_loss= 1.77926766872406\n",
      "avg_std= 0.19570869207382202\n",
      "dist std min max: 0.009628179483115673 0.19570869207382202 2.766521692276001\n",
      "hidden_states min max: -11.701255798339844 12.373567581176758\n",
      "hidden_state minus mean squared max: 83.48391723632812\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.15238189697266 3.7058234214782715\n",
      "loss  1768: 1.7793   grad norm: 2.0464          model param norm: 88.0334        \n",
      "\n",
      "quiet_star_policy_loss= -0.007917642593383789\n",
      "nll_loss= 1.7905189990997314\n",
      "avg_std= 0.1950804889202118\n",
      "dist std min max: 0.009568743407726288 0.1950804889202118 2.8144478797912598\n",
      "hidden_states min max: -13.949557304382324 12.223563194274902\n",
      "hidden_state minus mean squared max: 114.92736053466797\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.39085388183594 3.716024398803711\n",
      "loss  1769: 1.7826   grad norm: 2.0531          model param norm: 88.0346        \n",
      "\n",
      "quiet_star_policy_loss= -0.0011668027145788074\n",
      "nll_loss= 1.800418496131897\n",
      "avg_std= 0.19586603343486786\n",
      "dist std min max: 0.009811081923544407 0.19586603343486786 2.7374107837677\n",
      "hidden_states min max: -13.43444538116455 14.322328567504883\n",
      "hidden_state minus mean squared max: 116.79740142822266\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.77423095703125 3.7013063430786133\n",
      "loss  1770: 1.7993   grad norm: 2.0146          model param norm: 88.0360        \n",
      "\n",
      "quiet_star_policy_loss= -0.003060293151065707\n",
      "nll_loss= 1.778368353843689\n",
      "avg_std= 0.1967298686504364\n",
      "dist std min max: 0.009609443135559559 0.1967298686504364 2.772243022918701\n",
      "hidden_states min max: -12.74815559387207 12.623531341552734\n",
      "hidden_state minus mean squared max: 96.00836181640625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.69174194335938 3.7131381034851074\n",
      "loss  1771: 1.7753   grad norm: 2.0237          model param norm: 88.0374        \n",
      "\n",
      "quiet_star_policy_loss= -0.014081383123993874\n",
      "nll_loss= 1.8052290678024292\n",
      "avg_std= 0.1955796331167221\n",
      "dist std min max: 0.009609676897525787 0.1955796331167221 2.844442367553711\n",
      "hidden_states min max: -12.57553482055664 13.577726364135742\n",
      "hidden_state minus mean squared max: 112.06843566894531\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.07279968261719 3.709791660308838\n",
      "loss  1772: 1.7911   grad norm: 2.1326          model param norm: 88.0389        \n",
      "\n",
      "quiet_star_policy_loss= 0.007550096604973078\n",
      "nll_loss= 1.7863460779190063\n",
      "avg_std= 0.19726821780204773\n",
      "dist std min max: 0.009665319696068764 0.19726821780204773 2.973034620285034\n",
      "hidden_states min max: -12.464395523071289 12.257033348083496\n",
      "hidden_state minus mean squared max: 96.29674530029297\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.69419860839844 3.6957125663757324\n",
      "loss  1773: 1.7939   grad norm: 2.1535          model param norm: 88.0405        \n",
      "\n",
      "quiet_star_policy_loss= -0.025732850655913353\n",
      "nll_loss= 1.7902714014053345\n",
      "avg_std= 0.19642524421215057\n",
      "dist std min max: 0.009691301733255386 0.19642524421215057 2.7793967723846436\n",
      "hidden_states min max: -12.97879695892334 12.34388542175293\n",
      "hidden_state minus mean squared max: 112.96711730957031\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.2298355102539 3.713108539581299\n",
      "loss  1774: 1.7645   grad norm: 2.1707          model param norm: 88.0420        \n",
      "\n",
      "quiet_star_policy_loss= -0.005486130714416504\n",
      "nll_loss= 1.7879669666290283\n",
      "avg_std= 0.19759230315685272\n",
      "dist std min max: 0.009522021748125553 0.19759230315685272 2.8123152256011963\n",
      "hidden_states min max: -12.405675888061523 13.774175643920898\n",
      "hidden_state minus mean squared max: 105.32849884033203\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.08906555175781 3.72110652923584\n",
      "loss  1775: 1.7825   grad norm: 2.1764          model param norm: 88.0432        \n",
      "\n",
      "quiet_star_policy_loss= -0.032755136489868164\n",
      "nll_loss= 1.7905988693237305\n",
      "avg_std= 0.1970912516117096\n",
      "dist std min max: 0.009557024575769901 0.1970912516117096 2.801452875137329\n",
      "hidden_states min max: -13.176544189453125 13.262314796447754\n",
      "hidden_state minus mean squared max: 114.14005279541016\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.96241760253906 3.7229113578796387\n",
      "loss  1776: 1.7578   grad norm: 2.1958          model param norm: 88.0445        \n",
      "\n",
      "quiet_star_policy_loss= -0.030614232644438744\n",
      "nll_loss= 1.7855751514434814\n",
      "avg_std= 0.19640043377876282\n",
      "dist std min max: 0.009418444707989693 0.19640043377876282 2.8443970680236816\n",
      "hidden_states min max: -14.600345611572266 12.67963981628418\n",
      "hidden_state minus mean squared max: 128.71839904785156\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.69977569580078 3.7135443687438965\n",
      "loss  1777: 1.7550   grad norm: 2.0855          model param norm: 88.0456        \n",
      "\n",
      "quiet_star_policy_loss= -0.007320082280784845\n",
      "nll_loss= 1.7875041961669922\n",
      "avg_std= 0.19718880951404572\n",
      "dist std min max: 0.009407443925738335 0.19718880951404572 2.818660259246826\n",
      "hidden_states min max: -24.15194320678711 12.205442428588867\n",
      "hidden_state minus mean squared max: 739.9345092773438\n",
      "hidden_state minus mean divided by std max: 5.166583061218262\n",
      "log_prob min max: -104.83586120605469 3.74251127243042\n",
      "loss  1778: 1.7802   grad norm: 2.1336          model param norm: 88.0470        \n",
      "\n",
      "quiet_star_policy_loss= 0.0037186623085290194\n",
      "nll_loss= 1.7641953229904175\n",
      "avg_std= 0.19634823501110077\n",
      "dist std min max: 0.009756991639733315 0.19634823501110077 2.820085287094116\n",
      "hidden_states min max: -12.757527351379395 13.031881332397461\n",
      "hidden_state minus mean squared max: 113.11201477050781\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -103.13862609863281 3.701176643371582\n",
      "loss  1779: 1.7679   grad norm: 1.9946          model param norm: 88.0485        \n",
      "\n",
      "quiet_star_policy_loss= 0.021441226825118065\n",
      "nll_loss= 1.7994186878204346\n",
      "avg_std= 0.1955183744430542\n",
      "dist std min max: 0.009604468010365963 0.1955183744430542 2.9113729000091553\n",
      "hidden_states min max: -13.190285682678223 13.985658645629883\n",
      "hidden_state minus mean squared max: 106.43251037597656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.66854858398438 3.711085796356201\n",
      "loss  1780: 1.8209   grad norm: 2.1687          model param norm: 88.0498        \n",
      "\n",
      "quiet_star_policy_loss= -0.025147324427962303\n",
      "nll_loss= 1.7809852361679077\n",
      "avg_std= 0.19856856763362885\n",
      "dist std min max: 0.009568356908857822 0.19856856763362885 2.8388147354125977\n",
      "hidden_states min max: -13.401641845703125 12.927656173706055\n",
      "hidden_state minus mean squared max: 110.6126480102539\n",
      "hidden_state minus mean divided by std max: 5.035408020019531\n",
      "log_prob min max: -102.98310089111328 3.716561794281006\n",
      "loss  1781: 1.7558   grad norm: 2.2734          model param norm: 88.0511        \n",
      "\n",
      "quiet_star_policy_loss= -0.0014551877975463867\n",
      "nll_loss= 1.7865772247314453\n",
      "avg_std= 0.19675906002521515\n",
      "dist std min max: 0.009372872300446033 0.19675906002521515 2.889299154281616\n",
      "hidden_states min max: -13.089171409606934 15.213552474975586\n",
      "hidden_state minus mean squared max: 124.5565414428711\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -103.76193237304688 3.731771945953369\n",
      "loss  1782: 1.7851   grad norm: 2.1444          model param norm: 88.0527        \n",
      "\n",
      "quiet_star_policy_loss= -0.022932518273591995\n",
      "nll_loss= 1.7843506336212158\n",
      "avg_std= 0.19749237596988678\n",
      "dist std min max: 0.00947335921227932 0.19749237596988678 2.8658227920532227\n",
      "hidden_states min max: -12.586271286010742 12.620840072631836\n",
      "hidden_state minus mean squared max: 101.09139251708984\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.08480834960938 3.721315860748291\n",
      "loss  1783: 1.7614   grad norm: 2.0583          model param norm: 88.0541        \n",
      "\n",
      "quiet_star_policy_loss= -0.015194606967270374\n",
      "nll_loss= 1.788735270500183\n",
      "avg_std= 0.19773530960083008\n",
      "dist std min max: 0.009584763087332249 0.19773530960083008 2.846478223800659\n",
      "hidden_states min max: -13.00095272064209 12.669657707214355\n",
      "hidden_state minus mean squared max: 112.17549133300781\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -102.63314819335938 3.7188057899475098\n",
      "loss  1784: 1.7735   grad norm: 2.2018          model param norm: 88.0556        \n",
      "\n",
      "quiet_star_policy_loss= -0.026953697204589844\n",
      "nll_loss= 1.783132553100586\n",
      "avg_std= 0.19754160940647125\n",
      "dist std min max: 0.009686940349638462 0.19754160940647125 2.904111623764038\n",
      "hidden_states min max: -13.302640914916992 13.496683120727539\n",
      "hidden_state minus mean squared max: 163.8604736328125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.08208465576172 3.7113561630249023\n",
      "loss  1785: 1.7562   grad norm: 2.2567          model param norm: 88.0573        \n",
      "\n",
      "quiet_star_policy_loss= -0.00195827498100698\n",
      "nll_loss= 1.7762514352798462\n",
      "avg_std= 0.19753779470920563\n",
      "dist std min max: 0.009958828799426556 0.19753779470920563 2.8234522342681885\n",
      "hidden_states min max: -15.067831993103027 13.556143760681152\n",
      "hidden_state minus mean squared max: 136.980712890625\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.93124389648438 3.674476146697998\n",
      "loss  1786: 1.7743   grad norm: 2.1351          model param norm: 88.0587        \n",
      "\n",
      "quiet_star_policy_loss= -0.012015605345368385\n",
      "nll_loss= 1.780355453491211\n",
      "avg_std= 0.19733425974845886\n",
      "dist std min max: 0.009738529101014137 0.19733425974845886 2.7980246543884277\n",
      "hidden_states min max: -13.409200668334961 14.421541213989258\n",
      "hidden_state minus mean squared max: 119.1756591796875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.39771270751953 3.6991825103759766\n",
      "loss  1787: 1.7683   grad norm: 1.9334          model param norm: 88.0601        \n",
      "\n",
      "quiet_star_policy_loss= 0.016869163140654564\n",
      "nll_loss= 1.7916244268417358\n",
      "avg_std= 0.19623655080795288\n",
      "dist std min max: 0.009799294173717499 0.19623655080795288 2.799905776977539\n",
      "hidden_states min max: -13.634391784667969 13.057518005371094\n",
      "hidden_state minus mean squared max: 113.86329650878906\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -101.79663848876953 3.687532901763916\n",
      "loss  1788: 1.8085   grad norm: 2.2641          model param norm: 88.0618        \n",
      "\n",
      "quiet_star_policy_loss= -0.005593395326286554\n",
      "nll_loss= 1.8005081415176392\n",
      "avg_std= 0.19651344418525696\n",
      "dist std min max: 0.009952287189662457 0.19651344418525696 2.7974278926849365\n",
      "hidden_states min max: -14.812409400939941 13.906621932983398\n",
      "hidden_state minus mean squared max: 130.4490509033203\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.230712890625 3.6811752319335938\n",
      "loss  1789: 1.7949   grad norm: 2.1712          model param norm: 88.0632        \n",
      "\n",
      "quiet_star_policy_loss= -0.016253208741545677\n",
      "nll_loss= 1.7795019149780273\n",
      "avg_std= 0.19730068743228912\n",
      "dist std min max: 0.010077646933495998 0.19730068743228912 2.8683929443359375\n",
      "hidden_states min max: -14.6704683303833 13.339536666870117\n",
      "hidden_state minus mean squared max: 126.37464141845703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.47760009765625 3.6711044311523438\n",
      "loss  1790: 1.7632   grad norm: 2.0421          model param norm: 88.0649        \n",
      "\n",
      "quiet_star_policy_loss= -0.03472325578331947\n",
      "nll_loss= 1.8108464479446411\n",
      "avg_std= 0.19756589829921722\n",
      "dist std min max: 0.010080136358737946 0.19756589829921722 2.870272159576416\n",
      "hidden_states min max: -13.876152038574219 14.688798904418945\n",
      "hidden_state minus mean squared max: 111.73884582519531\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.44784545898438 3.6663570404052734\n",
      "loss  1791: 1.7761   grad norm: 4.2494          model param norm: 88.0666        \n",
      "\n",
      "quiet_star_policy_loss= -0.00704536447301507\n",
      "nll_loss= 1.8085272312164307\n",
      "avg_std= 0.19549721479415894\n",
      "dist std min max: 0.009606616571545601 0.19549721479415894 2.93827486038208\n",
      "hidden_states min max: -14.236522674560547 12.733541488647461\n",
      "hidden_state minus mean squared max: 124.63912200927734\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -100.48812866210938 3.7042808532714844\n",
      "loss  1792: 1.8015   grad norm: 2.0941          model param norm: 88.0686        \n",
      "\n",
      "quiet_star_policy_loss= -0.013908147811889648\n",
      "nll_loss= 1.7834981679916382\n",
      "avg_std= 0.19683675467967987\n",
      "dist std min max: 0.010028144344687462 0.19683675467967987 2.9602575302124023\n",
      "hidden_states min max: -13.33398151397705 12.565937042236328\n",
      "hidden_state minus mean squared max: 113.8023452758789\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.88443756103516 3.6746606826782227\n",
      "loss  1793: 1.7696   grad norm: 2.1116          model param norm: 88.0707        \n",
      "\n",
      "quiet_star_policy_loss= -0.00853743590414524\n",
      "nll_loss= 1.7788479328155518\n",
      "avg_std= 0.1964004635810852\n",
      "dist std min max: 0.009805618785321712 0.1964004635810852 2.9422061443328857\n",
      "hidden_states min max: -13.34512710571289 14.099714279174805\n",
      "hidden_state minus mean squared max: 113.65727233886719\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.90265655517578 3.6750731468200684\n",
      "loss  1794: 1.7703   grad norm: 2.0390          model param norm: 88.0724        \n",
      "\n",
      "quiet_star_policy_loss= -0.020206665620207787\n",
      "nll_loss= 1.7968796491622925\n",
      "avg_std= 0.19608940184116364\n",
      "dist std min max: 0.009660269133746624 0.19608940184116364 2.9470596313476562\n",
      "hidden_states min max: -13.360950469970703 12.527405738830566\n",
      "hidden_state minus mean squared max: 112.99246978759766\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.64906311035156 3.6989879608154297\n",
      "loss  1795: 1.7767   grad norm: 2.1841          model param norm: 88.0742        \n",
      "\n",
      "quiet_star_policy_loss= 0.00658681383356452\n",
      "nll_loss= 1.79192054271698\n",
      "avg_std= 0.1964128613471985\n",
      "dist std min max: 0.009916762821376324 0.1964128613471985 2.914886713027954\n",
      "hidden_states min max: -13.337250709533691 13.35594367980957\n",
      "hidden_state minus mean squared max: 109.05554962158203\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.55671691894531 3.6804752349853516\n",
      "loss  1796: 1.7985   grad norm: 2.1810          model param norm: 88.0759        \n",
      "\n",
      "quiet_star_policy_loss= 0.008686507120728493\n",
      "nll_loss= 1.7886555194854736\n",
      "avg_std= 0.1964579075574875\n",
      "dist std min max: 0.00983984861522913 0.1964579075574875 2.898055076599121\n",
      "hidden_states min max: -13.492198944091797 13.366000175476074\n",
      "hidden_state minus mean squared max: 123.88667297363281\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7441177368164 3.6885366439819336\n",
      "loss  1797: 1.7973   grad norm: 2.0089          model param norm: 88.0780        \n",
      "\n",
      "quiet_star_policy_loss= -0.028569532558321953\n",
      "nll_loss= 1.7975349426269531\n",
      "avg_std= 0.1945236772298813\n",
      "dist std min max: 0.010017414577305317 0.1945236772298813 2.9491896629333496\n",
      "hidden_states min max: -13.056207656860352 14.104912757873535\n",
      "hidden_state minus mean squared max: 109.82425689697266\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.3612289428711 3.6750593185424805\n",
      "loss  1798: 1.7690   grad norm: 2.0996          model param norm: 88.0799        \n",
      "\n",
      "quiet_star_policy_loss= -0.010821950621902943\n",
      "nll_loss= 1.7789734601974487\n",
      "avg_std= 0.19454288482666016\n",
      "dist std min max: 0.009942388162016869 0.19454288482666016 2.8660383224487305\n",
      "hidden_states min max: -13.113412857055664 13.060468673706055\n",
      "hidden_state minus mean squared max: 112.98131561279297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.56478118896484 3.68967342376709\n",
      "loss  1799: 1.7682   grad norm: 2.3066          model param norm: 88.0817        \n",
      "eval loss 1.8007071018218994\n",
      "\n",
      "quiet_star_policy_loss= -0.01652546040713787\n",
      "nll_loss= 1.7904905080795288\n",
      "avg_std= 0.19416551291942596\n",
      "dist std min max: 0.0099562406539917 0.19416551291942596 2.8943073749542236\n",
      "hidden_states min max: -13.046296119689941 12.814512252807617\n",
      "hidden_state minus mean squared max: 105.0494613647461\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -102.83553314208984 3.683206558227539\n",
      "loss  1800: 1.7740   grad norm: 2.0559          model param norm: 88.0834        \n",
      "\n",
      "quiet_star_policy_loss= 0.027573609724640846\n",
      "nll_loss= 1.8112680912017822\n",
      "avg_std= 0.19409172236919403\n",
      "dist std min max: 0.00987270101904869 0.19409172236919403 2.8987183570861816\n",
      "hidden_states min max: -13.51628303527832 12.924592971801758\n",
      "hidden_state minus mean squared max: 112.22635650634766\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.90499114990234 3.6913881301879883\n",
      "loss  1801: 1.8388   grad norm: 2.2152          model param norm: 88.0848        \n",
      "\n",
      "quiet_star_policy_loss= 0.021663714200258255\n",
      "nll_loss= 1.798288345336914\n",
      "avg_std= 0.1935906857252121\n",
      "dist std min max: 0.00972745567560196 0.1935906857252121 2.8512187004089355\n",
      "hidden_states min max: -13.56587028503418 14.075634956359863\n",
      "hidden_state minus mean squared max: 104.94062805175781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.51744842529297 3.706963062286377\n",
      "loss  1802: 1.8200   grad norm: 2.1787          model param norm: 88.0860        \n",
      "\n",
      "quiet_star_policy_loss= -0.0016550302971154451\n",
      "nll_loss= 1.7905851602554321\n",
      "avg_std= 0.19304704666137695\n",
      "dist std min max: 0.009866041131317616 0.19304704666137695 2.8851983547210693\n",
      "hidden_states min max: -13.033629417419434 12.73633861541748\n",
      "hidden_state minus mean squared max: 114.96174621582031\n",
      "hidden_state minus mean divided by std max: 5.166586875915527\n",
      "log_prob min max: -102.83806610107422 3.6851272583007812\n",
      "loss  1803: 1.7889   grad norm: 1.9982          model param norm: 88.0873        \n",
      "\n",
      "quiet_star_policy_loss= -0.017265964299440384\n",
      "nll_loss= 1.814341425895691\n",
      "avg_std= 0.19284501671791077\n",
      "dist std min max: 0.009913675487041473 0.19284501671791077 3.1599843502044678\n",
      "hidden_states min max: -17.37019920349121 12.36385726928711\n",
      "hidden_state minus mean squared max: 448.0135498046875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.5849838256836 3.689295768737793\n",
      "loss  1804: 1.7971   grad norm: 2.2893          model param norm: 88.0888        \n",
      "\n",
      "quiet_star_policy_loss= -0.01391900796443224\n",
      "nll_loss= 1.7822309732437134\n",
      "avg_std= 0.1942460983991623\n",
      "dist std min max: 0.00994839146733284 0.1942460983991623 2.898555278778076\n",
      "hidden_states min max: -12.687461853027344 13.719289779663086\n",
      "hidden_state minus mean squared max: 116.24674987792969\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.91043853759766 3.69028902053833\n",
      "loss  1805: 1.7683   grad norm: 2.1662          model param norm: 88.0900        \n",
      "\n",
      "quiet_star_policy_loss= -0.014697933569550514\n",
      "nll_loss= 1.801605224609375\n",
      "avg_std= 0.19211725890636444\n",
      "dist std min max: 0.009929917752742767 0.19211725890636444 2.937995195388794\n",
      "hidden_states min max: -12.795402526855469 14.057394027709961\n",
      "hidden_state minus mean squared max: 128.9207305908203\n",
      "hidden_state minus mean divided by std max: 5.166581630706787\n",
      "log_prob min max: -101.83712005615234 3.6843457221984863\n",
      "loss  1806: 1.7869   grad norm: 2.2326          model param norm: 88.0913        \n",
      "\n",
      "quiet_star_policy_loss= 0.0009305953863076866\n",
      "nll_loss= 1.8122504949569702\n",
      "avg_std= 0.1920979768037796\n",
      "dist std min max: 0.00981917418539524 0.1920979768037796 2.8643317222595215\n",
      "hidden_states min max: -13.057357788085938 14.371161460876465\n",
      "hidden_state minus mean squared max: 148.59234619140625\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -103.4228515625 3.703321933746338\n",
      "loss  1807: 1.8132   grad norm: 2.2408          model param norm: 88.0925        \n",
      "\n",
      "quiet_star_policy_loss= 0.0064099314622581005\n",
      "nll_loss= 1.7595551013946533\n",
      "avg_std= 0.1951645463705063\n",
      "dist std min max: 0.009643655270338058 0.1951645463705063 2.893005847930908\n",
      "hidden_states min max: -14.545348167419434 13.05575942993164\n",
      "hidden_state minus mean squared max: 127.85604095458984\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.15156555175781 3.7105631828308105\n",
      "loss  1808: 1.7660   grad norm: 2.0049          model param norm: 88.0940        \n",
      "\n",
      "quiet_star_policy_loss= -0.020770525559782982\n",
      "nll_loss= 1.786253809928894\n",
      "avg_std= 0.19280613958835602\n",
      "dist std min max: 0.009685083292424679 0.19280613958835602 2.870380401611328\n",
      "hidden_states min max: -14.018392562866211 13.542224884033203\n",
      "hidden_state minus mean squared max: 114.16323852539062\n",
      "hidden_state minus mean divided by std max: 5.1665802001953125\n",
      "log_prob min max: -102.59937286376953 3.7110729217529297\n",
      "loss  1809: 1.7655   grad norm: 2.1899          model param norm: 88.0953        \n",
      "\n",
      "quiet_star_policy_loss= -0.012294167652726173\n",
      "nll_loss= 1.7723616361618042\n",
      "avg_std= 0.19393229484558105\n",
      "dist std min max: 0.009880786761641502 0.19393229484558105 2.9724302291870117\n",
      "hidden_states min max: -14.044984817504883 14.525946617126465\n",
      "hidden_state minus mean squared max: 111.59941101074219\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -103.18102264404297 3.690068244934082\n",
      "loss  1810: 1.7601   grad norm: 2.1876          model param norm: 88.0968        \n",
      "\n",
      "quiet_star_policy_loss= -0.006047165486961603\n",
      "nll_loss= 1.794430136680603\n",
      "avg_std= 0.19212888181209564\n",
      "dist std min max: 0.009732512757182121 0.19212888181209564 2.843780755996704\n",
      "hidden_states min max: -13.296483993530273 13.587409973144531\n",
      "hidden_state minus mean squared max: 129.45785522460938\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.66191864013672 3.6960391998291016\n",
      "loss  1811: 1.7884   grad norm: 2.2359          model param norm: 88.0983        \n",
      "\n",
      "quiet_star_policy_loss= -0.008405208587646484\n",
      "nll_loss= 1.791752815246582\n",
      "avg_std= 0.19236409664154053\n",
      "dist std min max: 0.009586157277226448 0.19236409664154053 2.928515672683716\n",
      "hidden_states min max: -14.112476348876953 14.177452087402344\n",
      "hidden_state minus mean squared max: 116.37399291992188\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.22115325927734 3.712003707885742\n",
      "loss  1812: 1.7833   grad norm: 2.2858          model param norm: 88.0997        \n",
      "\n",
      "quiet_star_policy_loss= -0.0073563456535339355\n",
      "nll_loss= 1.7641159296035767\n",
      "avg_std= 0.19385449588298798\n",
      "dist std min max: 0.009603030048310757 0.19385449588298798 2.83803653717041\n",
      "hidden_states min max: -14.615562438964844 13.034015655517578\n",
      "hidden_state minus mean squared max: 129.87196350097656\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.77019500732422 3.708655834197998\n",
      "loss  1813: 1.7568   grad norm: 2.0947          model param norm: 88.1014        \n",
      "\n",
      "quiet_star_policy_loss= 0.009312868118286133\n",
      "nll_loss= 1.773681640625\n",
      "avg_std= 0.19328573346138\n",
      "dist std min max: 0.009596200659871101 0.19328573346138 2.874011278152466\n",
      "hidden_states min max: -13.543466567993164 14.173856735229492\n",
      "hidden_state minus mean squared max: 140.0661163330078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.70862579345703 3.720940589904785\n",
      "loss  1814: 1.7830   grad norm: 2.2268          model param norm: 88.1030        \n",
      "\n",
      "quiet_star_policy_loss= 0.001871347427368164\n",
      "nll_loss= 1.7742897272109985\n",
      "avg_std= 0.19316884875297546\n",
      "dist std min max: 0.00960405170917511 0.19316884875297546 2.9484446048736572\n",
      "hidden_states min max: -14.256723403930664 13.57040023803711\n",
      "hidden_state minus mean squared max: 116.0502700805664\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -101.76671600341797 3.714224338531494\n",
      "loss  1815: 1.7762   grad norm: 2.0839          model param norm: 88.1042        \n",
      "\n",
      "quiet_star_policy_loss= -0.006444406695663929\n",
      "nll_loss= 1.7727009057998657\n",
      "avg_std= 0.19340001046657562\n",
      "dist std min max: 0.009540283121168613 0.19340001046657562 2.864630699157715\n",
      "hidden_states min max: -13.593565940856934 14.241495132446289\n",
      "hidden_state minus mean squared max: 156.61973571777344\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.68949127197266 3.7303357124328613\n",
      "loss  1816: 1.7663   grad norm: 2.1828          model param norm: 88.1055        \n",
      "\n",
      "quiet_star_policy_loss= 0.007791757583618164\n",
      "nll_loss= 1.7679262161254883\n",
      "avg_std= 0.19366353750228882\n",
      "dist std min max: 0.009473183192312717 0.19366353750228882 2.8486239910125732\n",
      "hidden_states min max: -14.901493072509766 12.942278861999512\n",
      "hidden_state minus mean squared max: 137.6470947265625\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -103.3064193725586 3.7287845611572266\n",
      "loss  1817: 1.7757   grad norm: 2.2187          model param norm: 88.1065        \n",
      "\n",
      "quiet_star_policy_loss= -0.022899342700839043\n",
      "nll_loss= 1.7906376123428345\n",
      "avg_std= 0.19314365088939667\n",
      "dist std min max: 0.009336055256426334 0.19314365088939667 2.858678102493286\n",
      "hidden_states min max: -13.574063301086426 13.300216674804688\n",
      "hidden_state minus mean squared max: 113.27017974853516\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -13.705639839172363 3.7473182678222656\n",
      "loss  1818: 1.7677   grad norm: 2.3381          model param norm: 88.1073        \n",
      "\n",
      "quiet_star_policy_loss= -0.006861979141831398\n",
      "nll_loss= 1.7970373630523682\n",
      "avg_std= 0.19404621422290802\n",
      "dist std min max: 0.009367769584059715 0.19404621422290802 2.764327049255371\n",
      "hidden_states min max: -12.16568374633789 12.431188583374023\n",
      "hidden_state minus mean squared max: 119.72691345214844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.92363739013672 3.7464966773986816\n",
      "loss  1819: 1.7902   grad norm: 4.6922          model param norm: 88.1081        \n",
      "\n",
      "quiet_star_policy_loss= -0.021452760323882103\n",
      "nll_loss= 1.7841075658798218\n",
      "avg_std= 0.19330477714538574\n",
      "dist std min max: 0.009345171973109245 0.19330477714538574 2.9540953636169434\n",
      "hidden_states min max: -13.612953186035156 12.600229263305664\n",
      "hidden_state minus mean squared max: 127.17498016357422\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.37173461914062 3.753509044647217\n",
      "loss  1820: 1.7627   grad norm: 2.3607          model param norm: 88.1091        \n",
      "\n",
      "quiet_star_policy_loss= -0.019396424293518066\n",
      "nll_loss= 1.7969436645507812\n",
      "avg_std= 0.19292818009853363\n",
      "dist std min max: 0.009240795858204365 0.19292818009853363 2.905871629714966\n",
      "hidden_states min max: -12.811256408691406 13.211994171142578\n",
      "hidden_state minus mean squared max: 109.52574920654297\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.63765716552734 3.754974842071533\n",
      "loss  1821: 1.7775   grad norm: 2.2305          model param norm: 88.1097        \n",
      "\n",
      "quiet_star_policy_loss= 0.0111336475238204\n",
      "nll_loss= 1.7915111780166626\n",
      "avg_std= 0.19263720512390137\n",
      "dist std min max: 0.00927580427378416 0.19263720512390137 2.8226802349090576\n",
      "hidden_states min max: -14.432183265686035 13.332889556884766\n",
      "hidden_state minus mean squared max: 120.36151885986328\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.00733947753906 3.757284164428711\n",
      "loss  1822: 1.8026   grad norm: 2.1562          model param norm: 88.1105        \n",
      "\n",
      "quiet_star_policy_loss= 0.019370799884200096\n",
      "nll_loss= 1.7818735837936401\n",
      "avg_std= 0.1950572282075882\n",
      "dist std min max: 0.00924026221036911 0.1950572282075882 2.928823947906494\n",
      "hidden_states min max: -14.050826072692871 13.809633255004883\n",
      "hidden_state minus mean squared max: 120.70274353027344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.61851501464844 3.7609729766845703\n",
      "loss  1823: 1.8012   grad norm: 2.1963          model param norm: 88.1116        \n",
      "\n",
      "quiet_star_policy_loss= -0.00047817229642532766\n",
      "nll_loss= 1.8036327362060547\n",
      "avg_std= 0.19172151386737823\n",
      "dist std min max: 0.009176170453429222 0.19172151386737823 2.846811056137085\n",
      "hidden_states min max: -13.533273696899414 13.525930404663086\n",
      "hidden_state minus mean squared max: 136.4925079345703\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.13980865478516 3.7643160820007324\n",
      "loss  1824: 1.8032   grad norm: 2.4033          model param norm: 88.1128        \n",
      "\n",
      "quiet_star_policy_loss= -0.015230441465973854\n",
      "nll_loss= 1.7924724817276\n",
      "avg_std= 0.19416923820972443\n",
      "dist std min max: 0.009142880327999592 0.19416923820972443 2.8684678077697754\n",
      "hidden_states min max: -13.565584182739258 12.304975509643555\n",
      "hidden_state minus mean squared max: 112.38729095458984\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.00030517578125 3.7732181549072266\n",
      "loss  1825: 1.7772   grad norm: 2.4040          model param norm: 88.1141        \n",
      "\n",
      "quiet_star_policy_loss= -0.004415083210915327\n",
      "nll_loss= 1.7958815097808838\n",
      "avg_std= 0.19299447536468506\n",
      "dist std min max: 0.009154017083346844 0.19299447536468506 2.876098155975342\n",
      "hidden_states min max: -13.024822235107422 12.888689041137695\n",
      "hidden_state minus mean squared max: 122.71686553955078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.85247802734375 3.7698893547058105\n",
      "loss  1826: 1.7915   grad norm: 2.5426          model param norm: 88.1150        \n",
      "\n",
      "quiet_star_policy_loss= 0.003094744635745883\n",
      "nll_loss= 1.7840473651885986\n",
      "avg_std= 0.19344547390937805\n",
      "dist std min max: 0.009197832085192204 0.19344547390937805 2.875495672225952\n",
      "hidden_states min max: -12.814632415771484 12.313904762268066\n",
      "hidden_state minus mean squared max: 137.47015380859375\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.7437973022461 3.7611122131347656\n",
      "loss  1827: 1.7871   grad norm: 2.3926          model param norm: 88.1158        \n",
      "\n",
      "quiet_star_policy_loss= -0.01153111457824707\n",
      "nll_loss= 1.7764815092086792\n",
      "avg_std= 0.19476595520973206\n",
      "dist std min max: 0.009128626435995102 0.19476595520973206 2.836217164993286\n",
      "hidden_states min max: -14.887054443359375 13.874258995056152\n",
      "hidden_state minus mean squared max: 139.02688598632812\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.70089721679688 3.7738289833068848\n",
      "loss  1828: 1.7650   grad norm: 2.3838          model param norm: 88.1169        \n",
      "\n",
      "quiet_star_policy_loss= -0.005748963449150324\n",
      "nll_loss= 1.7818806171417236\n",
      "avg_std= 0.19429171085357666\n",
      "dist std min max: 0.009092478081583977 0.19429171085357666 3.1035890579223633\n",
      "hidden_states min max: -16.498613357543945 13.891069412231445\n",
      "hidden_state minus mean squared max: 321.29022216796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.41874694824219 3.768768787384033\n",
      "loss  1829: 1.7761   grad norm: 2.2031          model param norm: 88.1177        \n",
      "\n",
      "quiet_star_policy_loss= -0.019989920780062675\n",
      "nll_loss= 1.7961664199829102\n",
      "avg_std= 0.1913248598575592\n",
      "dist std min max: 0.008993401192128658 0.1913248598575592 2.9250946044921875\n",
      "hidden_states min max: -15.682108879089355 14.176238059997559\n",
      "hidden_state minus mean squared max: 150.77525329589844\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.75708770751953 3.7911691665649414\n",
      "loss  1830: 1.7762   grad norm: 2.2605          model param norm: 88.1185        \n",
      "\n",
      "quiet_star_policy_loss= -0.022028302773833275\n",
      "nll_loss= 1.8112694025039673\n",
      "avg_std= 0.19128243625164032\n",
      "dist std min max: 0.00876312144100666 0.19128243625164032 3.170146942138672\n",
      "hidden_states min max: -12.968757629394531 13.719444274902344\n",
      "hidden_state minus mean squared max: 126.44256591796875\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.56413269042969 3.8053746223449707\n",
      "loss  1831: 1.7892   grad norm: 2.4358          model param norm: 88.1195        \n",
      "\n",
      "quiet_star_policy_loss= -0.002621602965518832\n",
      "nll_loss= 1.7855318784713745\n",
      "avg_std= 0.1926208734512329\n",
      "dist std min max: 0.008826724253594875 0.1926208734512329 2.8755180835723877\n",
      "hidden_states min max: -13.253095626831055 13.437668800354004\n",
      "hidden_state minus mean squared max: 129.33058166503906\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.27389526367188 3.79722261428833\n",
      "loss  1832: 1.7829   grad norm: 2.4556          model param norm: 88.1209        \n",
      "\n",
      "quiet_star_policy_loss= -0.012685561552643776\n",
      "nll_loss= 1.7633007764816284\n",
      "avg_std= 0.19393011927604675\n",
      "dist std min max: 0.008861321024596691 0.19393011927604675 2.889983892440796\n",
      "hidden_states min max: -12.863914489746094 14.684946060180664\n",
      "hidden_state minus mean squared max: 146.31776428222656\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -101.83753204345703 3.8050055503845215\n",
      "loss  1833: 1.7506   grad norm: 2.2705          model param norm: 88.1220        \n",
      "\n",
      "quiet_star_policy_loss= -0.010397816076874733\n",
      "nll_loss= 1.7744487524032593\n",
      "avg_std= 0.19382831454277039\n",
      "dist std min max: 0.008778520859777927 0.19382831454277039 2.915464401245117\n",
      "hidden_states min max: -14.699645042419434 12.785405158996582\n",
      "hidden_state minus mean squared max: 131.96578979492188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.7326889038086 3.797069549560547\n",
      "loss  1834: 1.7641   grad norm: 2.3076          model param norm: 88.1234        \n",
      "\n",
      "quiet_star_policy_loss= 0.0013647556770592928\n",
      "nll_loss= 1.7990368604660034\n",
      "avg_std= 0.19171182811260223\n",
      "dist std min max: 0.008813323453068733 0.19171182811260223 2.9107861518859863\n",
      "hidden_states min max: -14.097635269165039 13.358469009399414\n",
      "hidden_state minus mean squared max: 120.64476013183594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.57415771484375 3.8125510215759277\n",
      "loss  1835: 1.8004   grad norm: 2.3344          model param norm: 88.1246        \n",
      "\n",
      "quiet_star_policy_loss= -0.01519765891134739\n",
      "nll_loss= 1.8013126850128174\n",
      "avg_std= 0.19311738014221191\n",
      "dist std min max: 0.00870496965944767 0.19311738014221191 2.9358959197998047\n",
      "hidden_states min max: -13.07512378692627 15.014328002929688\n",
      "hidden_state minus mean squared max: 134.91233825683594\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.49937438964844 3.8129782676696777\n",
      "loss  1836: 1.7861   grad norm: 2.1380          model param norm: 88.1259        \n",
      "\n",
      "quiet_star_policy_loss= -0.0092094661667943\n",
      "nll_loss= 1.8044735193252563\n",
      "avg_std= 0.19265864789485931\n",
      "dist std min max: 0.008813433349132538 0.19265864789485931 2.9239518642425537\n",
      "hidden_states min max: -14.971558570861816 14.318266868591309\n",
      "hidden_state minus mean squared max: 132.41468811035156\n",
      "hidden_state minus mean divided by std max: 5.166577339172363\n",
      "log_prob min max: -103.21126556396484 3.8090100288391113\n",
      "loss  1837: 1.7953   grad norm: 2.3508          model param norm: 88.1271        \n",
      "\n",
      "quiet_star_policy_loss= 0.00807499885559082\n",
      "nll_loss= 1.7845754623413086\n",
      "avg_std= 0.1942647248506546\n",
      "dist std min max: 0.008766883052885532 0.1942647248506546 2.964151382446289\n",
      "hidden_states min max: -14.222819328308105 14.43986988067627\n",
      "hidden_state minus mean squared max: 123.71772003173828\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.49850463867188 3.802778720855713\n",
      "loss  1838: 1.7927   grad norm: 2.2337          model param norm: 88.1284        \n",
      "\n",
      "quiet_star_policy_loss= -0.024200797080993652\n",
      "nll_loss= 1.7875537872314453\n",
      "avg_std= 0.19465267658233643\n",
      "dist std min max: 0.008671591058373451 0.19465267658233643 2.9342470169067383\n",
      "hidden_states min max: -13.508541107177734 13.788764953613281\n",
      "hidden_state minus mean squared max: 119.90202331542969\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -103.55461120605469 3.8159847259521484\n",
      "loss  1839: 1.7634   grad norm: 2.1980          model param norm: 88.1296        \n",
      "\n",
      "quiet_star_policy_loss= -0.01451950054615736\n",
      "nll_loss= 1.778542160987854\n",
      "avg_std= 0.19344571232795715\n",
      "dist std min max: 0.00861844327300787 0.19344571232795715 2.9357423782348633\n",
      "hidden_states min max: -13.584671020507812 14.059366226196289\n",
      "hidden_state minus mean squared max: 142.62869262695312\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.71715545654297 3.821815013885498\n",
      "loss  1840: 1.7640   grad norm: 2.0326          model param norm: 88.1307        \n",
      "\n",
      "quiet_star_policy_loss= -0.011927986517548561\n",
      "nll_loss= 1.7893160581588745\n",
      "avg_std= 0.19348488748073578\n",
      "dist std min max: 0.008689753711223602 0.19348488748073578 2.959421157836914\n",
      "hidden_states min max: -14.17849349975586 15.495972633361816\n",
      "hidden_state minus mean squared max: 132.7928924560547\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.3611831665039 3.824005603790283\n",
      "loss  1841: 1.7774   grad norm: 2.0418          model param norm: 88.1316        \n",
      "\n",
      "quiet_star_policy_loss= -0.018763208761811256\n",
      "nll_loss= 1.7817535400390625\n",
      "avg_std= 0.19364963471889496\n",
      "dist std min max: 0.00867550354450941 0.19364963471889496 2.9328126907348633\n",
      "hidden_states min max: -19.505918502807617 14.693303108215332\n",
      "hidden_state minus mean squared max: 360.5195617675781\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.4763412475586 3.8236613273620605\n",
      "loss  1842: 1.7630   grad norm: 2.2461          model param norm: 88.1324        \n",
      "\n",
      "quiet_star_policy_loss= -0.009446240030229092\n",
      "nll_loss= 1.789009928703308\n",
      "avg_std= 0.19217807054519653\n",
      "dist std min max: 0.00859084539115429 0.19217807054519653 3.038114547729492\n",
      "hidden_states min max: -37.536827087402344 14.591053009033203\n",
      "hidden_state minus mean squared max: 1182.28173828125\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -105.0701675415039 3.833627223968506\n",
      "loss  1843: 1.7796   grad norm: 2.2108          model param norm: 88.1330        \n",
      "\n",
      "quiet_star_policy_loss= -0.00963051337748766\n",
      "nll_loss= 1.7895386219024658\n",
      "avg_std= 0.19296427071094513\n",
      "dist std min max: 0.008658451028168201 0.19296427071094513 2.9854624271392822\n",
      "hidden_states min max: -13.658773422241211 15.00521469116211\n",
      "hidden_state minus mean squared max: 119.12413024902344\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.38777160644531 3.8247108459472656\n",
      "loss  1844: 1.7799   grad norm: 2.2123          model param norm: 88.1337        \n",
      "\n",
      "quiet_star_policy_loss= -0.024311399087309837\n",
      "nll_loss= 1.7768491506576538\n",
      "avg_std= 0.19231727719306946\n",
      "dist std min max: 0.00868169404566288 0.19231727719306946 2.9460740089416504\n",
      "hidden_states min max: -13.305360794067383 17.015718460083008\n",
      "hidden_state minus mean squared max: 161.77716064453125\n",
      "hidden_state minus mean divided by std max: 5.166579723358154\n",
      "log_prob min max: -102.4070053100586 3.8235349655151367\n",
      "loss  1845: 1.7525   grad norm: 2.3449          model param norm: 88.1346        \n",
      "\n",
      "quiet_star_policy_loss= 0.011366414837539196\n",
      "nll_loss= 1.7737849950790405\n",
      "avg_std= 0.19264726340770721\n",
      "dist std min max: 0.008510828018188477 0.19264726340770721 2.9854695796966553\n",
      "hidden_states min max: -12.59804916381836 13.959051132202148\n",
      "hidden_state minus mean squared max: 120.52692413330078\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.9689712524414 3.8378396034240723\n",
      "loss  1846: 1.7852   grad norm: 2.1679          model param norm: 88.1357        \n",
      "\n",
      "quiet_star_policy_loss= -0.01026738528162241\n",
      "nll_loss= 1.7632702589035034\n",
      "avg_std= 0.19523881375789642\n",
      "dist std min max: 0.008386869914829731 0.19523881375789642 2.9343414306640625\n",
      "hidden_states min max: -12.567547798156738 13.120388984680176\n",
      "hidden_state minus mean squared max: 110.07949829101562\n",
      "hidden_state minus mean divided by std max: 5.035404682159424\n",
      "log_prob min max: -11.81431770324707 3.849243640899658\n",
      "loss  1847: 1.7530   grad norm: 4.6705          model param norm: 88.1368        \n",
      "\n",
      "quiet_star_policy_loss= -0.011049485765397549\n",
      "nll_loss= 1.7883812189102173\n",
      "avg_std= 0.1920824497938156\n",
      "dist std min max: 0.008484533987939358 0.1920824497938156 2.953892230987549\n",
      "hidden_states min max: -12.096755027770996 13.89143180847168\n",
      "hidden_state minus mean squared max: 116.65794372558594\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.7812728881836 3.8402252197265625\n",
      "loss  1848: 1.7773   grad norm: 2.0921          model param norm: 88.1385        \n",
      "\n",
      "quiet_star_policy_loss= 0.0005093574873171747\n",
      "nll_loss= 1.788326621055603\n",
      "avg_std= 0.19222190976142883\n",
      "dist std min max: 0.008245249278843403 0.19222190976142883 2.9433236122131348\n",
      "hidden_states min max: -13.408571243286133 13.727776527404785\n",
      "hidden_state minus mean squared max: 100.42070770263672\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.40357208251953 3.878263473510742\n",
      "loss  1849: 1.7888   grad norm: 2.3080          model param norm: 88.1399        \n",
      "\n",
      "quiet_star_policy_loss= -0.018011892214417458\n",
      "nll_loss= 1.791109323501587\n",
      "avg_std= 0.19227924942970276\n",
      "dist std min max: 0.008362662978470325 0.19227924942970276 2.9438159465789795\n",
      "hidden_states min max: -13.136844635009766 14.061184883117676\n",
      "hidden_state minus mean squared max: 153.9637908935547\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.20623779296875 3.851194381713867\n",
      "loss  1850: 1.7731   grad norm: 2.1389          model param norm: 88.1410        \n",
      "\n",
      "quiet_star_policy_loss= -0.0007870197296142578\n",
      "nll_loss= 1.7660541534423828\n",
      "avg_std= 0.19135725498199463\n",
      "dist std min max: 0.008192924782633781 0.19135725498199463 2.947319269180298\n",
      "hidden_states min max: -13.139793395996094 14.773663520812988\n",
      "hidden_state minus mean squared max: 112.60528564453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -13.495610237121582 3.868720531463623\n",
      "loss  1851: 1.7653   grad norm: 2.1413          model param norm: 88.1420        \n",
      "\n",
      "quiet_star_policy_loss= 0.00025887490483000875\n",
      "nll_loss= 1.7741034030914307\n",
      "avg_std= 0.18970777094364166\n",
      "dist std min max: 0.007790585048496723 0.18970777094364166 2.9611916542053223\n",
      "hidden_states min max: -13.33798599243164 14.098319053649902\n",
      "hidden_state minus mean squared max: 141.83590698242188\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.61199188232422 3.9258041381835938\n",
      "loss  1852: 1.7744   grad norm: 2.2817          model param norm: 88.1428        \n",
      "\n",
      "quiet_star_policy_loss= -0.004590713884681463\n",
      "nll_loss= 1.7744568586349487\n",
      "avg_std= 0.19038429856300354\n",
      "dist std min max: 0.008103562518954277 0.19038429856300354 3.0056145191192627\n",
      "hidden_states min max: -12.394508361816406 15.223804473876953\n",
      "hidden_state minus mean squared max: 129.00125122070312\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.65058135986328 3.8787097930908203\n",
      "loss  1853: 1.7699   grad norm: 2.1468          model param norm: 88.1437        \n",
      "\n",
      "quiet_star_policy_loss= -0.02018272876739502\n",
      "nll_loss= 1.7777025699615479\n",
      "avg_std= 0.1893419623374939\n",
      "dist std min max: 0.007843301631510258 0.1893419623374939 3.052804946899414\n",
      "hidden_states min max: -13.516510009765625 14.391265869140625\n",
      "hidden_state minus mean squared max: 116.15040588378906\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.23175811767578 3.9089584350585938\n",
      "loss  1854: 1.7575   grad norm: 2.2046          model param norm: 88.1445        \n",
      "\n",
      "quiet_star_policy_loss= -0.004814338870346546\n",
      "nll_loss= 1.7822468280792236\n",
      "avg_std= 0.19000163674354553\n",
      "dist std min max: 0.007902250625193119 0.19000163674354553 3.012051820755005\n",
      "hidden_states min max: -12.275630950927734 13.633614540100098\n",
      "hidden_state minus mean squared max: 121.73345184326172\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.80980682373047 3.9201712608337402\n",
      "loss  1855: 1.7774   grad norm: 2.1405          model param norm: 88.1454        \n",
      "\n",
      "quiet_star_policy_loss= -0.009678698144853115\n",
      "nll_loss= 1.7829102277755737\n",
      "avg_std= 0.18961040675640106\n",
      "dist std min max: 0.007562915328890085 0.18961040675640106 3.094834089279175\n",
      "hidden_states min max: -15.226581573486328 15.73022747039795\n",
      "hidden_state minus mean squared max: 270.53497314453125\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -104.33277893066406 3.9536638259887695\n",
      "loss  1856: 1.7732   grad norm: 2.2130          model param norm: 88.1464        \n",
      "\n",
      "quiet_star_policy_loss= 0.0008780598873272538\n",
      "nll_loss= 1.795960783958435\n",
      "avg_std= 0.18867473304271698\n",
      "dist std min max: 0.007692317478358746 0.18867473304271698 3.012373447418213\n",
      "hidden_states min max: -12.859535217285156 13.994003295898438\n",
      "hidden_state minus mean squared max: 121.4959487915039\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.99311065673828 3.926661968231201\n",
      "loss  1857: 1.7968   grad norm: 2.2929          model param norm: 88.1476        \n",
      "\n",
      "quiet_star_policy_loss= -0.009865164756774902\n",
      "nll_loss= 1.7900028228759766\n",
      "avg_std= 0.1897282600402832\n",
      "dist std min max: 0.00785511452704668 0.1897282600402832 3.1304807662963867\n",
      "hidden_states min max: -12.216850280761719 14.068784713745117\n",
      "hidden_state minus mean squared max: 106.14557647705078\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.55586242675781 3.907318115234375\n",
      "loss  1858: 1.7801   grad norm: 2.1749          model param norm: 88.1488        \n",
      "\n",
      "quiet_star_policy_loss= -0.018783235922455788\n",
      "nll_loss= 1.773254632949829\n",
      "avg_std= 0.18933261930942535\n",
      "dist std min max: 0.007495205383747816 0.18933261930942535 3.0027451515197754\n",
      "hidden_states min max: -12.127494812011719 14.747077941894531\n",
      "hidden_state minus mean squared max: 132.04345703125\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.02662658691406 3.9636411666870117\n",
      "loss  1859: 1.7545   grad norm: 2.3126          model param norm: 88.1502        \n",
      "\n",
      "quiet_star_policy_loss= 0.014668864198029041\n",
      "nll_loss= 1.7973498106002808\n",
      "avg_std= 0.1894231140613556\n",
      "dist std min max: 0.007600810844451189 0.1894231140613556 3.0083134174346924\n",
      "hidden_states min max: -12.698623657226562 14.612662315368652\n",
      "hidden_state minus mean squared max: 127.82640838623047\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -102.97982788085938 3.9409356117248535\n",
      "loss  1860: 1.8120   grad norm: 2.6333          model param norm: 88.1515        \n",
      "\n",
      "quiet_star_policy_loss= -0.01462943572551012\n",
      "nll_loss= 1.7954124212265015\n",
      "avg_std= 0.18912483751773834\n",
      "dist std min max: 0.007506385911256075 0.18912483751773834 3.0433273315429688\n",
      "hidden_states min max: -12.414037704467773 14.789765357971191\n",
      "hidden_state minus mean squared max: 128.47332763671875\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.86447143554688 3.971388816833496\n",
      "loss  1861: 1.7808   grad norm: 2.3939          model param norm: 88.1527        \n",
      "\n",
      "quiet_star_policy_loss= -0.0028118491172790527\n",
      "nll_loss= 1.7994245290756226\n",
      "avg_std= 0.18796418607234955\n",
      "dist std min max: 0.007713632192462683 0.18796418607234955 3.037548542022705\n",
      "hidden_states min max: -12.516729354858398 16.063861846923828\n",
      "hidden_state minus mean squared max: 161.49429321289062\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -101.90388488769531 3.9253883361816406\n",
      "loss  1862: 1.7966   grad norm: 2.2462          model param norm: 88.1541        \n",
      "\n",
      "quiet_star_policy_loss= -0.0166913028806448\n",
      "nll_loss= 1.768823266029358\n",
      "avg_std= 0.1899523288011551\n",
      "dist std min max: 0.007829098962247372 0.1899523288011551 3.042811393737793\n",
      "hidden_states min max: -12.093636512756348 13.99738883972168\n",
      "hidden_state minus mean squared max: 110.71530151367188\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.78457641601562 3.9220919609069824\n",
      "loss  1863: 1.7521   grad norm: 2.4413          model param norm: 88.1555        \n",
      "\n",
      "quiet_star_policy_loss= 0.004228663630783558\n",
      "nll_loss= 1.7938823699951172\n",
      "avg_std= 0.18813079595565796\n",
      "dist std min max: 0.007743372116237879 0.18813079595565796 3.0589091777801514\n",
      "hidden_states min max: -12.098955154418945 16.256589889526367\n",
      "hidden_state minus mean squared max: 148.2973175048828\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.858642578125 3.9229931831359863\n",
      "loss  1864: 1.7981   grad norm: 2.3076          model param norm: 88.1573        \n",
      "\n",
      "quiet_star_policy_loss= -0.014554900117218494\n",
      "nll_loss= 1.7958897352218628\n",
      "avg_std= 0.18748001754283905\n",
      "dist std min max: 0.007743253838270903 0.18748001754283905 3.0455515384674072\n",
      "hidden_states min max: -13.251548767089844 14.147743225097656\n",
      "hidden_state minus mean squared max: 115.81537628173828\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.93994140625 3.9264578819274902\n",
      "loss  1865: 1.7813   grad norm: 2.2306          model param norm: 88.1589        \n",
      "\n",
      "quiet_star_policy_loss= 0.007572388742119074\n",
      "nll_loss= 1.7800981998443604\n",
      "avg_std= 0.18776878714561462\n",
      "dist std min max: 0.008162272162735462 0.18776878714561462 3.215930938720703\n",
      "hidden_states min max: -12.691062927246094 14.355460166931152\n",
      "hidden_state minus mean squared max: 111.49824523925781\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -103.2686538696289 3.87570858001709\n",
      "loss  1866: 1.7877   grad norm: 2.3203          model param norm: 88.1602        \n",
      "\n",
      "quiet_star_policy_loss= -0.023149728775024414\n",
      "nll_loss= 1.7821025848388672\n",
      "avg_std= 0.1883905977010727\n",
      "dist std min max: 0.007770506199449301 0.1883905977010727 3.0414931774139404\n",
      "hidden_states min max: -11.697677612304688 15.060611724853516\n",
      "hidden_state minus mean squared max: 165.0297393798828\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -104.08563232421875 3.895425319671631\n",
      "loss  1867: 1.7590   grad norm: 2.2294          model param norm: 88.1614        \n",
      "\n",
      "quiet_star_policy_loss= -0.009015226736664772\n",
      "nll_loss= 1.7928764820098877\n",
      "avg_std= 0.18709301948547363\n",
      "dist std min max: 0.008351114578545094 0.18709301948547363 3.1258814334869385\n",
      "hidden_states min max: -11.382129669189453 15.0045747756958\n",
      "hidden_state minus mean squared max: 136.2752227783203\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -102.12715148925781 3.8489174842834473\n",
      "loss  1868: 1.7839   grad norm: 2.2228          model param norm: 88.1625        \n",
      "\n",
      "quiet_star_policy_loss= -0.023876523599028587\n",
      "nll_loss= 1.8000777959823608\n",
      "avg_std= 0.1873203068971634\n",
      "dist std min max: 0.008555847220122814 0.1873203068971634 3.2255828380584717\n",
      "hidden_states min max: -11.269855499267578 14.46358871459961\n",
      "hidden_state minus mean squared max: 112.03534698486328\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -101.70484161376953 3.8333301544189453\n",
      "loss  1869: 1.7762   grad norm: 2.2067          model param norm: 88.1634        \n",
      "\n",
      "quiet_star_policy_loss= -0.005101633258163929\n",
      "nll_loss= 1.7860225439071655\n",
      "avg_std= 0.18745076656341553\n",
      "dist std min max: 0.008649451658129692 0.18745076656341553 3.001911163330078\n",
      "hidden_states min max: -13.326007843017578 13.748685836791992\n",
      "hidden_state minus mean squared max: 110.69254302978516\n",
      "hidden_state minus mean divided by std max: 5.166579246520996\n",
      "log_prob min max: -102.42345428466797 3.818103313446045\n",
      "loss  1870: 1.7809   grad norm: 2.2567          model param norm: 88.1644        \n",
      "\n",
      "quiet_star_policy_loss= -0.017567826434969902\n",
      "nll_loss= 1.7914577722549438\n",
      "avg_std= 0.18768295645713806\n",
      "dist std min max: 0.008351556025445461 0.18768295645713806 2.99827241897583\n",
      "hidden_states min max: -11.509003639221191 15.065096855163574\n",
      "hidden_state minus mean squared max: 137.81564331054688\n",
      "hidden_state minus mean divided by std max: 5.035406112670898\n",
      "log_prob min max: -102.33222961425781 3.8653526306152344\n",
      "loss  1871: 1.7739   grad norm: 2.2230          model param norm: 88.1656        \n",
      "\n",
      "quiet_star_policy_loss= -0.009143590927124023\n",
      "nll_loss= 1.7923301458358765\n",
      "avg_std= 0.18803273141384125\n",
      "dist std min max: 0.008775792084634304 0.18803273141384125 3.07490611076355\n",
      "hidden_states min max: -13.305624008178711 16.44635009765625\n",
      "hidden_state minus mean squared max: 178.5695343017578\n",
      "hidden_state minus mean divided by std max: 5.166582107543945\n",
      "log_prob min max: -104.12506866455078 3.799582004547119\n",
      "loss  1872: 1.7832   grad norm: 2.1737          model param norm: 88.1670        \n",
      "\n",
      "quiet_star_policy_loss= -0.0149392606690526\n",
      "nll_loss= 1.7726467847824097\n",
      "avg_std= 0.18854008615016937\n",
      "dist std min max: 0.008312231861054897 0.18854008615016937 2.9226109981536865\n",
      "hidden_states min max: -12.476853370666504 13.97092342376709\n",
      "hidden_state minus mean squared max: 121.35563659667969\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -103.14958953857422 3.864762783050537\n",
      "loss  1873: 1.7577   grad norm: 2.2677          model param norm: 88.1684        \n",
      "\n",
      "quiet_star_policy_loss= 0.0001685261813690886\n",
      "nll_loss= 1.7824795246124268\n",
      "avg_std= 0.18897387385368347\n",
      "dist std min max: 0.008650784380733967 0.18897387385368347 2.91636061668396\n",
      "hidden_states min max: -12.243467330932617 14.07690715789795\n",
      "hidden_state minus mean squared max: 132.5705108642578\n",
      "hidden_state minus mean divided by std max: 5.16657829284668\n",
      "log_prob min max: -102.9049301147461 3.8202552795410156\n",
      "loss  1874: 1.7826   grad norm: 2.2530          model param norm: 88.1697        \n",
      "\n",
      "quiet_star_policy_loss= 0.007305806502699852\n",
      "nll_loss= 1.748170018196106\n",
      "avg_std= 0.18754437565803528\n",
      "dist std min max: 0.009276967495679855 0.18754437565803528 2.8989298343658447\n",
      "hidden_states min max: -12.776357650756836 12.55990982055664\n",
      "hidden_state minus mean squared max: 106.94418334960938\n",
      "hidden_state minus mean divided by std max: 5.03540563583374\n",
      "log_prob min max: -102.71730041503906 3.7261390686035156\n",
      "loss  1875: 1.7555   grad norm: 4.2407          model param norm: 88.1712        \n",
      "\n",
      "quiet_star_policy_loss= 0.011354065500199795\n",
      "nll_loss= 1.7874828577041626\n",
      "avg_std= 0.1887962371110916\n",
      "dist std min max: 0.008852156810462475 0.1887962371110916 2.939277410507202\n",
      "hidden_states min max: -13.021402359008789 13.541674613952637\n",
      "hidden_state minus mean squared max: 117.70204162597656\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.0773696899414 3.803150177001953\n",
      "loss  1876: 1.7988   grad norm: 2.1062          model param norm: 88.1724        \n",
      "\n",
      "quiet_star_policy_loss= -0.009833288379013538\n",
      "nll_loss= 1.7922347784042358\n",
      "avg_std= 0.18751166760921478\n",
      "dist std min max: 0.009475012309849262 0.18751166760921478 3.0258359909057617\n",
      "hidden_states min max: -12.068113327026367 14.963491439819336\n",
      "hidden_state minus mean squared max: 135.21075439453125\n",
      "hidden_state minus mean divided by std max: 5.166581153869629\n",
      "log_prob min max: -103.50435638427734 3.7332282066345215\n",
      "loss  1877: 1.7824   grad norm: 2.1954          model param norm: 88.1736        \n",
      "\n",
      "quiet_star_policy_loss= -0.00762596121057868\n",
      "nll_loss= 1.7738851308822632\n",
      "avg_std= 0.19092917442321777\n",
      "dist std min max: 0.009385636076331139 0.19092917442321777 2.878833055496216\n",
      "hidden_states min max: -12.709771156311035 13.306356430053711\n",
      "hidden_state minus mean squared max: 109.84306335449219\n",
      "hidden_state minus mean divided by std max: 5.166578769683838\n",
      "log_prob min max: -103.82198333740234 3.732182025909424\n",
      "loss  1878: 1.7663   grad norm: 2.3350          model param norm: 88.1752        \n",
      "\n",
      "quiet_star_policy_loss= 0.008582079783082008\n",
      "nll_loss= 1.7965167760849\n",
      "avg_std= 0.1887034773826599\n",
      "dist std min max: 0.009142857976257801 0.1887034773826599 3.056704044342041\n",
      "hidden_states min max: -12.662041664123535 15.207152366638184\n",
      "hidden_state minus mean squared max: 115.38017272949219\n",
      "hidden_state minus mean divided by std max: 5.166580677032471\n",
      "log_prob min max: -102.84724426269531 3.7325005531311035\n",
      "loss  1879: 1.8051   grad norm: 2.2604          model param norm: 88.1768        \n",
      "\n",
      "quiet_star_policy_loss= -0.010071063414216042\n",
      "nll_loss= 1.775504469871521\n",
      "avg_std= 0.18966491520404816\n",
      "dist std min max: 0.009472907520830631 0.18966491520404816 3.0984251499176025\n",
      "hidden_states min max: -13.200054168701172 14.154671669006348\n",
      "hidden_state minus mean squared max: 118.24154663085938\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.1526870727539 3.7388052940368652\n",
      "loss  1880: 1.7654   grad norm: 2.2304          model param norm: 88.1782        \n",
      "\n",
      "quiet_star_policy_loss= -0.0025789260398596525\n",
      "nll_loss= 1.7832962274551392\n",
      "avg_std= 0.18856965005397797\n",
      "dist std min max: 0.009127755649387836 0.18856965005397797 3.0848793983459473\n",
      "hidden_states min max: -12.787574768066406 14.32365894317627\n",
      "hidden_state minus mean squared max: 114.12178039550781\n",
      "hidden_state minus mean divided by std max: 5.1665778160095215\n",
      "log_prob min max: -103.90121459960938 3.7463908195495605\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mipdb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ipdb\u001b[38;5;241m.\u001b[39mlaunch_ipdb_on_exception():\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_quiet_star_loss_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_nll\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQuietStarLanguageModelrGRU\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrgruhtilda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# epochs=1, batch_size=8000)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# eval loss 1.7922298908233643 for gru h_tilda 0.0871 std avg, got 1.75 with 0.12 std avg \u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#           with 10 samples gets to 1.66 loss, and 0.04 avg std. takes 23 minutes\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# for r gru h, seems semi stable with 10 samples. kind of similar to reparam. have restricted variance, so no reaching zero, but can still be unstable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#           optima that we are able to randomly sample towards. But we know this isn't true, and that there is a ton of symetry, \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#           but perhaps our starting distirbution can only sample one of the optima, and we just find that one?\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[119], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(get_loss_fn, eval_fn, model, batch_size, epochs, train_dl, eval_every, print_stuff)\u001b[0m\n\u001b[1;32m     40\u001b[0m     eval_losses\u001b[38;5;241m.\u001b[39mappend((i, eval_loss))\n\u001b[1;32m     41\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 42\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_stuff:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_quiet_star_loss_partial = partial(get_quiet_star_loss, policy_loss_beta=1, trice_samples=10, n_tokens_ahead=100000) # TODO: Change learning rate?? smaller because more influence of loss from early hidden representations is increased, potentially unstable?\n",
    "import ipdb\n",
    "\n",
    "with ipdb.launch_ipdb_on_exception():\n",
    "    train_model(get_quiet_star_loss_partial, lambda model: eval_loss_fn(model, get_nll), QuietStarLanguageModelrGRU(len(vocab), 100, 1, 'rgruhtilda').to(device), epochs=100) # epochs=1, batch_size=8000)\n",
    "\n",
    "# eval loss 1.7922298908233643 for gru h_tilda 0.0871 std avg, got 1.75 with 0.12 std avg \n",
    "#           with 10 samples gets to 1.66 loss, and 0.04 avg std. takes 23 minutes\n",
    "# for r gru h, seems semi stable with 10 samples. kind of similar to reparam. have restricted variance, so no reaching zero, but can still be unstable.\n",
    "#           Need to train fully with GPU, using mps it takes near an hour.\n",
    "\n",
    "# need to change the number of tokens ahead to be like all of them? Should I normalize wrt length? No, should just divide the nll per token by number of tokens, then do reverse cumsum\n",
    "# can use flip(dims=(1,)).cumsum(1).flip(dims=(1,))\n",
    "\n",
    "# is it possible to just use the version of the codebase that I developed to be model agnostic to work with this setting of creating a rationalization with a random linear layer?\n",
    "#           maybe, would require some abstraction over the generation procedure instead of just taking it as tokens, but this would be relatively ok to do. Then would test the exact quiet-star setting.\n",
    "#           Would this introduce complexity? - Each prefix would generate independantly and next token prediction would have to be done like quiet-star. I am just interested in NLL reduction based on some rationalization, and don't care too much if it is at the beginning or if it is throughout the generation process.\n",
    "#           Kind of like the idea of producing a rationalization specific to each prefix, but generating per token, and studying this optimization setting still is nice for learning about the single prefix optimization setting, just potentially the results might not transfer, like it might not work here, and still work there.\n",
    "#           Is it worth continuing with this idea? what do I gain? seeing if the reward model distillation idea has any viability. \n",
    "#           I think it does. Just have to get the model signal for its wide impact. Also might be necessary to improve the probability \n",
    "#           density model over randomly sampled state. Could choose to randomly sample from some vector set or something. \n",
    "#           This is just if the DPO doesn't seem to be working nicely tho... Fingers crossed that the optimization landscape has one clear \n",
    "#           optima that we are able to randomly sample towards. But we know this isn't true, and that there is a ton of symetry, \n",
    "#           but perhaps our starting distirbution can only sample one of the optima, and we just find that one?"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results from replications of quiet-star algorithm\n",
    "normal llm algorithm able to get 1.65 eval nll with 100 epochs, 100 hidden dim.\n",
    "\n",
    "policy loss beta = 1, with 100 epochs, and 100 hidden dim. 1000 hidden dim somehow performs worse. eval nll ~1.81 for two tokens -> 1.86\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "policy loss beta = 1000000, with 100 epochs, and 100 hidden. eval nll ~1.805 for two tokens -> 1.85\n",
    "\n",
    "![image-2.png](attachment:image-2.png)\n",
    "\n",
    "modifying the n_tokens_ahead from 1 to 2 to 16 had no effect. in fact at 16 it performed worse. notably, I don't modify the parameters of the lm head for those future tokens, unlike quiet-star which modifies the parameters of the base language model based on using the hidden representation context vector.\n",
    "\n",
    "may want to create nll graph and record the mean nll from eval because it is the metric I care about?\n",
    "\n",
    "when only training with positive rewards above the trice mean, there is a noticable drop in performance:\n",
    "eval loss 1.97 for both two tokens ahead and 1 token ahead, with policy loss beta 1. For plb 1000000  eval loss 2.07 Could point to substantial optimization room left on the table by not using the negative rewards. (they note this is for stability, so we could fix it with a reference policy (?). Also could use DPO method, but DPO method still needs to be proven that we can train an ok reward model. There are also probably other PPO works which deal with the fact of poor negative reward performance)\n",
    "\n",
    "get_quiet_star_loss_partial = partial(get_quiet_star_loss, policy_loss_beta=1, trice_samples=2, n_tokens_ahead=1)\n",
    "\n",
    "train_model(get_nll, lambda model: eval_loss_fn(model, get_quiet_star_loss_partial), QuietStarLanguageModelLSTM(len(vocab), 100, 1, reparameterization_trick=True).to(device), epochs=100)\n",
    "\n",
    "with reparameterization trick, and just training on NLL loss (because the score function trick isn't required with reparam), we can get eval loss 1.67, further more the avg std goes to about 0.009. This would be the equivelent of a gumbel trick. Doesn't nicely translate to language setting. (but why do they have different performances in the first place? shouldn't the expected gradient be the same? Answer: they are you just need to do more samples in the expectation for the score function trick to get good performance tho. maybe)\n",
    "\n",
    "can get 1.72 with 10 samples. trying with only positives gets 2.3 very sad! got 1.698 with 20 samples avg std goes to 0.014, normally with 2 it goes to 0.027\n",
    "\n",
    "For the DPO reward function, we are not just in the single next token setting, we are trying to get an idea of the performance achievable from some given latent representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dpo_loss(model: QuietStarLanguageModelLSTM, inputs: torch.Tensor, beta_2: float=1, target_model=None, num_samples=2, dpo_loss_beta:float=1, nll_loss_beta:float=1, forward_kl_reward:bool=True, reward_model=None, train_reward_model=None):\n",
    "    # need a reward model, or if the other model is defined, I can use this as a reward model, and match with it.\n",
    "    inputs = inputs.to(device)\n",
    "    labels = inputs.clone()\n",
    "    original_batch_size = inputs.size(0)\n",
    "    repeat_inputs = inputs.repeat_interleave(num_samples, dim=0)\n",
    "    repeat_labels = labels.repeat_interleave(num_samples, dim=0)\n",
    "    repeat_logits, repeat_hidden_states, repeat_log_prob_hidden_states, dist = model.get_logits_and_hidden_states_and_log_prob_hidden_states_dist(repeat_inputs)\n",
    "    repeat_log_prob_hidden_states = repeat_log_prob_hidden_states.sum(-1)\n",
    "    nll_loss = get_nll_from_logits_and_labels(repeat_logits, repeat_labels)\n",
    "    if target_model is not None:\n",
    "        with torch.no_grad():\n",
    "            target_model_repeat_logits = target_model(repeat_inputs)\n",
    "            if forward_kl_reward:\n",
    "                repeat_reward = (torch.softmax(target_model_repeat_logits, dim=-1) * torch.log_softmax(repeat_logits, dim=-1)).sum(-1)\n",
    "            else: # reverse KL didn't show much promise, just ignored what it was getting wrong I think. pretty piss poor exploration. the entropy encouragement beta_2 doesn't really help.\n",
    "                repeat_reward = (torch.softmax(repeat_logits, dim=-1) * (torch.log_softmax(target_model_repeat_logits, dim=-1) - torch.log_softmax(repeat_logits, dim=-1))).sum(-1)\n",
    "    elif reward_model is not None and train_reward_model is not None:\n",
    "        # train the reward model every so often to ensure it remains relevant to the current lm head and hidden state distribution.\n",
    "        train_reward_model(reward_model, model)\n",
    "        with torch.no_grad():\n",
    "            repeat_reward = reward_model(repeat_inputs, repeat_hidden_states)\n",
    "    else:\n",
    "        raise ValueError(\"must define either reward model or target model to train with DPO loss, i.e. need some way to get reward.\")\n",
    "    repeat_reward = repeat_reward.view(original_batch_size, num_samples, -1)\n",
    "    # this for numerical stability.\n",
    "    repeat_reward = repeat_reward - repeat_reward.max(dim=1, keepdim=True).values\n",
    "    repeat_exp_reward = (repeat_reward).exp()\n",
    "    repeat_gold_action_weight = repeat_exp_reward / repeat_exp_reward.sum(1, keepdim=True)\n",
    "    repeat_log_prob_hidden_states = repeat_log_prob_hidden_states.view(original_batch_size, num_samples, -1)\n",
    "    repeat_log_prob_hidden_states_divided_by_max_hidden_state_prob = repeat_log_prob_hidden_states - repeat_log_prob_hidden_states.max(1, keepdim=True).values # need to get the sum of the probabilities in the denominator\n",
    "    repeat_beta2_log_prob_hidden_states_divided_by_max_hidden_state_prob = beta_2 * repeat_log_prob_hidden_states_divided_by_max_hidden_state_prob\n",
    "    repeat_log_weight_on_hidden_states = repeat_beta2_log_prob_hidden_states_divided_by_max_hidden_state_prob - repeat_beta2_log_prob_hidden_states_divided_by_max_hidden_state_prob.exp().sum(1, keepdim=True).log()\n",
    "    dpo_loss = -(repeat_gold_action_weight.detach() * repeat_log_weight_on_hidden_states).sum(1).mean()\n",
    "    loss = dpo_loss * dpo_loss_beta + nll_loss * nll_loss_beta\n",
    "    print(f\"{dpo_loss= }\")\n",
    "    print(f\"{nll_loss= }\")\n",
    "    avg_std = dist.scale.mean()\n",
    "    print(f\"{avg_std= }\")\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(rnn_lm.cpu(), open(\"rnn_lm_1.51_eval_1.5_train.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_lm = pickle.load(open(\"rnn_lm_1.51_eval_1.5_train.pkl\", 'rb')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training steps total: 2800\n",
      "eval loss 4.366241931915283\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 0     6935.7587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1     6935.734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2     6935.71875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 3     6935.6904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 4     6935.6669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 5     6935.63623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 6     6935.6005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.1697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 7     6935.5703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.1225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 8     6935.5341796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.0730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 9     6935.4814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.0190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 10    6935.4306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.9535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 11    6935.365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.8727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 12    6935.2744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.7821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 13    6935.2021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 14    6935.0986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.8978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 15    6935.00439453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.8759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 16    6934.9296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.8526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 17    6934.90087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.8275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 18    6934.8701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.8056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 19    6934.8642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 20    6934.85888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7744, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 21    6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 22    6934.80322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 23    6934.75927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 24    6934.74609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2857, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 25    6934.724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 26    6934.7021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 27    6934.68115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 28    6934.69921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 29    6934.68798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 30    6934.66796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.2154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 31    6934.65625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.7054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 32    6934.6376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 33    6934.62451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 34    6934.61865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 35    6934.5966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 36    6934.58935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 37    6934.564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.1121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 38    6934.5625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 39    6934.53759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 40    6934.5126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 41    6934.50634765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 42    6934.5068359375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 43    6934.47607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.0188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 44    6934.4677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 45    6934.451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 46    6934.42529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 47    6934.4248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 48    6934.40869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.6008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 49    6934.41650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 50    6934.37890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 51    6934.37060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.9119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 52    6934.36474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 53    6934.3408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 54    6934.3349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 55    6934.3095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 56    6934.30810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 57    6934.30078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 58    6934.2880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.8046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 59    6934.2548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 60    6934.23974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 61    6934.22900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 62    6934.21435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 63    6934.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 64    6934.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 65    6934.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 66    6934.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 67    6934.18359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 68    6934.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 69    6934.15771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 70    6934.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 71    6934.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 72    6934.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 73    6934.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 74    6934.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.5017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 75    6934.0947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 76    6934.09814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6151, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 77    6934.07275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 78    6934.0869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6042, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 79    6934.06640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.6035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 80    6934.06005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 81    6934.04150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 82    6934.0263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 83    6934.009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 84    6934.02587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 85    6934.0419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 86    6934.02880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 87    6934.015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4595, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 88    6933.99267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 89    6933.9990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 90    6933.9931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 91    6933.99560546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 92    6933.99267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 93    6933.96923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 94    6933.982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4428, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 95    6933.96826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 96    6933.9609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.5004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 97    6933.96044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4379, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 98    6933.951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4357, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 99    6933.95068359375\n",
      "eval loss 2.4891104698181152\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 100   6933.951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 101   6933.94384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 102   6933.9384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 103   6933.92822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 104   6933.93017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 105   6933.92626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 106   6933.919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 107   6933.91748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 108   6933.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 109   6933.88916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 110   6933.89013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 111   6933.90966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.4008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 112   6933.880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 113   6933.88916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 114   6933.87646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 115   6933.8798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 116   6933.873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 117   6933.8759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.4093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 118   6933.87451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 119   6933.8564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 120   6933.8544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 121   6933.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 122   6933.84814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 123   6933.8515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 124   6933.84375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3938, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 125   6933.85400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 126   6933.84814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3748, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 127   6933.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 128   6933.84375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 129   6933.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 130   6933.8388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 131   6933.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 132   6933.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 133   6933.806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 134   6933.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 135   6933.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 136   6933.8046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 137   6933.80517578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 138   6933.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 139   6933.845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 140   6933.79541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3476, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 141   6933.79736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3410, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 142   6933.7900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 143   6933.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 144   6933.7822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 145   6933.78759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 146   6933.78173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 147   6933.78857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 148   6933.77197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 149   6933.77734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 150   6933.7685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3131, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 151   6933.77880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3268, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 152   6933.77001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 153   6933.7578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 154   6933.75146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 155   6933.76953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.3014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 156   6933.76416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 157   6933.75537109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 158   6933.74072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 159   6933.75146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 160   6933.73681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 161   6933.74658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 162   6933.7568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3204, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 163   6933.73193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 164   6933.73828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 165   6933.73828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 166   6933.74609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 167   6933.7197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 168   6933.724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 169   6933.74462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 170   6933.7119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 171   6933.7255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 172   6933.7294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 173   6933.7373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 174   6933.7119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 175   6933.71923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 176   6933.71484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 177   6933.697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 178   6933.71875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2310, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 179   6933.69677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 180   6933.712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.3010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 181   6933.69970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 182   6933.701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 183   6933.70361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 184   6933.70068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 185   6933.70166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 186   6933.6865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 187   6933.68359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 188   6933.69775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 189   6933.69189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 190   6933.67919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 191   6933.68212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 192   6933.67333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 193   6933.6689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2103, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 194   6933.67236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 195   6933.681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 196   6933.6708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 197   6933.68359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 198   6933.6640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 199   6933.6689453125\n",
      "eval loss 2.201657295227051\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.2087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 200   6933.67578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 201   6933.66015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 202   6933.66357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 203   6933.65283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 204   6933.65625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 205   6933.654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 206   6933.64990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 207   6933.65234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 208   6933.64892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 209   6933.6484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 210   6933.64794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 211   6933.65673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 212   6933.6416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 213   6933.6435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 214   6933.62939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2689, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 215   6933.64892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 216   6933.6533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 217   6933.64013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 218   6933.6357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 219   6933.63427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 220   6933.62646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 221   6933.62451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2641, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 222   6933.6396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 223   6933.63427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 224   6933.630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 225   6933.634765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 226   6933.62890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 227   6933.62353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 228   6933.60986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 229   6933.63134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 230   6933.60888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 231   6933.60546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2570, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 232   6933.6103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 233   6933.60595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 234   6933.6083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 235   6933.6083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 236   6933.60888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 237   6933.6259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 238   6933.60693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2550, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 239   6933.59716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 240   6933.595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 241   6933.6064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 242   6933.5966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 243   6933.60107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 244   6933.5771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 245   6933.59912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 246   6933.59033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 247   6933.5927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2468, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 248   6933.61376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 249   6933.583984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 250   6933.59716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 251   6933.59228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 252   6933.58642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 253   6933.57666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 254   6933.58837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 255   6933.57958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2429, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 256   6933.5849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 257   6933.57568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2440, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 258   6933.5859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 259   6933.57666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 260   6933.5771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2415, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 261   6933.58837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 262   6933.54931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 263   6933.59130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 264   6933.5771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 265   6933.5791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 266   6933.5791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2398, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 267   6933.572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 268   6933.54248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0911, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 269   6933.56005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 270   6933.55615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 271   6933.54443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 272   6933.56884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 273   6933.5576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2377, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 274   6933.5751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 275   6933.5556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.1018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 276   6933.5673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 277   6933.55126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 278   6933.544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 279   6933.56640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 280   6933.54833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 281   6933.5478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 282   6933.54638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 283   6933.5380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 284   6933.5654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 285   6933.546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 286   6933.54052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 287   6933.5478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 288   6933.54443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 289   6933.54345703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 290   6933.54833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 291   6933.5224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 292   6933.53076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 293   6933.5458984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 294   6933.5380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 295   6933.5361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 296   6933.5244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 297   6933.5185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 298   6933.53515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 299   6933.5224609375\n",
      "eval loss 2.0623865127563477\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 300   6933.56640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 301   6933.5390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 302   6933.5283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 303   6933.5224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2307, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 304   6933.53515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 305   6933.5224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2287, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 306   6933.5322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 307   6933.5185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 308   6933.5361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 309   6933.51953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 310   6933.5068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 311   6933.5185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 312   6933.51318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 313   6933.5146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2227, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 314   6933.521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2232, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 315   6933.525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 316   6933.5107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 317   6933.51953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 318   6933.5146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 319   6933.52587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 320   6933.5107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 321   6933.501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 322   6933.5087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 323   6933.51171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 324   6933.513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 325   6933.501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 326   6933.4990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 327   6933.51806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0410, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 328   6933.51025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 329   6933.5\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 330   6933.486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 331   6933.50830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 332   6933.4912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 333   6933.49755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 334   6933.48974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 335   6933.47705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 336   6933.48974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 337   6933.51220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 338   6933.4833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 339   6933.49365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 340   6933.50244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 341   6933.50341796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 342   6933.48876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 343   6933.48095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 344   6933.4873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 345   6933.49853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 346   6933.484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 347   6933.4765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 348   6933.47021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 349   6933.47265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 350   6933.4951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 351   6933.4912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 352   6933.48291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 353   6933.4892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 354   6933.47021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 355   6933.49267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 356   6933.47900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 357   6933.46630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 358   6933.46435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 359   6933.47119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 360   6933.486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0124, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 361   6933.4755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 362   6933.46728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 363   6933.47900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.2010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 364   6933.46142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 365   6933.47119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 366   6933.482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 367   6933.4609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 368   6933.45849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 369   6933.45361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 370   6933.4619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0006, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 371   6933.46630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 372   6933.45849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 373   6933.45654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 374   6933.4677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 375   6933.4599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 376   6933.470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 377   6933.47412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 378   6933.4716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 379   6933.45947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 380   6933.4599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 381   6933.453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 382   6933.46142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 383   6933.44091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 384   6933.46435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 385   6933.4462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 386   6933.4599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 387   6933.45703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 388   6933.453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 389   6933.45751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 390   6933.46240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(2.0005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 391   6933.4697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 392   6933.43310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 393   6933.44140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 394   6933.4658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 395   6933.4423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 396   6933.44970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 397   6933.44921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 398   6933.44677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 399   6933.44287109375\n",
      "eval loss 1.9782116413116455\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 400   6933.4560546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 401   6933.44970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 402   6933.439453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 403   6933.4384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9748, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 404   6933.4423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 405   6933.4326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 406   6933.4599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 407   6933.43359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 408   6933.42529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 409   6933.45068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 410   6933.45068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 411   6933.443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 412   6933.42333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 413   6933.42822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 414   6933.4306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 415   6933.44580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 416   6933.43505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 417   6933.43115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 418   6933.4326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 419   6933.41845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 420   6933.4248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 421   6933.45068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 422   6933.4228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 423   6933.41552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 424   6933.43994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 425   6933.4296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 426   6933.4267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 427   6933.4248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 428   6933.43603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 429   6933.42529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 430   6933.4150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 431   6933.412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1828, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 432   6933.43603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 433   6933.404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 434   6933.42724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 435   6933.41650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 436   6933.42138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 437   6933.4150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1814, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 438   6933.4228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1820, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 439   6933.40966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 440   6933.4375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 441   6933.41259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 442   6933.40625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 443   6933.42431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 444   6933.41455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 445   6933.4306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 446   6933.4130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9458, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1762, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 447   6933.41845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 448   6933.412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1806, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 449   6933.41064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 450   6933.4140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 451   6933.40283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 452   6933.41064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 453   6933.40087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 454   6933.419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 455   6933.3994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 456   6933.4052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 457   6933.41064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 458   6933.40234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 459   6933.408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 460   6933.39794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 461   6933.408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1747, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 462   6933.40625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 463   6933.400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 464   6933.4013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1743, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 465   6933.39306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 466   6933.39599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 467   6933.38916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1759, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 468   6933.4052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 469   6933.41943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1734, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 470   6933.39794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 471   6933.4013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 472   6933.40185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 473   6933.3828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 474   6933.39501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1723, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 475   6933.40283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 476   6933.39697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9260, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 477   6933.39453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 478   6933.39794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 479   6933.3759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 480   6933.4033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 481   6933.38671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 482   6933.3955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9257, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 483   6933.3935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 484   6933.39501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 485   6933.39208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 486   6933.3935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 487   6933.38720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 488   6933.39111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 489   6933.3818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 490   6933.38427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 491   6933.3837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 492   6933.37744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 493   6933.39453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 494   6933.39306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 495   6933.3818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 496   6933.376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1679, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 497   6933.37890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 498   6933.38330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1699, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 499   6933.37890625\n",
      "eval loss 1.9148280620574951\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9259, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 500   6933.39501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 501   6933.39453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9111, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1680, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 502   6933.3818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 503   6933.37255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1682, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 504   6933.3701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 505   6933.3896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 506   6933.37646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 507   6933.3837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 508   6933.38330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1696, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 509   6933.38232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 510   6933.38818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1698, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 511   6933.369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 512   6933.36181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 513   6933.3681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1677, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 514   6933.36669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 515   6933.3798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1704, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 516   6933.36328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9174, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1700, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 517   6933.3857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 518   6933.38427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 519   6933.388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9003, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 520   6933.36962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 521   6933.3603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 522   6933.37255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 523   6933.37255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 524   6933.35595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 525   6933.3759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 526   6933.3603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 527   6933.3603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 528   6933.3798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 529   6933.357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 530   6933.35546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1674, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 531   6933.36962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 532   6933.353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 533   6933.34716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1660, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 534   6933.34716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 535   6933.3662109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 536   6933.3583984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 537   6933.3603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 538   6933.373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 539   6933.37451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 540   6933.35693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 541   6933.38037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 542   6933.3740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8814, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1633, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 543   6933.35205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 544   6933.36279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1653, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 545   6933.35009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 546   6933.35302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 547   6933.35107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 548   6933.37060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 549   6933.35302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 550   6933.34326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 551   6933.35302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 552   6933.3740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 553   6933.357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 554   6933.35498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1632, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 555   6933.34912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 556   6933.3583984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 557   6933.376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1611, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 558   6933.3564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 559   6933.361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 560   6933.33837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 561   6933.357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 562   6933.35595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1602, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 563   6933.35205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 564   6933.33642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 565   6933.353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 566   6933.37353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1608, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 567   6933.34130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 568   6933.333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 569   6933.345703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 570   6933.330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 571   6933.34521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 572   6933.337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 573   6933.35107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 574   6933.3505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 575   6933.33935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 576   6933.357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1583, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 577   6933.35205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 578   6933.3359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 579   6933.3505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 580   6933.359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1568, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 581   6933.35205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1564, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 582   6933.32763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 583   6933.3515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 584   6933.322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 585   6933.35107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1554, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 586   6933.33544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 587   6933.341796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 588   6933.34228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 589   6933.3505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 590   6933.3330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 591   6933.34619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1561, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 592   6933.3388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 593   6933.34716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1548, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 594   6933.34814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1546, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 595   6933.34228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 596   6933.337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 597   6933.3330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 598   6933.3349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 599   6933.32470703125\n",
      "eval loss 1.8705170154571533\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 600   6933.326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 601   6933.3173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 602   6933.3173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 603   6933.32958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 604   6933.3359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 605   6933.32861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1524, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 606   6933.35107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 607   6933.3447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 608   6933.333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 609   6933.3193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 610   6933.33642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1516, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 611   6933.33251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 612   6933.3271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 613   6933.32666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 614   6933.33154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 615   6933.27978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1502, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 616   6933.33349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 617   6933.32080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 618   6933.32763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1496, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 619   6933.328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 620   6933.31787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 621   6933.32080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 622   6933.32861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 623   6933.30224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 624   6933.326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1510, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 625   6933.31982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1498, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 626   6933.326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 627   6933.31787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 628   6933.34814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1493, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 629   6933.31689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 630   6933.3173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 631   6933.33935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 632   6933.3173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 633   6933.32080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 634   6933.34375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 635   6933.31884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 636   6933.33349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 637   6933.32666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 638   6933.32275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 639   6933.3212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8628, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 640   6933.33154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 641   6933.31005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 642   6933.296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 643   6933.31640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 644   6933.3154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 645   6933.32568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 646   6933.30810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 647   6933.3193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 648   6933.32421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1459, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 649   6933.31982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 650   6933.3291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 651   6933.3271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 652   6933.3193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 653   6933.3154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1472, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 654   6933.30712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1449, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 655   6933.3154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 656   6933.3076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1446, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 657   6933.31005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 658   6933.3251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 659   6933.30615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 660   6933.30322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1427, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 661   6933.31689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 662   6933.3017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 663   6933.31884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 664   6933.30419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 665   6933.29541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 666   6933.3125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 667   6933.3134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 668   6933.2958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 669   6933.310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 670   6933.30322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 671   6933.27783203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 672   6933.31494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 673   6933.30078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1411, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 674   6933.3046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8244, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 675   6933.29296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 676   6933.2841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 677   6933.31787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1400, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 678   6933.2919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 679   6933.31982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1407, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 680   6933.30126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 681   6933.302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 682   6933.2919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1388, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 683   6933.330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 684   6933.3037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1386, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 685   6933.3076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1364, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 686   6933.31103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1373, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 687   6933.28369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 688   6933.310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 689   6933.29833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 690   6933.31787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 691   6933.2958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8196, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1371, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 692   6933.2880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 693   6933.302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 694   6933.30322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8264, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 695   6933.29736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 696   6933.30322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1381, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 697   6933.32177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8270, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1382, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 698   6933.29541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 699   6933.30517578125\n",
      "eval loss 1.8325846195220947\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 700   6933.2998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 701   6933.29248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 702   6933.29833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 703   6933.29541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 704   6933.2939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 705   6933.28955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 706   6933.3046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 707   6933.30126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 708   6933.30615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 709   6933.28662109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 710   6933.310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 711   6933.2841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 712   6933.287109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 713   6933.29248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 714   6933.3115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 715   6933.2841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 716   6933.29931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 717   6933.30224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 718   6933.302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 719   6933.29443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 720   6933.28564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 721   6933.3076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1362, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 722   6933.283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 723   6933.2958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8198, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 724   6933.2880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 725   6933.2958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8041, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1366, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 726   6933.2744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 727   6933.2880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 728   6933.2900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8117, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 729   6933.279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1358, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 730   6933.29931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8268, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 731   6933.29736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 732   6933.29150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 733   6933.30078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 734   6933.2958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8258, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 735   6933.294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 736   6933.29833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 737   6933.29150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 738   6933.27978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 739   6933.28857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 740   6933.2919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 741   6933.271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8138, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 742   6933.283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 743   6933.283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 744   6933.30078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 745   6933.27197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7922, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 746   6933.26123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 747   6933.296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 748   6933.2822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 749   6933.28515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8159, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 750   6933.28662109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 751   6933.2646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 752   6933.27197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 753   6933.294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 754   6933.2685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 755   6933.27685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 756   6933.26806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 757   6933.2734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 758   6933.29150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1352, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 759   6933.279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1347, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 760   6933.28076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 761   6933.28759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 762   6933.27490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 763   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1342, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 764   6933.27490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 765   6933.28662109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8168, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 766   6933.28759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 767   6933.27490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 768   6933.2900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 769   6933.279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 770   6933.2841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 771   6933.26953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 772   6933.28173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 773   6933.27197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 774   6933.2900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 775   6933.2763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 776   6933.28271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 777   6933.29150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 778   6933.265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1325, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 779   6933.29052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 780   6933.2724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 781   6933.26611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 782   6933.26171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 783   6933.2431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 784   6933.265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8197, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 785   6933.2890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8173, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1311, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 786   6933.2861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 787   6933.263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 788   6933.27099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 789   6933.2724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1320, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 790   6933.267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 791   6933.267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 792   6933.27392578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 793   6933.265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 794   6933.271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 795   6933.26025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 796   6933.26953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 797   6933.26953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 798   6933.26904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 799   6933.26123046875\n",
      "eval loss 1.8054598569869995\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 800   6933.25927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 801   6933.27197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 802   6933.2666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1301, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 803   6933.27880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 804   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 805   6933.267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 806   6933.27490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 807   6933.28369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1326, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 808   6933.26904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 809   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1315, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 810   6933.2880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 811   6933.28369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 812   6933.2578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7991, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 813   6933.26904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1288, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 814   6933.25341796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 815   6933.27099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 816   6933.26318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 817   6933.2607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 818   6933.24462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1269, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 819   6933.27783203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 820   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 821   6933.2724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7953, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 822   6933.265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 823   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 824   6933.27294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1294, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 825   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7959, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 826   6933.26513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 827   6933.271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 828   6933.27734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 829   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 830   6933.263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 831   6933.27685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 832   6933.27001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 833   6933.279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 834   6933.25732421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 835   6933.26220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 836   6933.28125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 837   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 838   6933.2587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1284, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 839   6933.2353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 840   6933.25\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 841   6933.25390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 842   6933.25537109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7897, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 843   6933.2587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 844   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 845   6933.26220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 846   6933.24365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 847   6933.26513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 848   6933.26953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 849   6933.26171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8024, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 850   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 851   6933.2802734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7873, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 852   6933.255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 853   6933.2578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 854   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 855   6933.24755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 856   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1302, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 857   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 858   6933.2705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1300, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 859   6933.27587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 860   6933.2548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 861   6933.236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 862   6933.25146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 863   6933.244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 864   6933.26025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 865   6933.2578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 866   6933.2490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 867   6933.25634765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 868   6933.2490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 869   6933.24267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 870   6933.2412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 871   6933.267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 872   6933.2421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 873   6933.2470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7795, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 874   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 875   6933.2568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 876   6933.244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 877   6933.25390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 878   6933.24951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 879   6933.24755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 880   6933.25537109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 881   6933.265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 882   6933.24072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 883   6933.25048828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 884   6933.27685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 885   6933.25732421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 886   6933.2529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7862, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 887   6933.2568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 888   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 889   6933.24462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 890   6933.25830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 891   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 892   6933.2509765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 893   6933.23779296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 894   6933.25634765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 895   6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 896   6933.2470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1231, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 897   6933.25439453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 898   6933.25146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 899   6933.25244140625\n",
      "eval loss 1.7832235097885132\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 900   6933.22607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 901   6933.2373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 902   6933.2373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 903   6933.251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 904   6933.2578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 905   6933.2431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 906   6933.25732421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 907   6933.22802734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 908   6933.2529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 909   6933.22900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 910   6933.23974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 911   6933.2490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1247, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 912   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 913   6933.2626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1259, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 914   6933.24951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 915   6933.24755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 916   6933.23828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 917   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1253, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 918   6933.2470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 919   6933.2685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 920   6933.2490234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 921   6933.24365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 922   6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 923   6933.2529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 924   6933.24658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1245, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 925   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 926   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 927   6933.2646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 928   6933.25830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 929   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 930   6933.248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 931   6933.2421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 932   6933.24169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 933   6933.244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 934   6933.251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 935   6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 936   6933.2294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 937   6933.23681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 938   6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 939   6933.2470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 940   6933.2373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 941   6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 942   6933.2431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1225, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 943   6933.244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 944   6933.2353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 945   6933.2529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 946   6933.24755859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1211, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 947   6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 948   6933.2314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1224, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 949   6933.23291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1226, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 950   6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.8158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1213, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 951   6933.2841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1223, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 952   6933.23974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 953   6933.234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 954   6933.2607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 955   6933.25146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 956   6933.2353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 957   6933.24658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 958   6933.2314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 959   6933.24072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7812, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 960   6933.25048828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 961   6933.240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 962   6933.21923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 963   6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 964   6933.2333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 965   6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 966   6933.22412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 967   6933.23046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1209, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 968   6933.23779296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1221, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 969   6933.20556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1206, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 970   6933.24853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 971   6933.2353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 972   6933.251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 973   6933.23876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 974   6933.2158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1216, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 975   6933.21044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 976   6933.24169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1218, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 977   6933.2431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 978   6933.2412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 979   6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 980   6933.23486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 981   6933.23046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 982   6933.23095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 983   6933.2392578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 984   6933.2236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7833, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 985   6933.25244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 986   6933.23876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 987   6933.23828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 988   6933.2294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 989   6933.2255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 990   6933.24169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 991   6933.22998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 992   6933.2314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 993   6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 994   6933.2431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 995   6933.2265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 996   6933.22607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 997   6933.2314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 998   6933.232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 999   6933.21533203125\n",
      "eval loss 1.7630140781402588\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1000  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1179, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1001  6933.232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1002  6933.220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1003  6933.2373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1004  6933.21240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1005  6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1006  6933.21044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1007  6933.25537109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1008  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1009  6933.23486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1010  6933.244140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1011  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1012  6933.2333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1013  6933.216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1014  6933.22802734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1015  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1016  6933.2119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1017  6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1018  6933.23046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1185, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1019  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1020  6933.24072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1021  6933.2119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1022  6933.23974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1023  6933.23046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1024  6933.22705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1025  6933.23583984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1026  6933.2421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1027  6933.216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1028  6933.21533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1029  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1030  6933.2333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1031  6933.2158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1162, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1032  6933.220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1033  6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1034  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1035  6933.23388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1036  6933.2119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1037  6933.22412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1038  6933.22900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1039  6933.23779296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1040  6933.22998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1160, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1041  6933.22705078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1042  6933.22314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1043  6933.2138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1044  6933.220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1045  6933.216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1046  6933.22509765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1047  6933.23095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1048  6933.21142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1049  6933.21533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1050  6933.2236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1051  6933.2236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1052  6933.22607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1053  6933.20947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1054  6933.19677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1055  6933.2197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1056  6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1156, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1057  6933.22412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1058  6933.20947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1059  6933.23681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1060  6933.21435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1061  6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1062  6933.21630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1063  6933.24560546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1064  6933.20751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1065  6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1066  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1067  6933.22607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1068  6933.232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1170, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1069  6933.2109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1165, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1070  6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1071  6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1072  6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7122, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1181, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1073  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1173, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1074  6933.2265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1075  6933.19873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1076  6933.232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7662, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1077  6933.234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1078  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1079  6933.21484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1080  6933.20947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1081  6933.23974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1082  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1176, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1083  6933.20751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1084  6933.21337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1085  6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1086  6933.228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1087  6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1154, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1088  6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1089  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1090  6933.21484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1091  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1092  6933.2099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1093  6933.22265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7472, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1094  6933.2177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1095  6933.23291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1152, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1096  6933.1962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1097  6933.20703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1098  6933.21044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1099  6933.1953125\n",
      "eval loss 1.7494885921478271\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1100  6933.21533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7419, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1101  6933.2119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1102  6933.2080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1103  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1104  6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1105  6933.2060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1106  6933.23291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1107  6933.22119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1108  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1109  6933.21923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1144, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1110  6933.2255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1151, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1111  6933.2080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1112  6933.2294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1113  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1114  6933.205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1115  6933.203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1116  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1117  6933.216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1130, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1118  6933.20263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1133, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1119  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1120  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1138, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1121  6933.22119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1122  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7206, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1123  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1124  6933.21533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1125  6933.20703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1134, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1126  6933.216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1127  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1128  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1129  6933.205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1130  6933.19970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1131  6933.1884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1132  6933.220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1133  6933.21630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1134  6933.2158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1135  6933.20947265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1136  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1137  6933.212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1138  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1139  6933.2109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1140  6933.2265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1124, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1141  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1142  6933.201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1143  6933.19189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1144  6933.20458984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1145  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1146  6933.22314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1147  6933.224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1148  6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1149  6933.2080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1150  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1151  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1152  6933.220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1153  6933.2041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1154  6933.22509765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1155  6933.2275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1156  6933.212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1157  6933.2109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1158  6933.20068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1159  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1160  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1161  6933.20849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1162  6933.201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1163  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1164  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7294, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1165  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1166  6933.18505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1167  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1135, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1168  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1169  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1170  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1171  6933.205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1172  6933.2216796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1173  6933.201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1107, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1174  6933.19189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1126, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1175  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1176  6933.20849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1177  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1118, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1178  6933.21484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1120, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1179  6933.1943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1180  6933.20703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1181  6933.2060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1182  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1183  6933.193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1110, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1184  6933.21337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1185  6933.20751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1186  6933.16650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1119, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1187  6933.20703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1188  6933.1953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1105, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1189  6933.201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1190  6933.208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1191  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1192  6933.203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1099, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1193  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1194  6933.21728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1195  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1196  6933.20556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7149, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1197  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1198  6933.20263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1199  6933.18798828125\n",
      "eval loss 1.7361209392547607\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1200  6933.2119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1201  6933.2001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1202  6933.1875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1203  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1204  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1205  6933.19970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7218, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1075, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1206  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1207  6933.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1208  6933.19189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1209  6933.208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7192, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1210  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1211  6933.203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1212  6933.19482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1213  6933.21875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1214  6933.20068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1215  6933.21337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1216  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1217  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1218  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1219  6933.1982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1220  6933.208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1098, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1221  6933.2060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1222  6933.2158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1223  6933.17236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1112, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1224  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1225  6933.20751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1226  6933.17626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1227  6933.19775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1228  6933.20849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1229  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1230  6933.19873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1231  6933.20458984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1232  6933.1826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1103, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1233  6933.19775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1234  6933.208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1086, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1235  6933.19873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1236  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1237  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1238  6933.2099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7165, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1109, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1239  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1097, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1240  6933.18505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1241  6933.19287109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1242  6933.18798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1243  6933.1904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1064, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1244  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1245  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1246  6933.1982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1247  6933.19482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1248  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1249  6933.212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1250  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7221, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1251  6933.193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1252  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1253  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7230, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1254  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1063, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1255  6933.19775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1256  6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1257  6933.20166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1258  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1259  6933.1845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1260  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1261  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1262  6933.22021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1263  6933.19970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1264  6933.18701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1265  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1266  6933.2099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1267  6933.20556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1071, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1268  6933.16650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1269  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1270  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7228, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1060, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1271  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1272  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1061, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1273  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1274  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7120, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1275  6933.18359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1276  6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1277  6933.1845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1278  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1279  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1280  6933.1806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1281  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1282  6933.18408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1283  6933.17431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1049, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1284  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1285  6933.19873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7144, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1286  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1287  6933.2080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1288  6933.19189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1289  6933.193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7231, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1290  6933.19384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1291  6933.19189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1292  6933.189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1293  6933.2021484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1294  6933.1953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1295  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1296  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7080, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1297  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1298  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7286, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1299  6933.197265625\n",
      "eval loss 1.7243698835372925\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1300  6933.1806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1301  6933.16943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1302  6933.1845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1303  6933.19970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7074, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1304  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6924, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1056, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1305  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1306  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1307  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1308  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1309  6933.1982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1310  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1311  6933.17626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1312  6933.21337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1313  6933.205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1314  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7252, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1315  6933.19580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1316  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1317  6933.1865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7291, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1318  6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1319  6933.2099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1320  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1321  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1322  6933.18505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1323  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1324  6933.2060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1325  6933.2138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1326  6933.17236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1327  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1328  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1329  6933.1923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1330  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1331  6933.1884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1332  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1333  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1051, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1334  6933.1904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1335  6933.1865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1336  6933.18359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7161, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1337  6933.1865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7096, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1338  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1339  6933.205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7177, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1340  6933.1884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1341  6933.18505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1342  6933.189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6867, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1343  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7207, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1344  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1345  6933.18408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1346  6933.1982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1347  6933.20654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1348  6933.171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1349  6933.1875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1350  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7160, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1351  6933.18408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1055, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1352  6933.197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7139, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1353  6933.18408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1354  6933.189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7105, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1355  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7130, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1356  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1357  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6985, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1358  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7205, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1359  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1360  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1361  6933.181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1362  6933.20166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7057, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1363  6933.17578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6860, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1364  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1365  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7050, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1366  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7251, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1367  6933.1953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1368  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1369  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1370  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1371  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7190, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1372  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7037, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1373  6933.17431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7119, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1374  6933.1826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1375  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7188, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1376  6933.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1377  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1378  6933.1962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1379  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1380  6933.1767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7008, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1381  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1382  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1383  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1384  6933.1943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1385  6933.181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1386  6933.18701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1387  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1388  6933.19921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1389  6933.18310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1390  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1391  6933.20361328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1392  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7059, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1393  6933.1767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1394  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7090, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1395  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1396  6933.1904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7098, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1397  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1398  6933.1728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1399  6933.1650390625\n",
      "eval loss 1.7170504331588745\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1400  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1401  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1402  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1403  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7007, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1404  6933.171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1405  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1406  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1407  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1408  6933.18359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1409  6933.1845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1410  6933.1875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1411  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7067, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1412  6933.17724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1413  6933.18701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1414  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1415  6933.16748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1046, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1416  6933.19091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7032, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1417  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1418  6933.1962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1419  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1041, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1420  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1421  6933.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1422  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7209, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1423  6933.18994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1424  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1425  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1426  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1427  6933.21630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1428  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6952, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1429  6933.166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1430  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1431  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7166, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1432  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1023, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1433  6933.1884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1434  6933.1904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7012, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1435  6933.171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6986, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1436  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1437  6933.1767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1438  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1439  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1440  6933.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7128, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1441  6933.18212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1442  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1443  6933.18701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6905, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1444  6933.162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7047, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1445  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1446  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1447  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1448  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1449  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1450  6933.18798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7094, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1451  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1452  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1453  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7114, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1454  6933.181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1455  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7113, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1456  6933.18212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1457  6933.18896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1458  6933.17724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6879, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1459  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1460  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1461  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1462  6933.2060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7073, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1463  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1464  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1465  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1466  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1467  6933.16455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1468  6933.1826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1469  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7084, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1470  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1471  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1472  6933.162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1473  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6910, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1474  6933.16015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1475  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1476  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7020, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1477  6933.17236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1478  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1479  6933.1748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1480  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1481  6933.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1482  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1483  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1484  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1485  6933.17431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1486  6933.18017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1487  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6964, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1488  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1489  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1490  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1491  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1492  6933.1767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1493  6933.14501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1022, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1494  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7101, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1495  6933.1806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1496  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1497  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1498  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1499  6933.17529296875\n",
      "eval loss 1.7050920724868774\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1500  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7126, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1501  6933.1826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1502  6933.1796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1503  6933.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6820, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1504  6933.15234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1505  6933.15673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1506  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6945, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1507  6933.16552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1508  6933.1748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1509  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7045, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1510  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1511  6933.1865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1512  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6997, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1513  6933.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1514  6933.18408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1515  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1516  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7058, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1517  6933.1767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1518  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1519  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1520  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1521  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1522  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1523  6933.17236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6942, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1524  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1525  6933.16748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1526  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1527  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1528  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1529  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1530  6933.169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1531  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1532  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1533  6933.16943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1534  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1535  6933.185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6899, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1536  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7099, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1537  6933.1806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1538  6933.15771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1539  6933.1806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1540  6933.16748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1541  6933.18212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7039, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1542  6933.1748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1543  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1544  6933.19140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1545  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1546  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1547  6933.169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1548  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1549  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1550  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1551  6933.15771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1552  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6941, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1553  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1554  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1555  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1556  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1557  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1558  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1559  6933.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1560  6933.16552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1561  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1034, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1562  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1563  6933.1787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1564  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1565  6933.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1566  6933.17578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1567  6933.17626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6951, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1568  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1569  6933.18603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7018, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1570  6933.171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1571  6933.15771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1572  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1573  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6968, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1574  6933.166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6927, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1575  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0998, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1576  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1577  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1578  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1008, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1579  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1010, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1580  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1581  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1582  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1583  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1584  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6961, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1033, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1585  6933.16748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7121, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1586  6933.18115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7055, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1587  6933.17626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6866, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1030, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1588  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1589  6933.17626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1590  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1591  6933.166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6944, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1592  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1006, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1593  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1594  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1595  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6804, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1016, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1596  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7092, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1597  6933.17822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7064, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1598  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1599  6933.1640625\n",
      "eval loss 1.6988015174865723\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1600  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1601  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6869, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1602  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1603  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1604  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1605  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6908, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1606  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1607  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1608  6933.16455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1609  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7065, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1610  6933.17724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1611  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1612  6933.15673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6947, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1613  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7048, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1614  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0997, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1615  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1616  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1617  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1618  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6999, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1619  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6928, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1620  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1004, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1621  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1622  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1623  6933.1748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1624  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6788, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1625  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1626  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6916, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1627  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6973, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1628  6933.16650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7010, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1629  6933.171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6813, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1630  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1631  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6850, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1632  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1633  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1634  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1635  6933.16015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1636  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1637  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1638  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1639  6933.1865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7005, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1640  6933.169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1641  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1642  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6974, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1643  6933.16748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1644  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1645  6933.16650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1646  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6854, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1647  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1648  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1649  6933.1845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1650  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1651  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6923, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1652  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1653  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1654  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1655  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1656  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1657  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1658  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1659  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1660  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1661  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1662  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1663  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1664  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6861, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1665  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1666  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7009, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1667  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1668  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1669  6933.177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1670  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1671  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1672  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1673  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6878, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1674  6933.158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1675  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1676  6933.16015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1677  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1678  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1679  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6871, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1680  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6851, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1681  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6914, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1682  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1683  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1684  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1685  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7011, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0970, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1686  6933.1728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1687  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1688  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1689  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1690  6933.158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1691  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1692  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1693  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1694  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1695  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1696  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1697  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1698  6933.162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0983, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1699  6933.1435546875\n",
      "eval loss 1.6919901371002197\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1700  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7060, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1701  6933.1748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1702  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1703  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7118, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1704  6933.18115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1705  6933.1669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1706  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1707  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1708  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1709  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1710  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1711  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6940, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1712  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1713  6933.17236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7001, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1714  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1715  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1716  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1717  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1718  6933.17529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1719  6933.1689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6983, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1720  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1721  6933.158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0982, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1722  6933.1640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6954, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1723  6933.166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1724  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1725  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1726  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6751, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1727  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0990, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1728  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6732, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1729  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1730  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6752, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0994, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1731  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1732  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1733  6933.1572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1734  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1735  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1736  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1737  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7028, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1738  6933.17333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1739  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1740  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6848, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0985, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1741  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6967, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1742  6933.166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7015, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1743  6933.1728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6842, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1744  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1745  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1001, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1746  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1747  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1748  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.1020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1749  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0995, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1750  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0996, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1751  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0993, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1752  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1753  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1754  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1755  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1756  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1757  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1758  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1759  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1760  6933.150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1761  6933.15673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7102, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1762  6933.17919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1763  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1764  6933.15673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1765  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1766  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1767  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1768  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6933, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1769  6933.16357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1770  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1771  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1772  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1773  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1774  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6979, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1775  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1776  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1777  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1778  6933.14501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6838, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1779  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1780  6933.15234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1781  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1782  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1783  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1784  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1785  6933.173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1786  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6876, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1787  6933.158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6718, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1788  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1789  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1790  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1791  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1792  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1793  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1794  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6755, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1795  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1796  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1797  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1798  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1799  6933.14501953125\n",
      "eval loss 1.689704418182373\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7106, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1800  6933.18115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1801  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6811, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1802  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1803  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1804  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1805  6933.150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1806  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1807  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1808  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1809  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1810  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1811  6933.16650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1812  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1813  6933.15234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1814  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1815  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1816  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1817  6933.150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1818  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1819  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1820  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1821  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1822  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1823  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1824  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1825  6933.15771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1826  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6847, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1827  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1828  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6995, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1829  6933.17138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1830  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1831  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1832  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1833  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1834  6933.16845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1835  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1836  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1837  6933.15869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1838  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1839  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1840  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1841  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1842  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1843  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1844  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1845  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1846  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1847  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1848  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1849  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1850  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6832, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1851  6933.15234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.7004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1852  6933.1708984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1853  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1854  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6852, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1855  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1856  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1857  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6863, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1858  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1859  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1860  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1861  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1862  6933.14013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0965, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1863  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6665, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1864  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1865  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1866  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1867  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1868  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1869  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1870  6933.16162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6767, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1871  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1872  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1873  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1874  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6786, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1875  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1876  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1877  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1878  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6843, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1879  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1880  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1881  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6896, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1882  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1883  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1884  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1885  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1886  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1887  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6803, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1888  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1889  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1890  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1891  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0939, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1892  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1893  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1894  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1895  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1896  6933.15625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1897  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1898  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1899  6933.13037109375\n",
      "eval loss 1.6799622774124146\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1900  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1901  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1902  6933.150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1903  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1904  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1905  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1906  6933.15478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1907  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1908  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1909  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1910  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1911  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1912  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1913  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1914  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1915  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1916  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1917  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1918  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6898, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1919  6933.16064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1920  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1921  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1922  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1923  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1924  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6845, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1925  6933.15380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1926  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1927  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1928  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1929  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1930  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1931  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1932  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1933  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1934  6933.15283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1935  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1936  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1937  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6839, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1938  6933.15576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1939  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1940  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1941  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1942  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1943  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1944  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1945  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1946  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1947  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1948  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1949  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1950  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1951  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1952  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1953  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6594, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1954  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0947, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1955  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1956  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1957  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1958  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1959  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1960  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1961  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1962  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1963  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6808, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1964  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1965  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1966  6933.1611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1967  6933.16259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1968  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1969  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1970  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6700, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1971  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1972  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1973  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1974  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1975  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1976  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1977  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1978  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1979  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1980  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1981  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1982  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1983  6933.1552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1984  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1985  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1986  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1987  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1988  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1989  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1990  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1991  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1992  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1993  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1994  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6919, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1995  6933.1630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0948, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1996  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1997  6933.14013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1998  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0958, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1999  6933.11181640625\n",
      "eval loss 1.6758227348327637\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2000  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2001  6933.1650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2002  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2003  6933.14013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2004  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2005  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2006  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2007  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2008  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2009  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2010  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2011  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2012  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2013  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6886, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2014  6933.1591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6909, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2015  6933.15966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2016  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2017  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6706, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2018  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2019  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0956, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2020  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2021  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2022  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6723, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2023  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2024  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2025  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2026  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2027  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2028  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2029  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2030  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2031  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2032  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2033  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2034  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6805, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2035  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2036  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2037  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2038  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2039  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2040  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2041  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2042  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2043  6933.16796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2044  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2045  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2046  6933.154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6797, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2047  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2048  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2049  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2050  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2051  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2052  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2053  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2054  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2055  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2056  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2057  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2058  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2059  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2060  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2061  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2062  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6727, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2063  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2064  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2065  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2066  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2067  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2068  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2069  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2070  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2071  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2072  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2073  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2074  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2075  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2076  6933.1494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2077  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2078  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2079  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2080  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2081  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6906, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2082  6933.162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2083  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2084  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2085  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2086  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2087  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2088  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2089  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2090  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2091  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2092  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2093  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2094  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0949, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2095  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2096  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2097  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2098  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2099  6933.115234375\n",
      "eval loss 1.6711008548736572\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0942, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2100  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2101  6933.14501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2102  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2103  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2104  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2105  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2106  6933.15185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2107  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2108  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2109  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2110  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2111  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2112  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2113  6933.14501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2114  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2115  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2116  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6630, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2117  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2118  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2119  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2120  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6740, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2121  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2122  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2123  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2124  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2125  6933.11865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6824, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2126  6933.1533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2127  6933.158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2128  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2129  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2130  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6801, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2131  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2132  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2133  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2134  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2135  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2136  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2137  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2138  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2139  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2140  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2141  6933.10693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2142  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2143  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2144  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2145  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2146  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2147  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2148  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2149  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2150  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2151  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2152  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2153  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2154  6933.14013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2155  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2156  6933.14404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2157  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2158  6933.10595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2159  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2160  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2161  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2162  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2163  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2164  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2165  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2166  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2167  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2168  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2169  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2170  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2171  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2172  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2173  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2174  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2175  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2176  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0929, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2177  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2178  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2179  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2180  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2181  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2182  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2183  6933.169921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2184  6933.142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2185  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2186  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2187  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2188  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2189  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2190  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2191  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2192  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2193  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2194  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2195  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2196  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2197  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0928, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2198  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0936, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2199  6933.13330078125\n",
      "eval loss 1.667157530784607\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2200  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2201  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2202  6933.14013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2203  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2204  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2205  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2206  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2207  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0916, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2208  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2209  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2210  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6717, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2211  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2212  6933.14990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2213  6933.103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2214  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2215  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2216  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2217  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2218  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2219  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6739, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2220  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6714, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2221  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2222  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2223  6933.1025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6777, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2224  6933.1484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2225  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2226  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2227  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2228  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2229  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2230  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2231  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2232  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2233  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2234  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6730, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2235  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2236  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2237  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2238  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2239  6933.09814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2240  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2241  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2242  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2243  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2244  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2245  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2246  6933.1474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2247  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2248  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2249  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2250  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2251  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2252  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2253  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2254  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2255  6933.1513671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2256  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2257  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2258  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2259  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2260  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2261  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2262  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2263  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2264  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2265  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2266  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2267  6933.10205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2268  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2269  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2270  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6663, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2271  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2272  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2273  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2274  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2275  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2276  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6382, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2277  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2278  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2279  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2280  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2281  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2282  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2283  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2284  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2285  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2286  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2287  6933.14892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2288  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2289  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2290  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2291  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2292  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2293  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6674, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2294  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2295  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2296  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2297  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2298  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2299  6933.11279296875\n",
      "eval loss 1.6615647077560425\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2300  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2301  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2302  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2303  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2304  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2305  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2306  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2307  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2308  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6681, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2309  6933.138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2310  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2311  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2312  6933.11865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2313  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2314  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2315  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2316  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6380, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2317  6933.1083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0911, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2318  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2319  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2320  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2321  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2322  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2323  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2324  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2325  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2326  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2327  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2328  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6398, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2329  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2330  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2331  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2332  6933.14599609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2333  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2334  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2335  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2336  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2337  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2338  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2339  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2340  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2341  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2342  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2343  6933.13525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2344  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2345  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2346  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2347  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2348  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2349  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2350  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2351  6933.1083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2352  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2353  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2354  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2355  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2356  6933.0966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2357  6933.09326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2358  6933.1328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2359  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2360  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2361  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2362  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2363  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6742, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2364  6933.14501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2365  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2366  6933.1416015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2367  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2368  6933.103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2369  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2370  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2371  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2372  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2373  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2374  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2375  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2376  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2377  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2378  6933.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2379  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2380  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2381  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2382  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2383  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2384  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2385  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2386  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2387  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2388  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2389  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2390  6933.1376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2391  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2392  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2393  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2394  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2395  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2396  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2397  6933.1025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2398  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2399  6933.1103515625\n",
      "eval loss 1.659974217414856\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2400  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6746, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2401  6933.1455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2402  6933.10546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2403  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2404  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2405  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2406  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2407  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2408  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2409  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2410  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2411  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2412  6933.10400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2413  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2414  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2415  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2416  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2417  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2418  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2419  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2420  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2421  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2422  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2423  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2424  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2425  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2426  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2427  6933.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2428  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2429  6933.09326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2430  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2431  6933.10546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2432  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2433  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2434  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2435  6933.15087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6552, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2436  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2437  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2438  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6689, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2439  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2440  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2441  6933.09619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2442  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2443  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2444  6933.14306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2445  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2446  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0912, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2447  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0906, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2448  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2449  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2450  6933.10693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2451  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2452  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2453  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0933, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2454  6933.10205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2455  6933.103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0922, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2456  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2457  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2458  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6315, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2459  6933.10205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2460  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2461  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2462  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0914, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2463  6933.1259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2464  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2465  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2466  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2467  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0901, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2468  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2469  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2470  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2471  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2472  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2473  6933.11279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2474  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2475  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2476  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2477  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2478  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2479  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2480  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2481  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2482  6933.12451171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2483  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2484  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2485  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6211, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2486  6933.091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2487  6933.10498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2488  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2489  6933.09716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2490  6933.1435546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2491  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2492  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2493  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2494  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2495  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2496  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2497  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2498  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2499  6933.142578125\n",
      "eval loss 1.6578757762908936\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2500  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2501  6933.10400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2502  6933.10888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2503  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2504  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6267, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2505  6933.09716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2506  6933.13671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2507  6933.14208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2508  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2509  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2510  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2511  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2512  6933.103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2513  6933.130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2514  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2515  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2516  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2517  6933.10009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2518  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2519  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2520  6933.09814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2521  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2522  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2523  6933.14794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2524  6933.10693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2525  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2526  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2527  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2528  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2529  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2530  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2531  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2532  6933.11865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2533  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2534  6933.14111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2535  6933.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2536  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2537  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6589, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2538  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0846, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2539  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2540  6933.10302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2541  6933.10888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2542  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2543  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2544  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2545  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2546  6933.1083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2547  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2548  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2549  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2550  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2551  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2552  6933.10302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2553  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2554  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2555  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2556  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2557  6933.10302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2558  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2559  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2560  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2561  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2562  6933.1015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2563  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2564  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6271, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2565  6933.09765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2566  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2567  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2568  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2569  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2570  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2571  6933.10888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2572  6933.13427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2573  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2574  6933.11279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6772, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2575  6933.146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2576  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2577  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2578  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6327, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2579  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2580  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2581  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2582  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2583  6933.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2584  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6393, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2585  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2586  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2587  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2588  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2589  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6687, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2590  6933.1396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2591  6933.1083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2592  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2593  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2594  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2595  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2596  6933.08642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2597  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2598  6933.10693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2599  6933.11474609375\n",
      "eval loss 1.6534593105316162\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2600  6933.12353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2601  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2602  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2603  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2604  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2605  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2606  6933.11767578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2607  6933.10498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2608  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2609  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2610  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2611  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6379, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2612  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2613  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0905, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2614  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2615  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6170, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2616  6933.08837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2617  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2618  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0896, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2619  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2620  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2621  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2622  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0886, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2623  6933.1337890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2624  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6313, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2625  6933.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2626  6933.1181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2627  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2628  6933.09765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2629  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2630  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2631  6933.14453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2632  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2633  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2634  6933.11328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2635  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6203, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2636  6933.09033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2637  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2638  6933.125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6392, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2639  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2640  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2641  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2642  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2643  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2644  6933.1025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2645  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2646  6933.1025390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2647  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2648  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2649  6933.11083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2650  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2651  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2652  6933.10009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2653  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2654  6933.12255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2655  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2656  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2657  6933.10888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2658  6933.09326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2659  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2660  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2661  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2662  6933.0859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2663  6933.13720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2664  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2665  6933.1015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2666  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6239, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2667  6933.09521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2668  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2669  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2670  6933.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2671  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2672  6933.1318359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2673  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2674  6933.095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2675  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2676  6933.13623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2677  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2678  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6266, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2679  6933.09716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2680  6933.1142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2681  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2682  6933.11865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2683  6933.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2684  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2685  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2686  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2687  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0873, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2688  6933.11279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2689  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2690  6933.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2691  6933.11962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2692  6933.09716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2693  6933.12158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2694  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2695  6933.12646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2696  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2697  6933.10009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2698  6933.134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2699  6933.13916015625\n",
      "eval loss 1.6489533185958862\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2700  6933.12841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2701  6933.09130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2702  6933.1298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2703  6933.1201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2704  6933.07861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2705  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2706  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2707  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2708  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2709  6933.10546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2710  6933.09326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2711  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0878, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2712  6933.10595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2713  6933.1015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6517, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2714  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0874, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2715  6933.10498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2716  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6308, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0870, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2717  6933.1015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0867, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2718  6933.13916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0875, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2719  6933.0966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2720  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2721  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6189, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2722  6933.08837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2723  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2724  6933.09716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6520, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2725  6933.1220703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2726  6933.12109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6295, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2727  6933.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2728  6933.1083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2729  6933.1171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2730  6933.12939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6222, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2731  6933.0927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2732  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2733  6933.09326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2734  6933.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2735  6933.10693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2736  6933.10400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2737  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6309, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2738  6933.1015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2739  6933.1162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6451, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2740  6933.11669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2741  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2742  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2743  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2744  6933.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2745  6933.13330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2746  6933.0751953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0842, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2747  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2748  6933.10400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2749  6933.103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2750  6933.12890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0839, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2751  6933.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2752  6933.13232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2753  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6217, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2754  6933.091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2755  6933.12744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2756  6933.09814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2757  6933.0859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2758  6933.115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2759  6933.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6389, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2760  6933.10791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2761  6933.1064453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2762  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2763  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2764  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6608, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2765  6933.13134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6415, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2766  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2767  6933.095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2768  6933.10546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0834, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2769  6933.09375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2770  6933.09228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0830, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2771  6933.14697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2772  6933.11474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2773  6933.13037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2774  6933.107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2775  6933.11572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2776  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2777  6933.10595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2778  6933.123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2779  6933.111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2780  6933.11376953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2781  6933.11279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2782  6933.10986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2783  6933.1103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2784  6933.10595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6326, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2785  6933.10400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2786  6933.09912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2787  6933.126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2788  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2789  6933.10107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0864, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2790  6933.10546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2791  6933.109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2792  6933.09521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6411, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2793  6933.11181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2794  6933.1123046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6183, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0854, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2795  6933.08740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2796  6933.1240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2797  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.6332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0848, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2798  6933.1044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(1.5907, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2799  6933.0595703125\n",
      "eval loss 1.646577000617981\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sot>accompanied?\\n'\n",
      " '\\n'\n",
      " 'AEdile:\\n'\n",
      " 'With old Menenius, and those senators\\n'\n",
      " \"That always favour'd him.\\n\"\n",
      " '\\n'\n",
      " 'SICINIUS:\\n'\n",
      " 'Ha',\n",
      " '<sot>et a devil\\n'\n",
      " 'His filth within being cast, he would appear\\n'\n",
      " 'A pond as deep as hell.\\n'\n",
      " '\\n'\n",
      " 'CLAUDIO:\\n'\n",
      " 'The prenz',\n",
      " '<sot>ance at my hand,\\n'\n",
      " 'That I yet know not?\\n'\n",
      " '\\n'\n",
      " 'FRIAR LAURENCE:\\n'\n",
      " 'Too familiar\\n'\n",
      " 'Is my dear son with such sour c',\n",
      " '<sot>ble man have mercy.\\n'\n",
      " '\\n'\n",
      " 'COMINIUS:\\n'\n",
      " 'Who shall ask it?\\n'\n",
      " \"The tribunes cannot do't for shame; the people\\n\"\n",
      " 'Des',\n",
      " '<sot>estroying wound\\n'\n",
      " 'And lie full low, graved in the hollow ground.\\n'\n",
      " '\\n'\n",
      " 'DUKE OF AUMERLE:\\n'\n",
      " 'Is Bushy, Green, a',\n",
      " \"<sot>t afeard; delay'd,\\n\"\n",
      " \"But nothing alter'd: what I was, I am;\\n\"\n",
      " 'More straining on for plucking back, not ',\n",
      " '<sot>hou leave the town and fight?\\n'\n",
      " 'Or shall we beat the stones about thine ears?\\n'\n",
      " '\\n'\n",
      " 'WARWICK:\\n'\n",
      " 'Alas, I am no',\n",
      " '<sot>d as white as it,\\n'\n",
      " \"Or Ethiopian's tooth, or the fann'd\\n\"\n",
      " \"snow that's bolted\\n\"\n",
      " 'By the northern blasts twi',\n",
      " \"<sot> to this place, i' the open air, before\\n\"\n",
      " 'I have got strength of limit. Now, my liege,\\n'\n",
      " 'Tell me what b',\n",
      " '<sot>new-made bridegroom from the city,\\n'\n",
      " 'For whom, and not for Tybalt, Juliet pined.\\n'\n",
      " 'You, to remove that ',\n",
      " '<sot>by the world--\\n'\n",
      " '\\n'\n",
      " 'QUEEN ELIZABETH:\\n'\n",
      " \"'Tis full of thy foul wrongs.\\n\"\n",
      " '\\n'\n",
      " 'KING RICHARD III:\\n'\n",
      " \"My father's death\",\n",
      " '<sot>:\\n'\n",
      " 'Thou shalt not stir a foot to seek a foe.\\n'\n",
      " '\\n'\n",
      " 'PRINCE:\\n'\n",
      " 'Rebellious subjects, enemies to peace,\\n'\n",
      " 'Profane',\n",
      " '<sot> my troth, Isabel, I loved thy brother:\\n'\n",
      " 'if the old fantastical duke of dark corners had been\\n'\n",
      " 'at hom',\n",
      " '<sot>o deserves greatness\\n'\n",
      " 'Deserves your hate; and your affections are\\n'\n",
      " \"A sick man's appetite, who desires\",\n",
      " '<sot>; but either\\n'\n",
      " 'Had borne the action of yourself, or else\\n'\n",
      " 'To him had left it solely.\\n'\n",
      " '\\n'\n",
      " 'AUFIDIUS:\\n'\n",
      " 'I unde',\n",
      " '<sot>d, wondrous well beloved,\\n'\n",
      " 'In Oxfordshire shalt muster up thy friends.\\n'\n",
      " 'My sovereign, with the loving',\n",
      " '<sot>, who is now queen.\\n'\n",
      " '\\n'\n",
      " 'ANTONIO:\\n'\n",
      " \"And the rarest that e'er came there.\\n\"\n",
      " '\\n'\n",
      " 'SEBASTIAN:\\n'\n",
      " 'Bate, I beseech you,',\n",
      " \"<sot>s'\\n\"\n",
      " \"gown for thy master's use!\\n\"\n",
      " '\\n'\n",
      " 'PETRUCHIO:\\n'\n",
      " \"Why, sir, what's your conceit in that?\\n\"\n",
      " '\\n'\n",
      " 'GRUMIO:\\n'\n",
      " 'O, sir, t',\n",
      " '<sot>:\\n'\n",
      " 'Go in to him, and fetch him out.\\n'\n",
      " '\\n'\n",
      " 'POMPEY:\\n'\n",
      " 'He is coming, sir, he is coming; I hear his straw rustl',\n",
      " '<sot>? yes, marry, sir:\\n'\n",
      " 'see where he looks out of the window.\\n'\n",
      " '\\n'\n",
      " 'VINCENTIO:\\n'\n",
      " \"Is't so, indeed.\\n\"\n",
      " '\\n'\n",
      " 'BIONDELLO:\\n'\n",
      " 'H',\n",
      " '<sot>ppiness with living woe;\\n'\n",
      " 'Think that thy babes were fairer than they were,\\n'\n",
      " 'And he that slew them fou',\n",
      " '<sot>wd:\\n'\n",
      " 'an unshunned consequence; it must be so. Art going\\n'\n",
      " 'to prison, Pompey?\\n'\n",
      " '\\n'\n",
      " 'POMPEY:\\n'\n",
      " 'Yes, faith, sir.',\n",
      " '<sot>\\n'\n",
      " '\\n'\n",
      " 'LADY CAPULET:\\n'\n",
      " 'That same villain, Romeo.\\n'\n",
      " '\\n'\n",
      " 'JULIET:\\n'\n",
      " '\\n'\n",
      " 'LADY CAPULET:\\n'\n",
      " 'That is, because the traitor murd',\n",
      " '<sot>de the dream.\\n'\n",
      " '\\n'\n",
      " 'BRAKENBURY:\\n'\n",
      " 'No marvel, my lord, though it affrighted you;\\n'\n",
      " 'I promise, I am afraid to ',\n",
      " '<sot>Did I let pass the abuse done to my niece?\\n'\n",
      " 'Did I impale him with the regal crown?\\n'\n",
      " 'Did I put Henry f',\n",
      " '<sot>nt\\n'\n",
      " \"That perish'd by the way: thoughts are no subjects;\\n\"\n",
      " 'Intents but merely thoughts.\\n'\n",
      " '\\n'\n",
      " 'MARIANA:\\n'\n",
      " 'Merel',\n",
      " '<sot>thee to do good.\\n'\n",
      " '\\n'\n",
      " 'QUEEN ELIZABETH:\\n'\n",
      " 'Shall I forget myself to be myself?\\n'\n",
      " '\\n'\n",
      " 'KING RICHARD III:\\n'\n",
      " 'Ay, if yo',\n",
      " \"<sot>nd make me die the thrall of Margaret's curse,\\n\"\n",
      " \"Nor mother, wife, nor England's counted queen.\\n\"\n",
      " '\\n'\n",
      " 'LORD',\n",
      " '<sot>e vile suspects.\\n'\n",
      " '\\n'\n",
      " 'GLOUCESTER:\\n'\n",
      " 'You may deny that you were not the cause\\n'\n",
      " \"Of my Lord Hastings' late im\",\n",
      " '<sot>\\n'\n",
      " \"And little look'd for at your helping hands.\\n\"\n",
      " '\\n'\n",
      " 'KING RICHARD II:\\n'\n",
      " 'Alack, why am I sent for to a king,',\n",
      " '<sot> of this contract to-night:\\n'\n",
      " 'It is too rash, too unadvised, too sudden;\\n'\n",
      " 'Too like the lightning, whic',\n",
      " '<sot>ORIZEL:\\n'\n",
      " 'Very nobly\\n'\n",
      " \"Have you deserved: it is my father's music\\n\"\n",
      " 'To speak your deeds, not little of hi',\n",
      " '<sot>end.\\n'\n",
      " '\\n'\n",
      " 'KING LEWIS XI:\\n'\n",
      " 'And still is friend to him and Margaret:\\n'\n",
      " 'But if your title to the crown be wea',\n",
      " '<sot>er?\\n'\n",
      " '\\n'\n",
      " 'CURTIS:\\n'\n",
      " 'All ready; and therefore, I pray thee, news.\\n'\n",
      " '\\n'\n",
      " 'GRUMIO:\\n'\n",
      " 'First, know, my horse is tired; ',\n",
      " '<sot>\\n'\n",
      " 'BUCKINGHAM:\\n'\n",
      " 'Have done! for shame, if not for charity.\\n'\n",
      " '\\n'\n",
      " 'QUEEN MARGARET:\\n'\n",
      " 'Urge neither charity nor sh',\n",
      " '<sot> to do in hell,\\n'\n",
      " 'When thou didst bower the spirit of a fiend\\n'\n",
      " 'In moral paradise of such sweet flesh?\\n',\n",
      " '<sot>\\n'\n",
      " 'LADY GREY:\\n'\n",
      " 'My love till death, my humble thanks, my prayers;\\n'\n",
      " 'That love which virtue begs and virtu',\n",
      " '<sot>O:\\n'\n",
      " 'Away, begone; the sport is at the best.\\n'\n",
      " '\\n'\n",
      " 'ROMEO:\\n'\n",
      " 'Ay, so I fear; the more is my unrest.\\n'\n",
      " '\\n'\n",
      " 'CAPULET:\\n',\n",
      " '<sot>l the rest,\\n'\n",
      " 'When weeping made you break the story off,\\n'\n",
      " 'of our two cousins coming into London.\\n'\n",
      " '\\n'\n",
      " 'DUKE',\n",
      " '<sot>?\\n'\n",
      " '\\n'\n",
      " 'KATHARINA:\\n'\n",
      " \"Well aim'd of such a young one.\\n\"\n",
      " '\\n'\n",
      " 'PETRUCHIO:\\n'\n",
      " 'Now, by Saint George, I am too young for ',\n",
      " '<sot>counsellor heart, the arm our soldier,\\n'\n",
      " 'Our steed the leg, the tongue our trumpeter.\\n'\n",
      " 'With other muni',\n",
      " \"<sot>'d with grievous taxes,\\n\"\n",
      " 'And quite lost their hearts: the nobles hath he fined\\n'\n",
      " 'For ancient quarrels,',\n",
      " '<sot>cannot countervail the exchange of joy\\n'\n",
      " 'That one short minute gives me in her sight:\\n'\n",
      " 'Do thou but clo',\n",
      " '<sot> drop into the rotten mouth of death.\\n'\n",
      " \"Here in these confines slily have I lurk'd,\\n\"\n",
      " 'To watch the wani',\n",
      " '<sot>hward.\\n'\n",
      " '\\n'\n",
      " 'Second Citizen:\\n'\n",
      " 'Why that way?\\n'\n",
      " '\\n'\n",
      " 'Third Citizen:\\n'\n",
      " 'To lose itself in a fog, where being three pa',\n",
      " '<sot>thee\\n'\n",
      " 'Where thou shalt rest, that thou mayst hear of us\\n'\n",
      " 'And we of thee: so if the time thrust forth\\n',\n",
      " '<sot> Corioli he scotched him and notched\\n'\n",
      " 'him like a carbon ado.\\n'\n",
      " '\\n'\n",
      " 'Second Servingman:\\n'\n",
      " 'An he had been cann',\n",
      " '<sot>antagenet, root him up who dares:\\n'\n",
      " 'Resolve thee, Richard; claim the English crown.\\n'\n",
      " '\\n'\n",
      " 'KING HENRY VI:\\n'\n",
      " 'M',\n",
      " '<sot>love\\n'\n",
      " \"Must climb a bird's nest soon when it is dark:\\n\"\n",
      " 'I am the drudge and toil in your delight,\\n'\n",
      " 'But y',\n",
      " '<sot>s,\\n'\n",
      " \"If you'll not here proclaim yourself our king,\\n\"\n",
      " \"I'll leave you to your fortune and be gone\\n\"\n",
      " 'To kee',\n",
      " \"<sot>ward marries Warwick's daughter.\\n\"\n",
      " '\\n'\n",
      " 'CLARENCE:\\n'\n",
      " 'Belike the elder; Clarence will have the younger.\\n'\n",
      " 'Now, ',\n",
      " '<sot>this frail sepulchre of our flesh,\\n'\n",
      " \"As now our flesh is banish'd from this land:\\n\"\n",
      " 'Confess thy treason',\n",
      " '<sot> York.\\n'\n",
      " '\\n'\n",
      " 'KING EDWARD IV:\\n'\n",
      " 'What fates impose, that men must needs abide;\\n'\n",
      " 'It boots not to resist both w',\n",
      " '<sot>p-revolving witty Buckingham\\n'\n",
      " 'No more shall be the neighbour to my counsel:\\n'\n",
      " 'Hath he so long held out',\n",
      " '<sot> marry her, if her dowry please.\\n'\n",
      " '\\n'\n",
      " 'GREMIO:\\n'\n",
      " 'So said, so done, is well.\\n'\n",
      " 'Hortensio, have you told him a',\n",
      " '<sot> Norfolk dead?\\n'\n",
      " '\\n'\n",
      " 'BISHOP OF CARLISLE:\\n'\n",
      " 'As surely as I live, my lord.\\n'\n",
      " '\\n'\n",
      " 'HENRY BOLINGBROKE:\\n'\n",
      " 'Sweet peace c',\n",
      " '<sot>o!\\n'\n",
      " '\\n'\n",
      " 'MENENIUS:\\n'\n",
      " \"What work's, my countrymen, in hand? where go you\\n\"\n",
      " 'With bats and clubs? The matter? sp',\n",
      " '<sot>then the Volsces stand but as at first,\\n'\n",
      " 'Ready, when time shall prompt them, to make road.\\n'\n",
      " \"Upon's ag\",\n",
      " \"<sot> princely eagle's bird,\\n\"\n",
      " \"Show thy descent by gazing 'gainst the sun:\\n\"\n",
      " 'For chair and dukedom, throne a',\n",
      " '<sot>n, and beat him back again.\\n'\n",
      " '\\n'\n",
      " 'CLARENCE:\\n'\n",
      " 'A little fire is quickly trodden out;\\n'\n",
      " \"Which, being suffer'd,\"]\n",
      "['ve the son the son th',\n",
      " 'ed the strition the s',\n",
      " 'ome the seed\\nThe so m',\n",
      " 't the son the see the',\n",
      " 'nd the son the son th',\n",
      " 'the seen the prisent\\n',\n",
      " 't the seen the prince',\n",
      " 't of the see the son\\n',\n",
      " 'e the son the son the',\n",
      " 'the shall she shall s',\n",
      " ' the son the see the ',\n",
      " 's to the so man the s',\n",
      " 'e and the son the son',\n",
      " ' the son the prince o',\n",
      " 'rs the so make the se',\n",
      " ' the seep the prince ',\n",
      " ' and the son the son ',\n",
      " 'he son the son the so',\n",
      " 'y the son\\nThe son the',\n",
      " 'e shall shall shall s',\n",
      " 'l be the son the sent',\n",
      " '\\n\\nPETRUCHIO:\\nWhat wit',\n",
      " 'er the seen the princ',\n",
      " 'the seen the son\\nThe ',\n",
      " 'or the son the seen t',\n",
      " ' the shall shall shal',\n",
      " 'u are the son the son',\n",
      " ' OF YORK:\\nWhat the sh',\n",
      " 'pers,\\nAnd the son the',\n",
      " '\\nAnd the some the son',\n",
      " 'h the son the prince ',\n",
      " 's some the prince.\\n\\nP',\n",
      " 'r the prince.\\n\\nPETRUC',\n",
      " 'and the seen the prin',\n",
      " 'all shall shall shall',\n",
      " '\\nCORIOLANUS:\\nI will s',\n",
      " 'e the son\\nThe son the',\n",
      " 'I will shall shall sh',\n",
      " ' VINCENTIO:\\nWhy, the ',\n",
      " 'the seen the son\\nThe ',\n",
      " 'son the son the son t',\n",
      " ' and the son the son ',\n",
      " 'ud to the so make the',\n",
      " 'ty of the seen the se',\n",
      " 'rtion\\nThe so man the ',\n",
      " 'The so the so man the',\n",
      " 'ot the son the son th',\n",
      " 'y lord of the see the',\n",
      " 'ou the son the son th',\n",
      " 'p the son the son the',\n",
      " 'and the son the son t',\n",
      " ' the son the seep the',\n",
      " 'ith the seen the pris',\n",
      " ' the son the seep the',\n",
      " 'nd the sent the seed\\n',\n",
      " 'ome the son the son t',\n",
      " 'eak the seen the seep',\n",
      " 'ain the son the son t',\n",
      " 'nd the seep the priso',\n",
      " ' and the son the seem']\n",
      "[' tkerelitnn \\n\\nCNN r::\\nThlh tfd tartstns  and theue soetters Thet t lar  aoieur d tis \\n\\nPeCINIUS:\\nIeve tou armonei uee Tf t l the ', '   tnmeail Tas sanl  tith n te ng tonte ae siuld tnpear Tngrstetn tead tn te l \\n\\nPOARDIO:\\nThe frisceentngel u \\nCSABELLA:\\nI, ttis ', ' tde tn te land  Ahet t wot tnow tot \\n\\nCiIAR LAURENCE:\\nTh  toienenl T  ty leat to  tith toch ao l tomelnt \\nA weeng the  shmeog  t', ' ee tek tive ta se \\n\\nPARINIUS:\\nWha shall s   tn \\n\\nhe srueese  aon ot te s tor toele  ahe crrple Toatnve toch arty tf tis tn the s', '  t eu ng tiuldsTnd tokstorl tev  aoace  tn the saudow toawsd \\n\\nPUKE VF YUMERLE:\\nA  tutii  aoead  and the sdrgitf tatl  im.dteat ', '  t fsr   aeain s \\nAut tot ing tnlor d  Ihat i wis  a wm \\nAyre toaiit og tf tor tresk ng teck  aot torlow.ng Ty loavt tptatl ng y', ' ewssoave the sh e tnd tanht \\n\\n  toall se se r the soargs tnout thesk anrt \\n\\nCARWICK:\\nWnlct a wm tot tommls tir  tor toaosde \\n\\n w', '  tn tiichrtn tt  Ar tdwas  nl d th  h  ar the satced Toow thet s tetdhr Te the sot hyr  teoct  thotk tf r \\n\\nCELIXENES:\\nIhat worl', ' th thes srace  af the suenttnd \\nae ore T wave too toaiage  tf tikety\\nWow  aa lokse \\nAhll te tiit teoss ng  a wave te   tnlne \\nAh', '  ss-ane teone eyud toom the soti\\n\\nAor tie   and tot tor thralt  aulien trtgs \\n\\nou  th tesene thet shr e tf traef toom tir \\nAu oe', ' e the sirdd m\\n\\nAUEEN ELIZABETH:\\nWTis tarl tf the larl biong  \\n\\nCING RICHARD III:\\nAy lather s toath -\\n\\nAUEEN ELIZABETH:\\nWhe loke ', ' \\nTheu shall tot thaletnmarl th tee  tnparn\\n\\nCEONCE:\\nTeaeaiynns aoclect   antmios th trrce \\nAroveitsy tf thes sovghterr -tann s t', ' ty lrunhe a  ilr \\na wove  the lrother\\n\\nAt yhe sud toicenteoen teke tf toyestomt ss,aiv te n An teue  ae sav toke  \\n\\nCUKE VINCENT', '  teatrve  toaat ess Tosenee  tou  sevh  and tou  brfection  tne Anbenk tek s t peatoy  ahassoatce  tyrt thet Thich tiuld ts eaat', ' \\nIet tnther Tav tetn sthe srcion tf tour  lf  ar tase Th tes tev tott tn to l y \\n\\nCUTIDIUS:\\nI wnters  nd ohe  ti l  and te theu ', '   thudeeus aill te iwe  \\nA  t eord  in  toall test r tpothe laoends \\n\\ny lonere nn  ahth the soveng totioen  \\nAeke th tes sn enc ', '  tha ss tot tuien \\n\\nPNTONIO:\\nTnd the sete   thet tner tone the e \\n\\nPeBASTIAN:\\nAushn t we teph tou  ahtew tid n\\n\\nPNGONIO:\\nT  thle', '   Tron tor the santer s tn  \\n\\nPETRUCHIO:\\nAha, tor, ahat s tou  boutesnytn thet \\n\\nPREMIO:\\nT  tir, the somsesnytn tead d thet tou ', ' \\nTootn th tes  and toa h tis tfr \\n\\nCEMPEY:\\nWe ss tomeng  tor, ae ss tomeng  a waarttes soaii oent\\ny\\n\\n\\nPNRORSON:\\nT  the srestp n ', ' \\nIot  aykri  aor,\\nAoe tiire ie sovk  tfr tf the solden \\n\\nPONCENTIO:\\nW    to  tn ead \\n\\nCAONDELLO:\\nWe l  ae l  ae l  Ie e s t mane', ' roress tith tikeng tir \\nAhesk thet the sene tti e totr d thet the  sire \\nAnd te shet shaa the  torl   thet ti ss \\nTu oer ng the ', ' ao\\nIn tndtasdes tomtenueste  an test te to \\nInt toong Th troson  aemeer  \\nCELPEY:\\nWot  aoir   aor,\\n\\nCACEO:\\nThy, ttis sot t  ne  ', ' AAUDY CAPULET:\\nWhet thye tillain  aomeo \\n\\nCULIET:\\nICADY AAPULET:\\nIhet ts  au oise the sruitir tasder d tike  \\n\\nCULIET:\\nTn, tykem', '  sthe seeat \\n\\nCAUMENIURY:\\nAowsayreny ay lord  aheu h tn t feeeht r tour\\nA wrovine  a wm t tein th tirr tourshll tn \\n\\nCAARENCE:\\nA', '  d t woa trrt the srore tete th te logsh \\n\\nod t wspen  tim tith the seair toown \\n\\nod t wrr terry toom tis somenerteght \\n\\nnd t  t', '   Ahet trrcve d te the soy  theu h   ane totmeclect\\n\\n\\nA  ent  tet ty se  theu h   \\n\\nCERCANA:\\nTynen   ty lord \\n\\nPUKE VINCENTIO:\\nT', '  e  th to tood \\n\\nPUEEN ELIZABETH:\\nToall t war et te elf th te te elf \\n\\nPING RICHARD III:\\nAn, tn you   lf d tesenbeonce tiong aou', '   teke te tod the sreott tf tarreied\\ns dorse\\n\\nTot terher  ahle  aot tdglond s tomnter tueen \\n\\nLARD OIOREEY:\\nTorl tf tite tole ts', '  till toceerti \\n\\nCLOUCESTER:\\nWou say teae thet tou tire tot the ponse Tf ty lord tasteng   tose tnpeoten ent\\n\\n\\nCOCERS:\\nWoa say  ', ' And toktle tovk d tor t  tour laad ng tivd  \\n\\nPING RICHARD III\\nAnace  aha wr t waet tor th t ming \\nAu ore t wave thaup tf  the s', ' tf thes somseaieioh meght\\n\\nT  ss th  tett  ah  tntreene   ah  tocgert\\nAh  toke the sovht eng  ahach to h tonre th te Tve tfe ton', ' NDOEL:\\nAere totle Tate tou toatrve   tf ts ty lather s tastneTh toeak tou  sead   aot tiktle tf tis bole Th teve the  teaonelrt ', '    \\n\\nLING RADIS MI:\\nTnd thall tn toiend th tes tnd tarreied \\nAut ts tou  shmhe th the soown te tilr \\nAn tyk t pear te tdward s t', '  e\\n\\nCORTIS:\\nInl teady  and the e ore  a wroy the   aovs \\n\\nPLEMIO:\\nTorst  anow  ay lawde tn thme   ay lanter tnd Tenteess torl d ', ' AuCKINGHAM:\\nIere te e  Ior toele  af yot tor toareny \\n\\nPUEEN EARGARET:\\nIneettovg er toareny oot toale th ty \\nTnteendnynle tith t', ' th te mt tirl  Ahin theu aod t tetnr the soeretyof t manndsT  tyret trrttent tf toch toeet toiss \\n\\nhs tner tetd tomtenn ng toch ', ' AoDY CREY:\\nWy lore thml teath  aa lasple thet    ay lrey\\nr.,\\nAhet Iive tiich teltue te ittnd tiliie tract  \\n\\nCING RDWARD IV:\\nIow', ' N\\nAnay, ae ane  ahe seert tn t  the seat \\n\\nLOMEO:\\nWn, to t waar  ahe sare tn ty lndaat \\n\\nBOPULET:\\nIoy, tontle ant arosore tot th', '  the seat  Ahin Ie p ng teye tou aeeat the sorrt tf e\\nAu tur soo mounin  tomeng tn o torg n \\n\\nCUKE VF YORK:\\nAhare iod t soave \\n\\n', ' \\n\\nCITHARINA:\\nIh l tnd d tf toch t mourg tf  \\n\\nCETRUCHIO:\\nWow  au tivnt tonvse  a wm th  tourg tor tour\\n\\nCITHARINA:\\nAot tou are t', ' hmrt d  wytirr   ahe sreitfr sondenss\\nArr toaad the soas ahe sr gue tfr boueber d \\n\\nhlh tfher tasdsest  tnd trrre tirl  T  thes ', ' s tith toaefeus toke   And tuerh tivt the r saart   the setle saevh sersord \\nTor t dlcnt tuirtyd   and tuerh tovt the r saart\\n\\n\\n', ' htter tomnter enn ohe sapeerce tf tuy Thet tfe toaudhtenese tove  te tn tir shrht \\nTo theu aet tooue tfr bevd  aith tiud tirds \\n', ' toewett o the seuhertaerrh tf teath \\n\\nere tn the e tomtergsttoave teve t wosded \\nAh teseh the sosttg tf tyne tnvery nens \\n\\nnfode', ' eard \\n\\nCecond Sitizen:\\nWha,what tis \\n\\nPRird Citizen:\\nAh tovt tn elf tn t marh thane te ng theoe trrti\\nTe lor tnay tith teyher te', ' he  Thine theu ahall teat  ahet sheu aay e terrttf tneTnd te mu the   to mn yhe shme theogt tor h Tnmonse tor the leaorr  ah wha', ' tlmiolsnhaashouehed tim tnd tot her\\nTis toke t monear tnvn\\n\\nCecond Mervingman:\\nWndtersav te n ton osll   trve   ae sasht tete Te', ' td ne td  aeyt tes tpotiissoye\\n\\n\\nAestnee the   tochard  aoocnsthe sdglamh toown \\n\\nPING RENRY VI:\\nAy lord   aevk tiine ihe soarne', ' lwe Tyst tooveltnmrtt s tovt to n tiir ts ts teyes\\nT wm the pees ertnd th nltn tour beaieht \\nAut tou shall se r the setnir ao n ', '   A  you ll tot te   triveeinsoour elf tfr bing \\nA ll sotve tourth tour sarthne tnd te troe Th tiep the  teck thet some th teche', ' ayd tekried aarwick d teyghter\\n\\n\\nBOARENCE:\\nAu iee the sas d  aoaiesce titl teve the sourg d,\\n\\now  tuother ting  aore ell  tnd th', '  es sooit ooe r h edtf tur saaas\\n\\nTn tot tfr saiss tn tecist d toom thes sodd \\nTomtest the lruaten  ane theu aai the peadii\\nAhrg', ' tork \\n\\nPING RDWARD IV:\\nIhat Iothrttnpere  thet ta  test tovd  tnod s\\nA  setth tot th teatneeteth titdetnd thmes\\n\\nCSFORD:\\nAhat wo', ' rseaenseng tithh totkingeil.Totmare toall se the sevghterr to te lomntelf\\nTevh tersh mivg tirl tfr tith ty tpdoce  \\nAnd torrs ti', ' takri ter  af te  wetne,traase \\n\\nPREMIO:\\nTo thyd  ao te e  tf ti l \\n\\newehnc on teve tou th d tem tnl ter satlt  \\n\\nLETRUCHIO:\\nA w', ' tordord toat \\n\\nCAOTOM YF YaPWIFTE:\\nTn toce y tn t wove  ay lord,\\n\\nDERRY BOLINGBROKE:\\nIoaet trrre tomterhitis soeet to l th the p', ' u\\n\\nCENENIUS:\\nIhat Iiut d  ty lomntey ant af tivd \\nIhere ioosourThth tech tnd toostt\\nIhe santer \\nIoeak  a wroy tour\\n\\nPirst Sitize', '  e  the sildeessooayd tet t  t  torst \\nTesr   ahar Ihme toall srovilethe   ah tyke teyt \\n\\npon d t ain \\n\\nCARINIUS:\\nAhe  sre tiud ', ' trisce y tnree s trdt \\nAoau the seaeont te toieng ttorn t the sec \\nTor toarl tnd teke  n  aheoue tnd ting er toy \\nAvth r thet ts', '   tnd te rhtes teck tnains\\n\\nCAARENCE:\\nAnboktle tore tn tueck y thuuyir afr \\nAhich  au ng tocfer d  aege   ton ot tuiede \\n\\nCARWIC']\n"
     ]
    }
   ],
   "source": [
    "get_dpo_loss_partial = partial(get_dpo_loss, target_model=rnn_lm, beta_2=0.0001, dpo_loss_beta=10000, num_samples=2)\n",
    "dpo_trained_model = QuietStarLanguageModelLSTM(len(vocab), 100, 1).to(device)\n",
    "train_model(get_dpo_loss_partial, lambda model: eval_loss_fn(model, get_nll), dpo_trained_model, epochs=100)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best obtainable 1.65 with normal nll 100 hidden dim 100 epochs.  got 1.82 eval with 100 hidden dim, and 100 epochs beta_2 = 0.0001\n",
    "\n",
    "\n",
    "\n",
    "dpo with beta value 0.0001, no reference model hence the high beta value. eval nll 1.86. with best obtainable 1.5? with normal nll with 1000 hidden dim 100 epochs\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "\n",
    "get_dpo_loss_partial = partial(get_dpo_loss, target_model=rnn_lm, beta_2=0.0001, dpo_loss_beta=10000) # ceiling is 1.65. obtained 1.79 matching quiet-star with positive and negative rewards.\n",
    "\n",
    "train_model(get_dpo_loss_partial, lambda model: eval_loss_fn(model, get_nll), QuietStarLanguageModelLSTM(len(vocab), 100, 1).to(device), epochs=100)\n",
    "\n",
    "\n",
    "eval 1.738 with ceiling of 1.657 with increase samples. beta_2=0.0001, dpo_loss_beta=10000, num_samples=10. 20 samples gets 1.728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, num_layers=1, dim_of_latent=None):\n",
    "        super().__init__()\n",
    "        if dim_of_latent is None:\n",
    "            dim_of_latent = hidden_dim\n",
    "        self.embed_tokens = torch.nn.Embedding(num_embeddings=vocab_size, embedding_dim=hidden_dim)\n",
    "        self.model = torch.nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, num_layers=num_layers)\n",
    "        self.output_layer = torch.nn.Linear(in_features=hidden_dim + dim_of_latent, out_features=1)\n",
    "    def forward(self, token_context, latent_states):\n",
    "        # produce the expected reward for a particular choice of latent state and context x. right? the expectation is that there is a latent state per token in the token_context tensor\n",
    "        x = self.embed_tokens(token_context)\n",
    "        x, _ = self.model(x)\n",
    "        x = self.output_layer(torch.concat((x, latent_states), dim=-1))\n",
    "        return x.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(dpo_trained_model.cpu(), open(\"dpo_trained_model_1.64_eval.pkl\", 'wb'))\n",
    "dpo_trained_model = pickle.load(open(\"dpo_trained_model_1.64_eval.pkl\", 'rb')).to(device)\n",
    "rnn_lm = pickle.load(open(\"rnn_lm_1.51_eval_1.5_train.pkl\", 'rb')).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the input_ids, and latent_states to train off of for the reward model.\n",
    "\n",
    "# create a function to collect the nlls input_ids and latent states. Could do this in the train model loop with a unique loss function, but I don't need to update the parameters of my model, so I could just create a new loop for this explicit purpose.\n",
    "def get_data_for_reward_model_training(model: QuietStarLanguageModelLSTM, repeat_sample, sample_for_train):\n",
    "    train_size = int(len(train_reward_model_dataset_shakespeare) // 10 * 0.9) * 10\n",
    "    input_and_latent_states_and_labels_dataset_train = []\n",
    "    input_and_latent_states_and_labels_dataset_eval = []\n",
    "    for _ in range(repeat_sample):\n",
    "        i = 0\n",
    "        for d in torch.utils.data.DataLoader(train_reward_model_dataset_shakespeare, batch_size=10, collate_fn=shakespeare_collate_fn): # no shuffle here can shuffle later if want to.\n",
    "            d = d.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits, hidden_states, _, _ = model.get_logits_and_hidden_states_and_log_prob_hidden_states_dist(d)\n",
    "                \n",
    "                shifted_logits = logits[:, :-1].contiguous()\n",
    "                shifted_input = d[:, :-1]\n",
    "                shifted_hidden_states = hidden_states[:, :-1].contiguous()\n",
    "                shifted_labels = d[:, 1:].contiguous()\n",
    "                is_train_data = i < train_size\n",
    "                if is_train_data:\n",
    "                    inputs_to_extend = input_and_latent_states_and_labels_dataset_train\n",
    "                else:\n",
    "                    inputs_to_extend = input_and_latent_states_and_labels_dataset_eval\n",
    "                if sample_for_train and is_train_data:\n",
    "                    targets = - torch.nn.CrossEntropyLoss(reduction='none')(shifted_logits.view(-1, logits.size(-1)), shifted_labels.view(-1)).view_as(shifted_labels)\n",
    "                else:\n",
    "                    # targets can also be decided based on the KL[D_ref(y | x) || policy(y | h)] with a reference policy, this is when we assume the reference policy represents the true data distribution.\n",
    "                    target_logits = rnn_lm(shifted_input)\n",
    "                    targets = (torch.softmax(target_logits, dim=-1) * torch.log_softmax(shifted_logits, dim=-1)).sum(-1) # has similar mean to just samples from the data, as expected, but the std between samples is smaller which makes sense, as we remove the variation from sampling and just directly get the expected KL on the distribution provided from the dpo model compared to the rnn_lm.\n",
    "                i += shifted_input.size(0)\n",
    "                inputs_to_extend.extend(zip(shifted_input.cpu(), shifted_hidden_states.cpu(), targets.cpu()))\n",
    "    # I can choose to reward model the dpo models KL with data it was trained on or with data it wasn't trained on? I really need to generalize to the reward of the data we are going to use for training with DPO. \n",
    "    # I should not train on that data tho, because then I would learn to give the exact loss achieved on that sample with that hidden state, which has the problem of not being the expected reward for that input/hidden state.\n",
    "    return input_and_latent_states_and_labels_dataset_train, input_and_latent_states_and_labels_dataset_eval\n",
    "input_and_latent_states_and_labels_dataset_train, input_and_latent_states_and_labels_dataset_eval = get_data_for_reward_model_training(dpo_trained_model, repeat_sample=10, sample_for_train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.6405), tensor(1.4577))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_from_data_distribution = torch.concat([input_and_latent_states_and_labels_dataset_train[i][-1] for i in range(len(input_and_latent_states_and_labels_dataset_train))])\n",
    "labels_from_data_distribution.mean(), labels_from_data_distribution.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.6699), tensor(0.9500))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_from_closed_from_kl = torch.concat([input_and_latent_states_and_labels_dataset_eval[i][-1] for i in range(len(input_and_latent_states_and_labels_dataset_eval))])\n",
    "labels_from_closed_from_kl.mean(), labels_from_closed_from_kl.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training steps total: 305\n",
      "eval loss 2.7291702694363065\n",
      "loss 0     2.7774062156677246\n",
      "loss 1     2.3098318576812744\n",
      "loss 2     1.8561772108078003\n",
      "loss 3     1.5061426162719727\n",
      "loss 4     1.2220115661621094\n",
      "loss 5     0.9549753665924072\n",
      "loss 6     0.7695146799087524\n",
      "loss 7     0.6432797312736511\n",
      "loss 8     0.5641867518424988\n",
      "loss 9     0.5374022126197815\n",
      "eval loss 0.5519882837931315\n",
      "loss 10    0.5478295683860779\n",
      "loss 11    0.5818609595298767\n",
      "loss 12    0.6107260584831238\n",
      "loss 13    0.6193346977233887\n",
      "loss 14    0.6370869874954224\n",
      "loss 15    0.6021691560745239\n",
      "loss 16    0.5674903392791748\n",
      "loss 17    0.5297573804855347\n",
      "loss 18    0.4988583028316498\n",
      "loss 19    0.45056289434432983\n",
      "eval loss 0.4159539010789659\n",
      "loss 20    0.41295671463012695\n",
      "loss 21    0.38520535826683044\n",
      "loss 22    0.36624735593795776\n",
      "loss 23    0.35001081228256226\n",
      "loss 24    0.3358851671218872\n",
      "loss 25    0.3260577321052551\n",
      "loss 26    0.32394763827323914\n",
      "loss 27    0.3230610489845276\n",
      "loss 28    0.3115394115447998\n",
      "loss 29    0.31150078773498535\n",
      "eval loss 0.30452783902486164\n",
      "loss 30    0.3096417188644409\n",
      "loss 31    0.3083253502845764\n",
      "loss 32    0.2995508313179016\n",
      "loss 33    0.29083138704299927\n",
      "loss 34    0.2887668013572693\n",
      "loss 35    0.2845281660556793\n",
      "loss 36    0.2743665874004364\n",
      "loss 37    0.26261210441589355\n",
      "loss 38    0.2569277286529541\n",
      "loss 39    0.2603013515472412\n",
      "eval loss 0.2471910317738851\n",
      "loss 40    0.2518223226070404\n",
      "loss 41    0.24499298632144928\n",
      "loss 42    0.2515409588813782\n",
      "loss 43    0.24381551146507263\n",
      "loss 44    0.24357479810714722\n",
      "loss 45    0.24382427334785461\n",
      "loss 46    0.24348482489585876\n",
      "loss 47    0.24175822734832764\n",
      "loss 48    0.24407720565795898\n",
      "loss 49    0.2440171092748642\n",
      "eval loss 0.23545710245768228\n",
      "loss 50    0.2334664762020111\n",
      "loss 51    0.23722413182258606\n",
      "loss 52    0.23319025337696075\n",
      "loss 53    0.23119434714317322\n",
      "loss 54    0.22552402317523956\n",
      "loss 55    0.22733494639396667\n",
      "loss 56    0.22605589032173157\n",
      "loss 57    0.22506479918956757\n",
      "loss 58    0.2219812273979187\n",
      "loss 59    0.22179511189460754\n",
      "eval loss 0.21792995929718018\n",
      "loss 60    0.22303369641304016\n",
      "loss 61    0.2198176085948944\n",
      "loss 62    0.21871286630630493\n",
      "loss 63    0.22113002836704254\n",
      "loss 64    0.21692517399787903\n",
      "loss 65    0.2203231304883957\n",
      "loss 66    0.22067378461360931\n",
      "loss 67    0.21462665498256683\n",
      "loss 68    0.214946448802948\n",
      "loss 69    0.2190709412097931\n",
      "eval loss 0.20976703696780735\n",
      "loss 70    0.213178813457489\n",
      "loss 71    0.21109916269779205\n",
      "loss 72    0.21416465938091278\n",
      "loss 73    0.2121727168560028\n",
      "loss 74    0.20793651044368744\n",
      "loss 75    0.2132469117641449\n",
      "loss 76    0.20876622200012207\n",
      "loss 77    0.20748871564865112\n",
      "loss 78    0.2078796774148941\n",
      "loss 79    0.20763510465621948\n",
      "eval loss 0.20476202170054117\n",
      "loss 80    0.20643189549446106\n",
      "loss 81    0.2078130841255188\n",
      "loss 82    0.20363514125347137\n",
      "loss 83    0.20602357387542725\n",
      "loss 84    0.20776638388633728\n",
      "loss 85    0.2076827883720398\n",
      "loss 86    0.20930099487304688\n",
      "loss 87    0.20855596661567688\n",
      "loss 88    0.20240193605422974\n",
      "loss 89    0.20901209115982056\n",
      "eval loss 0.2001775238249037\n",
      "loss 90    0.20622041821479797\n",
      "loss 91    0.204222172498703\n",
      "loss 92    0.202508807182312\n",
      "loss 93    0.20249342918395996\n",
      "loss 94    0.20272402465343475\n",
      "loss 95    0.20390585064888\n",
      "loss 96    0.20187412202358246\n",
      "loss 97    0.20062166452407837\n",
      "loss 98    0.19827058911323547\n",
      "loss 99    0.19964495301246643\n",
      "eval loss 0.19727957248687744\n",
      "loss 100   0.20100165903568268\n",
      "loss 101   0.20094695687294006\n",
      "loss 102   0.2027754783630371\n",
      "loss 103   0.20373816788196564\n",
      "loss 104   0.2015969157218933\n",
      "loss 105   0.1971244513988495\n",
      "loss 106   0.19964599609375\n",
      "loss 107   0.2008313536643982\n",
      "loss 108   0.20280739665031433\n",
      "loss 109   0.19938799738883972\n",
      "eval loss 0.1947863366868761\n",
      "loss 110   0.1982155442237854\n",
      "loss 111   0.19791388511657715\n",
      "loss 112   0.19353517889976501\n",
      "loss 113   0.1951644867658615\n",
      "loss 114   0.2025235891342163\n",
      "loss 115   0.20033420622348785\n",
      "loss 116   0.19740793108940125\n",
      "loss 117   0.19849273562431335\n",
      "loss 118   0.20050814747810364\n",
      "loss 119   0.19700764119625092\n",
      "eval loss 0.1925224330690172\n",
      "loss 120   0.19780531525611877\n",
      "loss 121   0.19804655015468597\n",
      "loss 122   0.1983514130115509\n",
      "loss 123   0.1954021453857422\n",
      "loss 124   0.198357492685318\n",
      "loss 125   0.19336184859275818\n",
      "loss 126   0.19501295685768127\n",
      "loss 127   0.19356560707092285\n",
      "loss 128   0.19835571944713593\n",
      "loss 129   0.1957067847251892\n",
      "eval loss 0.19089602099524605\n",
      "loss 130   0.1956218034029007\n",
      "loss 131   0.1915343552827835\n",
      "loss 132   0.19532379508018494\n",
      "loss 133   0.19748592376708984\n",
      "loss 134   0.1937367022037506\n",
      "loss 135   0.19049417972564697\n",
      "loss 136   0.19114123284816742\n",
      "loss 137   0.1948651820421219\n",
      "loss 138   0.1965554654598236\n",
      "loss 139   0.19328361749649048\n",
      "eval loss 0.18932648499806723\n",
      "loss 140   0.1916249394416809\n",
      "loss 141   0.19382338225841522\n",
      "loss 142   0.19674257934093475\n",
      "loss 143   0.18800967931747437\n",
      "loss 144   0.19004809856414795\n",
      "loss 145   0.1889592409133911\n",
      "loss 146   0.19459989666938782\n",
      "loss 147   0.1942158043384552\n",
      "loss 148   0.18976804614067078\n",
      "loss 149   0.194039985537529\n",
      "eval loss 0.18823077943589953\n",
      "loss 150   0.19090163707733154\n",
      "loss 151   0.19100259244441986\n",
      "loss 152   0.18802514672279358\n",
      "loss 153   0.19183772802352905\n",
      "loss 154   0.19216492772102356\n",
      "loss 155   0.1906658113002777\n",
      "loss 156   0.1887020468711853\n",
      "loss 157   0.19105857610702515\n",
      "loss 158   0.1894385814666748\n",
      "loss 159   0.1942920684814453\n",
      "eval loss 0.18717479705810547\n",
      "loss 160   0.19267310202121735\n",
      "loss 161   0.1923697143793106\n",
      "loss 162   0.19193151593208313\n",
      "loss 163   0.18734784424304962\n",
      "loss 164   0.18948397040367126\n",
      "loss 165   0.18890981376171112\n",
      "loss 166   0.18973664939403534\n",
      "loss 167   0.18722623586654663\n",
      "loss 168   0.19321127235889435\n",
      "loss 169   0.18878251314163208\n",
      "eval loss 0.1865031189388699\n",
      "loss 170   0.19416162371635437\n",
      "loss 171   0.19225594401359558\n",
      "loss 172   0.18744492530822754\n",
      "loss 173   0.1899631917476654\n",
      "loss 174   0.18954883515834808\n",
      "loss 175   0.19188809394836426\n",
      "loss 176   0.1912299245595932\n",
      "loss 177   0.18795764446258545\n",
      "loss 178   0.18743741512298584\n",
      "loss 179   0.1893881857395172\n",
      "eval loss 0.18535704082912868\n",
      "loss 180   0.18705692887306213\n",
      "loss 181   0.18735557794570923\n",
      "loss 182   0.18655085563659668\n",
      "loss 183   0.19376856088638306\n",
      "loss 184   0.1868399977684021\n",
      "loss 185   0.1905006617307663\n",
      "loss 186   0.19056057929992676\n",
      "loss 187   0.18873825669288635\n",
      "loss 188   0.19073793292045593\n",
      "loss 189   0.19207055866718292\n",
      "eval loss 0.1846945815616184\n",
      "loss 190   0.1867028772830963\n",
      "loss 191   0.1886586844921112\n",
      "loss 192   0.18677687644958496\n",
      "loss 193   0.1877845674753189\n",
      "loss 194   0.18395066261291504\n",
      "loss 195   0.1837969571352005\n",
      "loss 196   0.18835893273353577\n",
      "loss 197   0.18833325803279877\n",
      "loss 198   0.18729670345783234\n",
      "loss 199   0.1891794204711914\n",
      "eval loss 0.1840560038884481\n",
      "loss 200   0.19257202744483948\n",
      "loss 201   0.18800371885299683\n",
      "loss 202   0.1855514645576477\n",
      "loss 203   0.18697158992290497\n",
      "loss 204   0.1852586269378662\n",
      "loss 205   0.18784046173095703\n",
      "loss 206   0.1835126131772995\n",
      "loss 207   0.1893916130065918\n",
      "loss 208   0.18727761507034302\n",
      "loss 209   0.18599295616149902\n",
      "eval loss 0.18349528312683105\n",
      "loss 210   0.18789148330688477\n",
      "loss 211   0.18241772055625916\n",
      "loss 212   0.18794789910316467\n",
      "loss 213   0.18564969301223755\n",
      "loss 214   0.19176459312438965\n",
      "loss 215   0.18842822313308716\n",
      "loss 216   0.1836167871952057\n",
      "loss 217   0.18372583389282227\n",
      "loss 218   0.1874752789735794\n",
      "loss 219   0.1861017644405365\n",
      "eval loss 0.1828700436486138\n",
      "loss 220   0.18516376614570618\n",
      "loss 221   0.18864426016807556\n",
      "loss 222   0.1862790286540985\n",
      "loss 223   0.18753990530967712\n",
      "loss 224   0.18803533911705017\n",
      "loss 225   0.18452073633670807\n",
      "loss 226   0.1833282709121704\n",
      "loss 227   0.18743012845516205\n",
      "loss 228   0.18578758835792542\n",
      "loss 229   0.18895670771598816\n",
      "eval loss 0.18246091736687553\n",
      "loss 230   0.1868315488100052\n",
      "loss 231   0.1845788061618805\n",
      "loss 232   0.19408977031707764\n",
      "loss 233   0.18336862325668335\n",
      "loss 234   0.1861851066350937\n",
      "loss 235   0.18586742877960205\n",
      "loss 236   0.18621429800987244\n",
      "loss 237   0.18604065477848053\n",
      "loss 238   0.18295137584209442\n",
      "loss 239   0.19271133840084076\n",
      "eval loss 0.18204785717858207\n",
      "loss 240   0.18489059805870056\n",
      "loss 241   0.18575207889080048\n",
      "loss 242   0.18762226402759552\n",
      "loss 243   0.1835566610097885\n",
      "loss 244   0.18630599975585938\n",
      "loss 245   0.18731510639190674\n",
      "loss 246   0.1852254867553711\n",
      "loss 247   0.18518751859664917\n",
      "loss 248   0.18681444227695465\n",
      "loss 249   0.18460413813591003\n",
      "eval loss 0.18157716592152914\n",
      "loss 250   0.1852991133928299\n",
      "loss 251   0.1856815218925476\n",
      "loss 252   0.1839527189731598\n",
      "loss 253   0.18534782528877258\n",
      "loss 254   0.1827535331249237\n",
      "loss 255   0.18641623854637146\n",
      "loss 256   0.1837318241596222\n",
      "loss 257   0.18590903282165527\n",
      "loss 258   0.18560980260372162\n",
      "loss 259   0.18308910727500916\n",
      "eval loss 0.18122443887922499\n",
      "loss 260   0.18472887575626373\n",
      "loss 261   0.18138696253299713\n",
      "loss 262   0.18733273446559906\n",
      "loss 263   0.18293017148971558\n",
      "loss 264   0.1814313530921936\n",
      "loss 265   0.1850154548883438\n",
      "loss 266   0.18344195187091827\n",
      "loss 267   0.17777128517627716\n",
      "loss 268   0.181028813123703\n",
      "loss 269   0.18570487201213837\n",
      "eval loss 0.18064812819163004\n",
      "loss 270   0.1835126280784607\n",
      "loss 271   0.17854280769824982\n",
      "loss 272   0.18342849612236023\n",
      "loss 273   0.18479155004024506\n",
      "loss 274   0.18247920274734497\n",
      "loss 275   0.18150953948497772\n",
      "loss 276   0.18653634190559387\n",
      "loss 277   0.18255729973316193\n",
      "loss 278   0.18048548698425293\n",
      "loss 279   0.19193479418754578\n",
      "eval loss 0.18032916386922201\n",
      "loss 280   0.1847379207611084\n",
      "loss 281   0.18277305364608765\n",
      "loss 282   0.1854928433895111\n",
      "loss 283   0.18678522109985352\n",
      "loss 284   0.18043074011802673\n",
      "loss 285   0.18303854763507843\n",
      "loss 286   0.18204984068870544\n",
      "loss 287   0.1824892908334732\n",
      "loss 288   0.18145465850830078\n",
      "loss 289   0.1838294118642807\n",
      "eval loss 0.18012064033084446\n",
      "loss 290   0.18465566635131836\n",
      "loss 291   0.18593193590641022\n",
      "loss 292   0.18169589340686798\n",
      "loss 293   0.1812450885772705\n",
      "loss 294   0.1801709085702896\n",
      "loss 295   0.18154191970825195\n",
      "loss 296   0.1851355880498886\n",
      "loss 297   0.18459174036979675\n",
      "loss 298   0.18366441130638123\n",
      "loss 299   0.1821363866329193\n",
      "eval loss 0.17976082695855033\n",
      "loss 300   0.18436594307422638\n",
      "loss 301   0.17959663271903992\n",
      "loss 302   0.18565316498279572\n",
      "loss 303   0.1829720288515091\n",
      "loss 304   0.17751581966876984\n",
      "eval loss 0.17965598901112875\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Could change the data loader change the inputs and labels for this setting as it is no longer a language modeling setting, but a reward modeling setting.\n",
    "# should modify them so that inputs is context text and hidden_state chosen, and labels is what the correct (instant or expected) reward should have been\n",
    "def get_reward_model_loss(model: RewardModel, inputs: tuple[torch.Tensor, torch.Tensor, torch.Tensor]):\n",
    "    # expect to find input_ids, and a latent_states of similar dimension plus the dimension of the hidden state for the language model which produced the latent.\n",
    "    # get_logits_and_hidden_states\n",
    "    input_ids, latent_states, labels = inputs\n",
    "    input_ids, latent_states, labels = input_ids.to(device), latent_states.to(device), labels.to(device)\n",
    "    predicted_rewards = model(input_ids, latent_states)\n",
    "    diffs = labels - predicted_rewards\n",
    "    # print(f\"{labels= }\")\n",
    "    # print(f\"{predicted_rewards= }\")\n",
    "    # print(f\"{diffs= }\")\n",
    "    loss = 0.5 * (diffs).square().mean()\n",
    "    return loss\n",
    "def train_reward_model(reward_model, model, repeat_sample, sample_for_train, print_stuff):\n",
    "    input_and_latent_states_and_labels_dataset_train, input_and_latent_states_and_labels_dataset_eval = get_data_for_reward_model_training(model, repeat_sample=repeat_sample, sample_for_train=sample_for_train)\n",
    "    train_model(get_reward_model_loss, \n",
    "                lambda model: eval_loss_fn(model, get_reward_model_loss, dataloader=torch.utils.data.DataLoader(input_and_latent_states_and_labels_dataset_eval, batch_size=256)), \n",
    "                reward_model, \n",
    "                epochs=100,\n",
    "                train_dl=torch.utils.data.DataLoader(input_and_latent_states_and_labels_dataset_train, batch_size=256, shuffle=True),\n",
    "                eval_every=10,\n",
    "                print_stuff=print_stuff) # strange observation we aren't over fitting at all? This is nice I guess, but we should be able to overfit if we want to. I did 1000 hidden state, and tried 30 epochs, nothing.\n",
    "# we converge quickly, but is it to a good value? mse 0.7 for samples or 0.18 to the closed KL to reference model. Some of the rewards are very far off actually, not sure if this will be useful. \n",
    "# Will try first to use it in DPO, if doesn't work will then move to train ensemble. # matches eval 0.23 when trained on samples of log likelihood and matches eval to 0.17 when trained from the KL\n",
    "train_reward_model(RewardModel(len(vocab), hidden_dim=100).to(device), dpo_trained_model, repeat_sample=10, sample_for_train=False, print_stuff=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number training steps total: 2800\n",
      "eval loss 4.336419582366943\n",
      "Number training steps total: 40\n",
      "eval loss 9.647443771362305\n",
      "loss 0     9.659393310546875\n",
      "loss 1     9.393880844116211\n",
      "loss 2     9.164434432983398\n",
      "loss 3     8.995941162109375\n",
      "loss 4     8.695518493652344\n",
      "loss 5     8.451360702514648\n",
      "loss 6     8.15606689453125\n",
      "loss 7     8.059782981872559\n",
      "loss 8     7.59528923034668\n",
      "loss 9     7.285473823547363\n",
      "eval loss 6.952795028686523\n",
      "loss 10    6.901096343994141\n",
      "loss 11    6.999740123748779\n",
      "loss 12    6.0428996086120605\n",
      "loss 13    5.559576034545898\n",
      "loss 14    4.997659683227539\n",
      "loss 15    5.322469711303711\n",
      "loss 16    3.710531711578369\n",
      "loss 17    3.0752735137939453\n",
      "loss 18    2.4951343536376953\n",
      "loss 19    3.587360382080078\n",
      "eval loss 1.6079705953598022\n",
      "loss 20    1.4478669166564941\n",
      "loss 21    1.098055362701416\n",
      "loss 22    0.8249633312225342\n",
      "loss 23    2.082962989807129\n",
      "loss 24    0.4383693337440491\n",
      "loss 25    0.3284553289413452\n",
      "loss 26    0.2810075879096985\n",
      "loss 27    1.1991065740585327\n",
      "loss 28    0.2528616487979889\n",
      "loss 29    0.27280426025390625\n",
      "eval loss 0.3771148920059204\n",
      "loss 30    0.30472075939178467\n",
      "loss 31    1.1941286325454712\n",
      "loss 32    0.3548298478126526\n",
      "loss 33    0.38678038120269775\n",
      "loss 34    0.4011830687522888\n",
      "loss 35    1.1313326358795166\n",
      "loss 36    0.40147864818573\n",
      "loss 37    0.39530593156814575\n",
      "loss 38    0.37384164333343506\n",
      "loss 39    1.1012516021728516\n",
      "eval loss 0.3865692615509033\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9976, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 0     6935.806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1     6935.79833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 2     6935.7919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3141, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 3     6935.7841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3089, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 4     6935.78076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.3085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9974, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 5     6935.77783203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2993, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9977, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 6     6935.7783203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 7     6935.77685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9986, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 8     6935.7587890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(0.9992, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 9     6935.75439453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 10    6935.73486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 11    6935.7314453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2301, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 12    6935.69921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.2023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 13    6935.666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.1676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0062, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 14    6935.646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.1147, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 15    6935.58056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(4.0619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 16    6935.53125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.9821, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 17    6935.4521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.9068, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 18    6935.3701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.8284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 19    6935.296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.7564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 20    6935.2197265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.7017, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 21    6935.17041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 22    6935.12060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 23    6935.1005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5989, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 24    6935.072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0319, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 25    6935.0771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 26    6935.07763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 27    6935.119140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6104, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 28    6935.076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 29    6935.1279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 30    6935.1357421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0486, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 31    6935.13818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 32    6935.140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 33    6935.12548828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 34    6935.099609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6086, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 35    6935.07373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6081, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0716, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 36    6935.083984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5895, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0765, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 37    6935.0654296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 38    6935.06103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5949, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 39    6935.06884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.0932, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 40    6935.04931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.6115, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 41    6935.087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 42    6935.04736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5917, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 43    6935.0595703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 44    6935.03759765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5212, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 45    6934.98681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 46    6935.041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1432, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 47    6935.017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 48    6935.02685546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1514, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 49    6935.06005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 50    6935.00390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 51    6934.99609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1658, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 52    6934.990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1695, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 53    6935.0302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 54    6935.0205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 55    6934.978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1860, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 56    6934.9931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 57    6935.0166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 58    6935.02734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5171, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 59    6934.990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 60    6935.00146484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5158, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 61    6934.9912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2374, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 62    6935.00732421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5248, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 63    6934.9931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 64    6935.0166015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 65    6935.01611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 66    6935.01806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 67    6935.02294921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2841, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 68    6935.01611328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2919, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 69    6934.962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 70    6934.9873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.2999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 71    6934.9970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5284, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3009, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 72    6934.99658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3020, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 73    6935.0126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 74    6934.99951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5088, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3052, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 75    6934.98681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 76    6935.0126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5133, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 77    6934.9921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5085, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 78    6934.98095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 79    6934.99267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5193, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3217, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 80    6934.9921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5175, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3260, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 81    6934.99658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5276, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3305, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 82    6934.9931640625\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4982, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 83    6935.001953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5078, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3422, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 84    6934.9970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 85    6934.98388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5029, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 86    6934.96630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 87    6934.98046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 88    6934.96240234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5046, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 89    6934.9775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3784, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 90    6934.9970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 91    6934.974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 92    6934.97900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5043, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 93    6934.98974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4978, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3887, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 94    6934.9658203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 95    6934.970703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.3964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 96    6934.955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5000, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4000, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 97    6934.98486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 98    6934.9736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5052, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 99    6934.98779296875\n",
      "eval loss 3.5031802654266357\n",
      "Number training steps total: 40\n",
      "eval loss 474.5733337402344\n",
      "loss 0     472.347900390625\n",
      "loss 1     434.66265869140625\n",
      "loss 2     399.1605224609375\n",
      "loss 3     384.62548828125\n",
      "loss 4     332.60626220703125\n",
      "loss 5     302.45306396484375\n",
      "loss 6     273.0792541503906\n",
      "loss 7     264.61328125\n",
      "loss 8     220.198486328125\n",
      "loss 9     196.33477783203125\n",
      "eval loss 175.60699462890625\n",
      "loss 10    173.968017578125\n",
      "loss 11    169.1885986328125\n",
      "loss 12    134.2865447998047\n",
      "loss 13    116.41537475585938\n",
      "loss 14    100.68399810791016\n",
      "loss 15    97.38875579833984\n",
      "loss 16    72.80846405029297\n",
      "loss 17    61.02927780151367\n",
      "loss 18    50.857810974121094\n",
      "loss 19    51.4609489440918\n",
      "eval loss 34.346588134765625\n",
      "loss 20    33.51768493652344\n",
      "loss 21    26.67582893371582\n",
      "loss 22    20.741485595703125\n",
      "loss 23    22.179973602294922\n",
      "loss 24    11.77342414855957\n",
      "loss 25    8.423903465270996\n",
      "loss 26    5.888657569885254\n",
      "loss 27    8.547713279724121\n",
      "loss 28    2.4425816535949707\n",
      "loss 29    1.4819129705429077\n",
      "eval loss 1.0833498239517212\n",
      "loss 30    0.8846485614776611\n",
      "loss 31    2.8658359050750732\n",
      "loss 32    0.5811852812767029\n",
      "loss 33    0.776880145072937\n",
      "loss 34    1.1032482385635376\n",
      "loss 35    2.568047523498535\n",
      "loss 36    1.9641755819320679\n",
      "loss 37    2.429368495941162\n",
      "loss 38    2.9324378967285156\n",
      "loss 39    3.300844669342041\n",
      "eval loss 3.635862350463867\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 100   6934.97412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4855, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4191, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 101   6934.9560546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 102   6935.0029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4310, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 103   6934.96484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4793, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4365, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 104   6934.96044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4998, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4426, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 105   6934.97412109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 106   6934.96875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4962, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 107   6934.9775390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4980, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 108   6934.96875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4900, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 109   6934.955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4738, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 110   6934.9697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4790, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 111   6935.0068359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4929, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 112   6934.96826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4932, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.4935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 113   6934.96142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 114   6934.994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5021, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 115   6934.9833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4902, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 116   6934.96630859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 117   6934.97607421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5318, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 118   6934.97509765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 119   6934.94189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4889, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5466, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 120   6934.94677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4806, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5511, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 121   6934.93603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5542, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 122   6934.9541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4802, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 123   6934.9482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 124   6934.96142578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5623, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 125   6934.95703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 126   6934.9462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4865, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 127   6934.96044921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4725, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 128   6934.94140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4901, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 129   6934.95556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4920, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 130   6934.95556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5797, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 131   6934.96728515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4711, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 132   6934.92919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 133   6934.9326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4731, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 134   6934.94140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5809, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 135   6934.94189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 136   6934.9443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 137   6934.9404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 138   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 139   6934.9228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 140   6934.95556640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 141   6934.90478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.5978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 142   6934.93505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4816, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6027, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 143   6934.9462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 144   6934.94580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 145   6934.935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 146   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4726, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 147   6934.94091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4800, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 148   6934.96533203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 149   6934.9384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4555, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 150   6934.9248046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6506, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 151   6934.9375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 152   6934.92529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4586, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6617, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 153   6934.94140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 154   6934.9404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 155   6934.9453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 156   6934.94140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.6940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 157   6934.9326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7028, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 158   6934.91455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 159   6934.927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7197, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 160   6934.93359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 161   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7383, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 162   6934.94580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4497, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 163   6934.9267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 164   6934.93603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 165   6934.94677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4707, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7777, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 166   6934.9423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4110, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7872, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 167   6934.8974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.7967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 168   6934.9384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 169   6934.93212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8149, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 170   6934.9375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8257, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 171   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 172   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 173   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 174   6934.93994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 175   6934.94384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 176   6934.9453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 177   6934.93994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.8999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 178   6934.9326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 179   6934.9384765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 180   6934.93896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4581, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 181   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 182   6934.94921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9540, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 183   6934.9453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 184   6934.93896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(1.9858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 185   6934.931640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 186   6934.92919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 187   6934.93212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0343, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 188   6934.93603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4530, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0467, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 189   6934.90478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 190   6934.91552734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 191   6934.91796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 192   6934.93701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 193   6934.93017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 194   6934.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5157, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.0972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 195   6935.005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4691, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 196   6934.9453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 197   6934.93701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4599, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1238, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 198   6934.92333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 199   6934.935546875\n",
      "eval loss 3.4608025550842285\n",
      "Number training steps total: 40\n",
      "eval loss 97.71612548828125\n",
      "loss 0     96.40255737304688\n",
      "loss 1     80.29834747314453\n",
      "loss 2     66.32054901123047\n",
      "loss 3     63.48746871948242\n",
      "loss 4     41.85707092285156\n",
      "loss 5     31.96949005126953\n",
      "loss 6     23.616756439208984\n",
      "loss 7     22.120786666870117\n",
      "loss 8     11.215535163879395\n",
      "loss 9     6.890561103820801\n",
      "eval loss 4.258557319641113\n",
      "loss 10    3.9247260093688965\n",
      "loss 11    5.2729363441467285\n",
      "loss 12    0.923489511013031\n",
      "loss 13    0.7359163761138916\n",
      "loss 14    1.010823369026184\n",
      "loss 15    2.349539279937744\n",
      "loss 16    2.7548868656158447\n",
      "loss 17    3.829383611679077\n",
      "loss 18    4.876821517944336\n",
      "loss 19    5.3785400390625\n",
      "eval loss 6.291474342346191\n",
      "loss 20    6.349123001098633\n",
      "loss 21    6.840434551239014\n",
      "loss 22    6.957488059997559\n",
      "loss 23    6.304992198944092\n",
      "loss 24    6.499017715454102\n",
      "loss 25    5.929083824157715\n",
      "loss 26    5.254276275634766\n",
      "loss 27    4.9496588706970215\n",
      "loss 28    3.7964723110198975\n",
      "loss 29    3.102214813232422\n",
      "eval loss 2.4298810958862305\n",
      "loss 30    2.441866397857666\n",
      "loss 31    2.4073777198791504\n",
      "loss 32    1.4354679584503174\n",
      "loss 33    1.070289134979248\n",
      "loss 34    0.8412626385688782\n",
      "loss 35    2.883392810821533\n",
      "loss 36    0.6724417805671692\n",
      "loss 37    0.6991426944732666\n",
      "loss 38    0.7379236221313477\n",
      "loss 39    2.759315252304077\n",
      "eval loss 1.1255266666412354\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1414, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 200   6934.91650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4597, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 201   6934.9423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 202   6934.92138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 203   6934.9267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4872, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 204   6934.951171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 205   6934.943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 206   6934.912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 207   6934.9462890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.1989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 208   6934.9580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4548, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 209   6934.939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 210   6934.92333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 211   6934.9296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2355, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 212   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2431, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 213   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 214   6934.9375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 215   6934.962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 216   6934.9130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4368, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 217   6934.89990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.2980, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 218   6934.9111328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 219   6934.94677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4673, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 220   6934.94873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3169, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 221   6934.92529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 222   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3329, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 223   6934.89990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 224   6934.94580078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3482, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 225   6934.9521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 226   6934.9150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4293, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 227   6934.892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 228   6934.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.3961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 229   6934.96923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 230   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4163, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 231   6934.91162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 232   6934.91845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 233   6934.935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 234   6934.939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4475, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 235   6934.90869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 236   6934.89453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 237   6934.9208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 238   6934.962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4344, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 239   6934.9267578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4865, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 240   6934.9091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.4989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 241   6934.9365234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5092, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 242   6934.91455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 243   6934.9072265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4278, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 244   6934.8994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 245   6934.94873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 246   6934.91455078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 247   6934.92236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 248   6934.94091796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.5904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 249   6934.9208984375\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6024, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 250   6934.94482421875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 251   6934.9296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 252   6934.8857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 253   6934.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6730, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 254   6934.91748046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4300, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.6889, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 255   6934.9033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 256   6934.94677734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4578, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7174, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 257   6934.916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7346, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 258   6934.90087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 259   6934.88720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4516, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 260   6934.92724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 261   6934.93212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4287, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.7945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 262   6934.8994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.8145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 263   6934.91015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.8353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 264   6934.927734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.8626, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 265   6934.90576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4456, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.8910, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 266   6934.90966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4153, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.9192, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 267   6934.87841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4272, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.9418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 268   6934.90576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.9635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 269   6934.87890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(2.9810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 270   6934.89453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.0012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 271   6934.90673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.0235, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 272   6934.9033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.0518, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 273   6934.92529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.0840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 274   6934.89453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4213, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.1207, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 275   6934.89453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.1557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 276   6934.91259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.1907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 277   6934.91015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4132, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.2278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 278   6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.2662, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 279   6934.8974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4254, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.3011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 280   6934.89453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.3348, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 281   6934.92529296875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4460, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.3565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 282   6934.9482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.3852, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 283   6934.94189453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.4137, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 284   6934.91259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.4484, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 285   6934.9208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.4791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 286   6934.916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.5111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 287   6934.896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.5477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 288   6934.95263671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.5885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 289   6934.9306640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4367, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.6261, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 290   6934.91796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.6692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 291   6934.90966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.7047, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 292   6934.9150390625\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.7445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 293   6934.94287109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.7913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 294   6934.9384765625\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.8369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 295   6934.9375\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.8824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 296   6934.9482421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.9396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 297   6934.8876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(3.9987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 298   6934.919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4405, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.0589, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 299   6934.9228515625\n",
      "eval loss 3.4513320922851562\n",
      "Number training steps total: 40\n",
      "eval loss 81.23530578613281\n",
      "loss 0     80.33263397216797\n",
      "loss 1     64.1990966796875\n",
      "loss 2     49.60596466064453\n",
      "loss 3     43.99616241455078\n",
      "loss 4     26.415800094604492\n",
      "loss 5     18.092653274536133\n",
      "loss 6     11.452333450317383\n",
      "loss 7     9.785103797912598\n",
      "loss 8     3.242565155029297\n",
      "loss 9     1.4201478958129883\n",
      "eval loss 0.9857640862464905\n",
      "loss 10    0.8366053104400635\n",
      "loss 11    2.8744616508483887\n",
      "loss 12    2.2290706634521484\n",
      "loss 13    3.5737695693969727\n",
      "loss 14    4.996722221374512\n",
      "loss 15    6.01690673828125\n",
      "loss 16    7.3308024406433105\n",
      "loss 17    7.8817033767700195\n",
      "loss 18    8.215587615966797\n",
      "loss 19    7.081971168518066\n",
      "eval loss 7.300054550170898\n",
      "loss 20    7.401268005371094\n",
      "loss 21    6.6336669921875\n",
      "loss 22    5.664681434631348\n",
      "loss 23    4.4429168701171875\n",
      "loss 24    3.6379361152648926\n",
      "loss 25    2.762962579727173\n",
      "loss 26    1.992577075958252\n",
      "loss 27    2.2788004875183105\n",
      "loss 28    1.0428887605667114\n",
      "loss 29    0.8303123116493225\n",
      "eval loss 0.9048261046409607\n",
      "loss 30    0.7687501907348633\n",
      "loss 31    2.673618793487549\n",
      "loss 32    1.0074098110198975\n",
      "loss 33    1.167420506477356\n",
      "loss 34    1.3787870407104492\n",
      "loss 35    3.779843807220459\n",
      "loss 36    1.562791109085083\n",
      "loss 37    1.5654780864715576\n",
      "loss 38    1.5357333421707153\n",
      "loss 39    3.264207363128662\n",
      "eval loss 1.4765260219573975\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 300   6934.9208984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 301   6934.92236328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 302   6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2850, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 303   6934.8955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3285, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 304   6934.91943359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3824, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 305   6934.921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 306   6934.91650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4858, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 307   6934.9443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4242, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5175, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 308   6934.89990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 309   6934.921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4350, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5877, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 310   6934.9150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4181, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 311   6934.90283203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6591, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 312   6934.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6957, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 313   6934.923828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 314   6934.90576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 315   6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7882, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 316   6934.8984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4324, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 317   6934.91162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 318   6934.8671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 319   6934.92724609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 320   6934.9375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7420, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 321   6934.93408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 322   6934.90625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4066, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7002, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 323   6934.88671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4299, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 324   6934.884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4220, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 325   6934.8935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4296, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6512, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 326   6934.90673828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6409, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 327   6934.86572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 328   6934.900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4125, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 329   6934.89697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 330   6934.8896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4146, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 331   6934.90185546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4201, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 332   6934.89697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4773, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 333   6934.916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 334   6934.8857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4112, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 335   6934.89892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 336   6934.90234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 337   6934.90625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 338   6934.8955078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 339   6934.8974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2419, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 340   6934.876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2158, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 341   6934.90478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 342   6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4233, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1686, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 343   6934.89501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 344   6934.90234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1477, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 345   6934.92529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 346   6934.89892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 347   6934.92138671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 348   6934.908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4261, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1664, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 349   6934.90771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 350   6934.92431640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 351   6934.9130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4145, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 352   6934.87939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 353   6934.90966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.1975, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 354   6934.87744140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2015, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 355   6934.921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4162, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 356   6934.90087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 357   6934.91015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 358   6934.89697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4250, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.2801, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 359   6934.884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 360   6934.90576171875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4518, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 361   6934.953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4459, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 362   6934.904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.3714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 363   6934.921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4129, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4019, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 364   6934.86083984375\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4313, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 365   6934.9228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4601, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 366   6934.93994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.4964, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 367   6934.8828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 368   6934.92529296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 369   6934.90869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.5894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 370   6934.87353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 371   6934.91162109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3977, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 372   6934.85986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 373   6934.892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4109, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6705, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 374   6934.88525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.6844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 375   6934.90380859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7032, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 376   6934.92041015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4356, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7188, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 377   6934.896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4235, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 378   6934.8798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 379   6934.89501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.7792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 380   6934.93896484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4137, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8123, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 381   6934.8701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 382   6934.9013671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 383   6934.912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8726, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 384   6934.8828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8586, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 385   6934.90087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 386   6934.9052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8501, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 387   6934.8994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8571, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 388   6934.90234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8652, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 389   6934.900390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8714, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 390   6934.884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 391   6934.916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 392   6934.89990234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 393   6934.9140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 394   6934.90087890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4429, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8778, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 395   6934.93505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 396   6934.919921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 397   6934.88232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8733, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 398   6934.88525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8787, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 399   6934.892578125\n",
      "eval loss 3.4320144653320312\n",
      "Number training steps total: 40\n",
      "eval loss 45.328285217285156\n",
      "loss 0     44.91571807861328\n",
      "loss 1     30.868947982788086\n",
      "loss 2     19.80207633972168\n",
      "loss 3     15.53866958618164\n",
      "loss 4     5.639863014221191\n",
      "loss 5     2.2660720348358154\n",
      "loss 6     1.0071213245391846\n",
      "loss 7     2.424870252609253\n",
      "loss 8     2.7761030197143555\n",
      "loss 9     4.563668251037598\n",
      "eval loss 6.184124946594238\n",
      "loss 10    6.210068702697754\n",
      "loss 11    6.751412868499756\n",
      "loss 12    7.868800640106201\n",
      "loss 13    7.66220235824585\n",
      "loss 14    6.931751251220703\n",
      "loss 15    5.542818069458008\n",
      "loss 16    4.4958953857421875\n",
      "loss 17    3.224013090133667\n",
      "loss 18    2.1557095050811768\n",
      "loss 19    2.044647216796875\n",
      "eval loss 1.0074394941329956\n",
      "loss 20    0.9341372847557068\n",
      "loss 21    0.7981010675430298\n",
      "loss 22    0.9135246276855469\n",
      "loss 23    2.7136411666870117\n",
      "loss 24    1.5106709003448486\n",
      "loss 25    1.8369669914245605\n",
      "loss 26    2.0368008613586426\n",
      "loss 27    3.966846466064453\n",
      "loss 28    1.9777544736862183\n",
      "loss 29    1.7034450769424438\n",
      "eval loss 1.6458945274353027\n",
      "loss 30    1.5034055709838867\n",
      "loss 31    2.4557583332061768\n",
      "loss 32    0.988986074924469\n",
      "loss 33    0.8070629835128784\n",
      "loss 34    0.7453613877296448\n",
      "loss 35    1.9318674802780151\n",
      "loss 36    0.8273570537567139\n",
      "loss 37    0.9577515125274658\n",
      "loss 38    1.078543782234192\n",
      "loss 39    1.6553664207458496\n",
      "eval loss 1.251722812652588\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8795, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 400   6934.875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 401   6934.921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 402   6934.91650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4176, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8627, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 403   6934.88525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8488, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 404   6934.92333984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8367, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 405   6934.90771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4253, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 406   6934.88623046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4182, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 407   6934.91015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4185, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8399, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 408   6934.8974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 409   6934.8994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4116, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 410   6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 411   6934.89697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8951, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 412   6934.92626953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4136, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 413   6934.869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4186, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9234, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 414   6934.8837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9350, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 415   6934.9150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4236, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 416   6934.88818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9480, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 417   6934.91259765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 418   6934.90771484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9356, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 419   6934.83935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4292, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9141, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 420   6934.89501953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 421   6934.90576171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4123, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8592, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 422   6934.8837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3856, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8473, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 423   6934.8564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3950, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 424   6934.87890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4100, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8460, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 425   6934.88330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4034, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 426   6934.8779296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 427   6934.8720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 428   6934.89892578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4082, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 429   6934.87060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3988, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.8988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 430   6934.8837890625\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9166, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 431   6934.90478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3957, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9520, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 432   6934.86669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4208, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(4.9862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 433   6934.89404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 434   6934.89404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4049, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0353, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 435   6934.8681640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4023, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0519, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 436   6934.86572265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 437   6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4061, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 438   6934.88720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 439   6934.88037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3936, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.0921, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 440   6934.865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4075, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 441   6934.87939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4191, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1180, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 442   6934.88427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 443   6934.91650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4237, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1394, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 444   6934.908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 445   6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4093, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1635, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 446   6934.87158203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1779, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 447   6934.8662109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 448   6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.1968, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 449   6934.90625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4033, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2132, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 450   6934.890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4022, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 451   6934.88134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3893, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 452   6934.8369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3965, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 453   6934.86669921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3926, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 454   6934.85009765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 455   6934.86474609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3796, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 456   6934.87646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3864, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 457   6934.85888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3887, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.2885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 458   6934.876953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 459   6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3955, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 460   6934.87109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4016, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3354, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 461   6934.8642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4031, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 462   6934.87255859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3443, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 463   6934.875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3778, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 464   6934.87060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 465   6934.880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3939, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.3880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 466   6934.8544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.4077, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 467   6934.875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3749, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.4236, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 468   6934.8466796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3948, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.4417, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 469   6934.8701171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.4671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 470   6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.4863, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 471   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3840, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5106, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 472   6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3799, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 473   6934.861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3684, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5252, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 474   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 475   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3963, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 476   6934.88916015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3892, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.5881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 477   6934.85498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.6139, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 478   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.6293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 479   6934.861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.6433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 480   6934.87353515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4056, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.6720, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 481   6934.8857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3734, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 482   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 483   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 484   6934.84765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 485   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4026, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8026, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 486   6934.873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3775, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 487   6934.84912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8045, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 488   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 489   6934.873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3646, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 490   6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3835, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 491   6934.8544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7622, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 492   6934.86279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3574, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7549, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 493   6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7531, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 494   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7562, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 495   6934.84326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3741, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 496   6934.84521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3817, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 497   6934.85986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3562, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7630, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 498   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3542, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 499   6934.84033203125\n",
      "eval loss 3.377016544342041\n",
      "Number training steps total: 40\n",
      "eval loss 15.469330787658691\n",
      "loss 0     15.190585136413574\n",
      "loss 1     7.3432488441467285\n",
      "loss 2     2.7102150917053223\n",
      "loss 3     1.885204553604126\n",
      "loss 4     1.2512240409851074\n",
      "loss 5     2.7465200424194336\n",
      "loss 6     4.12947416305542\n",
      "loss 7     4.737872123718262\n",
      "loss 8     4.428744316101074\n",
      "loss 9     3.53702449798584\n",
      "eval loss 2.456549644470215\n",
      "loss 10    2.4762394428253174\n",
      "loss 11    1.9692199230194092\n",
      "loss 12    0.9163581728935242\n",
      "loss 13    0.7509542107582092\n",
      "loss 14    0.9155958294868469\n",
      "loss 15    2.463392734527588\n",
      "loss 16    1.6026067733764648\n",
      "loss 17    1.7364428043365479\n",
      "loss 18    1.6735601425170898\n",
      "loss 19    2.7382259368896484\n",
      "eval loss 1.2987098693847656\n",
      "loss 20    1.1870331764221191\n",
      "loss 21    0.8896867036819458\n",
      "loss 22    0.7331321835517883\n",
      "loss 23    1.3314568996429443\n",
      "loss 24    0.8080185651779175\n",
      "loss 25    0.9470285773277283\n",
      "loss 26    1.0889348983764648\n",
      "loss 27    1.6886811256408691\n",
      "loss 28    1.1708475351333618\n",
      "loss 29    1.0752450227737427\n",
      "eval loss 0.9698303937911987\n",
      "loss 30    0.9338518381118774\n",
      "loss 31    1.2098703384399414\n",
      "loss 32    0.7159274816513062\n",
      "loss 33    0.6456555724143982\n",
      "loss 34    0.6761881709098816\n",
      "loss 35    1.7001632452011108\n",
      "loss 36    0.7688145041465759\n",
      "loss 37    0.7552754282951355\n",
      "loss 38    0.741629421710968\n",
      "loss 39    1.3606762886047363\n",
      "eval loss 0.744304895401001\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 500   6934.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 501   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3825, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 502   6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7580, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 503   6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 504   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3931, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7424, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 505   6934.85400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 506   6934.85400390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 507   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 508   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3781, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7292, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 509   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7229, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 510   6934.8505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 511   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7392, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 512   6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7308, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 513   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 514   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3884, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 515   6934.873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7094, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 516   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7044, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 517   6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3829, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7018, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 518   6934.85888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 519   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 520   6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3883, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7142, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 521   6934.87060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3640, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 522   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 523   6934.84375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7567, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 524   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7746, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 525   6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7871, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 526   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7880, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 527   6934.85986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3672, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7917, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 528   6934.84375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7931, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 529   6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3757, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 530   6934.85302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7847, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 531   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3703, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7944, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 532   6934.8515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 533   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3791, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 534   6934.84765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 535   6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3891, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 536   6934.87060546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3715, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 537   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 538   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8624, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 539   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8661, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 540   6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8711, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 541   6934.8603515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3339, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 542   6934.78564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8812, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 543   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8924, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 544   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 545   6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3765, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9085, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 546   6934.85302734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3699, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9111, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 547   6934.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 548   6934.8642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 549   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9495, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 550   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3702, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 551   6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3423, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 552   6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3750, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9600, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 553   6934.8466796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 554   6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9456, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 555   6934.83837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 556   6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 557   6934.8564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 558   6934.8369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3194, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8915, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 559   6934.77783203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8615, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 560   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3705, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8351, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 561   6934.84228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3675, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 562   6934.84765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7941, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 563   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 564   6934.85498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3642, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7735, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 565   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 566   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3716, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7596, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 567   6934.86328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7541, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 568   6934.8466796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7532, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 569   6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3827, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7517, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 570   6934.865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 571   6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7555, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 572   6934.84130859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 573   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7553, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 574   6934.853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 575   6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 576   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7376, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 577   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3683, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 578   6934.853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 579   6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 580   6934.84716796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 581   6934.84912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7195, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 582   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 583   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7230, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 584   6934.8525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 585   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3633, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 586   6934.8388671875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3810, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 587   6934.89404296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.7757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 588   6934.8544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3695, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8021, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 589   6934.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8309, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 590   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8515, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 591   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8672, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 592   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.8843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 593   6934.849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 594   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 595   6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9418, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 596   6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 597   6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 598   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3618, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(5.9927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 599   6934.837890625\n",
      "eval loss 3.363651990890503\n",
      "Number training steps total: 40\n",
      "eval loss 20.177404403686523\n",
      "loss 0     19.742000579833984\n",
      "loss 1     9.954048156738281\n",
      "loss 2     3.714132308959961\n",
      "loss 3     1.6206592321395874\n",
      "loss 4     0.8956887722015381\n",
      "loss 5     2.5569307804107666\n",
      "loss 6     4.367677688598633\n",
      "loss 7     4.893536567687988\n",
      "loss 8     5.369350433349609\n",
      "loss 9     4.508989334106445\n",
      "eval loss 3.20037579536438\n",
      "loss 10    3.2292675971984863\n",
      "loss 11    2.3273818492889404\n",
      "loss 12    1.055046558380127\n",
      "loss 13    0.6057007908821106\n",
      "loss 14    0.6260429620742798\n",
      "loss 15    1.7215694189071655\n",
      "loss 16    1.4480056762695312\n",
      "loss 17    1.7890708446502686\n",
      "loss 18    1.9096918106079102\n",
      "loss 19    2.893868923187256\n",
      "eval loss 1.566821813583374\n",
      "loss 20    1.4508914947509766\n",
      "loss 21    1.0791528224945068\n",
      "loss 22    0.7349414229393005\n",
      "loss 23    1.4026658535003662\n",
      "loss 24    0.5662827491760254\n",
      "loss 25    0.7001082897186279\n",
      "loss 26    0.884526252746582\n",
      "loss 27    1.2636404037475586\n",
      "loss 28    1.152536392211914\n",
      "loss 29    1.1193561553955078\n",
      "eval loss 1.0149719715118408\n",
      "loss 30    0.9919449090957642\n",
      "loss 31    1.380678653717041\n",
      "loss 32    0.6694521903991699\n",
      "loss 33    0.5656970739364624\n",
      "loss 34    0.5433787107467651\n",
      "loss 35    1.2415122985839844\n",
      "loss 36    0.6371995806694031\n",
      "loss 37    0.6046543121337891\n",
      "loss 38    0.6376259326934814\n",
      "loss 39    1.630067229270935\n",
      "eval loss 0.6702311635017395\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0145, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 600   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3830, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 601   6934.84326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0489, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 602   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3783, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 603   6934.8515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 604   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 605   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 606   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3834, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0807, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 607   6934.84814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0861, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 608   6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.0904, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 609   6934.85693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 610   6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 611   6934.86181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3610, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 612   6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1481, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 613   6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1676, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 614   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 615   6934.86181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1940, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 616   6934.83984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3381, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2040, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 617   6934.80517578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 618   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 619   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3790, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2402, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 620   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 621   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 622   6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2766, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 623   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2885, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 624   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 625   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3096, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 626   6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 627   6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3325, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 628   6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3598, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3248, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 629   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 630   6934.80859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 631   6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3743, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 632   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2979, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 633   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 634   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 635   6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 636   6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 637   6934.8115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 638   6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2196, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 639   6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 640   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 641   6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1833, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 642   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 643   6934.80126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1837, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 644   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1767, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 645   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1710, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 646   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3521, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1559, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 647   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1461, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 648   6934.845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1401, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 649   6934.83544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 650   6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1390, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 651   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1458, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 652   6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1539, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 653   6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3653, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1703, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 654   6934.85791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1857, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 655   6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2113, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 656   6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2378, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 657   6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3577, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 658   6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2817, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 659   6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 660   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3013, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 661   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3048, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 662   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3036, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 663   6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2971, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 664   6934.7939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 665   6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 666   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3538, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2782, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 667   6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3178, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2755, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 668   6934.7861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2791, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 669   6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2781, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 670   6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3329, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 671   6934.75634765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 672   6934.80322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2333, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 673   6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2183, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 674   6934.80126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 675   6934.8564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3489, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.1978, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 676   6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3503, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 677   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2070, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 678   6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 679   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 680   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2114, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 681   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3199, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2276, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 682   6934.796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 683   6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 684   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 685   6934.8037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 686   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.2907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 687   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 688   6934.7802734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3275, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 689   6934.7978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3690, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3395, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 690   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3478, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 691   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3611, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 692   6934.8388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3530, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 693   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 694   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3635, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3644, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 695   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3666, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 696   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 697   6934.80908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.3966, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 698   6934.8017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 699   6934.791015625\n",
      "eval loss 3.350749969482422\n",
      "Number training steps total: 40\n",
      "eval loss 14.098998069763184\n",
      "loss 0     13.70347785949707\n",
      "loss 1     5.485711097717285\n",
      "loss 2     1.3985035419464111\n",
      "loss 3     1.4493274688720703\n",
      "loss 4     2.5778250694274902\n",
      "loss 5     4.377145767211914\n",
      "loss 6     4.879174709320068\n",
      "loss 7     3.750208616256714\n",
      "loss 8     3.0816807746887207\n",
      "loss 9     1.8021466732025146\n",
      "eval loss 0.9620959162712097\n",
      "loss 10    0.9236763715744019\n",
      "loss 11    1.2216781377792358\n",
      "loss 12    0.9056775569915771\n",
      "loss 13    1.416243553161621\n",
      "loss 14    1.768447995185852\n",
      "loss 15    2.865206480026245\n",
      "loss 16    1.7226611375808716\n",
      "loss 17    1.417137622833252\n",
      "loss 18    0.9932727813720703\n",
      "loss 19    1.2508316040039062\n",
      "eval loss 0.6990215182304382\n",
      "loss 20    0.6501457691192627\n",
      "loss 21    0.7730203866958618\n",
      "loss 22    0.9956313967704773\n",
      "loss 23    1.4311145544052124\n",
      "loss 24    1.2974616289138794\n",
      "loss 25    1.2302212715148926\n",
      "loss 26    1.0509142875671387\n",
      "loss 27    1.1800696849822998\n",
      "loss 28    0.716286301612854\n",
      "loss 29    0.6330019235610962\n",
      "eval loss 0.7021970152854919\n",
      "loss 30    0.6190578937530518\n",
      "loss 31    1.749479055404663\n",
      "loss 32    0.7570618987083435\n",
      "loss 33    0.7899740934371948\n",
      "loss 34    0.7328428030014038\n",
      "loss 35    1.3252146244049072\n",
      "loss 36    0.6286765336990356\n",
      "loss 37    0.624312698841095\n",
      "loss 38    0.6200085878372192\n",
      "loss 39    1.139427900314331\n",
      "eval loss 0.7483411431312561\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3651, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4121, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 700   6934.84521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3676, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 701   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3474, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 702   6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 703   6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3202, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 704   6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4087, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 705   6934.84326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4155, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 706   6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 707   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4267, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 708   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 709   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4565, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 710   6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3601, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4697, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 711   6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3167, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.4923, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 712   6934.791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.5067, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 713   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.5296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 714   6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.5545, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 715   6934.80126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.5702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 716   6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.5926, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 717   6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.6177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 718   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.6452, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 719   6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.6681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 720   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.6950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 721   6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3401, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.7148, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 722   6934.81591796875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.7250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 723   6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.7490, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 724   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.7772, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 725   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3358, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.8057, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 726   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3246, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.8372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 727   6934.794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3551, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.8815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 728   6934.84521484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3643, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.9324, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 729   6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(6.9763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 730   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.0128, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 731   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.0558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 732   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.0866, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 733   6934.7998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.1115, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 734   6934.78369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.1306, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 735   6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.1522, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 736   6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.1770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 737   6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2035, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 738   6934.83935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 739   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3647, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2349, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 740   6934.8583984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2469, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 741   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 742   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2776, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 743   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3593, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.2943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 744   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3496, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.3279, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 745   6934.80615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.3536, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 746   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.3745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 747   6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.3954, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 748   6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4164, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 749   6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 750   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4479, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 751   6934.8369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4525, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 752   6934.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4604, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 753   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 754   6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4893, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 755   6934.8515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3483, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4913, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 756   6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3395, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 757   6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.4849, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 758   6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 759   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5147, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 760   6934.7958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 761   6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3464, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 762   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5576, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 763   6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5620, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 764   6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5673, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 765   6934.85498046875\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.5829, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 766   6934.86279296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.6080, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 767   6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.6291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 768   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3656, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.6578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 769   6934.85205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.6952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 770   6934.8037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.7344, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 771   6934.78369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3564, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.7647, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 772   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.7792, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 773   6934.85498046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.8012, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 774   6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.8369, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 775   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.8681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 776   6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.8856, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 777   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.9031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 778   6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.9385, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 779   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3247, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(7.9876, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 780   6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 781   6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.0722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 782   6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3595, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.1210, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 783   6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.1636, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 784   6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.2153, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 785   6934.81298828125\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.2634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 786   6934.86962890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3659, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.3244, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 787   6934.8505859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.3835, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 788   6934.79052734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.4363, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 789   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.4764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 790   6934.8349609375\n",
      "dpo_loss= tensor(0.6932, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.5332, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 791   6934.84228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.6042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 792   6934.853515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3544, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.6739, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 793   6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.7389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 794   6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.7969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 795   6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.8433, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 796   6934.810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.8883, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 797   6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.9330, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 798   6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3819, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(8.9988, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 799   6934.85107421875\n",
      "eval loss 3.3502988815307617\n",
      "Number training steps total: 40\n",
      "eval loss 6.802669048309326\n",
      "loss 0     6.667962074279785\n",
      "loss 1     1.659214735031128\n",
      "loss 2     0.9582318067550659\n",
      "loss 3     2.3735525608062744\n",
      "loss 4     3.300790309906006\n",
      "loss 5     2.803986072540283\n",
      "loss 6     1.7606685161590576\n",
      "loss 7     1.0929328203201294\n",
      "loss 8     0.7643868327140808\n",
      "loss 9     1.0470695495605469\n",
      "eval loss 1.5774985551834106\n",
      "loss 10    1.492794394493103\n",
      "loss 11    2.1867876052856445\n",
      "loss 12    1.408266544342041\n",
      "loss 13    1.073293924331665\n",
      "loss 14    0.7854676246643066\n",
      "loss 15    1.1784889698028564\n",
      "loss 16    0.8428682684898376\n",
      "loss 17    1.0966047048568726\n",
      "loss 18    1.1931477785110474\n",
      "loss 19    1.2130409479141235\n",
      "eval loss 0.9630485773086548\n",
      "loss 20    0.9312087297439575\n",
      "loss 21    0.7443513870239258\n",
      "loss 22    0.6480779647827148\n",
      "loss 23    1.0423831939697266\n",
      "loss 24    0.7397797107696533\n",
      "loss 25    0.7854585647583008\n",
      "loss 26    0.7840516567230225\n",
      "loss 27    1.2516573667526245\n",
      "loss 28    0.6253567934036255\n",
      "loss 29    0.5777245759963989\n",
      "eval loss 0.6545842885971069\n",
      "loss 30    0.6226674318313599\n",
      "loss 31    0.8708086013793945\n",
      "loss 32    0.7197152972221375\n",
      "loss 33    0.712593674659729\n",
      "loss 34    0.6715165376663208\n",
      "loss 35    0.7352673411369324\n",
      "loss 36    0.5475541949272156\n",
      "loss 37    0.5665530562400818\n",
      "loss 38    0.5465197563171387\n",
      "loss 39    0.8812174797058105\n",
      "eval loss 0.616690993309021\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 800   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.1237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 801   6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3514, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.1881, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 802   6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.2487, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 803   6934.857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.3117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 804   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.3681, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 805   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.4215, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 806   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3720, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.4618, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 807   6934.86181640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.5065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 808   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3523, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.5558, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 809   6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.6069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 810   6934.80908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.6494, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 811   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.7140, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 812   6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.7793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 813   6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.8339, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 814   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.8973, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 815   6934.83837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3512, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.9421, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 816   6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(9.9757, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 817   6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 818   6934.8115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0189, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 819   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 820   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0314, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 821   6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3666, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0552, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 822   6934.8564453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.0799, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 823   6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.1083, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 824   6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3682, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.1447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 825   6934.8447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.1855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 826   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3667, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.2334, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 827   6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.2650, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 828   6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.3136, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 829   6934.861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3383, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.3805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 830   6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3374, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.4455, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 831   6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.4900, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 832   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.5186, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 833   6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.5341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 834   6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.5628, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 835   6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3533, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.5952, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 836   6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.6264, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 837   6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.6544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 838   6934.791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3822, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.6750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 839   6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.6897, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 840   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3736, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7059, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 841   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3688, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7258, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 842   6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3487, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7335, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 843   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7437, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 844   6934.7958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7556, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 845   6934.85107421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7607, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 846   6934.79736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 847   6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7523, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 848   6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3263, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7521, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 849   6934.80615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7582, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 850   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.7796, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 851   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.8212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 852   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.8684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 853   6934.80419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.8890, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 854   6934.8037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 855   6934.7939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9241, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 856   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 857   6934.841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9563, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 858   6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9718, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 859   6934.80712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3571, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9938, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 860   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3469, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0182, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 861   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0167, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 862   6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 863   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3409, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 864   6934.80615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3387, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9831, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 865   6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9808, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 866   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3240, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 867   6934.78173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9821, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 868   6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 869   6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3316, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9832, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 870   6934.79736328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 871   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9813, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 872   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(10.9907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 873   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3488, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0131, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 874   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0406, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 875   6934.7958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0640, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 876   6934.79345703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.0902, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 877   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.1286, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 878   6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.1668, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 879   6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3394, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.2053, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 880   6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.2663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 881   6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.3108, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 882   6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.3327, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 883   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3536, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.3793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 884   6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.4297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 885   6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.4802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 886   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.5280, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 887   6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.5793, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 888   6934.8046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.6293, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 889   6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3447, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.6891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 890   6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.7694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 891   6934.7841796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.8331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 892   6934.80712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.8888, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 893   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(11.9574, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 894   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3605, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 895   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.0751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 896   6934.8466796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3585, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.1266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 897   6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.1678, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 898   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.2193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 899   6934.8291015625\n",
      "eval loss 3.356168031692505\n",
      "Number training steps total: 40\n",
      "eval loss 6.568957805633545\n",
      "loss 0     6.412354469299316\n",
      "loss 1     1.3955695629119873\n",
      "loss 2     0.9217827320098877\n",
      "loss 3     2.4015965461730957\n",
      "loss 4     3.1921262741088867\n",
      "loss 5     2.5187835693359375\n",
      "loss 6     1.3772733211517334\n",
      "loss 7     0.9990038871765137\n",
      "loss 8     0.6942309141159058\n",
      "loss 9     1.1417381763458252\n",
      "eval loss 1.5976170301437378\n",
      "loss 10    1.5324256420135498\n",
      "loss 11    2.488593101501465\n",
      "loss 12    1.163703441619873\n",
      "loss 13    0.7744649648666382\n",
      "loss 14    0.5858086347579956\n",
      "loss 15    1.0352632999420166\n",
      "loss 16    0.9428454041481018\n",
      "loss 17    1.146467685699463\n",
      "loss 18    1.1630818843841553\n",
      "loss 19    1.0957015752792358\n",
      "eval loss 0.7441421747207642\n",
      "loss 20    0.7377020120620728\n",
      "loss 21    0.5898518562316895\n",
      "loss 22    0.5749490261077881\n",
      "loss 23    1.0683095455169678\n",
      "loss 24    0.7577806711196899\n",
      "loss 25    0.758346438407898\n",
      "loss 26    0.6889315843582153\n",
      "loss 27    0.9285705089569092\n",
      "loss 28    0.5130113363265991\n",
      "loss 29    0.5824828147888184\n",
      "eval loss 0.6415603160858154\n",
      "loss 30    0.641445517539978\n",
      "loss 31    0.8454462885856628\n",
      "loss 32    0.704262912273407\n",
      "loss 33    0.638103187084198\n",
      "loss 34    0.5645330548286438\n",
      "loss 35    0.9941200017929077\n",
      "loss 36    0.5155491828918457\n",
      "loss 37    0.5281456112861633\n",
      "loss 38    0.5737673044204712\n",
      "loss 39    0.9163656234741211\n",
      "eval loss 0.5521522760391235\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.2543, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 900   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.2688, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 901   6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.2811, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 902   6934.80859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3336, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.2907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 903   6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3461, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3003, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 904   6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 905   6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3494, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 906   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3360, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 907   6934.80419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3168, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 908   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3491, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3282, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 909   6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3639, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 910   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 911   6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.3960, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 912   6934.80419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.4161, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 913   6934.8388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.4528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 914   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3376, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.4925, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 915   6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.5219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 916   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.5610, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 917   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3142, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.5908, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 918   6934.787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6289, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 919   6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6551, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 920   6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6903, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 921   6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3560, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 922   6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 923   6934.7978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7513, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 924   6934.8525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 925   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7785, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 926   6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3476, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7771, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 927   6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7788, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 928   6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3515, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7920, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 929   6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3670, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 930   6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 931   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8405, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 932   6934.84814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 933   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3480, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8898, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 934   6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9025, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 935   6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 936   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3657, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9671, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 937   6934.85205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 938   6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 939   6934.81982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0572, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 940   6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0894, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 941   6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3769, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 942   6934.85693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3600, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 943   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 944   6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3648, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 945   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3450, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2178, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 946   6934.80908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3638, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2250, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 947   6934.8486328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2321, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 948   6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3509, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2581, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 949   6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2810, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 950   6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4152, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 951   6934.89697265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3719, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 952   6934.84326171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3685, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 953   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3649, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 954   6934.83544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 955   6934.84765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 956   6934.8466796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3484, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3058, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 957   6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3102, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 958   6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3101, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 959   6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3289, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 960   6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3500, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3095, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 961   6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3416, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 962   6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 963   6934.837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3531, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3079, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 964   6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2907, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 965   6934.8388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2737, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 966   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 967   6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 968   6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 969   6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1687, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 970   6934.80517578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3539, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1297, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 971   6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3438, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 972   6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0312, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 973   6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3475, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9930, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 974   6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 975   6934.81982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9463, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 976   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 977   6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9203, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 978   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3525, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9246, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 979   6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9441, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 980   6934.83837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3446, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9750, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 981   6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3698, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0031, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 982   6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0341, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 983   6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 984   6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3498, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0579, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 985   6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0868, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 986   6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3603, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0981, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 987   6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3563, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 988   6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3596, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 989   6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0605, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 990   6934.83203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3449, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0372, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 991   6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9934, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 992   6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3347, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9616, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 993   6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 994   6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8945, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 995   6934.83984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 996   6934.7978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3314, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8360, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 997   6934.79150390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3537, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8100, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 998   6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8039, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 999   6934.8134765625\n",
      "eval loss 3.3488426208496094\n",
      "Number training steps total: 40\n",
      "eval loss 4.525561809539795\n",
      "loss 0     4.400494575500488\n",
      "loss 1     0.619893491268158\n",
      "loss 2     1.4807372093200684\n",
      "loss 3     2.2374463081359863\n",
      "loss 4     2.0542235374450684\n",
      "loss 5     1.047122836112976\n",
      "loss 6     0.5249643325805664\n",
      "loss 7     1.3270295858383179\n",
      "loss 8     1.1248704195022583\n",
      "loss 9     1.2056891918182373\n",
      "eval loss 1.0090508460998535\n",
      "loss 10    0.9630488753318787\n",
      "loss 11    1.0773322582244873\n",
      "loss 12    0.5074377059936523\n",
      "loss 13    0.658698558807373\n",
      "loss 14    0.8785518407821655\n",
      "loss 15    0.9877583384513855\n",
      "loss 16    0.8444875478744507\n",
      "loss 17    0.631767749786377\n",
      "loss 18    0.46336090564727783\n",
      "loss 19    0.8144025802612305\n",
      "eval loss 0.620817244052887\n",
      "loss 20    0.5719641447067261\n",
      "loss 21    0.6337036490440369\n",
      "loss 22    0.5993194580078125\n",
      "loss 23    0.8567044138908386\n",
      "loss 24    0.4530501961708069\n",
      "loss 25    0.46894699335098267\n",
      "loss 26    0.5363353490829468\n",
      "loss 27    0.8200318217277527\n",
      "loss 28    0.5684781670570374\n",
      "loss 29    0.5137197971343994\n",
      "eval loss 0.46592283248901367\n",
      "loss 30    0.4602787494659424\n",
      "loss 31    0.7861544489860535\n",
      "loss 32    0.44510897994041443\n",
      "loss 33    0.48078322410583496\n",
      "loss 34    0.43851616978645325\n",
      "loss 35    0.9268602728843689\n",
      "loss 36    0.4203537702560425\n",
      "loss 37    0.43402594327926636\n",
      "loss 38    0.4681701362133026\n",
      "loss 39    0.6173575520515442\n",
      "eval loss 0.47129344940185547\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7844, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1000  6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7684, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1001  6934.80322265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7404, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1002  6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7082, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1003  6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3704, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6853, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1004  6934.85693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3604, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6780, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1005  6934.83837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3519, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6749, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1006  6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3148, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1007  6934.796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3606, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6448, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1008  6934.83544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1009  6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3553, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6239, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1010  6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3567, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6283, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1011  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6416, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1012  6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6585, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1013  6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6725, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1014  6934.86865234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.6943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1015  6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3634, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7199, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1016  6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3641, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7474, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1017  6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7646, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1018  6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1019  6934.8046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.7991, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1020  6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1021  6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8391, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1022  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1023  6934.81982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3722, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8598, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1024  6934.8447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3631, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8625, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1025  6934.83984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8741, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1026  6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1027  6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8729, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1028  6934.85205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8740, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1029  6934.83740234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.8859, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1030  6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9068, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1031  6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9205, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1032  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3669, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1033  6934.845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(12.9804, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1034  6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3615, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0212, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1035  6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0242, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1036  6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0184, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1037  6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0127, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1038  6934.83935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0150, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1039  6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3712, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0270, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1040  6934.8525390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3677, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1041  6934.8544921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3238, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0485, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1042  6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3400, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1043  6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3779, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0727, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1044  6934.85693359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3627, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0822, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1045  6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3613, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.0937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1046  6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3269, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1146, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1047  6934.79541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3502, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1291, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1048  6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3318, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1049  6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1050  6934.7958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3255, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2088, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1051  6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1052  6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.2826, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1053  6934.845703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3202, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1054  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.3655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1055  6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3637, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.4198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1056  6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3482, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.4659, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1057  6934.81982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.4892, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1058  6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1059  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3369, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5451, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1060  6934.80615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5736, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1061  6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3385, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5851, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1062  6934.80615234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5891, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1063  6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3441, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5717, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1064  6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3378, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5573, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1065  6934.798828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5439, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1066  6934.81982421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3570, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5470, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1067  6934.8388671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5619, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1068  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3668, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5786, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1069  6934.84228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.5955, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1070  6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3568, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6074, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1071  6934.8369140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1072  6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3357, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6499, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1073  6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3485, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6578, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1074  6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3156, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6603, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1075  6934.80029296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3652, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6721, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1076  6934.83935546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3430, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.6927, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1077  6934.81591796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3561, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7073, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1078  6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3372, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1079  6934.80126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7157, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1080  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3375, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7042, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1081  6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1082  6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7065, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1083  6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7159, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1084  6934.81396484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3435, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7233, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1085  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3320, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1086  6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3572, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.7609, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1087  6934.8447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3448, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.8043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1088  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3412, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.8491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1089  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3504, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.8962, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1090  6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.9338, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1091  6934.78125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3619, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.9393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1092  6934.8310546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3397, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.9503, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1093  6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3343, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(13.9836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1094  6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.0011, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1095  6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.0219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1096  6934.7998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.0403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1097  6934.82373046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3437, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.0742, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1098  6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.1193, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1099  6934.82421875\n",
      "eval loss 3.3458452224731445\n",
      "Number training steps total: 40\n",
      "eval loss 1.9524911642074585\n",
      "loss 0     1.8637381792068481\n",
      "loss 1     0.7581672072410583\n",
      "loss 2     1.5624580383300781\n",
      "loss 3     0.9480037093162537\n",
      "loss 4     0.4259272813796997\n",
      "loss 5     0.5911530256271362\n",
      "loss 6     0.8297735452651978\n",
      "loss 7     1.2675501108169556\n",
      "loss 8     0.42208132147789\n",
      "loss 9     0.4936100244522095\n",
      "eval loss 0.6905682682991028\n",
      "loss 10    0.6876773834228516\n",
      "loss 11    0.8258926272392273\n",
      "loss 12    0.5739806890487671\n",
      "loss 13    0.4203113317489624\n",
      "loss 14    0.41169777512550354\n",
      "loss 15    1.0346629619598389\n",
      "loss 16    0.5172945857048035\n",
      "loss 17    0.4086228311061859\n",
      "loss 18    0.3953183889389038\n",
      "loss 19    0.6840709447860718\n",
      "eval loss 0.532543957233429\n",
      "loss 20    0.5199946165084839\n",
      "loss 21    0.5219656229019165\n",
      "loss 22    0.4334298074245453\n",
      "loss 23    0.7772250175476074\n",
      "loss 24    0.39896082878112793\n",
      "loss 25    0.4032612442970276\n",
      "loss 26    0.41932129859924316\n",
      "loss 27    0.7250317931175232\n",
      "loss 28    0.3960454761981964\n",
      "loss 29    0.4330574870109558\n",
      "eval loss 0.4649794101715088\n",
      "loss 30    0.4474533200263977\n",
      "loss 31    0.6301919221878052\n",
      "loss 32    0.397052526473999\n",
      "loss 33    0.38049477338790894\n",
      "loss 34    0.37702447175979614\n",
      "loss 35    0.7592549324035645\n",
      "loss 36    0.3956771790981293\n",
      "loss 37    0.3732735216617584\n",
      "loss 38    0.39206570386886597\n",
      "loss 39    0.7056107521057129\n",
      "eval loss 0.42845088243484497\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3403, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.1597, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1100  6934.810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.1984, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1101  6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.2299, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1102  6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.2544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1103  6934.80712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3290, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.2745, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1104  6934.7998046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3227, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.2959, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1105  6934.791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1106  6934.80712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3117, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1107  6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3352, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1108  6934.8115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3450, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1109  6934.80908203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1110  6934.8115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3507, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.3999, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1111  6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3501, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.4361, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1112  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.4751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1113  6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3377, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.5219, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1114  6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.5534, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1115  6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.5884, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1116  6934.81884765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3321, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.6187, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1117  6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.6505, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1118  6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3432, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.6800, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1119  6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3607, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.7143, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1120  6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4097, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.7497, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1121  6934.880859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3609, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.7783, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1122  6934.8349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4556, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.8172, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1123  6934.9296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.8614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1124  6934.90869140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.9005, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1125  6934.912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.9403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1126  6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4275, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.9643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1127  6934.904296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4265, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(14.9845, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1128  6934.90478515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0029, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1129  6934.91650390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0129, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1130  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4770, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0243, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1131  6934.94873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0340, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1132  6934.89794921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4844, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0447, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1133  6934.966796875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0683, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1134  6934.84228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.5140, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.0855, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1135  6934.98974609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3815, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1136  6934.85205078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4709, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1359, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1137  6934.93994140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4179, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1138  6934.888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4143, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1557, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1139  6934.88720703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3918, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1140  6934.85986328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1805, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1141  6934.873046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4070, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1719, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1142  6934.8828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1613, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1143  6934.859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.4051, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1612, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1144  6934.87646484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1685, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1145  6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3625, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1731, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1146  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3623, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1815, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1147  6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3728, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1690, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1148  6934.84375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3735, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1593, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1149  6934.84912109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3580, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1508, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1150  6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1453, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1151  6934.857421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3468, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1152  6934.8232421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3792, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1153  6934.85888671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3554, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1637, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1154  6934.83642578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3632, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.1937, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1155  6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.2304, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1156  6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.2614, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1157  6934.82861328125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3622, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.2836, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1158  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.3038, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1159  6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3661, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.3208, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1160  6934.8408203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3349, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.3396, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1161  6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.3528, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1162  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3210, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.3764, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1163  6934.7939453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.4069, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1164  6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3508, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.4317, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1165  6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3338, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.4715, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1166  6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3590, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.5043, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1167  6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3384, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.5442, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1168  6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.5879, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1169  6934.8056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3546, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.6295, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1170  6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3386, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.6642, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1171  6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.6909, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1172  6934.8134765625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3371, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7089, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1173  6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3529, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7251, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1174  6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7526, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1175  6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1176  6934.84228515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7577, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1177  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1178  6934.8037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7751, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1179  6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.7969, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1180  6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1181  6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3591, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8403, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1182  6934.8291015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3639, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8590, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1183  6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3366, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8655, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1184  6934.80126953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3404, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1185  6934.810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3612, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8840, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1186  6934.83154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3388, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.8918, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1187  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3499, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1188  6934.83251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3219, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9177, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1189  6934.791015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3362, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9266, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1190  6934.80810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3471, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9298, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1191  6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3434, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9274, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1192  6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3226, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9214, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1193  6934.79443359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3306, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9190, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1194  6934.80224609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3417, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1195  6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1196  6934.810546875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1197  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3545, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1198  6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3535, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9237, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1199  6934.8291015625\n",
      "eval loss 3.346595048904419\n",
      "Number training steps total: 40\n",
      "eval loss 1.4049967527389526\n",
      "loss 0     1.3061624765396118\n",
      "loss 1     1.2395057678222656\n",
      "loss 2     1.1027055978775024\n",
      "loss 3     0.6417241096496582\n",
      "loss 4     0.5180269479751587\n",
      "loss 5     0.7082341909408569\n",
      "loss 6     0.552847146987915\n",
      "loss 7     0.7315799593925476\n",
      "loss 8     0.5267598032951355\n",
      "loss 9     0.702908456325531\n",
      "eval loss 0.5994454622268677\n",
      "loss 10    0.5956709980964661\n",
      "loss 11    0.6619117856025696\n",
      "loss 12    0.4039045572280884\n",
      "loss 13    0.432657927274704\n",
      "loss 14    0.4719306528568268\n",
      "loss 15    0.8191458582878113\n",
      "loss 16    0.37513217329978943\n",
      "loss 17    0.45522356033325195\n",
      "loss 18    0.49943578243255615\n",
      "loss 19    0.6903828382492065\n",
      "eval loss 0.4151007831096649\n",
      "loss 20    0.3916497230529785\n",
      "loss 21    0.38116544485092163\n",
      "loss 22    0.3927593231201172\n",
      "loss 23    0.8460501432418823\n",
      "loss 24    0.3741339445114136\n",
      "loss 25    0.3699987530708313\n",
      "loss 26    0.41759341955184937\n",
      "loss 27    0.569128155708313\n",
      "loss 28    0.4025782644748688\n",
      "loss 29    0.39413338899612427\n",
      "eval loss 0.40569913387298584\n",
      "loss 30    0.3615967929363251\n",
      "loss 31    0.6599330902099609\n",
      "loss 32    0.3760339617729187\n",
      "loss 33    0.36929410696029663\n",
      "loss 34    0.3798130750656128\n",
      "loss 35    0.6764482259750366\n",
      "loss 36    0.39792555570602417\n",
      "loss 37    0.37626540660858154\n",
      "loss 38    0.3674551844596863\n",
      "loss 39    0.6599353551864624\n",
      "eval loss 0.404167115688324\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9171, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1200  6934.82177734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9104, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1201  6934.83447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9014, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1202  6934.82421875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3334, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9066, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1203  6934.79541015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9198, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1204  6934.8427734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3427, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1205  6934.81689453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9648, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1206  6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(15.9963, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1207  6934.80712890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.0254, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1208  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3522, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.0457, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1209  6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3280, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.0712, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1210  6934.802734375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3470, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.0987, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1211  6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3439, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.1272, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1212  6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.1560, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1213  6934.80859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3493, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.1819, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1214  6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3391, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.2090, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1215  6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3559, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.2389, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1216  6934.830078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.2763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1217  6934.8046875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3365, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.3116, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1218  6934.8095703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3302, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.3471, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1219  6934.79833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3532, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.3728, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1220  6934.8271484375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3433, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.4007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1221  6934.814453125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3282, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.4240, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1222  6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3342, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.4537, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1223  6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3359, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.4946, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1224  6934.80859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.5316, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1225  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3617, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.5663, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1226  6934.83837890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3486, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.6050, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1227  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.6370, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1228  6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3407, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.6752, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1229  6934.8125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.7122, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1230  6934.8212890625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.7408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1231  6934.833984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3566, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.7692, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1232  6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.7953, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1233  6934.81103515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3420, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.8281, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1234  6934.81787109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.8634, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1235  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3620, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.9007, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1236  6934.83349609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3312, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.9368, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1237  6934.8017578125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3527, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.9702, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1238  6934.82568359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3467, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(16.9989, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1239  6934.82275390625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3569, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.0263, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1240  6934.826171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.0566, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1241  6934.8173828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3462, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.0943, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1242  6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3303, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.1296, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1243  6934.80419921875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3710, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.1654, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1244  6934.849609375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3680, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.2017, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1245  6934.84619140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.2445, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1246  6934.84423828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3511, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.2862, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1247  6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.3337, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1248  6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3524, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.3722, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1249  6934.82666015625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3664, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.4054, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1250  6934.8447265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3541, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.4434, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1251  6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3540, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.4760, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1252  6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5037, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1253  6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3584, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5328, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1254  6934.82763671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5529, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1255  6934.81494140625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3513, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1256  6934.822265625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3466, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5802, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1257  6934.8193359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3505, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.5961, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1258  6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3440, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.6201, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1259  6934.83056640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3445, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.6827, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1260  6934.8154296875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3477, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.7384, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1261  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.7763, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1262  6934.82080078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3426, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.8220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1263  6934.818359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3363, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.8594, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1264  6934.8076171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3644, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.8967, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1265  6934.8359375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3413, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.9277, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1266  6934.81201171875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3346, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.9621, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1267  6934.8037109375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(17.9843, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1268  6934.84033203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.0093, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1269  6934.82470703125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3453, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.0256, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1270  6934.81640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3399, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.0544, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1271  6934.8115234375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3455, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.0823, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1272  6934.828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.1200, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1273  6934.81005859375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.1491, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1274  6934.806640625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3305, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.1816, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1275  6934.7978515625\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3588, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.2076, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1276  6934.8251953125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.2249, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1277  6934.81298828125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3431, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.2408, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1278  6934.8203125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3348, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.2693, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1279  6934.80078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.2895, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1280  6934.79638671875\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3575, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.3081, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1281  6934.8330078125\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3528, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.3273, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1282  6934.82958984375\n",
      "dpo_loss= tensor(0.6931, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "nll_loss= tensor(3.3234, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "avg_std= tensor(18.3397, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "loss 1283  6934.79931640625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m get_dpo_loss_partial \u001b[38;5;241m=\u001b[39m partial(get_dpo_loss, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, dpo_loss_beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, reward_model\u001b[38;5;241m=\u001b[39mreward_model, train_reward_model\u001b[38;5;241m=\u001b[39mtrain_reward_model_partial)\n\u001b[1;32m     12\u001b[0m dpo_trained_model_with_reward_model \u001b[38;5;241m=\u001b[39m QuietStarLanguageModelLSTM(\u001b[38;5;28mlen\u001b[39m(vocab), \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 13\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_dpo_loss_partial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_nll\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpo_trained_model_with_reward_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(get_loss_fn, eval_fn, model, batch_size, epochs, train_dl, eval_every, print_stuff)\u001b[0m\n\u001b[1;32m     35\u001b[0m     eval_losses\u001b[38;5;241m.\u001b[39mappend((i, eval_loss))\n\u001b[1;32m     36\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 37\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mget_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m print_stuff:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<5\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[35], line 37\u001b[0m, in \u001b[0;36mget_dpo_loss\u001b[0;34m(model, inputs, beta_2, target_model, num_samples, dpo_loss_beta, nll_loss_beta, forward_kl_reward, reward_model, train_reward_model)\u001b[0m\n\u001b[1;32m     35\u001b[0m dpo_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(repeat_gold_action_weight\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m*\u001b[39m repeat_log_weight_on_hidden_states)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m dpo_loss \u001b[38;5;241m*\u001b[39m dpo_loss_beta \u001b[38;5;241m+\u001b[39m nll_loss \u001b[38;5;241m*\u001b[39m nll_loss_beta\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdpo_loss\u001b[49m\u001b[38;5;132;43;01m= }\u001b[39;49;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnll_loss\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m dist \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_hidden_state_dist(repeat_inputs)\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/_tensor.py:523\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    520\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    521\u001b[0m     )\n\u001b[1;32m    522\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/_tensor_str.py:708\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    707\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 708\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/_tensor_str.py:625\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    623\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    624\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 625\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    628\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/_tensor_str.py:357\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    355\u001b[0m     )\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m/coc/flash9/jbjorner3/miniforge3/envs/quiet-star-replicate-v1/lib/python3.12/site-packages/torch/_tensor_str.py:145\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 145\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "reward_model = RewardModel(len(vocab), hidden_dim=100).to(device)\n",
    "num_calls_trainer = 0\n",
    "def train_reward_model_partial(reward_model, model):\n",
    "    global num_calls_trainer\n",
    "    if num_calls_trainer % 100 == 0:\n",
    "        num_calls_trainer += 1\n",
    "        return train_reward_model(reward_model, model, sample_for_train=False, print_stuff=True)\n",
    "    else:\n",
    "        num_calls_trainer += 1\n",
    "        return None # train_reward_model(reward_model, model, sample_for_train=False, print_stuff=False)\n",
    "get_dpo_loss_partial = partial(get_dpo_loss, beta_2=0.0001, dpo_loss_beta=10000, num_samples=2, reward_model=reward_model, train_reward_model=train_reward_model_partial)\n",
    "dpo_trained_model_with_reward_model = QuietStarLanguageModelLSTM(len(vocab), 100, 1).to(device)\n",
    "train_model(get_dpo_loss_partial, lambda model: eval_loss_fn(model, get_nll), dpo_trained_model_with_reward_model, epochs=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quiet-star-v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
